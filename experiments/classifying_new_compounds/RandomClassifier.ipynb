{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9606e0f7-ce9d-43af-8fd2-72cd6ba96d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "from seq2seq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2d1c6a-f935-470f-943c-b16f544beceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a string that simulates a list to a real list\n",
    "def convert_string_list(element):\n",
    "    # Delete [] of the string\n",
    "    element = element[0:len(element)]\n",
    "    # Create a list that contains each code as e.g. 'A'\n",
    "    ATC_list = list(element.split('; '))\n",
    "    for index, code in enumerate(ATC_list):\n",
    "        # Delete '' of the code\n",
    "        ATC_list[index] = code[0:len(code)] \n",
    "    return ATC_list\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def create_partitions(df, seed):\n",
    "    # Create a new column that indicates if the compound has more than 1 ATC code associated (1) or not (0)\n",
    "    df['multiple_ATC'] = df['ATC Codes'].apply(lambda x: len(convert_string_list(x)) > 1)\n",
    "    \n",
    "    # Divide the dataset depending on multiple_ATC column\n",
    "    group_more_than_one = df[df['multiple_ATC']]  # Compounds with more than one ATC code associated\n",
    "    group_one = df[~df['multiple_ATC']]          # Compounds with just one ATC code associated\n",
    "\n",
    "    conteo_longitudes = Counter(len(convert_string_list(codes)) for codes in group_more_than_one['ATC Codes'])\n",
    "    group_more_than_one = group_more_than_one.reset_index(drop=True)\n",
    "    group_one = group_one.reset_index(drop=True)\n",
    "\n",
    "    # Divide each set into train, validation and test subsets\n",
    "    train_more, test_more = train_test_split(group_more_than_one, test_size=0.2, random_state=seed)\n",
    "    train_one, test_one = train_test_split(group_one, test_size=0.2, random_state=seed)\n",
    "    train_more, val_more = train_test_split(train_more, test_size=0.15, random_state=seed)\n",
    "    train_one, val_one = train_test_split(train_one, test_size=0.15, random_state=seed)\n",
    "    \n",
    "    # Combine each set\n",
    "    train_set = pd.concat([train_more, train_one])\n",
    "    test_set = pd.concat([test_more, test_one])\n",
    "    val_set = pd.concat([val_more, val_one])\n",
    "    train_set = shuffle(train_set, random_state = seed)\n",
    "    test_set = shuffle(test_set, random_state = seed)\n",
    "    val_set = shuffle(val_set, random_state = seed)\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def multiplicate_rows(df):\n",
    "    # Duplicate each compound the number of ATC codes associated to it, copying its SMILES in new rows\n",
    "    new_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        atc_codes = row['ATC Codes']\n",
    "        atc_codes_list = convert_string_list(atc_codes)\n",
    "        \n",
    "        if len(atc_codes_list) > 1:\n",
    "            for code in atc_codes_list:\n",
    "                if len(code) == 5:\n",
    "                    new_row = row.copy()\n",
    "                    new_row['ATC Codes'] = code\n",
    "                    new_rows.append(new_row)\n",
    "        else:\n",
    "            if len(atc_codes_list[0]) == 5:\n",
    "                new_rows.append(row)\n",
    "    \n",
    "    new_set = pd.DataFrame(new_rows)\n",
    "    new_set = new_set.reset_index(drop=True)\n",
    "\n",
    "    return new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d828d612-7497-4982-bcd8-4b021c556939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trini\\AppData\\Local\\Temp\\ipykernel_11588\\2322825274.py:187: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 4 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 0 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 0 compounds\n",
      "Mean: Precision            0.000935\n",
      "Recall               0.001939\n",
      "F1                   0.001150\n",
      "Precision_level3     0.004984\n",
      "Recall_level3        0.010984\n",
      "F1_level3            0.006379\n",
      "Precision_level2     0.019834\n",
      "Recall_level2        0.047889\n",
      "F1_level2            0.026805\n",
      "Precision level 1    0.117082\n",
      "Precision level 2    0.171435\n",
      "Precision level 3    0.245895\n",
      "Precision level 4    0.174452\n",
      "Recall level 1       0.268985\n",
      "Recall level 2       0.180428\n",
      "Recall level 3       0.246539\n",
      "Recall level 4       0.174452\n",
      "dtype: float64\n",
      "Std: Precision            0.000766\n",
      "Recall               0.001972\n",
      "F1                   0.001045\n",
      "Precision_level3     0.001389\n",
      "Recall_level3        0.004137\n",
      "F1_level3            0.002128\n",
      "Precision_level2     0.002345\n",
      "Recall_level2        0.007038\n",
      "F1_level2            0.003594\n",
      "Precision level 1    0.008355\n",
      "Precision level 2    0.026085\n",
      "Precision level 3    0.067402\n",
      "Precision level 4    0.140623\n",
      "Recall level 1       0.018660\n",
      "Recall level 2       0.027286\n",
      "Recall level 3       0.060857\n",
      "Recall level 4       0.140623\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "seeds = [42, 123, 47899, 2025, 1, 20, 99, 1020, 345, 78] \n",
    "columns = [\n",
    "    'Seed', \n",
    "    'Precision', 'Recall', 'F1',\n",
    "    'Precision_level3', 'Recall_level3', 'F1_level3',\n",
    "    'Precision_level2', 'Recall_level2', 'F1_level2',\n",
    "    'Precision level 1', 'Precision level 2', 'Precision level 3', 'Precision level 4',\n",
    "    'Recall level 1', 'Recall level 2', 'Recall level 3', 'Recall level 4',\n",
    "    '#Compounds that have at least one match'\n",
    "]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seeds(seed)\n",
    "    \n",
    "    train_set = pd.read_csv(f'Datasets/train_set{seed}.csv')\n",
    "    test_set = pd.read_csv(f'Datasets/test_set{seed}.csv')\n",
    "    val_set = pd.read_csv(f'Datasets/val_set{seed}.csv')\n",
    "    \n",
    "    new_train_set = multiplicate_rows(train_set)\n",
    "    new_val_set = multiplicate_rows(val_set)\n",
    "    new_test_set = multiplicate_rows(test_set)\n",
    "\n",
    "\n",
    "    X_train = new_train_set['Neutralized SMILES']\n",
    "    y_train = new_train_set['ATC Codes']\n",
    "    X_test = new_test_set['Neutralized SMILES']\n",
    "    X_test2 = test_set['Neutralized SMILES']\n",
    "    y_test = new_test_set['ATC Codes']\n",
    "    \n",
    "    atc_nivel1 = []\n",
    "    for y in y_train:\n",
    "        atc_nivel1.append(y[0])\n",
    "    y_train_nivel1 = pd.DataFrame(atc_nivel1)\n",
    "    y_train_nivel1 = y_train_nivel1.reset_index(drop=True)\n",
    "\n",
    "    atc_nivel2 = []\n",
    "    for y in y_train:\n",
    "        atc_nivel2.append(y[1:3])\n",
    "    y_train_nivel2 = pd.DataFrame(atc_nivel2)\n",
    "    y_train_nivel2 = y_train_nivel2.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    atc_nivel3 = []\n",
    "    for y in y_train:\n",
    "        atc_nivel3.append(y[3:4])\n",
    "    y_train_nivel3 = pd.DataFrame(atc_nivel3)\n",
    "    y_train_nivel3 = y_train_nivel3.reset_index(drop=True)\n",
    "\n",
    "    atc_nivel4 = []\n",
    "    for y in y_train:\n",
    "        atc_nivel4.append(y[4:5])\n",
    "    y_train_nivel4 = pd.DataFrame(atc_nivel4)\n",
    "    y_train_nivel4 = y_train_nivel4.reset_index(drop=True)\n",
    "\n",
    "    # Contar la frecuencia de cada elemento\n",
    "    conteo1 = Counter(atc_nivel1)\n",
    "    # Calcular la probabilidad de cada elemento\n",
    "    total_elementos1 = len(atc_nivel1)\n",
    "    probabilidades1 = {elemento: frecuencia / total_elementos1 for elemento, frecuencia in conteo1.items()}\n",
    "    nivel1 = np.random.choice(list(probabilidades1.keys()), size=(len(X_test2),20), p=list(probabilidades1.values()))\n",
    "\n",
    "    # Contar la frecuencia de cada elemento\n",
    "    conteo2 = Counter(atc_nivel2)\n",
    "    \n",
    "    # Calcular la probabilidad de cada elemento\n",
    "    total_elementos2 = len(atc_nivel2)\n",
    "    probabilidades2 = {elemento: frecuencia / total_elementos2 for elemento, frecuencia in conteo2.items()}\n",
    "    nivel2 = np.random.choice(list(probabilidades2.keys()), size=(len(X_test2),20), p=list(probabilidades2.values()))\n",
    "        \n",
    "    # Contar la frecuencia de cada elemento\n",
    "    conteo3 = Counter(atc_nivel3)\n",
    "    \n",
    "    # Calcular la probabilidad de cada elemento\n",
    "    total_elementos3 = len(atc_nivel3)\n",
    "    probabilidades3 = {elemento: frecuencia / total_elementos3 for elemento, frecuencia in conteo3.items()}\n",
    "    nivel3 = np.random.choice(list(probabilidades3.keys()), size=(len(X_test2),20), p=list(probabilidades3.values()))\n",
    "\n",
    "    # Contar la frecuencia de cada elemento\n",
    "    conteo4 = Counter(atc_nivel4)\n",
    "    \n",
    "    # Calcular la probabilidad de cada elemento\n",
    "    total_elementos4 = len(atc_nivel4)\n",
    "    probabilidades4 = {elemento: frecuencia / total_elementos4 for elemento, frecuencia in conteo4.items()}\n",
    "    nivel4 = np.random.choice(list(probabilidades4.keys()), size=(len(X_test2),20), p=list(probabilidades4.values()))\n",
    "\n",
    "    predictions = []\n",
    "    for i, atc1 in enumerate(nivel1):\n",
    "        codes = []\n",
    "        for j, code1 in enumerate(atc1):\n",
    "            codes.append(code1 + nivel2[i][j] + nivel3[i][j] + nivel4[i][j])\n",
    "        predictions.append(codes)\n",
    "    predictions_clean = []\n",
    "    counter4_lessthan3 = 0\n",
    "    for preds in predictions:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            if len(clean_pred) == 5:\n",
    "                interm.append(clean_pred)\n",
    "        if len(interm) >= 3:\n",
    "            predictions_clean.append(interm[0:3])\n",
    "        else:\n",
    "            counter4_lessthan3 += 1\n",
    "            predictions_clean.append(interm)\n",
    "    print(f\"The model predicted less than 3 ATC codes of level 4 for {counter4_lessthan3} compounds\")                 \n",
    "    predictions_clean_level3 = []\n",
    "    counter3_lessthan3 = 0\n",
    "    for preds in predictions:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            pred_3 = clean_pred[0:4]\n",
    "            if len(pred_3) == 4 and pred_3 not in interm:\n",
    "                interm.append(pred_3)\n",
    "        if len(interm) >= 3:\n",
    "            predictions_clean_level3.append(interm[0:3])\n",
    "        else:\n",
    "            counter3_lessthan3 += 1\n",
    "            predictions_clean_level3.append(interm)\n",
    "    print(f\"The model predicted less than 3 ATC codes of level 3 for {counter3_lessthan3} compounds\")       \n",
    "    predictions_clean_level2 = []\n",
    "    counter2_lessthan3 = 0\n",
    "    for preds in predictions:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            pred_2 = clean_pred[0:3]\n",
    "            if len(pred_2) == 3 and pred_2 not in interm:\n",
    "                interm.append(pred_2)\n",
    "        if len(interm) >= 3:\n",
    "            predictions_clean_level2.append(interm[0:3])\n",
    "        else:\n",
    "            counter2_lessthan3 += 1\n",
    "            predictions_clean_level2.append(interm)\n",
    "    print(f\"The model predicted less than 3 ATC codes of level 2 for {counter2_lessthan3} compounds\")       \n",
    "    precision_1, precision_2, precision_3, precision_4 = defined_metrics.precision(predictions_clean, f'Datasets/test_set{seed}.csv', 'ATC Codes')\n",
    "    recall_1, recall_2, recall_3, recall_4, counter_compound_match = defined_metrics.recall(predictions_clean, f'Datasets/test_set{seed}.csv', 'ATC Codes')\n",
    "    precisions, recalls, f1s = defined_metrics.complete_metrics(predictions_clean, f'Datasets/test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_level3, recalls_level3, f1s_level3 = defined_metrics.complete_metrics_level3(predictions_clean_level3, f'Datasets/test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_level2, recalls_level2, f1s_level2 = defined_metrics.complete_metrics_level2(predictions_clean_level2, f'Datasets/test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_average = sum(precisions)/len(precisions)\n",
    "    recalls_average = sum(recalls)/len(recalls)\n",
    "    f1s_average = sum(f1s)/len(f1s)\n",
    "\n",
    "    precisions_average_level3 = sum(precisions_level3)/len(precisions_level3)\n",
    "    recalls_average_level3 = sum(recalls_level3)/len(recalls_level3)\n",
    "    f1s_average_level3 = sum(f1s_level3)/len(f1s_level3)\n",
    "\n",
    "    precisions_average_level2 = sum(precisions_level2)/len(precisions_level2)\n",
    "    recalls_average_level2 = sum(recalls_level2)/len(recalls_level2)\n",
    "    f1s_average_level2 = sum(f1s_level2)/len(f1s_level2)\n",
    "    \n",
    "    precisions_average_level3 = sum(precisions_level3)/len(precisions_level3)\n",
    "    recalls_average_level3 = sum(recalls_level3)/len(recalls_level3)\n",
    "    f1s_average_level3 = sum(f1s_level3)/len(f1s_level3)\n",
    "        \n",
    "    metrics = {\n",
    "        'Precision': precisions_average, \n",
    "        'Recall': recalls_average,\n",
    "        'F1': f1s_average,\n",
    "        'Precision_level3': precisions_average_level3, \n",
    "        'Recall_level3': recalls_average_level3,\n",
    "        'F1_level3': f1s_average_level3,\n",
    "        'Precision_level2': precisions_average_level2, \n",
    "        'Recall_level2': recalls_average_level2,\n",
    "        'F1_level2': f1s_average_level2,\n",
    "        'Precision level 1': precision_1,\n",
    "        'Precision level 2': precision_2,\n",
    "        'Precision level 3': precision_3,\n",
    "        'Precision level 4': precision_4,\n",
    "        'Recall level 1': recall_1,\n",
    "        'Recall level 2': recall_2,\n",
    "        'Recall level 3': recall_3,\n",
    "        'Recall level 4': recall_4,\n",
    "        '#Compounds that have at least one match': counter_compound_match\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Build the row\n",
    "    row = {\n",
    "        'Seed': seed,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "metrics_df.to_csv(\"distributionbasedrandom_metrics.csv\", index=False)\n",
    "print(\"Mean:\", metrics_df.mean(numeric_only=True))\n",
    "print(\"Std:\", metrics_df.std(numeric_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
