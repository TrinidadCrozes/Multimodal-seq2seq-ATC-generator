{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314e872e-827f-45f7-99ae-5d36af0c3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "from seq2seq import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdddabfe-85fe-4e2f-94c4-4152d60f7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "# Convert a string that simulates a list to a real list\n",
    "def convert_string_list(element):\n",
    "    # Delete [] of the string\n",
    "    element = element[1:len(element)-1]\n",
    "    # Create a list that contains each code as e.g. 'A'\n",
    "    ATC_list = list(element.split(', '))\n",
    "    for index, code in enumerate(ATC_list):\n",
    "        # Delete '' of the code\n",
    "        ATC_list[index] = code[1:len(code)-1]\n",
    "    return ATC_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80d739-0bfe-4df9-a0a9-2ba732615233",
   "metadata": {},
   "source": [
    "## SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9368eced-7217-4178-aa30-cfd0b680995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_1(seed):\n",
    "    train_set = pd.read_csv(f'train_set.csv')\n",
    "    val_set = pd.read_csv(f'val_set.csv')\n",
    "    test_set = pd.read_csv(f'test_set.csv')\n",
    "    train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns from train set\n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns from test set\n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Divide in X and y\n",
    "    X_train = train_set.drop('ATC_level1', axis = 1)\n",
    "    y_train = train_set['ATC_level1']\n",
    "    X_test = test_set.drop('ATC_level1', axis = 1)\n",
    "    y_test = test_set['ATC_level1']\n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_2(seed):\n",
    "    train_set = pd.read_csv(f'train_set.csv')\n",
    "    val_set = pd.read_csv(f'val_set.csv')\n",
    "    test_set = pd.read_csv(f'test_set.csv')\n",
    "    train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC level 1 code\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level1_list = convert_string_list(row['ATC_level1'])\n",
    "        for code in ATC_level1_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level1'] = code\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level2_list = convert_string_list(row['ATC_level2'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level2_list:\n",
    "            if code[0] == row['ATC_level1']:\n",
    "                new_row = row.copy()\n",
    "                new_row['ATC_level2'] = code[1:len(code)]\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "    \n",
    "    new_test_set2 = test_set\n",
    "    \n",
    "    X_train = new_train_set2.drop('ATC_level2', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level2']\n",
    "    X_test = new_test_set2.drop('ATC_level2', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level2']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_3(seed):\n",
    "    train_set = pd.read_csv(f'train_set.csv')\n",
    "    val_set = pd.read_csv(f'val_set.csv')\n",
    "    test_set = pd.read_csv(f'test_set.csv')\n",
    "    train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC code\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level2_list = convert_string_list(row['ATC_level2'])\n",
    "        for code in ATC_level2_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level2'] = code\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level3_list = convert_string_list(row['ATC_level3'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level3_list:\n",
    "            if code[0:3] == row['ATC_level2']:\n",
    "                new_row = row.copy()\n",
    "                new_row['ATC_level3'] = code[3:len(code)]\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "\n",
    "    new_test_set2 = test_set\n",
    "\n",
    "    X_train = new_train_set2.drop('ATC_level3', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level3']\n",
    "    X_test = new_test_set2.drop('ATC_level3', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level3']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_4(seed):\n",
    "    train_set = pd.read_csv(f'train_set.csv')\n",
    "    val_set = pd.read_csv(f'val_set.csv')\n",
    "    test_set = pd.read_csv(f'test_set.csv')\n",
    "    train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC code\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level3_list = convert_string_list(row['ATC_level3'])\n",
    "        for code in ATC_level3_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level3'] = code\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level4_list = convert_string_list(row['ATC_level4'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level4_list:\n",
    "            if code[0:4] == row['ATC_level3']:\n",
    "                if code[4:len(code)] != '':\n",
    "                    new_row = row.copy()\n",
    "                    new_row['ATC_level4'] = code[4:len(code)]\n",
    "                    new_rows.append(new_row)\n",
    "\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "\n",
    "    new_test_set2 = test_set\n",
    "\n",
    "    X_train = new_train_set2.drop('ATC_level4', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level4']\n",
    "    X_test = new_test_set2.drop('ATC_level4', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level4']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be774c2-0840-48f1-bb1f-79596dc9e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level1(X_test):\n",
    "    X_test = np.asarray(X_test).astype(np.float32)\n",
    "    X_test[pd.isna(X_test)] = np.nanmedian(X_test)\n",
    "    X_test = scaler1.transform(X_test)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fbdde0-31ff-4dee-8ac2-a350caaf885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level2(X_test1, pred_df_level1):\n",
    "    X_test1.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "    X_test1['ATC_level1'] = pred_df_level1['pred_1']\n",
    "    ATC_level1 = X_test1['ATC_level1']\n",
    "    categorical_atc = atc_level1_labels_encoder2.transform(ATC_level1)\n",
    "    df_level1 = pd.DataFrame(categorical_atc, columns=atc_level1_labels2)\n",
    "    X_test1 = pd.concat([X_test1, df_level1], axis = 1)\n",
    "    X_test1.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "\n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler2.transform(X_test1)\n",
    "\n",
    "    return X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820c220c-e5a8-4f3e-95ce-020147944292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level3(X_test1, pred_df_level1, pred_df_level2):\n",
    "    predicted_codes = []\n",
    "    \n",
    "    for index, pred1 in enumerate(pred_df_level1['pred_1']):\n",
    "        pred2 = str(pred_df_level2.at[pred_df_level2.index[index], 'pred_2']).zfill(2)\n",
    "        prediction = pred1 + '' + pred2\n",
    "        predicted_codes.append(prediction)\n",
    "        \n",
    "    X_test1['ATC_level2'] = predicted_codes\n",
    "    \n",
    "    ATC_level22 = X_test1['ATC_level2']\n",
    "    ATC_level1_3 = ATC_level22.copy()\n",
    "    ATC_level2_3 = ATC_level22.copy()\n",
    "    for index, atc in enumerate(ATC_level22):\n",
    "        ATC_level1_3[index] = []\n",
    "        ATC_level1_3[index].append(atc[0:1])\n",
    "        ATC_level2_3[index] = []\n",
    "        ATC_level2_3[index].append(atc[1:3])\n",
    "\n",
    "    X_test1 = X_test1.drop(labels=['ATC_level2'], axis=\"columns\")\n",
    "    \n",
    "    categorical_atc1_3 = atc_level1_labels_encoder3.transform(ATC_level1_3)\n",
    "    categorical_atc2_3 = atc_level2_labels_encoder3.transform(ATC_level2_3)\n",
    "    df_level1_3 = pd.DataFrame(categorical_atc1_3, columns=atc_level1_labels3)\n",
    "    df_level2_3 = pd.DataFrame(categorical_atc2_3, columns=atc_level2_labels3)\n",
    "    X_test1 = pd.concat([X_test1, df_level1_3, df_level2_3], axis = 1)                         \n",
    "    \n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler3.transform(X_test1)\n",
    "\n",
    "    return X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577054fb-9916-413f-88c1-b7b8ff5f2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level4(X_test1, pred_df_level1, pred_df_level2, pred_df_level3):\n",
    "    predicted_codes = []\n",
    "    \n",
    "    for index, pred1 in enumerate(pred_df_level1['pred_1']):\n",
    "        pred2 = str(pred_df_level2.at[pred_df_level2.index[index], 'pred_2']).zfill(2)\n",
    "        pred3 = pred_df_level3.at[pred_df_level3.index[index], 'pred_3']\n",
    "        prediction = pred1 + '' + pred2 + '' + pred3\n",
    "        predicted_codes.append(prediction)\n",
    "        \n",
    "    X_test1['ATC_level3'] = predicted_codes\n",
    "    \n",
    "    ATC_level33 = X_test1['ATC_level3']\n",
    "    ATC_level1_4 = ATC_level33.copy()\n",
    "    ATC_level2_4 = ATC_level33.copy()\n",
    "    ATC_level3_4 = ATC_level33.copy()\n",
    "    for index, atc in enumerate(ATC_level33):\n",
    "        ATC_level1_4[index] = []\n",
    "        ATC_level1_4[index].append(atc[0:1])\n",
    "        ATC_level2_4[index] = []\n",
    "        ATC_level2_4[index].append(atc[1:3])\n",
    "        ATC_level3_4[index] = []\n",
    "        ATC_level3_4[index].append(atc[3:4])\n",
    "\n",
    "    X_test1 = X_test1.drop(labels=['ATC_level3'], axis=\"columns\")\n",
    "\n",
    "    categorical_atc1_4 = atc_level1_labels_encoder4.transform(ATC_level1_4)\n",
    "    categorical_atc2_4 = atc_level2_labels_encoder4.transform(ATC_level2_4)\n",
    "    categorical_atc3_4 = atc_level3_labels_encoder4.transform(ATC_level3_4)\n",
    "    df_level1_4 = pd.DataFrame(categorical_atc1_4, columns=atc_level1_labels4)\n",
    "    df_level2_4 = pd.DataFrame(categorical_atc2_4, columns=atc_level2_labels4)\n",
    "    df_level3_4 = pd.DataFrame(categorical_atc3_4, columns=atc_level3_labels4)\n",
    "    X_test1 = pd.concat([X_test1, df_level1_4, df_level2_4, df_level3_4], axis = 1)                         \n",
    "   \n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler4.transform(X_test1)\n",
    "\n",
    "    return X_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b05d4-f303-4f07-a559-52ffa8c9cd4e",
   "metadata": {},
   "source": [
    "## PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc14585c-31cf-45f1-bf75-3ee51e0f363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, previous_predictions = None, index = None):\n",
    "    \"\"\"Genera predicciones para todos los niveles, o solo para un índice si se repite\"\"\"\n",
    "    \n",
    "    def sample_prediction(model, X_test, encoder, previous_predictions = None, index = None):\n",
    "        \"\"\"Calcula la predicción con probabilidad ponderada para un índice o todos\"\"\"\n",
    "        if previous_predictions is None:\n",
    "            # Primera vez: calcular todas las predicciones\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            y_prob_matrix = np.array(y_prob)[:, :, 1].T\n",
    "            predictions = [\n",
    "                random.choices(encoder.classes_, weights=row, k=1)[0] for row in y_prob_matrix\n",
    "            ]\n",
    "        else:\n",
    "            # Mantener las predicciones anteriores y cambiar solo la del índice dado\n",
    "            predictions = previous_predictions.copy()\n",
    "            if index is not None:\n",
    "                y_prob = model.predict_proba(X_test)\n",
    "                row = np.array(y_prob)[:, :, 1].T[index]\n",
    "                predictions[index] = random.choices(encoder.classes_, weights=row, k=1)[0]\n",
    "    \n",
    "        return predictions\n",
    "    \n",
    "    # Nivel 1\n",
    "    X_test1 = test_set_level1(X_test1)\n",
    "    pred_1 = sample_prediction(model1, X_test1, mlb, previous_predictions['pred_1'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level1 = pd.DataFrame(pred_1, columns=['pred_1'])\n",
    "\n",
    "    # Nivel 2\n",
    "    X_test2 = test_set_level2(X_test2, pred_df_level1)\n",
    "    pred_2 = sample_prediction(model2, X_test2, y_labels_encoder2, previous_predictions['pred_2'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level2 = pd.DataFrame(pred_2, columns=['pred_2'])\n",
    "\n",
    "    # Nivel 3\n",
    "    X_test3 = test_set_level3(X_test3, pred_df_level1, pred_df_level2)\n",
    "    pred_3 = sample_prediction(model3, X_test3, y_labels_encoder3, previous_predictions['pred_3'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level3 = pd.DataFrame(pred_3, columns=['pred_3'])\n",
    "\n",
    "    # Nivel 4\n",
    "    X_test4 = test_set_level4(X_test4, pred_df_level1, pred_df_level2, pred_df_level3)\n",
    "    pred_4 = sample_prediction(model4, X_test4, y_labels_encoder4, previous_predictions['pred_4'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level4 = pd.DataFrame(pred_4, columns=['pred_4'])\n",
    "\n",
    "    return pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c1e846-cc8c-4a1e-aabc-707dc71dd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "def random_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4):\n",
    "    final_predictions = [[] for _ in range(len(X_test1))]\n",
    "    max_attempts = 30\n",
    "    for i in range(10):\n",
    "        pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4 = generate_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4)\n",
    "        for index in range(len(pred_df_level1)):\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                pred1 = pred_df_level1.at[index, 'pred_1']\n",
    "                pred2 = str(pred_df_level2.at[index, \"pred_2\"]).zfill(2)\n",
    "                pred3 = pred_df_level3.at[index, \"pred_3\"]\n",
    "                pred4 = pred_df_level4.at[index, \"pred_4\"]\n",
    "                prediction = pred1 + pred2 + pred3 + pred4\n",
    "                \n",
    "                if prediction not in final_predictions[index]:\n",
    "                    final_predictions[index].append(prediction)\n",
    "                    break  # Salimos del bucle cuando obtenemos una predicción nueva\n",
    "\n",
    "                # print(f\"{attempts}- Prediction {prediction} already found in {final_predictions[index]}, reclassifying index {index}...\")\n",
    "                previous_predictions = pd.concat([pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4], axis = 1)\n",
    "                # Recalcular la clasificación completa solo para este índice\n",
    "                pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4 = generate_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, previous_predictions, index)\n",
    "                attempts += 1\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d062b35d-e773-4c33-99a8-216e7832b7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n",
      "The model predicted less than 10 ATC codes of level 4 for a compound\n"
     ]
    }
   ],
   "source": [
    "seed=78\n",
    "set_seeds(seed)\n",
    "X_train1, y_train1, X_test1, y_test1 = train_test_1(seed)\n",
    "X_train2, y_train2, X_test2, y_test2 = train_test_2(seed)\n",
    "X_train3, y_train3, X_test3, y_test3 = train_test_3(seed)\n",
    "X_train4, y_train4, X_test4, y_test4 = train_test_4(seed)\n",
    "# LEVEL1\n",
    "# Get all available labels describing the level 1 ATC code\n",
    "labels1 = set()\n",
    "for lista in y_train1:\n",
    "    lista = convert_string_list(lista)\n",
    "    for code in lista:\n",
    "        labels1.add(code)\n",
    "for lista in y_test1:\n",
    "    lista = convert_string_list(lista)\n",
    "    for code in lista:\n",
    "        labels1.add(code)\n",
    "        \n",
    "labels1 = sorted(list(labels1))\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([labels1])\n",
    "y_new = y_train1.copy()\n",
    "for index, lista in enumerate(y_train1):\n",
    "    y_new[index] = []\n",
    "    lista = convert_string_list(lista)\n",
    "    for i, label in enumerate(lista):\n",
    "        y_new[index].append(lista[i])\n",
    "y_categorical1 = mlb.transform(y_new)\n",
    "\n",
    "X_train1 = np.asarray(X_train1).astype(np.float32)\n",
    "y_categorical1 = np.asarray(y_categorical1).astype(np.float32)\n",
    "# Complete NaN values in each column with the median\n",
    "X_train1[pd.isna(X_train1)] = np.nanmedian(X_train1)\n",
    "# Define an instance of the MinMaxScaler\n",
    "scaler1 = MinMaxScaler()\n",
    "# Fit the scaler to the data and transform it\n",
    "X_train1 = scaler1.fit_transform(X_train1)\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=100, max_depth = 40, min_samples_split = 2, min_samples_leaf = 1, class_weight = None, random_state=seed)\n",
    "# Train the model\n",
    "rf1.fit(X_train1, y_categorical1)\n",
    "\n",
    "#LEVEL 2\n",
    "# Get all available labels describing the level 1 ATC code\n",
    "labels2 = set()\n",
    "for code in y_train2:\n",
    "    labels2.add(code)\n",
    "        \n",
    "labels2 = sorted(list(labels2))\n",
    "y_labels_encoder2 = MultiLabelBinarizer()\n",
    "y_labels_encoder2.fit([labels2])\n",
    "encoded_y_train2 = y_labels_encoder2.transform(y_train2.values.reshape(-1, 1))\n",
    "atc_level1_labels2 = set()\n",
    "for _, row in X_train2.iterrows():\n",
    "    atc_level1_labels2.add(row['ATC_level1'])\n",
    "        \n",
    "atc_level1_labels2 = sorted(list(atc_level1_labels2))\n",
    "atc_level1_labels_encoder2 = MultiLabelBinarizer()\n",
    "atc_level1_labels_encoder2.fit([atc_level1_labels2])\n",
    "ATC_level11 = X_train2['ATC_level1']\n",
    "ATC_level1_2 = ATC_level11.copy()\n",
    "for index, lista in enumerate(ATC_level11):\n",
    "    ATC_level1_2[index] = []\n",
    "    ATC_level1_2[index].append(lista)\n",
    "categorical_atc2 = atc_level1_labels_encoder2.transform(ATC_level1_2)\n",
    "X_train2.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "df_level1_2 = pd.DataFrame(categorical_atc2, columns=atc_level1_labels2)\n",
    "X_train2 = pd.concat([X_train2, df_level1_2], axis = 1)\n",
    "X_train2 = np.asarray(X_train2).astype(np.float32)\n",
    "encoded_y_train2 = np.asarray(encoded_y_train2).astype(np.float32)\n",
    "# Complete NaN values in each column with the median\n",
    "X_train2[pd.isna(X_train2)] = np.nanmedian(X_train2)\n",
    "# Define an instance of the MinMaxScaler\n",
    "scaler2 = MinMaxScaler()\n",
    "# Fit the scaler to the data and transform it\n",
    "X_train2 = scaler2.fit_transform(X_train2)\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "# Train the model\n",
    "rf2.fit(X_train2, encoded_y_train2)\n",
    "\n",
    "#LEVEL 3\n",
    "# Get all available labels describing the level 1 ATC code\n",
    "labels3 = set()\n",
    "for code in y_train3:\n",
    "    labels3.add(code)\n",
    "        \n",
    "labels3 = sorted(list(labels3))\n",
    "y_labels_encoder3 = MultiLabelBinarizer()\n",
    "y_labels_encoder3.fit([labels3])\n",
    "encoded_y_train3 = y_labels_encoder3.transform(y_train3.values.reshape(-1, 1))\n",
    "\n",
    "atc_level1_labels3 = set()\n",
    "atc_level2_labels3 = set()\n",
    "for _, row in X_train3.iterrows():\n",
    "    atc_level1_labels3.add(row['ATC_level2'][0:1])\n",
    "    atc_level2_labels3.add(row['ATC_level2'][1:3])\n",
    "for _, row in X_test3.iterrows():\n",
    "    lista = convert_string_list(row['ATC_level2'])\n",
    "    for code in lista:\n",
    "        atc_level1_labels3.add(code[0:1])\n",
    "        atc_level2_labels3.add(code[1:3])\n",
    "        \n",
    "atc_level1_labels3 = sorted(list(atc_level1_labels3))\n",
    "atc_level2_labels3 = sorted(list(atc_level2_labels3))\n",
    "atc_level1_labels_encoder3 = MultiLabelBinarizer()\n",
    "atc_level1_labels_encoder3.fit([atc_level1_labels3])\n",
    "atc_level2_labels_encoder3 = MultiLabelBinarizer()\n",
    "atc_level2_labels_encoder3.fit([atc_level2_labels3])\n",
    "ATC_level22 = X_train3['ATC_level2']\n",
    "ATC_level1_3 = ATC_level22.copy()\n",
    "ATC_level2_3 = ATC_level22.copy()\n",
    "for index, lista in enumerate(ATC_level22):\n",
    "    ATC_level1_3[index] = []\n",
    "    ATC_level1_3[index].append(lista[0:1])\n",
    "for index, lista in enumerate(ATC_level22):\n",
    "    ATC_level2_3[index] = []\n",
    "    ATC_level2_3[index].append(lista[1:3])\n",
    "X_train3.drop(labels=['ATC_level2'], axis=\"columns\", inplace=True)\n",
    "categorical_atc1_3 = atc_level1_labels_encoder3.transform(ATC_level1_3)\n",
    "categorical_atc2_3 = atc_level2_labels_encoder3.transform(ATC_level2_3)\n",
    "df_level1_3 = pd.DataFrame(categorical_atc1_3, columns=atc_level1_labels3)\n",
    "df_level2_3 = pd.DataFrame(categorical_atc2_3, columns=atc_level2_labels3)\n",
    "X_train3 = pd.concat([X_train3, df_level1_3, df_level2_3], axis = 1)\n",
    "X_train3 = np.asarray(X_train3).astype(np.float32)\n",
    "encoded_y_train3 = np.asarray(encoded_y_train3).astype(np.float32)\n",
    "# Complete NaN values in each column with the median\n",
    "X_train3[pd.isna(X_train3)] = np.nanmedian(X_train3)\n",
    "# Define an instance of the MinMaxScaler\n",
    "scaler3 = MinMaxScaler()\n",
    "# Fit the scaler to the data and transform it\n",
    "X_train3 = scaler3.fit_transform(X_train3)\n",
    "\n",
    "rf3 = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "# Train the model\n",
    "rf3.fit(X_train3, encoded_y_train3)\n",
    "#LEVEL 4\n",
    "# Get all available labels describing the level 1 ATC code\n",
    "labels4 = set()\n",
    "for code in y_train4:\n",
    "    labels4.add(code)\n",
    "        \n",
    "labels4 = sorted(list(labels4))\n",
    "y_labels_encoder4 = MultiLabelBinarizer()\n",
    "y_labels_encoder4.fit([labels4])\n",
    "encoded_y_train4 = y_labels_encoder4.transform(y_train4.values.reshape(-1, 1))\n",
    "\n",
    "atc_level1_labels4 = set()\n",
    "atc_level2_labels4 = set()\n",
    "atc_level3_labels4 = set()\n",
    "for _, row in X_train4.iterrows():\n",
    "    atc_level1_labels4.add(row['ATC_level3'][0:1])\n",
    "    atc_level2_labels4.add(row['ATC_level3'][1:3])\n",
    "    atc_level3_labels4.add(row['ATC_level3'][3:4])\n",
    "for _, row in X_test4.iterrows():\n",
    "    lista = convert_string_list(row['ATC_level3'])\n",
    "    for code in lista:\n",
    "        atc_level1_labels4.add(code[0:1])\n",
    "        atc_level2_labels4.add(code[1:3])\n",
    "        atc_level3_labels4.add(code[3:4])\n",
    "\n",
    "atc_level1_labels4 = sorted(list(atc_level1_labels4))\n",
    "atc_level2_labels4 = sorted(list(atc_level2_labels4))\n",
    "atc_level3_labels4 = sorted(list(atc_level3_labels4))\n",
    "atc_level1_labels_encoder4 = MultiLabelBinarizer()\n",
    "atc_level1_labels_encoder4.fit([atc_level1_labels4])\n",
    "atc_level2_labels_encoder4 = MultiLabelBinarizer()\n",
    "atc_level2_labels_encoder4.fit([atc_level2_labels4])\n",
    "atc_level3_labels_encoder4 = MultiLabelBinarizer()\n",
    "atc_level3_labels_encoder4.fit([atc_level3_labels4])\n",
    "ATC_level33 = X_train4['ATC_level3']\n",
    "ATC_level1_4 = ATC_level33.copy()\n",
    "for index, lista in enumerate(ATC_level33):\n",
    "    ATC_level1_4[index] = []\n",
    "    ATC_level1_4[index].append(lista[0:1])\n",
    "ATC_level2_4 = ATC_level33.copy()\n",
    "for index, lista in enumerate(ATC_level33):\n",
    "    ATC_level2_4[index] = []\n",
    "    ATC_level2_4[index].append(lista[1:3])\n",
    "ATC_level3_4 = ATC_level33.copy()\n",
    "for index, lista in enumerate(ATC_level33):\n",
    "    ATC_level3_4[index] = []\n",
    "    ATC_level3_4[index].append(lista[3:4])\n",
    "X_train4.drop(labels=['ATC_level3'], axis=\"columns\", inplace=True)\n",
    "categorical_atc1_4 = atc_level1_labels_encoder4.transform(ATC_level1_4)\n",
    "categorical_atc2_4 = atc_level2_labels_encoder4.transform(ATC_level2_4)\n",
    "categorical_atc3_4 = atc_level3_labels_encoder4.transform(ATC_level3_4)\n",
    "df_level1_4 = pd.DataFrame(categorical_atc1_4, columns=atc_level1_labels4)\n",
    "df_level2_4 = pd.DataFrame(categorical_atc2_4, columns=atc_level2_labels4)\n",
    "df_level3_4 = pd.DataFrame(categorical_atc3_4, columns=atc_level3_labels4)\n",
    "X_train4 = pd.concat([X_train4, df_level1_4, df_level2_4, df_level3_4], axis = 1)\n",
    "X_train4 = np.asarray(X_train4).astype(np.float32)\n",
    "encoded_y_train4 = np.asarray(encoded_y_train4).astype(np.float32)\n",
    "# Complete NaN values in each column with the median\n",
    "X_train4[pd.isna(X_train4)] = np.nanmedian(X_train4)\n",
    "# Define an instance of the MinMaxScaler\n",
    "scaler4 = MinMaxScaler()\n",
    "# Fit the scaler to the data and transform it\n",
    "X_train4 = scaler4.fit_transform(X_train4)\n",
    "\n",
    "rf4 = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "\n",
    "rf4.fit(X_train4, encoded_y_train4)\n",
    "\n",
    "#TEST\n",
    "X_test3.drop(labels=['ATC_level2'], axis=\"columns\", inplace=True)\n",
    "X_test4.drop(labels=['ATC_level3'], axis=\"columns\", inplace = True)\n",
    "\n",
    "predictions = random_predictions(rf1, X_test1, rf2, X_test2, rf3, X_test3, rf4, X_test4)\n",
    "predictions_clean = []\n",
    "for preds in predictions:\n",
    "    interm = []\n",
    "    for pred in preds:\n",
    "        clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "        if len(clean_pred) == 5:\n",
    "            interm.append(clean_pred)\n",
    "    if len(interm) == 10:\n",
    "        predictions_clean.append(interm)\n",
    "    else: \n",
    "        print(\"The model predicted less than 10 ATC codes of level 4 for a compound\")\n",
    "        predictions_clean.append(interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2731ea24-0a71-47e9-8ecf-c713b16eb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions1, recalls1, f1s1 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 1)\n",
    "precisions2, recalls2, f1s2 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 2)\n",
    "precisions3, recalls3, f1s3 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 3)\n",
    "precisions4, recalls4, f1s4 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 4)\n",
    "precisions5, recalls5, f1s5 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 5)\n",
    "precisions6, recalls6, f1s6 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 6)\n",
    "precisions7, recalls7, f1s7 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 7)\n",
    "precisions8, recalls8, f1s8 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 8)\n",
    "precisions9, recalls9, f1s9 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 9)\n",
    "precisions10, recalls10, f1s10 = defined_metrics.complete_metrics(predictions_clean, f'test_set.csv', 'ATC Codes', 10)\n",
    "\n",
    "precisions_average1 = sum(precisions1)/len(precisions1)\n",
    "recalls_average1 = sum(recalls1)/len(recalls1)\n",
    "f1s_average1 = sum(f1s1)/len(f1s1)\n",
    "\n",
    "precisions_average2 = sum(precisions2)/len(precisions2)\n",
    "recalls_average2 = sum(recalls2)/len(recalls2)\n",
    "f1s_average2 = sum(f1s2)/len(f1s2)\n",
    "\n",
    "precisions_average3 = sum(precisions3)/len(precisions3)\n",
    "recalls_average3 = sum(recalls3)/len(recalls3)\n",
    "f1s_average3 = sum(f1s3)/len(f1s3)\n",
    "\n",
    "precisions_average4 = sum(precisions4)/len(precisions4)\n",
    "recalls_average4 = sum(recalls4)/len(recalls4)\n",
    "f1s_average4 = sum(f1s4)/len(f1s4)\n",
    "\n",
    "precisions_average5 = sum(precisions5)/len(precisions5)\n",
    "recalls_average5 = sum(recalls5)/len(recalls5)\n",
    "f1s_average5 = sum(f1s5)/len(f1s5)\n",
    "\n",
    "precisions_average6 = sum(precisions6)/len(precisions6)\n",
    "recalls_average6 = sum(recalls6)/len(recalls6)\n",
    "f1s_average6 = sum(f1s6)/len(f1s6)\n",
    "\n",
    "precisions_average7 = sum(precisions7)/len(precisions7)\n",
    "recalls_average7 = sum(recalls7)/len(recalls7)\n",
    "f1s_average7 = sum(f1s7)/len(f1s7)\n",
    "\n",
    "precisions_average8 = sum(precisions8)/len(precisions8)\n",
    "recalls_average8 = sum(recalls8)/len(recalls8)\n",
    "f1s_average8 = sum(f1s8)/len(f1s8)\n",
    "\n",
    "precisions_average9 = sum(precisions9)/len(precisions9)\n",
    "recalls_average9 = sum(recalls9)/len(recalls9)\n",
    "f1s_average9 = sum(f1s9)/len(f1s9)\n",
    "\n",
    "precisions_average10 = sum(precisions10)/len(precisions10)\n",
    "recalls_average10 = sum(recalls10)/len(recalls10)\n",
    "f1s_average10 = sum(f1s10)/len(f1s10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "127b3e36-877b-43ea-9426-855dfe1f5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_average_k = []\n",
    "precisions_average_k.append(precisions_average1)\n",
    "precisions_average_k.append(precisions_average2)\n",
    "precisions_average_k.append(precisions_average3)\n",
    "precisions_average_k.append(precisions_average4)\n",
    "precisions_average_k.append(precisions_average5)\n",
    "precisions_average_k.append(precisions_average6)\n",
    "precisions_average_k.append(precisions_average7)\n",
    "precisions_average_k.append(precisions_average8)\n",
    "precisions_average_k.append(precisions_average9)\n",
    "precisions_average_k.append(precisions_average10)\n",
    "recalls_average_k = []\n",
    "recalls_average_k.append(recalls_average1)\n",
    "recalls_average_k.append(recalls_average2)\n",
    "recalls_average_k.append(recalls_average3)\n",
    "recalls_average_k.append(recalls_average4)\n",
    "recalls_average_k.append(recalls_average5)\n",
    "recalls_average_k.append(recalls_average6)\n",
    "recalls_average_k.append(recalls_average7)\n",
    "recalls_average_k.append(recalls_average8)\n",
    "recalls_average_k.append(recalls_average9)\n",
    "recalls_average_k.append(recalls_average10)\n",
    "f1s_average_k = []\n",
    "f1s_average_k.append(f1s_average1)\n",
    "f1s_average_k.append(f1s_average2)\n",
    "f1s_average_k.append(f1s_average3)\n",
    "f1s_average_k.append(f1s_average4)\n",
    "f1s_average_k.append(f1s_average5)\n",
    "f1s_average_k.append(f1s_average6)\n",
    "f1s_average_k.append(f1s_average7)\n",
    "f1s_average_k.append(f1s_average8)\n",
    "f1s_average_k.append(f1s_average9)\n",
    "f1s_average_k.append(f1s_average10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bf06a29-d3b7-4f91-9d78-58fa0592e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(\"df_metrics.csv\")\n",
    "new_rows = pd.DataFrame(columns=[\"model\", \"precision\", \"recall\", \"f1\"])\n",
    "new_rows[\"precision\"] = precisions_average_k\n",
    "new_rows[\"recall\"] = recalls_average_k\n",
    "new_rows[\"f1\"] = f1s_average_k\n",
    "new_rows[\"model\"] = \"Random Forest\"\n",
    "df_results = pd.concat([df_results , new_rows], ignore_index=True)\n",
    "df_results.to_csv(\"df_metrics.csv\", index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
