{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e872e-827f-45f7-99ae-5d36af0c3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../..')))\n",
    "from seq2seq import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from randomforest_hyp import hyperparametersselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdddabfe-85fe-4e2f-94c4-4152d60f7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "# Convert a string that simulates a list to a real list\n",
    "def convert_string_list(element):\n",
    "    # Delete [] of the string\n",
    "    element = element[1:len(element)-1]\n",
    "    # Create a list that contains each code as e.g. 'A'\n",
    "    ATC_list = list(element.split(', '))\n",
    "    for index, code in enumerate(ATC_list):\n",
    "        # Delete '' of the code\n",
    "        ATC_list[index] = code[1:len(code)-1]\n",
    "    return ATC_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80d739-0bfe-4df9-a0a9-2ba732615233",
   "metadata": {},
   "source": [
    "## SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9368eced-7217-4178-aa30-cfd0b680995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_1(seed):\n",
    "    train_set = pd.read_csv(f'../Datasets/SplitATC_Rep_complete_train_set{seed}.csv')\n",
    "    test_set = pd.read_csv(f'../Datasets/SplitATC_Rep_test_set{seed}.csv')\n",
    "    \n",
    "    # Delete unnecessary columns from train set\n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns from test set\n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Divide in X and y\n",
    "    X_train = train_set.drop('ATC_level1', axis = 1)\n",
    "    y_train = train_set['ATC_level1']\n",
    "    X_test = test_set.drop('ATC_level1', axis = 1)\n",
    "    y_test = test_set['ATC_level1']\n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_2(seed):\n",
    "    train_set = pd.read_csv(f'../Datasets/SplitATC_Rep_complete_train_set{seed}.csv')\n",
    "    \n",
    "    test_set = pd.read_csv(f'../Datasets/SplitATC_Rep_test_set{seed}.csv')\n",
    "    \n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC level 1 code\n",
    "    new_rows = []\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level1_list = convert_string_list(row['ATC_level1'])\n",
    "        for code in ATC_level1_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level1'] = code\n",
    "            new_rows.append(new_row)\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level2_list = convert_string_list(row['ATC_level2'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level2_list:\n",
    "            if code[0] == row['ATC_level1']:\n",
    "                new_row = row.copy()\n",
    "                new_row['ATC_level2'] = code[1:len(code)]\n",
    "                new_rows.append(new_row)\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "    \n",
    "    new_test_set2 = test_set\n",
    "    \n",
    "    X_train = new_train_set2.drop('ATC_level2', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level2']\n",
    "    X_test = new_test_set2.drop('ATC_level2', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level2']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_3(seed):\n",
    "    train_set = pd.read_csv(f'../Datasets/SplitATC_Rep_complete_train_set{seed}.csv')\n",
    "    \n",
    "    test_set = pd.read_csv(f'../Datasets/SplitATC_Rep_test_set{seed}.csv')\n",
    "    \n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC code\n",
    "    new_rows = []\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level2_list = convert_string_list(row['ATC_level2'])\n",
    "        for code in ATC_level2_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level2'] = code\n",
    "            new_rows.append(new_row)\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level3_list = convert_string_list(row['ATC_level3'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level3_list:\n",
    "            if code[0:3] == row['ATC_level2']:\n",
    "                new_row = row.copy()\n",
    "                new_row['ATC_level3'] = code[3:len(code)]\n",
    "                new_rows.append(new_row)\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "    new_test_set2 = test_set\n",
    "    X_train = new_train_set2.drop('ATC_level3', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level3']\n",
    "    X_test = new_test_set2.drop('ATC_level3', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level3']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_4(seed):\n",
    "    train_set = pd.read_csv(f'../Datasets/SplitATC_Rep_complete_train_set{seed}.csv')\n",
    "    \n",
    "    test_set = pd.read_csv(f'../Datasets/SplitATC_Rep_test_set{seed}.csv')\n",
    "    \n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    test_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    test_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    test_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC code\n",
    "    new_rows = []\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level3_list = convert_string_list(row['ATC_level3'])\n",
    "        for code in ATC_level3_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level3'] = code\n",
    "            new_rows.append(new_row)\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level4_list = convert_string_list(row['ATC_level4'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level4_list:\n",
    "            if code[0:4] == row['ATC_level3']:\n",
    "                if code[4:len(code)] != '':\n",
    "                    new_row = row.copy()\n",
    "                    new_row['ATC_level4'] = code[4:len(code)]\n",
    "                    new_rows.append(new_row)\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "    new_test_set2 = test_set\n",
    "    X_train = new_train_set2.drop('ATC_level4', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level4']\n",
    "    X_test = new_test_set2.drop('ATC_level4', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level4']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be774c2-0840-48f1-bb1f-79596dc9e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level1(scaler1, X_test):\n",
    "    X_test = np.asarray(X_test).astype(np.float32)\n",
    "    X_test[pd.isna(X_test)] = np.nanmedian(X_test)\n",
    "    X_test = scaler1.transform(X_test)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fbdde0-31ff-4dee-8ac2-a350caaf885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level2(scaler2, X_test1, pred_df_level1, atc_level1_labels_encoder2):\n",
    "    X_test1.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "    X_test1['ATC_level1'] = pred_df_level1['pred_1']\n",
    "    ATC_level1 = X_test1['ATC_level1']\n",
    "    categorical_atc = atc_level1_labels_encoder2.transform(ATC_level1)\n",
    "    df_level1 = pd.DataFrame(categorical_atc, columns=atc_level1_labels_encoder2.classes_)\n",
    "    X_test1 = pd.concat([X_test1, df_level1], axis = 1)\n",
    "    X_test1.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler2.transform(X_test1)\n",
    "    return X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820c220c-e5a8-4f3e-95ce-020147944292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level3(scaler3, X_test1, pred_df_level1, pred_df_level2, atc_level1_labels_encoder3, atc_level2_labels_encoder3):\n",
    "    predicted_codes = []\n",
    "    \n",
    "    for index, pred1 in enumerate(pred_df_level1['pred_1']):\n",
    "        pred2 = str(pred_df_level2.at[pred_df_level2.index[index], 'pred_2']).zfill(2)\n",
    "        prediction = pred1 + '' + pred2\n",
    "        predicted_codes.append(prediction)\n",
    "        \n",
    "    X_test1['ATC_level2'] = predicted_codes\n",
    "    \n",
    "    ATC_level22 = X_test1['ATC_level2']\n",
    "    ATC_level1_3 = ATC_level22.copy()\n",
    "    ATC_level2_3 = ATC_level22.copy()\n",
    "    for index, atc in enumerate(ATC_level22):\n",
    "        ATC_level1_3[index] = []\n",
    "        ATC_level1_3[index].append(atc[0:1])\n",
    "        ATC_level2_3[index] = []\n",
    "        ATC_level2_3[index].append(atc[1:3])\n",
    "    X_test1 = X_test1.drop(labels=['ATC_level2'], axis=\"columns\")\n",
    "    \n",
    "    categorical_atc1_3 = atc_level1_labels_encoder3.transform(ATC_level1_3)\n",
    "    categorical_atc2_3 = atc_level2_labels_encoder3.transform(ATC_level2_3)\n",
    "    df_level1_3 = pd.DataFrame(categorical_atc1_3, columns=atc_level1_labels_encoder3.classes_)\n",
    "    df_level2_3 = pd.DataFrame(categorical_atc2_3, columns=atc_level2_labels_encoder3.classes_)\n",
    "    X_test1 = pd.concat([X_test1, df_level1_3, df_level2_3], axis = 1)                         \n",
    "    \n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler3.transform(X_test1)\n",
    "    return X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577054fb-9916-413f-88c1-b7b8ff5f2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level4(scaler4, X_test1, pred_df_level1, pred_df_level2, pred_df_level3, atc_level1_labels_encoder4, atc_level2_labels_encoder4, atc_level3_labels_encoder4):\n",
    "    predicted_codes = []\n",
    "    \n",
    "    for index, pred1 in enumerate(pred_df_level1['pred_1']):\n",
    "        pred2 = str(pred_df_level2.at[pred_df_level2.index[index], 'pred_2']).zfill(2)\n",
    "        pred3 = pred_df_level3.at[pred_df_level3.index[index], 'pred_3']\n",
    "        prediction = pred1 + '' + pred2 + '' + pred3\n",
    "        predicted_codes.append(prediction)\n",
    "        \n",
    "    X_test1['ATC_level3'] = predicted_codes\n",
    "    \n",
    "    ATC_level33 = X_test1['ATC_level3']\n",
    "    ATC_level1_4 = ATC_level33.copy()\n",
    "    ATC_level2_4 = ATC_level33.copy()\n",
    "    ATC_level3_4 = ATC_level33.copy()\n",
    "    for index, atc in enumerate(ATC_level33):\n",
    "        ATC_level1_4[index] = []\n",
    "        ATC_level1_4[index].append(atc[0:1])\n",
    "        ATC_level2_4[index] = []\n",
    "        ATC_level2_4[index].append(atc[1:3])\n",
    "        ATC_level3_4[index] = []\n",
    "        ATC_level3_4[index].append(atc[3:4])\n",
    "    X_test1 = X_test1.drop(labels=['ATC_level3'], axis=\"columns\")\n",
    "    categorical_atc1_4 = atc_level1_labels_encoder4.transform(ATC_level1_4)\n",
    "    categorical_atc2_4 = atc_level2_labels_encoder4.transform(ATC_level2_4)\n",
    "    categorical_atc3_4 = atc_level3_labels_encoder4.transform(ATC_level3_4)\n",
    "    df_level1_4 = pd.DataFrame(categorical_atc1_4, columns=atc_level1_labels_encoder4.classes_)\n",
    "    df_level2_4 = pd.DataFrame(categorical_atc2_4, columns=atc_level2_labels_encoder4.classes_)\n",
    "    df_level3_4 = pd.DataFrame(categorical_atc3_4, columns=atc_level3_labels_encoder4.classes_)\n",
    "    X_test1 = pd.concat([X_test1, df_level1_4, df_level2_4, df_level3_4], axis = 1)                         \n",
    "   \n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler4.transform(X_test1)\n",
    "    return X_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b05d4-f303-4f07-a559-52ffa8c9cd4e",
   "metadata": {},
   "source": [
    "## PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc14585c-31cf-45f1-bf75-3ee51e0f363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(scaler1, scaler2, scaler3, scaler4, model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, mlb, y_labels_encoder2, y_labels_encoder3, y_labels_encoder4, atc_level1_labels_encoder2, atc_level1_labels_encoder3, atc_level2_labels_encoder3, atc_level1_labels_encoder4, atc_level2_labels_encoder4, atc_level3_labels_encoder4, previous_predictions = None, index = None):\n",
    "    \"\"\"Genera predicciones para todos los niveles, o solo para un índice si se repite\"\"\"\n",
    "    \n",
    "    def sample_prediction(model, X_test, encoder, previous_predictions = None, index = None):\n",
    "        \"\"\"Calcula la predicción con probabilidad ponderada para un índice o todos\"\"\"\n",
    "        if previous_predictions is None:\n",
    "            # Primera vez: calcular todas las predicciones\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            y_prob_matrix = np.array(y_prob)[:, :, 1].T\n",
    "            predictions = [\n",
    "                random.choices(encoder.classes_, weights=row, k=1)[0] for row in y_prob_matrix\n",
    "            ]\n",
    "        else:\n",
    "            # Mantener las predicciones anteriores y cambiar solo la del índice dado\n",
    "            predictions = previous_predictions.copy()\n",
    "            if index is not None:\n",
    "                y_prob = model.predict_proba(X_test)\n",
    "                row = np.array(y_prob)[:, :, 1].T[index]\n",
    "                predictions[index] = random.choices(encoder.classes_, weights=row, k=1)[0]\n",
    "    \n",
    "        return predictions\n",
    "    \n",
    "    # Nivel 1\n",
    "    X_test1 = test_set_level1(scaler1, X_test1)\n",
    "    pred_1 = sample_prediction(model1, X_test1, mlb, previous_predictions['pred_1'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level1 = pd.DataFrame(pred_1, columns=['pred_1'])\n",
    "\n",
    "    # Nivel 2\n",
    "    X_test2 = test_set_level2(scaler2, X_test2, pred_df_level1, atc_level1_labels_encoder2)\n",
    "    pred_2 = sample_prediction(model2, X_test2, y_labels_encoder2, previous_predictions['pred_2'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level2 = pd.DataFrame(pred_2, columns=['pred_2'])\n",
    "\n",
    "    # Nivel 3\n",
    "    X_test3 = test_set_level3(scaler3, X_test3, pred_df_level1, pred_df_level2, atc_level1_labels_encoder3, atc_level2_labels_encoder3)\n",
    "    pred_3 = sample_prediction(model3, X_test3, y_labels_encoder3, previous_predictions['pred_3'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level3 = pd.DataFrame(pred_3, columns=['pred_3'])\n",
    "\n",
    "    # Nivel 4\n",
    "    X_test4 = test_set_level4(scaler4, X_test4, pred_df_level1, pred_df_level2, pred_df_level3, atc_level1_labels_encoder4, atc_level2_labels_encoder4, atc_level3_labels_encoder4)\n",
    "    pred_4 = sample_prediction(model4, X_test4, y_labels_encoder4, previous_predictions['pred_4'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level4 = pd.DataFrame(pred_4, columns=['pred_4'])\n",
    "\n",
    "    return pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c1e846-cc8c-4a1e-aabc-707dc71dd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predictions(scaler1, scaler2, scaler3, scaler4, model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, mlb, y_labels_encoder2, y_labels_encoder3, y_labels_encoder4, atc_level1_labels_encoder2, atc_level1_labels_encoder3, atc_level2_labels_encoder3, atc_level1_labels_encoder4, atc_level2_labels_encoder4, atc_level3_labels_encoder4):\n",
    "    final_predictions = [[] for _ in range(len(X_test1))]\n",
    "    max_attempts = 30\n",
    "    for i in range(3):\n",
    "        pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4 = generate_predictions(scaler1, scaler2, scaler3, scaler4, model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, mlb, y_labels_encoder2, y_labels_encoder3, y_labels_encoder4,  atc_level1_labels_encoder2, atc_level1_labels_encoder3, atc_level2_labels_encoder3, atc_level1_labels_encoder4, atc_level2_labels_encoder4, atc_level3_labels_encoder4)\n",
    "\n",
    "        for index in range(len(pred_df_level1)):\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                pred1 = pred_df_level1.at[index, 'pred_1']\n",
    "                pred2 = str(pred_df_level2.at[index, \"pred_2\"]).zfill(2)\n",
    "                pred3 = pred_df_level3.at[index, \"pred_3\"]\n",
    "                pred4 = pred_df_level4.at[index, \"pred_4\"]\n",
    "                prediction = pred1 + pred2 + pred3 + pred4\n",
    "\n",
    "                if prediction not in final_predictions[index]:\n",
    "                    final_predictions[index].append(prediction)\n",
    "                    break  # Salimos del bucle cuando obtenemos una predicción nueva\n",
    "\n",
    "                # print(f\"Prediction {prediction} already found in {final_predictions[index]}, reclassifying index {index}...\")\n",
    "                previous_predictions = pd.concat([pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4], axis = 1)\n",
    "                # Recalcular la clasificación completa solo para este índice\n",
    "                pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4 = generate_predictions(scaler1, scaler2, scaler3, scaler4, model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, mlb, y_labels_encoder2, y_labels_encoder3, y_labels_encoder4, atc_level1_labels_encoder2, atc_level1_labels_encoder3, atc_level2_labels_encoder3, atc_level1_labels_encoder4, atc_level2_labels_encoder4, atc_level3_labels_encoder4, previous_predictions, index)\n",
    "                attempts += 1\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d062b35d-e773-4c33-99a8-216e7832b7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators                                           500\n",
      "max_depth                                               20\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.415629\n",
      "Precision nivel2                                  0.538889\n",
      "Precision nivel3                                  0.644737\n",
      "Precision nivel4                                  0.561404\n",
      "Recall nivel1                                     0.671024\n",
      "Recall nivel2                                     0.684848\n",
      "Recall nivel3                                     0.743421\n",
      "Recall nivel4                                     0.690058\n",
      "Drugs that have at least one match    [330, 228, 171, 118]\n",
      "F1 nivel1                                         0.513314\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 2 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 220 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 411 compounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trini\\AppData\\Local\\Temp\\ipykernel_56016\\3329730929.py:319: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators                                           100\n",
      "max_depth                                               40\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.421853\n",
      "Precision nivel2                                  0.558764\n",
      "Precision nivel3                                  0.627803\n",
      "Precision nivel4                                  0.557199\n",
      "Recall nivel1                                     0.663728\n",
      "Recall nivel2                                     0.668693\n",
      "Recall nivel3                                     0.753363\n",
      "Recall nivel4                                     0.704142\n",
      "Drugs that have at least one match    [329, 223, 169, 119]\n",
      "F1 nivel1                                         0.515845\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 9 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 226 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 418 compounds\n",
      "n_estimators                                           500\n",
      "max_depth                                               20\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.381743\n",
      "Precision nivel2                                  0.559748\n",
      "Precision nivel3                                  0.642636\n",
      "Precision nivel4                                  0.565625\n",
      "Recall nivel1                                     0.641252\n",
      "Recall nivel2                                     0.671122\n",
      "Recall nivel3                                     0.714729\n",
      "Recall nivel4                                      0.68125\n",
      "Drugs that have at least one match    [318, 215, 160, 110]\n",
      "F1 nivel1                                         0.478582\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 3 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 223 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 430 compounds\n",
      "n_estimators                                           500\n",
      "max_depth                                               20\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.391425\n",
      "Precision nivel2                                  0.555026\n",
      "Precision nivel3                                  0.601246\n",
      "Precision nivel4                                   0.57265\n",
      "Recall nivel1                                     0.643845\n",
      "Recall nivel2                                     0.671429\n",
      "Recall nivel3                                     0.726636\n",
      "Recall nivel4                                     0.691239\n",
      "Drugs that have at least one match    [315, 214, 156, 109]\n",
      "F1 nivel1                                         0.486862\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 3 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 244 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 434 compounds\n",
      "n_estimators                                          100\n",
      "max_depth                                            None\n",
      "min_samples_split                                       5\n",
      "min_samples_leaf                                        1\n",
      "class_weight                                         None\n",
      "Precision nivel1                                 0.396266\n",
      "Precision nivel2                                 0.517382\n",
      "Precision nivel3                                 0.586854\n",
      "Precision nivel4                                 0.558166\n",
      "Recall nivel1                                    0.654046\n",
      "Recall nivel2                                    0.648262\n",
      "Recall nivel3                                    0.694836\n",
      "Recall nivel4                                    0.643177\n",
      "Drugs that have at least one match    [326, 213, 149, 97]\n",
      "F1 nivel1                                        0.493522\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 2 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 177 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 330 compounds\n",
      "n_estimators                                           500\n",
      "max_depth                                               20\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.374136\n",
      "Precision nivel2                                  0.504154\n",
      "Precision nivel3                                  0.670051\n",
      "Precision nivel4                                   0.57906\n",
      "Recall nivel1                                     0.646888\n",
      "Recall nivel2                                     0.608255\n",
      "Recall nivel3                                     0.780034\n",
      "Recall nivel4                                     0.692308\n",
      "Drugs that have at least one match    [321, 197, 156, 109]\n",
      "F1 nivel1                                         0.474081\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 3 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 220 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 444 compounds\n",
      "n_estimators                                           500\n",
      "max_depth                                               40\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.410443\n",
      "Precision nivel2                                  0.500986\n",
      "Precision nivel3                                   0.64455\n",
      "Precision nivel4                                  0.554852\n",
      "Recall nivel1                                     0.687068\n",
      "Recall nivel2                                     0.619822\n",
      "Recall nivel3                                     0.744076\n",
      "Recall nivel4                                      0.68038\n",
      "Drugs that have at least one match    [338, 211, 158, 108]\n",
      "F1 nivel1                                         0.513894\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 5 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 241 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 424 compounds\n",
      "n_estimators                                            50\n",
      "max_depth                                             None\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.401107\n",
      "Precision nivel2                                   0.54441\n",
      "Precision nivel3                                  0.630468\n",
      "Precision nivel4                                  0.704499\n",
      "Recall nivel1                                     0.644191\n",
      "Recall nivel2                                     0.682445\n",
      "Recall nivel3                                     0.732278\n",
      "Recall nivel4                                     0.822086\n",
      "Drugs that have at least one match    [319, 221, 163, 135]\n",
      "F1 nivel1                                         0.494384\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 7 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 244 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 417 compounds\n",
      "n_estimators                                            50\n",
      "max_depth                                             None\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.383817\n",
      "Precision nivel2                                  0.555736\n",
      "Precision nivel3                                  0.659558\n",
      "Precision nivel4                                  0.564182\n",
      "Recall nivel1                                     0.624654\n",
      "Recall nivel2                                     0.680736\n",
      "Recall nivel3                                     0.751975\n",
      "Recall nivel4                                     0.700828\n",
      "Drugs that have at least one match    [308, 211, 161, 114]\n",
      "F1 nivel1                                         0.475478\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 6 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 220 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 421 compounds\n",
      "n_estimators                                           100\n",
      "max_depth                                             None\n",
      "min_samples_split                                        2\n",
      "min_samples_leaf                                         1\n",
      "class_weight                                          None\n",
      "Precision nivel1                                  0.387967\n",
      "Precision nivel2                                  0.552576\n",
      "Precision nivel3                                  0.597095\n",
      "Precision nivel4                                  0.593418\n",
      "Recall nivel1                                     0.639177\n",
      "Recall nivel2                                     0.674553\n",
      "Recall nivel3                                     0.715596\n",
      "Recall nivel4                                     0.694268\n",
      "Drugs that have at least one match    [317, 218, 157, 109]\n",
      "F1 nivel1                                         0.482852\n",
      "Name: 0, dtype: object\n",
      "The model predicted less than 3 ATC codes of level 4 for 5 compounds\n",
      "The model predicted less than 3 ATC codes of level 3 for 208 compounds\n",
      "The model predicted less than 3 ATC codes of level 2 for 406 compounds\n",
      "Mean: Precision            0.048226\n",
      "Recall               0.129591\n",
      "F1                   0.068962\n",
      "Precision_level3     0.147876\n",
      "Recall_level3        0.320168\n",
      "F1_level3            0.195299\n",
      "Precision_level2     0.286093\n",
      "Recall_level2        0.516689\n",
      "F1_level2            0.352083\n",
      "Precision level 1    0.483355\n",
      "Precision level 2    0.625288\n",
      "Precision level 3    0.514990\n",
      "Precision level 4    0.325576\n",
      "Recall level 1       0.681746\n",
      "Recall level 2       0.749167\n",
      "Recall level 3       0.612606\n",
      "Recall level 4       0.400061\n",
      "dtype: float64\n",
      "Std: Precision            0.006052\n",
      "Recall               0.014754\n",
      "F1                   0.008234\n",
      "Precision_level3     0.011719\n",
      "Recall_level3        0.024218\n",
      "F1_level3            0.014904\n",
      "Precision_level2     0.016684\n",
      "Recall_level2        0.019892\n",
      "F1_level2            0.017577\n",
      "Precision level 1    0.011366\n",
      "Precision level 2    0.029155\n",
      "Precision level 3    0.026957\n",
      "Precision level 4    0.025592\n",
      "Recall level 1       0.005855\n",
      "Recall level 2       0.027164\n",
      "Recall level 3       0.025548\n",
      "Recall level 4       0.025134\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "seeds = [42, 123, 47899, 2025, 1, 20, 99, 1020, 345, 78] \n",
    "columns = [\n",
    "    'Seed', \n",
    "    'Precision', 'Recall', 'F1',\n",
    "    'Precision_level3', 'Recall_level3', 'F1_level3',\n",
    "    'Precision_level2', 'Recall_level2', 'F1_level2',\n",
    "    'Precision level 1', 'Precision level 2', 'Precision level 3', 'Precision level 4',\n",
    "    'Recall level 1', 'Recall level 2', 'Recall level 3', 'Recall level 4',\n",
    "    '#Compounds that have at least one match'\n",
    "]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seeds(seed)\n",
    "    X_train1, y_train1, X_test1, y_test1 = train_test_1(seed)\n",
    "    X_train2, y_train2, X_test2, y_test2 = train_test_2(seed)\n",
    "    X_train3, y_train3, X_test3, y_test3 = train_test_3(seed)\n",
    "    X_train4, y_train4, X_test4, y_test4 = train_test_4(seed)\n",
    "    \n",
    "    if os.path.exists(f\"sortedrandomforest_results{seed}.csv\"):\n",
    "        best_hyperparameters = (pd.read_csv(f\"sortedrandomforest_results{seed}.csv\", keep_default_na=False)).loc[0]\n",
    "    else:\n",
    "        best_hyperparameters = hyperparametersselection(seed)\n",
    "\n",
    "    best_hyperparameters['max_depth'] = None if best_hyperparameters['max_depth'] == \"None\" else int(best_hyperparameters['max_depth'])\n",
    "    best_hyperparameters = best_hyperparameters.replace(\"None\", None)\n",
    "    print(best_hyperparameters)\n",
    "    # LEVEL1\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels1 = set()\n",
    "    for lista in y_train1:\n",
    "        lista = convert_string_list(lista)\n",
    "        for code in lista:\n",
    "            labels1.add(code)\n",
    "    for lista in y_test1:\n",
    "        lista = convert_string_list(lista)\n",
    "        for code in lista:\n",
    "            labels1.add(code)\n",
    "            \n",
    "    labels1 = sorted(list(labels1))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit([labels1])\n",
    "    y_new = y_train1.copy()\n",
    "    for index, lista in enumerate(y_train1):\n",
    "        y_new[index] = []\n",
    "        lista = convert_string_list(lista)\n",
    "        for i, label in enumerate(lista):\n",
    "            y_new[index].append(lista[i])\n",
    "    y_categorical1 = mlb.transform(y_new)\n",
    "    \n",
    "    X_train1 = np.asarray(X_train1).astype(np.float32)\n",
    "    y_categorical1 = np.asarray(y_categorical1).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train1[pd.isna(X_train1)] = np.nanmedian(X_train1)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler1 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train1 = scaler1.fit_transform(X_train1)\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=best_hyperparameters['n_estimators'], max_depth=best_hyperparameters['max_depth'], min_samples_split=best_hyperparameters['min_samples_split'], min_samples_leaf=best_hyperparameters['min_samples_leaf'], class_weight = best_hyperparameters['class_weight'], random_state=seed)\n",
    "    # Train the model\n",
    "    rf1.fit(X_train1, y_categorical1)\n",
    "    \n",
    "    #LEVEL 2\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels2 = set()\n",
    "    for code in y_train2:\n",
    "        labels2.add(code)\n",
    "            \n",
    "    labels2 = sorted(list(labels2))\n",
    "    y_labels_encoder2 = MultiLabelBinarizer()\n",
    "    y_labels_encoder2.fit([labels2])\n",
    "    encoded_y_train2 = y_labels_encoder2.transform(y_train2.values.reshape(-1, 1))\n",
    "    atc_level1_labels2 = set()\n",
    "    for _, row in X_train2.iterrows():\n",
    "        atc_level1_labels2.add(row['ATC_level1'])\n",
    "            \n",
    "    atc_level1_labels2 = sorted(list(atc_level1_labels2))\n",
    "    atc_level1_labels_encoder2 = MultiLabelBinarizer()\n",
    "    atc_level1_labels_encoder2.fit([atc_level1_labels2])\n",
    "    ATC_level11 = X_train2['ATC_level1']\n",
    "    ATC_level1_2 = ATC_level11.copy()\n",
    "    for index, lista in enumerate(ATC_level11):\n",
    "        ATC_level1_2[index] = []\n",
    "        ATC_level1_2[index].append(lista)\n",
    "    categorical_atc2 = atc_level1_labels_encoder2.transform(ATC_level1_2)\n",
    "    X_train2.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "    df_level1_2 = pd.DataFrame(categorical_atc2, columns=atc_level1_labels2)\n",
    "    X_train2 = pd.concat([X_train2, df_level1_2], axis = 1)\n",
    "    X_train2 = np.asarray(X_train2).astype(np.float32)\n",
    "    encoded_y_train2 = np.asarray(encoded_y_train2).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train2[pd.isna(X_train2)] = np.nanmedian(X_train2)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler2 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train2 = scaler2.fit_transform(X_train2)\n",
    "    \n",
    "    rf2 = RandomForestClassifier(n_estimators=best_hyperparameters['n_estimators'], max_depth=best_hyperparameters['max_depth'], min_samples_split=best_hyperparameters['min_samples_split'], min_samples_leaf=best_hyperparameters['min_samples_leaf'], class_weight = best_hyperparameters['class_weight'], random_state=seed)\n",
    "    # Train the model\n",
    "    rf2.fit(X_train2, encoded_y_train2)\n",
    "    \n",
    "    #LEVEL 3\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels3 = set()\n",
    "    for code in y_train3:\n",
    "        labels3.add(code)\n",
    "            \n",
    "    labels3 = sorted(list(labels3))\n",
    "    y_labels_encoder3 = MultiLabelBinarizer()\n",
    "    y_labels_encoder3.fit([labels3])\n",
    "    encoded_y_train3 = y_labels_encoder3.transform(y_train3.values.reshape(-1, 1))\n",
    "    \n",
    "    atc_level1_labels3 = set()\n",
    "    atc_level2_labels3 = set()\n",
    "    for _, row in X_train3.iterrows():\n",
    "        atc_level1_labels3.add(row['ATC_level2'][0:1])\n",
    "        atc_level2_labels3.add(row['ATC_level2'][1:3])\n",
    "    for _, row in X_test3.iterrows():\n",
    "        lista = convert_string_list(row['ATC_level2'])\n",
    "        for code in lista:\n",
    "            atc_level1_labels3.add(code[0:1])\n",
    "            atc_level2_labels3.add(code[1:3])\n",
    "            \n",
    "    atc_level1_labels3 = sorted(list(atc_level1_labels3))\n",
    "    atc_level2_labels3 = sorted(list(atc_level2_labels3))\n",
    "    atc_level1_labels_encoder3 = MultiLabelBinarizer()\n",
    "    atc_level1_labels_encoder3.fit([atc_level1_labels3])\n",
    "    atc_level2_labels_encoder3 = MultiLabelBinarizer()\n",
    "    atc_level2_labels_encoder3.fit([atc_level2_labels3])\n",
    "    ATC_level22 = X_train3['ATC_level2']\n",
    "    ATC_level1_3 = ATC_level22.copy()\n",
    "    ATC_level2_3 = ATC_level22.copy()\n",
    "    for index, lista in enumerate(ATC_level22):\n",
    "        ATC_level1_3[index] = []\n",
    "        ATC_level1_3[index].append(lista[0:1])\n",
    "    for index, lista in enumerate(ATC_level22):\n",
    "        ATC_level2_3[index] = []\n",
    "        ATC_level2_3[index].append(lista[1:3])\n",
    "    X_train3.drop(labels=['ATC_level2'], axis=\"columns\", inplace=True)\n",
    "    categorical_atc1_3 = atc_level1_labels_encoder3.transform(ATC_level1_3)\n",
    "    categorical_atc2_3 = atc_level2_labels_encoder3.transform(ATC_level2_3)\n",
    "    df_level1_3 = pd.DataFrame(categorical_atc1_3, columns=atc_level1_labels3)\n",
    "    df_level2_3 = pd.DataFrame(categorical_atc2_3, columns=atc_level2_labels3)\n",
    "    X_train3 = pd.concat([X_train3, df_level1_3, df_level2_3], axis = 1)\n",
    "    X_train3 = np.asarray(X_train3).astype(np.float32)\n",
    "    encoded_y_train3 = np.asarray(encoded_y_train3).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train3[pd.isna(X_train3)] = np.nanmedian(X_train3)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler3 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train3 = scaler3.fit_transform(X_train3)\n",
    "    \n",
    "    rf3 = RandomForestClassifier(n_estimators=best_hyperparameters['n_estimators'], max_depth=best_hyperparameters['max_depth'], min_samples_split=best_hyperparameters['min_samples_split'], min_samples_leaf=best_hyperparameters['min_samples_leaf'], class_weight = best_hyperparameters['class_weight'], random_state=seed)\n",
    "    # Train the model\n",
    "    rf3.fit(X_train3, encoded_y_train3)\n",
    "    #LEVEL 4\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels4 = set()\n",
    "    for code in y_train4:\n",
    "        labels4.add(code)\n",
    "            \n",
    "    labels4 = sorted(list(labels4))\n",
    "    y_labels_encoder4 = MultiLabelBinarizer()\n",
    "    y_labels_encoder4.fit([labels4])\n",
    "    encoded_y_train4 = y_labels_encoder4.transform(y_train4.values.reshape(-1, 1))\n",
    "    \n",
    "    atc_level1_labels4 = set()\n",
    "    atc_level2_labels4 = set()\n",
    "    atc_level3_labels4 = set()\n",
    "    for _, row in X_train4.iterrows():\n",
    "        atc_level1_labels4.add(row['ATC_level3'][0:1])\n",
    "        atc_level2_labels4.add(row['ATC_level3'][1:3])\n",
    "        atc_level3_labels4.add(row['ATC_level3'][3:4])\n",
    "    for _, row in X_test4.iterrows():\n",
    "        lista = convert_string_list(row['ATC_level3'])\n",
    "        for code in lista:\n",
    "            atc_level1_labels4.add(code[0:1])\n",
    "            atc_level2_labels4.add(code[1:3])\n",
    "            atc_level3_labels4.add(code[3:4])\n",
    "    \n",
    "    atc_level1_labels4 = sorted(list(atc_level1_labels4))\n",
    "    atc_level2_labels4 = sorted(list(atc_level2_labels4))\n",
    "    atc_level3_labels4 = sorted(list(atc_level3_labels4))\n",
    "    atc_level1_labels_encoder4 = MultiLabelBinarizer()\n",
    "    atc_level1_labels_encoder4.fit([atc_level1_labels4])\n",
    "    atc_level2_labels_encoder4 = MultiLabelBinarizer()\n",
    "    atc_level2_labels_encoder4.fit([atc_level2_labels4])\n",
    "    atc_level3_labels_encoder4 = MultiLabelBinarizer()\n",
    "    atc_level3_labels_encoder4.fit([atc_level3_labels4])\n",
    "    ATC_level33 = X_train4['ATC_level3']\n",
    "    ATC_level1_4 = ATC_level33.copy()\n",
    "    for index, lista in enumerate(ATC_level33):\n",
    "        ATC_level1_4[index] = []\n",
    "        ATC_level1_4[index].append(lista[0:1])\n",
    "    ATC_level2_4 = ATC_level33.copy()\n",
    "    for index, lista in enumerate(ATC_level33):\n",
    "        ATC_level2_4[index] = []\n",
    "        ATC_level2_4[index].append(lista[1:3])\n",
    "    ATC_level3_4 = ATC_level33.copy()\n",
    "    for index, lista in enumerate(ATC_level33):\n",
    "        ATC_level3_4[index] = []\n",
    "        ATC_level3_4[index].append(lista[3:4])\n",
    "    X_train4.drop(labels=['ATC_level3'], axis=\"columns\", inplace=True)\n",
    "    categorical_atc1_4 = atc_level1_labels_encoder4.transform(ATC_level1_4)\n",
    "    categorical_atc2_4 = atc_level2_labels_encoder4.transform(ATC_level2_4)\n",
    "    categorical_atc3_4 = atc_level3_labels_encoder4.transform(ATC_level3_4)\n",
    "    df_level1_4 = pd.DataFrame(categorical_atc1_4, columns=atc_level1_labels4)\n",
    "    df_level2_4 = pd.DataFrame(categorical_atc2_4, columns=atc_level2_labels4)\n",
    "    df_level3_4 = pd.DataFrame(categorical_atc3_4, columns=atc_level3_labels4)\n",
    "    X_train4 = pd.concat([X_train4, df_level1_4, df_level2_4, df_level3_4], axis = 1)\n",
    "    X_train4 = np.asarray(X_train4).astype(np.float32)\n",
    "    encoded_y_train4 = np.asarray(encoded_y_train4).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train4[pd.isna(X_train4)] = np.nanmedian(X_train4)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler4 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train4 = scaler4.fit_transform(X_train4)\n",
    "    \n",
    "    rf4 = RandomForestClassifier(n_estimators=best_hyperparameters['n_estimators'], max_depth=best_hyperparameters['max_depth'], min_samples_split=best_hyperparameters['min_samples_split'], min_samples_leaf=best_hyperparameters['min_samples_leaf'], class_weight = best_hyperparameters['class_weight'], random_state=seed)\n",
    "    rf4.fit(X_train4, encoded_y_train4)\n",
    "    \n",
    "    #TEST\n",
    "    X_test3.drop(labels=['ATC_level2'], axis=\"columns\", inplace=True)\n",
    "    X_test4.drop(labels=['ATC_level3'], axis=\"columns\", inplace = True)\n",
    "\n",
    "    predictions = random_predictions(scaler1, scaler2, scaler3, scaler4, rf1, X_test1, rf2, X_test2, rf3, X_test3, rf4, X_test4, mlb, y_labels_encoder2, y_labels_encoder3, y_labels_encoder4, atc_level1_labels_encoder2, atc_level1_labels_encoder3, atc_level2_labels_encoder3, atc_level1_labels_encoder4, atc_level2_labels_encoder4, atc_level3_labels_encoder4)\n",
    "    predictions_clean = []\n",
    "    counter4_lessthan3 = 0\n",
    "    for preds in predictions:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            if len(clean_pred) == 5:\n",
    "                interm.append(clean_pred)\n",
    "        if len(interm) == 3:\n",
    "            predictions_clean.append(interm)\n",
    "        else:\n",
    "            counter4_lessthan3 += 1\n",
    "            predictions_clean.append(interm)\n",
    "    print(f\"The model predicted less than 3 ATC codes of level 4 for {counter4_lessthan3} compounds\")                 \n",
    "    predictions_clean_level3 = []\n",
    "    counter3_lessthan3 = 0\n",
    "    for preds in predictions:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            pred_3 = clean_pred[0:4]\n",
    "            if len(pred_3) == 4 and pred_3 not in interm:\n",
    "                interm.append(pred_3)\n",
    "        if len(interm) == 3:\n",
    "            predictions_clean_level3.append(interm)\n",
    "        else:\n",
    "            counter3_lessthan3 += 1\n",
    "            predictions_clean_level3.append(interm)\n",
    "    print(f\"The model predicted less than 3 ATC codes of level 3 for {counter3_lessthan3} compounds\")       \n",
    "    predictions_clean_level2 = []\n",
    "    counter2_lessthan3 = 0\n",
    "    for preds in predictions:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            pred_2 = clean_pred[0:3]\n",
    "            if len(pred_2) == 3 and pred_2 not in interm:\n",
    "                interm.append(pred_2)\n",
    "        if len(interm) == 3:\n",
    "            predictions_clean_level2.append(interm)\n",
    "        else:\n",
    "            counter2_lessthan3 += 1\n",
    "            predictions_clean_level2.append(interm)\n",
    "    print(f\"The model predicted less than 3 ATC codes of level 2 for {counter2_lessthan3} compounds\")       \n",
    "    precision_1, precision_2, precision_3, precision_4 = defined_metrics.precision(predictions_clean, f'../Datasets/SplitATC_Rep_test_set{seed}.csv', 'ATC Codes')\n",
    "    recall_1, recall_2, recall_3, recall_4, counter_compound_match = defined_metrics.recall(predictions_clean, f'../Datasets/SplitATC_Rep_test_set{seed}.csv', 'ATC Codes')\n",
    "    precisions, recalls, f1s = defined_metrics.complete_metrics(predictions_clean, f'../Datasets/SplitATC_Rep_test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_level3, recalls_level3, f1s_level3 = defined_metrics.complete_metrics_level3(predictions_clean_level3, f'../Datasets/SplitATC_Rep_test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_level2, recalls_level2, f1s_level2 = defined_metrics.complete_metrics_level2(predictions_clean_level2, f'../Datasets/SplitATC_Rep_test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_average = sum(precisions)/len(precisions)\n",
    "    recalls_average = sum(recalls)/len(recalls)\n",
    "    f1s_average = sum(f1s)/len(f1s)\n",
    "\n",
    "    precisions_average_level3 = sum(precisions_level3)/len(precisions_level3)\n",
    "    recalls_average_level3 = sum(recalls_level3)/len(recalls_level3)\n",
    "    f1s_average_level3 = sum(f1s_level3)/len(f1s_level3)\n",
    "\n",
    "    precisions_average_level2 = sum(precisions_level2)/len(precisions_level2)\n",
    "    recalls_average_level2 = sum(recalls_level2)/len(recalls_level2)\n",
    "    f1s_average_level2 = sum(f1s_level2)/len(f1s_level2)\n",
    "        \n",
    "    metrics = {\n",
    "        'Precision': precisions_average, \n",
    "        'Recall': recalls_average,\n",
    "        'F1': f1s_average,\n",
    "        'Precision_level3': precisions_average_level3, \n",
    "        'Recall_level3': recalls_average_level3,\n",
    "        'F1_level3': f1s_average_level3,\n",
    "        'Precision_level2': precisions_average_level2, \n",
    "        'Recall_level2': recalls_average_level2,\n",
    "        'F1_level2': f1s_average_level2,\n",
    "        'Precision level 1': precision_1,\n",
    "        'Precision level 2': precision_2,\n",
    "        'Precision level 3': precision_3,\n",
    "        'Precision level 4': precision_4,\n",
    "        'Recall level 1': recall_1,\n",
    "        'Recall level 2': recall_2,\n",
    "        'Recall level 3': recall_3,\n",
    "        'Recall level 4': recall_4,\n",
    "        '#Compounds that have at least one match': counter_compound_match\n",
    "    }\n",
    "    \n",
    "    # Build the row\n",
    "    row = {\n",
    "        'Seed': seed,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "metrics_df.to_csv(\"randomforest_metrics.csv\", index=False)\n",
    "print(\"Mean:\", metrics_df.mean(numeric_only=True))\n",
    "print(\"Std:\", metrics_df.std(numeric_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
