{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a67c3-3164-402b-9c3d-ed5f98d41c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../..')))\n",
    "from seq2seq import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bilstm_hyp import hyperparametersselection\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566324b4-cf15-44bc-9f49-c6c019367e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa18d3e-cd26-4b6a-85c0-cc550fb60feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a string that simulates a list to a real list\n",
    "def convert_string_list(element):\n",
    "    # Delete [] of the string\n",
    "    element = element[0:len(element)]\n",
    "    # Create a list that contains each code as e.g. 'A'\n",
    "    ATC_list = list(element.split('; '))\n",
    "    for index, code in enumerate(ATC_list):\n",
    "        # Delete '' of the code\n",
    "        ATC_list[index] = code[0:len(code)]\n",
    "    return ATC_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e165f52-d611-4847-a003-d336dd9fe491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplicate_rows(df):\n",
    "    # Duplicate each compound the number of ATC codes associated to it, copying its SMILES in new rows\n",
    "    new_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        atc_codes = row['ATC Codes']\n",
    "        atc_codes_list = convert_string_list(atc_codes)\n",
    "        \n",
    "        if len(atc_codes_list) > 1:\n",
    "            for code in atc_codes_list:\n",
    "                if len(code) == 5:\n",
    "                    new_row = row.copy()\n",
    "                    new_row['ATC Codes'] = code\n",
    "                    new_rows.append(new_row)\n",
    "        else:\n",
    "            if len(atc_codes_list[0]) == 5:\n",
    "                new_rows.append(row)\n",
    "    \n",
    "    new_set = pd.DataFrame(new_rows)\n",
    "    new_set = new_set.reset_index(drop=True)\n",
    "\n",
    "    return new_set\n",
    "\n",
    "# Create vocabularies\n",
    "# Tokenize the data\n",
    "def source(df):\n",
    "    source = []\n",
    "    for compound in df['Neutralized SMILES']:\n",
    "        # A list containing each SMILES character separated\n",
    "        source.append(list(compound))\n",
    "    return source\n",
    "def target(df):\n",
    "    target = []\n",
    "    for codes in df['ATC Codes']:  \n",
    "        code = convert_string_list(codes) \n",
    "        # A list of lists, each one containing each ATC code character separated \n",
    "        for c in code:\n",
    "            list_c = list(c)\n",
    "            target.append(list_c)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3256e6-ad40-454b-80a0-e415b76ab19e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 46 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 128\n",
      "Decoder embedding dimension: 128\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 1,069,346\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3024, 702])\n",
      "Y_train.shape: torch.Size([3024, 7])\n",
      "X_dev.shape: torch.Size([538, 295])\n",
      "Y_dev.shape: torch.Size([538, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 1e-05\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.0681 |     56.294 |   1.5027 |     46.097 |     0.0\n",
      "    2 |   1.4036 |     45.569 |   1.3538 |     46.004 |     0.1\n",
      "    3 |   1.3164 |     44.466 |   1.2979 |     43.897 |     0.1\n",
      "    4 |   1.2681 |     43.634 |   1.2587 |     43.587 |     0.2\n",
      "    5 |   1.2368 |     43.094 |   1.2327 |     42.441 |     0.2\n",
      "    6 |   1.2088 |     42.323 |   1.2090 |     42.937 |     0.2\n",
      "    7 |   1.1762 |     41.474 |   1.1807 |     41.233 |     0.3\n",
      "    8 |   1.1452 |     40.074 |   1.1455 |     39.932 |     0.3\n",
      "    9 |   1.1054 |     38.106 |   1.1238 |     39.374 |     0.3\n",
      "   10 |   1.0668 |     36.491 |   1.0887 |     38.011 |     0.4\n",
      "   11 |   1.0223 |     34.177 |   1.0495 |     35.812 |     0.4\n",
      "   12 |   0.9709 |     32.512 |   1.0288 |     36.059 |     0.5\n",
      "   13 |   0.9201 |     30.379 |   1.0037 |     34.542 |     0.5\n",
      "   14 |   0.8726 |     28.853 |   0.9706 |     32.931 |     0.5\n",
      "   15 |   0.8232 |     26.857 |   0.9478 |     32.001 |     0.6\n",
      "   16 |   0.7717 |     24.956 |   0.9356 |     30.514 |     0.6\n",
      "   17 |   0.7158 |     22.928 |   0.9245 |     29.616 |     0.6\n",
      "   18 |   0.6654 |     20.955 |   0.9149 |     29.926 |     0.7\n",
      "   19 |   0.6226 |     19.703 |   0.9043 |     29.089 |     0.7\n",
      "   20 |   0.5786 |     18.430 |   0.8937 |     28.222 |     0.8\n",
      "   21 |   0.5363 |     16.970 |   0.9007 |     27.757 |     0.8\n",
      "   22 |   0.4901 |     15.399 |   0.8939 |     27.292 |     0.8\n",
      "   23 |   0.4522 |     13.878 |   0.9061 |     27.014 |     0.9\n",
      "   24 |   0.4131 |     12.693 |   0.8938 |     25.682 |     0.9\n",
      "Early stopping\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trini\\AppData\\Local\\Temp\\ipykernel_22256\\1946271227.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 45 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 64\n",
      "Decoder embedding dimension: 64\n",
      "Encoder hidden units: 64\n",
      "Encoder layers: 4\n",
      "Decoder hidden units: 64\n",
      "Decoder layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 503,906\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3028, 649])\n",
      "Y_train.shape: torch.Size([3028, 7])\n",
      "X_dev.shape: torch.Size([534, 702])\n",
      "Y_dev.shape: torch.Size([534, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.4315 |     64.993 |   1.8863 |     48.876 |     0.0\n",
      "    2 |   1.6453 |     47.545 |   1.4866 |     45.350 |     0.1\n",
      "    3 |   1.4441 |     46.109 |   1.4053 |     45.350 |     0.1\n",
      "    4 |   1.3882 |     46.020 |   1.3582 |     44.975 |     0.1\n",
      "    5 |   1.3484 |     45.492 |   1.3270 |     44.351 |     0.1\n",
      "    6 |   1.3176 |     44.452 |   1.2964 |     43.789 |     0.2\n",
      "    7 |   1.2910 |     44.243 |   1.2732 |     43.102 |     0.2\n",
      "    8 |   1.2703 |     43.929 |   1.2577 |     43.539 |     0.2\n",
      "    9 |   1.2502 |     43.566 |   1.2330 |     43.165 |     0.3\n",
      "   10 |   1.2323 |     43.417 |   1.2183 |     42.104 |     0.3\n",
      "   11 |   1.2166 |     42.575 |   1.2074 |     41.667 |     0.3\n",
      "   12 |   1.2007 |     42.129 |   1.1885 |     41.417 |     0.4\n",
      "   13 |   1.1853 |     41.562 |   1.1780 |     41.292 |     0.4\n",
      "   14 |   1.1699 |     41.188 |   1.1673 |     40.574 |     0.4\n",
      "   15 |   1.1550 |     40.203 |   1.1537 |     39.357 |     0.4\n",
      "   16 |   1.1417 |     40.109 |   1.1426 |     39.232 |     0.5\n",
      "   17 |   1.1291 |     39.278 |   1.1340 |     40.169 |     0.5\n",
      "   18 |   1.1185 |     39.239 |   1.1249 |     39.232 |     0.5\n",
      "   19 |   1.1010 |     38.480 |   1.1169 |     39.419 |     0.6\n",
      "   20 |   1.0880 |     38.094 |   1.1037 |     38.390 |     0.6\n",
      "   21 |   1.0773 |     37.698 |   1.0943 |     38.233 |     0.6\n",
      "   22 |   1.0632 |     37.225 |   1.0935 |     38.296 |     0.6\n",
      "   23 |   1.0524 |     36.702 |   1.0876 |     38.670 |     0.7\n",
      "   24 |   1.0363 |     36.113 |   1.0665 |     37.016 |     0.7\n",
      "   25 |   1.0267 |     35.750 |   1.0656 |     36.954 |     0.7\n",
      "   26 |   1.0118 |     35.331 |   1.0549 |     36.829 |     0.8\n",
      "   27 |   1.0001 |     34.621 |   1.0423 |     36.548 |     0.8\n",
      "   28 |   0.9852 |     34.412 |   1.0364 |     35.893 |     0.8\n",
      "   29 |   0.9711 |     33.631 |   1.0273 |     35.019 |     0.9\n",
      "   30 |   0.9551 |     33.009 |   1.0168 |     34.956 |     0.9\n",
      "   31 |   0.9392 |     32.078 |   1.0151 |     34.644 |     0.9\n",
      "   32 |   0.9221 |     31.605 |   1.0078 |     34.488 |     0.9\n",
      "   33 |   0.9083 |     31.027 |   0.9997 |     33.958 |     1.0\n",
      "   34 |   0.8896 |     30.367 |   0.9863 |     33.302 |     1.0\n",
      "   35 |   0.8732 |     29.778 |   0.9845 |     33.333 |     1.0\n",
      "   36 |   0.8547 |     29.084 |   0.9774 |     32.615 |     1.1\n",
      "   37 |   0.8423 |     28.314 |   0.9724 |     32.865 |     1.1\n",
      "   38 |   0.8207 |     27.427 |   0.9576 |     32.491 |     1.1\n",
      "   39 |   0.8073 |     27.059 |   0.9650 |     32.147 |     1.1\n",
      "   40 |   0.7898 |     26.420 |   0.9588 |     31.960 |     1.2\n",
      "   41 |   0.7728 |     25.683 |   0.9474 |     31.273 |     1.2\n",
      "   42 |   0.7546 |     25.292 |   0.9655 |     32.022 |     1.2\n",
      "   43 |   0.7470 |     24.708 |   0.9622 |     31.367 |     1.3\n",
      "   44 |   0.7322 |     24.169 |   0.9603 |     31.648 |     1.3\n",
      "   45 |   0.7042 |     23.074 |   0.9512 |     30.868 |     1.3\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 44 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 64\n",
      "Decoder embedding dimension: 64\n",
      "Encoder hidden units: 64\n",
      "Encoder layers: 3\n",
      "Decoder hidden units: 64\n",
      "Decoder layers: 3\n",
      "Dropout: 0.2\n",
      "Trainable parameters: 421,410\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3021, 702])\n",
      "Y_train.shape: torch.Size([3021, 7])\n",
      "X_dev.shape: torch.Size([541, 250])\n",
      "Y_dev.shape: torch.Size([541, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 1e-05\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.5427 |     67.842 |   1.9593 |     54.498 |     0.0\n",
      "    2 |   1.7561 |     49.906 |   1.5862 |     45.903 |     0.1\n",
      "    3 |   1.5166 |     46.050 |   1.4638 |     45.903 |     0.1\n",
      "    4 |   1.4426 |     46.094 |   1.4256 |     45.903 |     0.1\n",
      "    5 |   1.4165 |     46.077 |   1.4066 |     45.903 |     0.1\n",
      "    6 |   1.3974 |     46.011 |   1.3860 |     45.903 |     0.2\n",
      "    7 |   1.3821 |     46.017 |   1.3705 |     46.149 |     0.2\n",
      "    8 |   1.3671 |     46.127 |   1.3529 |     45.903 |     0.2\n",
      "    9 |   1.3484 |     45.791 |   1.3429 |     45.656 |     0.2\n",
      "   10 |   1.3329 |     45.542 |   1.3316 |     45.441 |     0.3\n",
      "   11 |   1.3171 |     45.272 |   1.3104 |     45.933 |     0.3\n",
      "   12 |   1.3005 |     44.853 |   1.2991 |     45.317 |     0.3\n",
      "   13 |   1.2899 |     44.544 |   1.2889 |     43.839 |     0.3\n",
      "   14 |   1.2759 |     44.091 |   1.2784 |     43.346 |     0.4\n",
      "   15 |   1.2631 |     43.843 |   1.2648 |     43.746 |     0.4\n",
      "   16 |   1.2510 |     43.529 |   1.2528 |     43.376 |     0.4\n",
      "   17 |   1.2373 |     42.867 |   1.2410 |     42.791 |     0.4\n",
      "   18 |   1.2241 |     42.320 |   1.2340 |     42.668 |     0.5\n",
      "   19 |   1.2138 |     42.243 |   1.2280 |     42.298 |     0.5\n",
      "   20 |   1.2009 |     41.989 |   1.2190 |     42.391 |     0.5\n",
      "   21 |   1.1914 |     41.625 |   1.2083 |     42.083 |     0.5\n",
      "   22 |   1.1796 |     41.239 |   1.1976 |     41.343 |     0.6\n",
      "   23 |   1.1678 |     40.814 |   1.1962 |     41.158 |     0.6\n",
      "   24 |   1.1556 |     40.671 |   1.1807 |     41.035 |     0.6\n",
      "   25 |   1.1493 |     40.136 |   1.1731 |     40.635 |     0.6\n",
      "   26 |   1.1359 |     39.937 |   1.1710 |     40.943 |     0.7\n",
      "   27 |   1.1229 |     39.601 |   1.1558 |     41.097 |     0.7\n",
      "   28 |   1.1092 |     39.176 |   1.1631 |     41.189 |     0.7\n",
      "   29 |   1.1000 |     38.900 |   1.1520 |     40.173 |     0.8\n",
      "   30 |   1.0905 |     38.420 |   1.1362 |     39.587 |     0.8\n",
      "   31 |   1.0721 |     37.466 |   1.1263 |     39.495 |     0.8\n",
      "   32 |   1.0597 |     37.195 |   1.1257 |     39.187 |     0.8\n",
      "   33 |   1.0478 |     36.892 |   1.1212 |     40.018 |     0.9\n",
      "   34 |   1.0337 |     35.948 |   1.1128 |     38.879 |     0.9\n",
      "   35 |   1.0204 |     35.573 |   1.1169 |     38.817 |     0.9\n",
      "   36 |   1.0056 |     35.182 |   1.1051 |     37.831 |     0.9\n",
      "   37 |   0.9917 |     34.382 |   1.1078 |     38.293 |     1.0\n",
      "   38 |   0.9772 |     33.962 |   1.1026 |     37.246 |     1.0\n",
      "   39 |   0.9682 |     33.560 |   1.0935 |     36.661 |     1.0\n",
      "   40 |   0.9515 |     32.517 |   1.1016 |     36.445 |     1.0\n",
      "   41 |   0.9314 |     31.744 |   1.0978 |     36.198 |     1.1\n",
      "   42 |   0.9200 |     31.309 |   1.0872 |     35.829 |     1.1\n",
      "   43 |   0.9071 |     30.757 |   1.0814 |     35.613 |     1.1\n",
      "   44 |   0.8954 |     30.288 |   1.0753 |     35.213 |     1.1\n",
      "   45 |   0.8801 |     29.422 |   1.0920 |     35.551 |     1.2\n",
      "   46 |   0.8715 |     28.964 |   1.0766 |     34.258 |     1.2\n",
      "   47 |   0.8599 |     28.605 |   1.0815 |     34.720 |     1.2\n",
      "   48 |   0.8420 |     27.976 |   1.0797 |     34.319 |     1.2\n",
      "   49 |   0.8258 |     27.364 |   1.0610 |     33.795 |     1.3\n",
      "   50 |   0.8111 |     26.994 |   1.0675 |     33.703 |     1.3\n",
      "   51 |   0.7958 |     26.183 |   1.0668 |     33.457 |     1.3\n",
      "   52 |   0.7850 |     25.897 |   1.0686 |     33.457 |     1.3\n",
      "   53 |   0.7683 |     25.306 |   1.0803 |     32.964 |     1.4\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 43 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 128\n",
      "Decoder embedding dimension: 64\n",
      "Encoder hidden units: 64\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.1\n",
      "Trainable parameters: 507,682\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3042, 702])\n",
      "Y_train.shape: torch.Size([3042, 7])\n",
      "X_dev.shape: torch.Size([520, 467])\n",
      "Y_dev.shape: torch.Size([520, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.0917 |     57.961 |   1.5294 |     45.769 |     0.0\n",
      "    2 |   1.4305 |     45.891 |   1.3796 |     45.705 |     0.0\n",
      "    3 |   1.3456 |     45.075 |   1.3253 |     44.519 |     0.1\n",
      "    4 |   1.3059 |     44.664 |   1.2945 |     44.872 |     0.1\n",
      "    5 |   1.2835 |     44.187 |   1.2624 |     43.590 |     0.1\n",
      "    6 |   1.2580 |     44.017 |   1.2397 |     43.141 |     0.1\n",
      "    7 |   1.2386 |     43.557 |   1.2211 |     42.885 |     0.1\n",
      "    8 |   1.2196 |     42.982 |   1.1961 |     41.346 |     0.2\n",
      "    9 |   1.2013 |     42.362 |   1.1834 |     40.929 |     0.2\n",
      "   10 |   1.1847 |     41.809 |   1.1577 |     40.353 |     0.2\n",
      "   11 |   1.1601 |     40.675 |   1.1426 |     39.904 |     0.2\n",
      "   12 |   1.1310 |     39.892 |   1.1215 |     38.718 |     0.2\n",
      "   13 |   1.1033 |     38.746 |   1.0836 |     37.564 |     0.3\n",
      "   14 |   1.0776 |     37.410 |   1.0643 |     37.596 |     0.3\n",
      "   15 |   1.0496 |     36.418 |   1.0362 |     35.865 |     0.3\n",
      "   16 |   1.0266 |     35.530 |   1.0181 |     35.481 |     0.3\n",
      "   17 |   0.9999 |     35.026 |   1.0075 |     34.840 |     0.3\n",
      "   18 |   0.9767 |     33.629 |   0.9878 |     33.814 |     0.4\n",
      "   19 |   0.9520 |     32.490 |   0.9569 |     32.468 |     0.4\n",
      "   20 |   0.9300 |     31.684 |   0.9440 |     32.147 |     0.4\n",
      "   21 |   0.9023 |     30.391 |   0.9301 |     32.244 |     0.4\n",
      "   22 |   0.8848 |     30.150 |   0.9218 |     31.378 |     0.4\n",
      "   23 |   0.8585 |     29.000 |   0.8999 |     30.449 |     0.5\n",
      "   24 |   0.8323 |     28.019 |   0.8864 |     30.128 |     0.5\n",
      "   25 |   0.8166 |     27.356 |   0.8903 |     30.321 |     0.5\n",
      "   26 |   0.7861 |     26.353 |   0.8782 |     29.295 |     0.5\n",
      "   27 |   0.7634 |     25.690 |   0.8742 |     28.782 |     0.5\n",
      "   28 |   0.7413 |     24.485 |   0.8539 |     28.462 |     0.6\n",
      "   29 |   0.7264 |     24.080 |   0.8580 |     28.045 |     0.6\n",
      "   30 |   0.7107 |     23.280 |   0.8474 |     27.564 |     0.6\n",
      "   31 |   0.6835 |     22.743 |   0.8350 |     27.051 |     0.6\n",
      "   32 |   0.6555 |     21.570 |   0.8453 |     27.724 |     0.6\n",
      "   33 |   0.6465 |     21.422 |   0.8506 |     27.179 |     0.7\n",
      "   34 |   0.6185 |     19.746 |   0.8476 |     27.404 |     0.7\n",
      "   35 |   0.6044 |     19.773 |   0.8423 |     26.827 |     0.7\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 46 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 128\n",
      "Decoder embedding dimension: 128\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 3\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 3\n",
      "Dropout: 0.2\n",
      "Trainable parameters: 1,662,242\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3027, 702])\n",
      "Y_train.shape: torch.Size([3027, 7])\n",
      "X_dev.shape: torch.Size([535, 258])\n",
      "Y_dev.shape: torch.Size([535, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.2128 |     60.687 |   1.6130 |     45.763 |     0.1\n",
      "    2 |   1.4825 |     46.223 |   1.4180 |     45.732 |     0.1\n",
      "    3 |   1.3933 |     45.970 |   1.3713 |     44.922 |     0.2\n",
      "    4 |   1.3561 |     45.700 |   1.3357 |     45.296 |     0.2\n",
      "    5 |   1.3212 |     45.303 |   1.3193 |     44.953 |     0.3\n",
      "    6 |   1.2986 |     44.698 |   1.2954 |     44.611 |     0.4\n",
      "    7 |   1.2727 |     44.164 |   1.2602 |     43.396 |     0.4\n",
      "    8 |   1.2461 |     43.635 |   1.2454 |     43.956 |     0.5\n",
      "    9 |   1.2261 |     42.864 |   1.2288 |     44.299 |     0.5\n",
      "   10 |   1.2133 |     42.815 |   1.2083 |     42.118 |     0.6\n",
      "   11 |   1.1994 |     42.418 |   1.2111 |     43.115 |     0.7\n",
      "   12 |   1.1880 |     42.330 |   1.1975 |     42.212 |     0.7\n",
      "   13 |   1.1737 |     41.895 |   1.1899 |     42.523 |     0.8\n",
      "   14 |   1.1613 |     41.190 |   1.1678 |     41.745 |     0.9\n",
      "   15 |   1.1493 |     40.976 |   1.1570 |     42.243 |     0.9\n",
      "   16 |   1.1390 |     40.442 |   1.1435 |     41.900 |     1.0\n",
      "   17 |   1.1272 |     40.084 |   1.1490 |     41.215 |     1.0\n",
      "   18 |   1.1161 |     39.896 |   1.1245 |     39.564 |     1.1\n",
      "   19 |   1.1048 |     39.208 |   1.1171 |     41.402 |     1.2\n",
      "   20 |   1.0938 |     39.126 |   1.0974 |     38.816 |     1.2\n",
      "   21 |   1.0821 |     38.828 |   1.0861 |     39.190 |     1.3\n",
      "   22 |   1.0748 |     38.283 |   1.0862 |     39.065 |     1.3\n",
      "   23 |   1.0615 |     37.892 |   1.0674 |     37.165 |     1.4\n",
      "   24 |   1.0469 |     37.166 |   1.0635 |     38.349 |     1.5\n",
      "   25 |   1.0368 |     36.813 |   1.0504 |     37.259 |     1.5\n",
      "   26 |   1.0240 |     36.070 |   1.0475 |     37.383 |     1.6\n",
      "   27 |   1.0124 |     35.916 |   1.0414 |     36.791 |     1.6\n",
      "   28 |   0.9993 |     35.448 |   1.0238 |     36.231 |     1.7\n",
      "   29 |   0.9842 |     34.715 |   1.0186 |     35.888 |     1.8\n",
      "   30 |   0.9783 |     34.523 |   1.0134 |     35.327 |     1.8\n",
      "   31 |   0.9643 |     34.011 |   1.0033 |     35.016 |     1.9\n",
      "   32 |   0.9518 |     33.449 |   0.9998 |     34.891 |     2.0\n",
      "   33 |   0.9357 |     32.651 |   0.9898 |     34.611 |     2.0\n",
      "   34 |   0.9312 |     32.491 |   0.9745 |     33.302 |     2.1\n",
      "   35 |   0.9216 |     32.480 |   0.9884 |     33.988 |     2.1\n",
      "   36 |   0.9109 |     31.637 |   1.0160 |     35.981 |     2.2\n",
      "   37 |   0.8926 |     30.966 |   0.9773 |     33.769 |     2.3\n",
      "   38 |   0.8877 |     30.812 |   0.9876 |     33.956 |     2.3\n",
      "   39 |   0.8790 |     30.608 |   0.9669 |     32.804 |     2.4\n",
      "   40 |   0.8729 |     30.663 |   0.9620 |     32.835 |     2.4\n",
      "   41 |   0.8607 |     29.964 |   0.9630 |     33.396 |     2.5\n",
      "   42 |   0.8482 |     29.275 |   0.9568 |     33.333 |     2.6\n",
      "   43 |   0.8393 |     28.989 |   0.9545 |     33.583 |     2.6\n",
      "   44 |   0.8265 |     28.538 |   0.9513 |     32.773 |     2.7\n",
      "   45 |   0.8204 |     28.251 |   0.9417 |     32.056 |     2.7\n",
      "   46 |   0.8105 |     27.937 |   0.9443 |     32.804 |     2.8\n",
      "   47 |   0.7978 |     27.591 |   0.9350 |     31.776 |     2.9\n",
      "   48 |   0.7908 |     26.908 |   0.9350 |     31.869 |     2.9\n",
      "   49 |   0.7786 |     26.693 |   0.9290 |     32.056 |     3.0\n",
      "   50 |   0.7700 |     26.842 |   0.9298 |     32.118 |     3.1\n",
      "   51 |   0.7615 |     26.054 |   0.9179 |     30.841 |     3.1\n",
      "   52 |   0.7467 |     25.405 |   0.9115 |     31.028 |     3.2\n",
      "   53 |   0.7399 |     25.394 |   0.9255 |     30.935 |     3.2\n",
      "   54 |   0.7291 |     24.749 |   0.9254 |     31.184 |     3.3\n",
      "   55 |   0.7197 |     24.392 |   0.9078 |     29.782 |     3.4\n",
      "   56 |   0.7096 |     24.116 |   0.9165 |     30.405 |     3.4\n",
      "   57 |   0.6999 |     23.791 |   0.9191 |     30.779 |     3.5\n",
      "   58 |   0.6876 |     23.142 |   0.9194 |     30.249 |     3.5\n",
      "   59 |   0.6857 |     23.114 |   0.9107 |     29.346 |     3.6\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 45 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 32\n",
      "Decoder embedding dimension: 128\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 4\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 3\n",
      "Dropout: 0.2\n",
      "Trainable parameters: 2,020,290\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3025, 702])\n",
      "Y_train.shape: torch.Size([3025, 7])\n",
      "X_dev.shape: torch.Size([537, 467])\n",
      "Y_dev.shape: torch.Size([537, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.2282 |     60.700 |   1.6271 |     46.151 |     0.7\n",
      "    2 |   1.4922 |     46.028 |   1.4142 |     46.151 |     1.4\n",
      "    3 |   1.4041 |     46.099 |   1.3726 |     46.493 |     2.0\n",
      "    4 |   1.3625 |     45.983 |   1.3329 |     45.996 |     2.7\n",
      "    5 |   1.3287 |     45.466 |   1.3066 |     45.282 |     3.4\n",
      "    6 |   1.3035 |     45.251 |   1.3005 |     45.313 |     4.1\n",
      "    7 |   1.2848 |     44.496 |   1.2722 |     44.507 |     4.8\n",
      "    8 |   1.2559 |     43.614 |   1.2409 |     43.203 |     5.5\n",
      "    9 |   1.2393 |     43.300 |   1.2397 |     44.134 |     6.2\n",
      "   10 |   1.2263 |     43.311 |   1.2185 |     42.458 |     6.8\n",
      "   11 |   1.2149 |     42.948 |   1.2140 |     42.520 |     7.5\n",
      "   12 |   1.2025 |     42.364 |   1.2004 |     42.241 |     8.2\n",
      "   13 |   1.1914 |     42.242 |   1.1906 |     42.210 |     8.9\n",
      "   14 |   1.1803 |     42.061 |   1.1803 |     41.186 |     9.6\n",
      "   15 |   1.1692 |     41.526 |   1.1720 |     41.092 |    10.3\n",
      "   16 |   1.1589 |     41.581 |   1.1693 |     41.775 |    10.9\n",
      "   17 |   1.1485 |     41.300 |   1.1604 |     41.744 |    11.6\n",
      "   18 |   1.1414 |     41.063 |   1.1546 |     41.713 |    12.3\n",
      "   19 |   1.1320 |     40.964 |   1.1475 |     41.124 |    13.0\n",
      "   20 |   1.1254 |     40.871 |   1.1403 |     40.906 |    13.7\n",
      "   21 |   1.1187 |     40.645 |   1.1349 |     40.658 |    14.3\n",
      "   22 |   1.1148 |     40.672 |   1.1362 |     40.658 |    15.0\n",
      "   23 |   1.1072 |     40.248 |   1.1344 |     41.186 |    15.7\n",
      "   24 |   1.1051 |     40.446 |   1.1293 |     40.658 |    16.4\n",
      "   25 |   1.1002 |     40.231 |   1.1238 |     40.534 |    17.1\n",
      "   26 |   1.0971 |     40.331 |   1.1218 |     40.255 |    17.8\n",
      "   27 |   1.0909 |     40.028 |   1.1311 |     40.720 |    18.4\n",
      "   28 |   1.0889 |     40.138 |   1.1155 |     40.006 |    19.1\n",
      "   29 |   1.0866 |     40.055 |   1.1155 |     40.006 |    19.8\n",
      "   30 |   1.0838 |     39.917 |   1.1204 |     40.348 |    20.5\n",
      "   31 |   1.0805 |     40.077 |   1.1175 |     40.627 |    21.2\n",
      "   32 |   1.0789 |     39.857 |   1.1144 |     40.565 |    21.9\n",
      "   33 |   1.0766 |     40.072 |   1.1139 |     39.913 |    22.5\n",
      "   34 |   1.0719 |     39.614 |   1.1097 |     39.665 |    23.2\n",
      "   35 |   1.0721 |     39.455 |   1.1060 |     39.634 |    23.9\n",
      "   36 |   1.0653 |     39.322 |   1.1067 |     39.603 |    24.6\n",
      "   37 |   1.0597 |     38.981 |   1.1001 |     39.944 |    25.3\n",
      "   38 |   1.0591 |     39.140 |   1.1015 |     39.479 |    25.9\n",
      "   39 |   1.0542 |     38.573 |   1.0972 |     39.417 |    26.6\n",
      "   40 |   1.0508 |     38.595 |   1.0896 |     39.013 |    27.3\n",
      "   41 |   1.0477 |     38.573 |   1.0969 |     39.727 |    28.0\n",
      "   42 |   1.0432 |     38.314 |   1.0891 |     38.703 |    28.7\n",
      "   43 |   1.0390 |     38.198 |   1.0877 |     38.889 |    29.4\n",
      "   44 |   1.0360 |     38.044 |   1.0869 |     39.230 |    30.0\n",
      "   45 |   1.0322 |     38.281 |   1.0845 |     38.734 |    30.7\n",
      "   46 |   1.0271 |     37.857 |   1.0852 |     39.013 |    31.4\n",
      "   47 |   1.0279 |     38.072 |   1.0824 |     38.889 |    32.1\n",
      "   48 |   1.0212 |     37.504 |   1.0754 |     38.423 |    32.8\n",
      "   49 |   1.0184 |     37.708 |   1.0755 |     38.703 |    33.5\n",
      "   50 |   1.0116 |     37.118 |   1.0719 |     38.020 |    34.1\n",
      "   51 |   1.0093 |     37.212 |   1.0739 |     38.268 |    34.8\n",
      "   52 |   1.0076 |     37.080 |   1.0739 |     38.206 |    35.5\n",
      "   53 |   1.0027 |     36.760 |   1.0722 |     37.741 |    36.2\n",
      "   54 |   0.9986 |     36.419 |   1.0632 |     37.182 |    36.9\n",
      "   55 |   0.9927 |     36.298 |   1.0704 |     37.772 |    37.5\n",
      "   56 |   0.9896 |     36.050 |   1.0587 |     37.554 |    38.2\n",
      "   57 |   0.9882 |     36.342 |   1.0647 |     37.678 |    38.9\n",
      "   58 |   0.9902 |     36.231 |   1.0551 |     36.499 |    39.6\n",
      "   59 |   0.9805 |     36.022 |   1.0502 |     37.213 |    40.3\n",
      "   60 |   0.9739 |     35.570 |   1.0474 |     36.903 |    41.0\n",
      "   61 |   0.9668 |     35.074 |   1.0437 |     36.685 |    41.7\n",
      "   62 |   0.9662 |     35.438 |   1.0416 |     36.530 |    42.3\n",
      "   63 |   0.9585 |     34.959 |   1.0380 |     36.282 |    43.0\n",
      "   64 |   0.9558 |     34.683 |   1.0335 |     35.878 |    43.7\n",
      "   65 |   0.9510 |     34.424 |   1.0518 |     37.120 |    44.4\n",
      "   66 |   0.9456 |     34.187 |   1.0346 |     36.592 |    45.1\n",
      "   67 |   0.9378 |     34.138 |   1.0302 |     35.227 |    45.7\n",
      "   68 |   0.9399 |     34.000 |   1.0297 |     35.599 |    46.4\n",
      "   69 |   0.9339 |     33.857 |   1.0283 |     35.133 |    47.1\n",
      "   70 |   0.9314 |     33.598 |   1.0216 |     35.444 |    47.8\n",
      "   71 |   0.9212 |     33.014 |   1.0280 |     35.909 |    48.5\n",
      "   72 |   0.9210 |     33.234 |   1.0336 |     35.444 |    49.2\n",
      "   73 |   0.9187 |     33.262 |   1.0246 |     35.227 |    49.8\n",
      "   74 |   0.9116 |     32.760 |   1.0320 |     35.878 |    50.5\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 45 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 128\n",
      "Decoder embedding dimension: 128\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.1\n",
      "Trainable parameters: 1,069,218\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3038, 702])\n",
      "Y_train.shape: torch.Size([3038, 7])\n",
      "X_dev.shape: torch.Size([524, 221])\n",
      "Y_dev.shape: torch.Size([524, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 1e-05\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   1.9943 |     55.168 |   1.4687 |     45.483 |     0.0\n",
      "    2 |   1.3880 |     45.408 |   1.3301 |     44.847 |     0.1\n",
      "    3 |   1.3085 |     44.558 |   1.2924 |     44.625 |     0.1\n",
      "    4 |   1.2703 |     43.806 |   1.2473 |     44.116 |     0.2\n",
      "    5 |   1.2350 |     43.230 |   1.2194 |     42.494 |     0.2\n",
      "    6 |   1.2078 |     42.391 |   1.1940 |     41.094 |     0.2\n",
      "    7 |   1.1746 |     41.348 |   1.1608 |     40.363 |     0.3\n",
      "    8 |   1.1439 |     40.465 |   1.1468 |     39.885 |     0.3\n",
      "    9 |   1.1184 |     38.984 |   1.1228 |     38.581 |     0.4\n",
      "   10 |   1.0771 |     37.388 |   1.0851 |     37.277 |     0.4\n",
      "   11 |   1.0432 |     36.235 |   1.0663 |     36.101 |     0.5\n",
      "   12 |   1.0035 |     34.595 |   1.0466 |     35.973 |     0.5\n",
      "   13 |   0.9612 |     33.097 |   1.0118 |     34.288 |     0.5\n",
      "   14 |   0.9242 |     31.216 |   0.9985 |     33.492 |     0.6\n",
      "   15 |   0.8774 |     29.438 |   0.9631 |     32.284 |     0.6\n",
      "   16 |   0.8336 |     27.880 |   0.9511 |     30.662 |     0.7\n",
      "   17 |   0.7899 |     26.295 |   0.9186 |     30.344 |     0.7\n",
      "   18 |   0.7388 |     24.194 |   0.9072 |     28.880 |     0.7\n",
      "   19 |   0.6962 |     22.586 |   0.8929 |     29.198 |     0.8\n",
      "   20 |   0.6631 |     21.571 |   0.9006 |     28.022 |     0.8\n",
      "   21 |   0.6241 |     20.095 |   0.8831 |     27.863 |     0.9\n",
      "   22 |   0.5777 |     18.499 |   0.8910 |     27.481 |     0.9\n",
      "   23 |   0.5500 |     17.709 |   0.8979 |     26.686 |     0.9\n",
      "   24 |   0.5170 |     16.705 |   0.8937 |     26.686 |     1.0\n",
      "   25 |   0.4839 |     15.498 |   0.9134 |     26.908 |     1.0\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 46 items>\n",
      "Target index: <Seq2Seq Index with 33 items>\n",
      "Encoder embedding dimension: 64\n",
      "Decoder embedding dimension: 64\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 64\n",
      "Decoder layers: 3\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 766,497\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3025, 702])\n",
      "Y_train.shape: torch.Size([3025, 7])\n",
      "X_dev.shape: torch.Size([537, 337])\n",
      "Y_dev.shape: torch.Size([537, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.5287 |     68.435 |   2.0160 |     55.059 |     0.1\n",
      "    2 |   1.7806 |     50.336 |   1.5909 |     47.083 |     0.1\n",
      "    3 |   1.4998 |     45.851 |   1.4543 |     47.114 |     0.2\n",
      "    4 |   1.4241 |     45.851 |   1.4158 |     47.114 |     0.3\n",
      "    5 |   1.3978 |     45.807 |   1.3943 |     47.083 |     0.4\n",
      "    6 |   1.3817 |     45.719 |   1.3838 |     47.145 |     0.5\n",
      "    7 |   1.3640 |     45.361 |   1.3657 |     46.151 |     0.5\n",
      "    8 |   1.3445 |     44.826 |   1.3443 |     46.120 |     0.6\n",
      "    9 |   1.3242 |     44.457 |   1.3275 |     46.772 |     0.7\n",
      "   10 |   1.3064 |     44.320 |   1.2997 |     45.779 |     0.8\n",
      "   11 |   1.2892 |     44.066 |   1.2911 |     45.624 |     0.8\n",
      "   12 |   1.2726 |     43.785 |   1.2733 |     45.065 |     0.9\n",
      "   13 |   1.2592 |     43.851 |   1.2568 |     45.065 |     1.0\n",
      "   14 |   1.2459 |     43.620 |   1.2408 |     45.127 |     1.1\n",
      "   15 |   1.2325 |     43.532 |   1.2363 |     45.158 |     1.1\n",
      "   16 |   1.2220 |     43.339 |   1.2262 |     44.817 |     1.2\n",
      "   17 |   1.2130 |     43.146 |   1.2226 |     45.096 |     1.3\n",
      "   18 |   1.2041 |     42.782 |   1.2075 |     43.700 |     1.4\n",
      "   19 |   1.1956 |     42.716 |   1.2023 |     43.948 |     1.4\n",
      "   20 |   1.1892 |     42.380 |   1.1950 |     43.855 |     1.5\n",
      "   21 |   1.1816 |     42.468 |   1.1847 |     43.141 |     1.6\n",
      "   22 |   1.1738 |     41.972 |   1.1872 |     42.365 |     1.7\n",
      "   23 |   1.1674 |     41.477 |   1.1811 |     43.048 |     1.7\n",
      "   24 |   1.1615 |     41.295 |   1.1706 |     42.024 |     1.8\n",
      "   25 |   1.1543 |     41.091 |   1.1702 |     41.465 |     1.9\n",
      "   26 |   1.1489 |     40.893 |   1.1588 |     42.086 |     2.0\n",
      "   27 |   1.1425 |     40.540 |   1.1652 |     41.620 |     2.0\n",
      "   28 |   1.1371 |     40.193 |   1.1544 |     41.713 |     2.1\n",
      "   29 |   1.1301 |     40.088 |   1.1565 |     41.341 |     2.2\n",
      "   30 |   1.1258 |     39.846 |   1.1449 |     41.310 |     2.3\n",
      "   31 |   1.1185 |     39.581 |   1.1433 |     40.720 |     2.3\n",
      "   32 |   1.1147 |     39.554 |   1.1324 |     40.317 |     2.4\n",
      "   33 |   1.1055 |     39.190 |   1.1293 |     40.441 |     2.5\n",
      "   34 |   1.1018 |     39.223 |   1.1220 |     39.851 |     2.6\n",
      "   35 |   1.0984 |     38.893 |   1.1232 |     40.192 |     2.6\n",
      "   36 |   1.0926 |     38.815 |   1.1239 |     40.317 |     2.7\n",
      "   37 |   1.0891 |     38.848 |   1.1156 |     39.634 |     2.8\n",
      "   38 |   1.0813 |     38.485 |   1.1086 |     39.789 |     2.9\n",
      "   39 |   1.0793 |     38.479 |   1.1104 |     39.851 |     2.9\n",
      "   40 |   1.0786 |     38.424 |   1.1148 |     40.130 |     3.0\n",
      "   41 |   1.0691 |     37.989 |   1.1014 |     39.758 |     3.1\n",
      "   42 |   1.0636 |     38.033 |   1.0931 |     39.603 |     3.2\n",
      "   43 |   1.0622 |     37.923 |   1.0987 |     39.199 |     3.2\n",
      "   44 |   1.0565 |     37.713 |   1.0919 |     38.268 |     3.3\n",
      "   45 |   1.0518 |     37.521 |   1.0958 |     39.479 |     3.4\n",
      "   46 |   1.0478 |     37.372 |   1.0877 |     38.641 |     3.5\n",
      "   47 |   1.0443 |     37.306 |   1.0856 |     38.610 |     3.5\n",
      "   48 |   1.0395 |     37.185 |   1.0801 |     38.361 |     3.6\n",
      "   49 |   1.0347 |     36.948 |   1.0831 |     38.951 |     3.7\n",
      "   50 |   1.0324 |     36.810 |   1.0822 |     38.579 |     3.8\n",
      "   51 |   1.0244 |     36.601 |   1.0701 |     37.213 |     3.8\n",
      "   52 |   1.0181 |     36.077 |   1.0623 |     37.306 |     3.9\n",
      "   53 |   1.0128 |     35.758 |   1.0617 |     37.927 |     4.0\n",
      "   54 |   1.0079 |     35.455 |   1.0601 |     37.027 |     4.1\n",
      "   55 |   1.0033 |     35.300 |   1.0541 |     36.313 |     4.1\n",
      "   56 |   1.0049 |     35.306 |   1.0613 |     36.530 |     4.2\n",
      "   57 |   0.9944 |     34.727 |   1.0495 |     36.406 |     4.3\n",
      "   58 |   0.9847 |     34.386 |   1.0536 |     37.182 |     4.4\n",
      "   59 |   0.9806 |     34.391 |   1.0490 |     36.065 |     4.4\n",
      "   60 |   0.9704 |     34.011 |   1.0408 |     35.785 |     4.5\n",
      "   61 |   0.9650 |     33.708 |   1.0341 |     35.692 |     4.6\n",
      "   62 |   0.9603 |     33.482 |   1.0366 |     35.133 |     4.7\n",
      "   63 |   0.9485 |     33.058 |   1.0360 |     35.320 |     4.8\n",
      "   64 |   0.9432 |     32.810 |   1.0340 |     35.599 |     4.8\n",
      "   65 |   0.9344 |     32.512 |   1.0202 |     35.847 |     4.9\n",
      "   66 |   0.9263 |     31.901 |   1.0301 |     35.382 |     5.0\n",
      "   67 |   0.9180 |     31.868 |   1.0124 |     34.916 |     5.1\n",
      "   68 |   0.9115 |     31.598 |   1.0117 |     34.575 |     5.1\n",
      "   69 |   0.9032 |     31.361 |   1.0223 |     35.351 |     5.2\n",
      "   70 |   0.8994 |     31.262 |   1.0066 |     34.389 |     5.3\n",
      "   71 |   0.8856 |     30.826 |   1.0144 |     34.668 |     5.4\n",
      "   72 |   0.8712 |     30.198 |   1.0059 |     34.171 |     5.4\n",
      "   73 |   0.8664 |     29.570 |   1.0100 |     34.947 |     5.5\n",
      "   74 |   0.8603 |     29.895 |   0.9869 |     33.799 |     5.6\n",
      "   75 |   0.8492 |     29.052 |   0.9965 |     33.861 |     5.7\n",
      "   76 |   0.8387 |     28.749 |   0.9906 |     33.426 |     5.7\n",
      "   77 |   0.8304 |     28.408 |   1.0001 |     33.551 |     5.8\n",
      "   78 |   0.8222 |     28.160 |   0.9801 |     32.992 |     5.9\n",
      "   79 |   0.8146 |     27.818 |   0.9796 |     32.992 |     6.0\n",
      "   80 |   0.8019 |     27.152 |   0.9819 |     32.495 |     6.0\n",
      "   81 |   0.7923 |     26.986 |   0.9792 |     33.023 |     6.1\n",
      "   82 |   0.7833 |     26.523 |   0.9870 |     32.619 |     6.2\n",
      "   83 |   0.7770 |     26.320 |   0.9795 |     32.433 |     6.3\n",
      "   84 |   0.7630 |     25.780 |   0.9840 |     32.713 |     6.3\n",
      "   85 |   0.7590 |     25.736 |   0.9707 |     32.030 |     6.4\n",
      "   86 |   0.7473 |     25.190 |   0.9712 |     31.626 |     6.5\n",
      "   87 |   0.7373 |     24.694 |   0.9823 |     32.061 |     6.6\n",
      "   88 |   0.7226 |     24.209 |   0.9627 |     31.813 |     6.6\n",
      "   89 |   0.7101 |     23.983 |   0.9615 |     31.161 |     6.7\n",
      "   90 |   0.7032 |     23.658 |   0.9672 |     32.092 |     6.8\n",
      "   91 |   0.6892 |     23.157 |   0.9558 |     31.192 |     6.9\n",
      "   92 |   0.6983 |     23.262 |   0.9792 |     31.099 |     6.9\n",
      "   93 |   0.6935 |     23.135 |   0.9631 |     30.850 |     7.0\n",
      "   94 |   0.6582 |     21.840 |   0.9659 |     31.316 |     7.1\n",
      "   95 |   0.6469 |     21.262 |   0.9623 |     30.168 |     7.2\n",
      "   96 |   0.6378 |     20.915 |   0.9486 |     29.454 |     7.3\n",
      "   97 |   0.6234 |     20.413 |   0.9640 |     30.261 |     7.3\n",
      "   98 |   0.6137 |     19.873 |   0.9650 |     29.950 |     7.4\n",
      "   99 |   0.6118 |     19.719 |   0.9809 |     30.354 |     7.5\n",
      "  100 |   0.5917 |     19.019 |   0.9604 |     29.112 |     7.6\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 45 items>\n",
      "Target index: <Seq2Seq Index with 33 items>\n",
      "Encoder embedding dimension: 32\n",
      "Decoder embedding dimension: 128\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 3\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 3\n",
      "Dropout: 0.2\n",
      "Trainable parameters: 1,559,233\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3033, 702])\n",
      "Y_train.shape: torch.Size([3033, 7])\n",
      "X_dev.shape: torch.Size([529, 267])\n",
      "Y_dev.shape: torch.Size([529, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.1536 |     59.518 |   1.5828 |     45.432 |     0.1\n",
      "    2 |   1.4737 |     46.285 |   1.4088 |     45.432 |     0.1\n",
      "    3 |   1.3956 |     46.175 |   1.3696 |     45.274 |     0.2\n",
      "    4 |   1.3499 |     45.703 |   1.3277 |     44.991 |     0.2\n",
      "    5 |   1.3196 |     45.390 |   1.2997 |     44.266 |     0.3\n",
      "    6 |   1.2945 |     44.774 |   1.2789 |     43.888 |     0.4\n",
      "    7 |   1.2697 |     44.318 |   1.2548 |     43.856 |     0.4\n",
      "    8 |   1.2417 |     43.834 |   1.2329 |     43.195 |     0.5\n",
      "    9 |   1.2222 |     42.999 |   1.2230 |     42.628 |     0.5\n",
      "   10 |   1.2072 |     42.648 |   1.2117 |     41.588 |     0.6\n",
      "   11 |   1.1894 |     42.147 |   1.1913 |     40.328 |     0.7\n",
      "   12 |   1.1782 |     41.730 |   1.1796 |     40.769 |     0.7\n",
      "   13 |   1.1642 |     41.318 |   1.1758 |     40.895 |     0.8\n",
      "   14 |   1.1540 |     40.977 |   1.1614 |     39.792 |     0.8\n",
      "   15 |   1.1427 |     40.576 |   1.1665 |     40.832 |     0.9\n",
      "   16 |   1.1308 |     40.433 |   1.1482 |     40.265 |     1.0\n",
      "   17 |   1.1242 |     40.290 |   1.1446 |     40.296 |     1.0\n",
      "   18 |   1.1174 |     40.153 |   1.1393 |     39.761 |     1.1\n",
      "   19 |   1.1055 |     39.741 |   1.1270 |     39.477 |     1.1\n",
      "   20 |   1.0995 |     39.482 |   1.1307 |     40.044 |     1.2\n",
      "   21 |   1.0916 |     39.482 |   1.1184 |     39.099 |     1.3\n",
      "   22 |   1.0861 |     39.532 |   1.1138 |     38.941 |     1.3\n",
      "   23 |   1.0783 |     39.169 |   1.1077 |     38.973 |     1.4\n",
      "   24 |   1.0740 |     38.861 |   1.1077 |     39.509 |     1.4\n",
      "   25 |   1.0661 |     38.680 |   1.1040 |     39.603 |     1.5\n",
      "   26 |   1.0607 |     38.636 |   1.1034 |     39.572 |     1.6\n",
      "   27 |   1.0691 |     38.966 |   1.1300 |     39.824 |     1.6\n",
      "   28 |   1.0677 |     38.823 |   1.1148 |     40.044 |     1.7\n",
      "   29 |   1.0498 |     38.488 |   1.0894 |     38.815 |     1.7\n",
      "   30 |   1.0421 |     38.048 |   1.0897 |     38.689 |     1.8\n",
      "   31 |   1.0417 |     38.004 |   1.0877 |     39.477 |     1.9\n",
      "   32 |   1.0365 |     38.081 |   1.0895 |     39.572 |     1.9\n",
      "   33 |   1.0332 |     37.823 |   1.0881 |     38.469 |     2.0\n",
      "   34 |   1.0246 |     37.411 |   1.0865 |     38.658 |     2.0\n",
      "   35 |   1.0220 |     37.493 |   1.0804 |     38.154 |     2.1\n",
      "   36 |   1.0160 |     37.235 |   1.0777 |     38.595 |     2.2\n",
      "   37 |   1.0118 |     37.108 |   1.0733 |     38.185 |     2.2\n",
      "   38 |   1.0081 |     36.845 |   1.0645 |     37.618 |     2.3\n",
      "   39 |   1.0044 |     36.971 |   1.0704 |     38.500 |     2.3\n",
      "   40 |   0.9968 |     36.400 |   1.0625 |     37.965 |     2.4\n",
      "   41 |   0.9932 |     35.751 |   1.0606 |     36.736 |     2.5\n",
      "   42 |   0.9885 |     36.070 |   1.0558 |     36.957 |     2.5\n",
      "   43 |   0.9852 |     36.031 |   1.0588 |     37.839 |     2.6\n",
      "   44 |   0.9784 |     35.652 |   1.0518 |     37.744 |     2.6\n",
      "   45 |   0.9717 |     35.169 |   1.0509 |     37.177 |     2.7\n",
      "   46 |   0.9641 |     34.844 |   1.0472 |     36.894 |     2.8\n",
      "   47 |   0.9567 |     34.586 |   1.0400 |     36.515 |     2.8\n",
      "   48 |   0.9469 |     34.086 |   1.0343 |     35.948 |     2.9\n",
      "   49 |   0.9426 |     33.938 |   1.0234 |     35.507 |     2.9\n",
      "   50 |   0.9349 |     33.663 |   1.0270 |     35.539 |     3.0\n",
      "   51 |   0.9320 |     33.267 |   1.0192 |     35.003 |     3.1\n",
      "   52 |   0.9195 |     32.740 |   1.0096 |     34.405 |     3.1\n",
      "   53 |   0.9148 |     32.833 |   1.0124 |     34.562 |     3.2\n",
      "   54 |   0.9048 |     32.196 |   1.0060 |     34.121 |     3.2\n",
      "   55 |   0.9006 |     31.679 |   1.0056 |     34.342 |     3.3\n",
      "   56 |   0.8937 |     31.712 |   1.0076 |     34.436 |     3.4\n",
      "   57 |   0.8850 |     31.119 |   1.0088 |     35.003 |     3.4\n",
      "   58 |   0.8734 |     31.025 |   1.0080 |     34.058 |     3.5\n",
      "   59 |   0.8791 |     31.075 |   1.0071 |     34.373 |     3.5\n",
      "   60 |   0.8730 |     30.707 |   0.9963 |     33.743 |     3.6\n",
      "   61 |   0.8564 |     30.047 |   0.9965 |     33.459 |     3.7\n",
      "   62 |   0.8466 |     29.635 |   0.9916 |     33.680 |     3.7\n",
      "   63 |   0.8407 |     29.355 |   0.9772 |     32.829 |     3.8\n",
      "   64 |   0.8393 |     29.157 |   0.9932 |     33.333 |     3.8\n",
      "   65 |   0.8244 |     28.844 |   0.9833 |     32.703 |     3.9\n",
      "   66 |   0.8200 |     28.888 |   0.9865 |     33.207 |     4.0\n",
      "   67 |   0.8210 |     28.668 |   0.9799 |     32.105 |     4.0\n",
      "Early stopping\n",
      "\n",
      "Model: Seq2Seq Bi-LSTM\n",
      "Source index: <Seq2Seq Index with 46 items>\n",
      "Target index: <Seq2Seq Index with 34 items>\n",
      "Encoder embedding dimension: 64\n",
      "Decoder embedding dimension: 128\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 3\n",
      "Decoder hidden units: 64\n",
      "Decoder layers: 3\n",
      "Dropout: 0.1\n",
      "Trainable parameters: 1,213,218\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([3017, 702])\n",
      "Y_train.shape: torch.Size([3017, 7])\n",
      "X_dev.shape: torch.Size([545, 323])\n",
      "Y_dev.shape: torch.Size([545, 7])\n",
      "Epochs: 150\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0001\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n",
      "    1 |   2.5092 |     67.667 |   1.9292 |     58.410 |     0.2\n",
      "    2 |   1.6973 |     49.309 |   1.5066 |     45.963 |     0.3\n",
      "    3 |   1.4667 |     46.150 |   1.4119 |     45.963 |     0.5\n",
      "    4 |   1.4065 |     46.017 |   1.3732 |     45.841 |     0.6\n",
      "    5 |   1.3645 |     45.741 |   1.3417 |     44.832 |     0.8\n",
      "    6 |   1.3324 |     45.675 |   1.3129 |     45.474 |     0.9\n",
      "    7 |   1.3065 |     45.216 |   1.2981 |     45.382 |     1.1\n",
      "    8 |   1.2906 |     44.763 |   1.2873 |     44.098 |     1.2\n",
      "    9 |   1.2731 |     44.266 |   1.2715 |     44.190 |     1.4\n",
      "   10 |   1.2580 |     44.023 |   1.2529 |     44.312 |     1.5\n",
      "   11 |   1.2413 |     43.713 |   1.2449 |     44.067 |     1.7\n",
      "   12 |   1.2290 |     43.365 |   1.2304 |     44.128 |     1.8\n",
      "   13 |   1.2112 |     42.575 |   1.2123 |     42.661 |     2.0\n",
      "   14 |   1.1991 |     42.377 |   1.2038 |     42.263 |     2.1\n",
      "   15 |   1.1903 |     41.885 |   1.1975 |     42.508 |     2.3\n",
      "   16 |   1.1779 |     41.614 |   1.1865 |     42.049 |     2.4\n",
      "   17 |   1.1666 |     41.172 |   1.1734 |     41.223 |     2.6\n",
      "   18 |   1.1565 |     40.918 |   1.1713 |     41.529 |     2.7\n",
      "   19 |   1.1482 |     40.846 |   1.1618 |     41.774 |     2.9\n",
      "   20 |   1.1390 |     40.355 |   1.1559 |     40.765 |     3.0\n",
      "   21 |   1.1288 |     40.078 |   1.1566 |     40.398 |     3.2\n",
      "   22 |   1.1258 |     39.714 |   1.1485 |     40.000 |     3.3\n",
      "   23 |   1.1181 |     39.913 |   1.1314 |     39.939 |     3.5\n",
      "   24 |   1.1087 |     39.344 |   1.1417 |     40.673 |     3.6\n",
      "   25 |   1.1034 |     38.957 |   1.1246 |     39.511 |     3.8\n",
      "   26 |   1.0938 |     38.791 |   1.1250 |     39.083 |     3.9\n",
      "   27 |   1.0889 |     38.957 |   1.1184 |     38.869 |     4.1\n",
      "   28 |   1.0772 |     38.046 |   1.1055 |     38.410 |     4.2\n",
      "   29 |   1.0757 |     38.416 |   1.1010 |     38.257 |     4.4\n",
      "   30 |   1.0705 |     38.095 |   1.1015 |     38.226 |     4.5\n",
      "   31 |   1.0640 |     37.548 |   1.0992 |     38.440 |     4.7\n",
      "   32 |   1.0574 |     37.195 |   1.1043 |     37.768 |     4.8\n",
      "   33 |   1.0499 |     36.764 |   1.0895 |     38.349 |     4.9\n",
      "   34 |   1.0441 |     37.327 |   1.0896 |     37.339 |     5.1\n",
      "   35 |   1.0416 |     36.874 |   1.0910 |     37.676 |     5.3\n",
      "   36 |   1.0331 |     36.620 |   1.0857 |     37.034 |     5.4\n",
      "   37 |   1.0339 |     36.526 |   1.0836 |     37.890 |     5.5\n",
      "   38 |   1.0223 |     36.007 |   1.0747 |     36.391 |     5.7\n",
      "   39 |   1.0198 |     36.046 |   1.0780 |     37.095 |     5.8\n",
      "   40 |   1.0132 |     35.747 |   1.0784 |     36.972 |     6.0\n",
      "   41 |   1.0072 |     35.294 |   1.0712 |     36.911 |     6.1\n",
      "   42 |   1.0026 |     35.444 |   1.0737 |     36.942 |     6.3\n",
      "   43 |   0.9959 |     35.223 |   1.0709 |     36.544 |     6.4\n",
      "   44 |   0.9897 |     34.875 |   1.0669 |     35.841 |     6.6\n",
      "   45 |   0.9827 |     34.377 |   1.0602 |     36.055 |     6.7\n",
      "   46 |   0.9765 |     34.278 |   1.0605 |     36.208 |     6.9\n",
      "   47 |   0.9759 |     34.107 |   1.0569 |     35.688 |     7.0\n",
      "   48 |   0.9700 |     33.952 |   1.0572 |     36.208 |     7.2\n",
      "   49 |   0.9718 |     33.930 |   1.0611 |     35.810 |     7.3\n",
      "   50 |   0.9635 |     33.709 |   1.0588 |     36.453 |     7.5\n",
      "   51 |   0.9497 |     33.366 |   1.0473 |     35.780 |     7.6\n",
      "   52 |   0.9388 |     33.052 |   1.0397 |     35.627 |     7.8\n",
      "   53 |   0.9361 |     32.897 |   1.0394 |     35.749 |     7.9\n",
      "   54 |   0.9285 |     32.626 |   1.0303 |     35.566 |     8.1\n",
      "   55 |   0.9195 |     31.952 |   1.0318 |     35.566 |     8.2\n",
      "   56 |   0.9137 |     31.814 |   1.0352 |     35.138 |     8.4\n",
      "   57 |   0.9068 |     31.858 |   1.0374 |     34.648 |     8.5\n",
      "   58 |   0.8987 |     31.229 |   1.0277 |     35.107 |     8.7\n",
      "   59 |   0.8891 |     30.792 |   1.0247 |     34.771 |     8.8\n",
      "   60 |   0.8796 |     30.455 |   1.0277 |     35.535 |     9.0\n",
      "   61 |   0.8735 |     30.367 |   1.0181 |     34.434 |     9.1\n",
      "   62 |   0.8680 |     30.052 |   1.0129 |     34.067 |     9.3\n",
      "   63 |   0.8568 |     29.422 |   1.0113 |     34.128 |     9.4\n",
      "   64 |   0.8503 |     29.279 |   1.0163 |     34.495 |     9.6\n",
      "   65 |   0.8296 |     28.428 |   0.9970 |     33.303 |     9.7\n",
      "   66 |   0.8280 |     28.350 |   1.0003 |     33.425 |     9.9\n",
      "   67 |   0.8245 |     28.439 |   0.9999 |     32.691 |    10.0\n",
      "   68 |   0.8080 |     27.444 |   0.9979 |     33.058 |    10.2\n",
      "   69 |   0.8006 |     27.422 |   0.9866 |     32.416 |    10.3\n",
      "   70 |   0.7908 |     27.025 |   0.9948 |     32.141 |    10.5\n",
      "   71 |   0.7811 |     26.384 |   1.0109 |     32.752 |    10.6\n",
      "   72 |   0.7746 |     26.240 |   1.0117 |     32.661 |    10.8\n",
      "   73 |   0.7672 |     26.157 |   0.9880 |     32.049 |    10.9\n",
      "   74 |   0.7469 |     25.373 |   0.9808 |     31.682 |    11.1\n",
      "   75 |   0.7394 |     24.854 |   0.9864 |     31.682 |    11.2\n",
      "   76 |   0.7319 |     24.583 |   0.9943 |     32.049 |    11.4\n",
      "   77 |   0.7218 |     24.599 |   0.9997 |     31.529 |    11.5\n",
      "   78 |   0.7086 |     23.622 |   0.9968 |     31.590 |    11.7\n",
      "Early stopping\n",
      "\n",
      "Mean: Precision            0.076435\n",
      "Recall               0.197887\n",
      "F1                   0.107289\n",
      "Precision_level3     0.121923\n",
      "Recall_level3        0.320004\n",
      "F1_level3            0.171855\n",
      "Precision_level2     0.164805\n",
      "Recall_level2        0.438063\n",
      "F1_level2            0.234222\n",
      "Precision level 1    0.331713\n",
      "Precision level 2    0.675735\n",
      "Precision level 3    0.675489\n",
      "Precision level 4    0.574655\n",
      "Recall level 1       0.530996\n",
      "Recall level 2       0.737384\n",
      "Recall level 3       0.726980\n",
      "Recall level 4       0.661604\n",
      "dtype: float64\n",
      "Std: Precision            0.027423\n",
      "Recall               0.075560\n",
      "F1                   0.039705\n",
      "Precision_level3     0.041216\n",
      "Recall_level3        0.111120\n",
      "F1_level3            0.059114\n",
      "Precision_level2     0.046628\n",
      "Recall_level2        0.119765\n",
      "F1_level2            0.065597\n",
      "Precision level 1    0.095972\n",
      "Precision level 2    0.073897\n",
      "Precision level 3    0.036385\n",
      "Precision level 4    0.030297\n",
      "Recall level 1       0.097024\n",
      "Recall level 2       0.087755\n",
      "Recall level 3       0.060771\n",
      "Recall level 4       0.038009\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "seeds = [42, 123, 47899, 2025, 1, 20, 99, 1020, 345, 78] \n",
    "columns = [\n",
    "    'Seed', \n",
    "    'Precision', 'Recall', 'F1',\n",
    "    'Precision_level3', 'Recall_level3', 'F1_level3',\n",
    "    'Precision_level2', 'Recall_level2', 'F1_level2',\n",
    "    'Precision level 1', 'Precision level 2', 'Precision level 3', 'Precision level 4',\n",
    "    'Recall level 1', 'Recall level 2', 'Recall level 3', 'Recall level 4',\n",
    "    '#Compounds that have at least one match'\n",
    "]\n",
    "metrics_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seeds(seed)\n",
    "\n",
    "    train_set = pd.read_csv(f'../Datasets/Rep_train_set{seed}.csv')\n",
    "    test_set = pd.read_csv(f'../Datasets/Rep_test_set{seed}.csv')\n",
    "    val_set = pd.read_csv(f'../Datasets/Rep_val_set{seed}.csv')\n",
    "    \n",
    "    new_train_set = multiplicate_rows(train_set)\n",
    "    new_val_set = multiplicate_rows(val_set)\n",
    "    new_test_set = multiplicate_rows(test_set)\n",
    "    \n",
    "    source_train = source(new_train_set)\n",
    "    source_test = source(new_test_set)\n",
    "    # Test set without duplicated compounds\n",
    "    source_test2 = source(test_set)\n",
    "    source_val = source(new_val_set)\n",
    "    # Val set without duplicated compounds\n",
    "    source_val2 = source(val_set)\n",
    "    \n",
    "    target_train = target(new_train_set)\n",
    "    target_test = target(new_test_set)\n",
    "    target_val = target(new_val_set)\n",
    "    \n",
    "    # An Index object represents a mapping from the vocabulary to integers (indices) to feed into the models\n",
    "    source_index = index.Index(source_train)\n",
    "    target_index = index.Index(target_train)\n",
    "    \n",
    "    # Create tensors\n",
    "    X_train = source_index.text2tensor(source_train)\n",
    "    y_train = target_index.text2tensor(target_train)\n",
    "    X_val = source_index.text2tensor(source_val)\n",
    "    X_val2 = source_index.text2tensor(source_val2)\n",
    "    y_val = target_index.text2tensor(target_val)     \n",
    "    X_test = source_index.text2tensor(source_test)\n",
    "    X_test2 = source_index.text2tensor(source_test2)\n",
    "    y_test = target_index.text2tensor(target_test)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        X_train = X_train.to(\"cuda\")\n",
    "        y_train = y_train.to(\"cuda\")\n",
    "        X_val = X_val.to(\"cuda\")\n",
    "        X_val2 = X_val2.to(\"cuda\")\n",
    "        y_val = y_val.to(\"cuda\")\n",
    "        X_test= X_test.to(\"cuda\")\n",
    "        y_test = y_test.to(\"cuda\")\n",
    "        X_test2 = X_test2.to(\"cuda\")\n",
    "\n",
    "    if os.path.exists(f\"sortedbilstm_results{seed}.csv\"):\n",
    "        best_hyperparameters = (pd.read_csv(f\"sortedbilstm_results{seed}.csv\")).loc[0]\n",
    "    else:\n",
    "        best_hyperparameters = hyperparametersselection(seed, source_index, target_index, X_train, X_val, X_val2, y_train, y_val)\n",
    "\n",
    "    model = models.BiLSTM(\n",
    "                source_index, \n",
    "                target_index,\n",
    "                encoder_embedding_dimension = best_hyperparameters['encoder_embedding_dim'],\n",
    "                decoder_embedding_dimension = best_hyperparameters['decoder_embedding_dim'],\n",
    "                encoder_hidden_units = int(best_hyperparameters['enc_hidden_units']), \n",
    "                encoder_layers = best_hyperparameters['enc_layers'],\n",
    "                decoder_hidden_units = int(best_hyperparameters['dec_hidden_units']),\n",
    "                decoder_layers = best_hyperparameters['dec_layers'],\n",
    "                dropout = best_hyperparameters['dropout'])   \n",
    "    model.to(\"cuda\")\n",
    "    q = model.fit(X_train, \n",
    "            y_train,\n",
    "            X_val, \n",
    "            y_val, \n",
    "            batch_size = 32, \n",
    "            epochs = 150, \n",
    "            learning_rate = best_hyperparameters['learning_rate'], \n",
    "            weight_decay = best_hyperparameters['weight_decay'],\n",
    "            progress_bar = 0, \n",
    "            save_path = None)\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n",
    "    loss, error_rate = model.evaluate(X_test, y_test, batch_size = 32) \n",
    "\n",
    "    predictions, log_probabilities = search_algorithms.beam_search(\n",
    "        model, \n",
    "        X_test2, # Make predictions with test set \n",
    "        predictions = 6, # max length of the predicted sequence\n",
    "        beam_width = 10,\n",
    "        batch_size = 32, \n",
    "        progress_bar = 0\n",
    "    )\n",
    "    output_beam = [target_index.tensor2text(p) for p in predictions]\n",
    "\n",
    "    predictions_clean = []\n",
    "    for preds in output_beam:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            if len(clean_pred) == 5:\n",
    "                interm.append(clean_pred)\n",
    "            if len(interm) == 3:\n",
    "                break\n",
    "        predictions_clean.append(interm)\n",
    "    predictions_clean_level3 = []\n",
    "    for preds in output_beam:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            pred_3 = clean_pred[0:4]\n",
    "            if len(pred_3) == 4 and pred_3 not in interm:\n",
    "                interm.append(pred_3)\n",
    "        predictions_clean_level3.append(interm[0:3])\n",
    "    predictions_clean_level2 = []\n",
    "    for preds in output_beam:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            pred_2 = clean_pred[0:3]\n",
    "            if len(pred_2) == 3 and pred_2 not in interm:\n",
    "                interm.append(pred_2)\n",
    "        predictions_clean_level2.append(interm[0:3])\n",
    "    precision_1, precision_2, precision_3, precision_4 = defined_metrics.precision(predictions_clean, f'../Datasets/Rep_test_set{seed}.csv', 'ATC Codes')\n",
    "    recall_1, recall_2, recall_3, recall_4, counter_compound_match = defined_metrics.recall(predictions_clean, f'../Datasets/Rep_test_set{seed}.csv', 'ATC Codes')\n",
    "    precisions, recalls, f1s = defined_metrics.complete_metrics(predictions_clean, f'../Datasets/Rep_test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_level3, recalls_level3, f1s_level3 = defined_metrics.complete_metrics_level3(predictions_clean_level3, f'../Datasets/Rep_test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_level2, recalls_level2, f1s_level2 = defined_metrics.complete_metrics_level2(predictions_clean_level2, f'../Datasets/Rep_test_set{seed}.csv', 'ATC Codes', 3)\n",
    "    precisions_average = sum(precisions)/len(precisions)\n",
    "    recalls_average = sum(recalls)/len(recalls)\n",
    "    f1s_average = sum(f1s)/len(f1s)\n",
    "\n",
    "    precisions_average_level3 = sum(precisions_level3)/len(precisions_level3)\n",
    "    recalls_average_level3 = sum(recalls_level3)/len(recalls_level3)\n",
    "    f1s_average_level3 = sum(f1s_level3)/len(f1s_level3)\n",
    "\n",
    "    precisions_average_level2 = sum(precisions_level2)/len(precisions_level2)\n",
    "    recalls_average_level2 = sum(recalls_level2)/len(recalls_level2)\n",
    "    f1s_average_level2 = sum(f1s_level2)/len(f1s_level2)\n",
    "    \n",
    "    metrics = {\n",
    "        'Precision': precisions_average, \n",
    "        'Recall': recalls_average,\n",
    "        'F1': f1s_average,\n",
    "        'Precision_level3': precisions_average_level3, \n",
    "        'Recall_level3': recalls_average_level3,\n",
    "        'F1_level3': f1s_average_level3,\n",
    "        'Precision_level2': precisions_average_level2, \n",
    "        'Recall_level2': recalls_average_level2,\n",
    "        'F1_level2': f1s_average_level2,\n",
    "        'Precision level 1': precision_1,\n",
    "        'Precision level 2': precision_2,\n",
    "        'Precision level 3': precision_3,\n",
    "        'Precision level 4': precision_4,\n",
    "        'Recall level 1': recall_1,\n",
    "        'Recall level 2': recall_2,\n",
    "        'Recall level 3': recall_3,\n",
    "        'Recall level 4': recall_4,\n",
    "        '#Compounds that have at least one match': counter_compound_match\n",
    "    }\n",
    "    \n",
    "    # Build the row\n",
    "    row = {\n",
    "        'Seed': seed,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "metrics_df.to_csv(\"bilstm_metrics.csv\", index=False)\n",
    "print(\"Mean:\", metrics_df.mean(numeric_only=True))\n",
    "print(\"Std:\", metrics_df.std(numeric_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
