Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,662,242

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2244 |     61.612 |   1.6204 |     51.090 |     0.1
    2 |   1.4891 |     46.184 |   1.4224 |     45.763 |     0.1
    3 |   1.4021 |     46.036 |   1.3854 |     45.701 |     0.2
    4 |   1.3590 |     45.733 |   1.3355 |     45.421 |     0.2
    5 |   1.3180 |     45.259 |   1.3078 |     44.735 |     0.3
    6 |   1.2928 |     44.940 |   1.2837 |     44.798 |     0.4
    7 |   1.2692 |     44.549 |   1.2629 |     43.956 |     0.4
    8 |   1.2470 |     43.685 |   1.2465 |     43.209 |     0.5
    9 |   1.2260 |     43.189 |   1.2263 |     44.050 |     0.5
   10 |   1.2073 |     42.605 |   1.2093 |     43.271 |     0.6
   11 |   1.1872 |     42.187 |   1.1868 |     42.835 |     0.7
   12 |   1.1736 |     41.796 |   1.1752 |     41.558 |     0.7
   13 |   1.1537 |     40.673 |   1.1700 |     41.308 |     0.8
   14 |   1.1461 |     40.755 |   1.1516 |     40.872 |     0.8
   15 |   1.1302 |     40.337 |   1.1401 |     40.125 |     0.9
   16 |   1.1192 |     40.188 |   1.1366 |     40.685 |     1.0
   17 |   1.1115 |     39.852 |   1.1188 |     39.377 |     1.0
   18 |   1.0995 |     39.263 |   1.1149 |     39.408 |     1.1
   19 |   1.0900 |     39.021 |   1.1078 |     39.875 |     1.1
   20 |   1.0794 |     38.740 |   1.1020 |     39.003 |     1.2
   21 |   1.0728 |     38.592 |   1.0940 |     38.474 |     1.3
   22 |   1.0650 |     38.272 |   1.0864 |     38.411 |     1.3
   23 |   1.0579 |     38.019 |   1.0826 |     39.003 |     1.4
   24 |   1.0480 |     37.711 |   1.0648 |     38.037 |     1.4
   25 |   1.0377 |     37.210 |   1.0635 |     37.477 |     1.5
   26 |   1.0311 |     36.890 |   1.0626 |     37.227 |     1.6
   27 |   1.0199 |     36.863 |   1.0488 |     37.445 |     1.6
   28 |   1.0086 |     35.943 |   1.0411 |     36.760 |     1.7
   29 |   0.9995 |     35.657 |   1.0271 |     36.417 |     1.7
   30 |   0.9871 |     35.194 |   1.0233 |     35.950 |     1.8
   31 |   0.9811 |     34.798 |   1.0181 |     36.667 |     1.9
   32 |   0.9693 |     34.253 |   1.0056 |     35.670 |     1.9
   33 |   0.9578 |     33.922 |   0.9889 |     35.327 |     2.0
   34 |   0.9467 |     33.339 |   1.0062 |     35.639 |     2.1
   35 |   0.9422 |     33.526 |   0.9842 |     34.206 |     2.1
   36 |   0.9285 |     32.937 |   0.9772 |     34.611 |     2.2
   37 |   0.9148 |     32.238 |   0.9723 |     34.455 |     2.2
   38 |   0.9065 |     31.726 |   0.9687 |     33.489 |     2.3
   39 |   0.8952 |     31.423 |   0.9618 |     33.022 |     2.4
   40 |   0.8847 |     30.757 |   0.9528 |     32.866 |     2.4
   41 |   0.8787 |     30.779 |   0.9552 |     32.648 |     2.5
   42 |   0.8689 |     30.454 |   0.9416 |     32.243 |     2.5
   43 |   0.8539 |     30.013 |   0.9474 |     32.430 |     2.6
   44 |   0.8512 |     29.798 |   0.9471 |     32.461 |     2.7
   45 |   0.8422 |     29.391 |   0.9266 |     31.464 |     2.7
   46 |   0.8327 |     29.050 |   0.9308 |     31.464 |     2.8
   47 |   0.8196 |     28.708 |   0.9331 |     31.184 |     2.8
   48 |   0.8121 |     28.273 |   0.9275 |     30.810 |     2.9
   49 |   0.8027 |     27.915 |   0.9216 |     31.184 |     3.0
   50 |   0.7955 |     27.480 |   0.9131 |     30.841 |     3.0
   51 |   0.7892 |     27.337 |   0.9170 |     30.249 |     3.1
   52 |   0.7793 |     26.974 |   0.9098 |     30.561 |     3.1
   53 |   0.7665 |     26.467 |   0.9104 |     30.343 |     3.2
   54 |   0.7598 |     26.098 |   0.9158 |     30.872 |     3.3
   55 |   0.7502 |     25.939 |   0.9161 |     30.592 |     3.3
   56 |   0.7409 |     25.746 |   0.9009 |     29.969 |     3.4
   57 |   0.7307 |     24.931 |   0.9126 |     29.439 |     3.4
   58 |   0.7204 |     24.700 |   0.8941 |     30.156 |     3.5
   59 |   0.7133 |     24.094 |   0.8921 |     29.907 |     3.6
   60 |   0.7024 |     24.276 |   0.9130 |     30.156 |     3.6
   61 |   0.6995 |     24.083 |   0.8999 |     28.910 |     3.7
   62 |   0.6827 |     23.235 |   0.8966 |     28.318 |     3.8
   63 |   0.6782 |     22.949 |   0.8895 |     27.975 |     3.8
   64 |   0.6696 |     22.993 |   0.8869 |     28.255 |     3.9
   65 |   0.6558 |     22.487 |   0.8921 |     28.629 |     3.9
   66 |   0.6560 |     22.387 |   0.8863 |     27.539 |     4.0
   67 |   0.6374 |     21.567 |   0.8955 |     28.100 |     4.1
   68 |   0.6364 |     21.633 |   0.8828 |     28.162 |     4.1
   69 |   0.6269 |     21.088 |   0.8914 |     27.508 |     4.2
   70 |   0.6135 |     20.625 |   0.8795 |     27.290 |     4.2
   71 |   0.6089 |     20.455 |   0.8889 |     28.037 |     4.3
   72 |   0.6055 |     20.466 |   0.8884 |     28.006 |     4.4
   73 |   0.5939 |     20.031 |   0.8787 |     26.667 |     4.4
   74 |   0.5877 |     19.921 |   0.8929 |     27.695 |     4.5
   75 |   0.5774 |     19.502 |   0.8833 |     27.165 |     4.5
   76 |   0.5673 |     19.254 |   0.8943 |     27.445 |     4.6
   77 |   0.5704 |     19.387 |   0.9049 |     27.695 |     4.7
   78 |   0.5618 |     19.023 |   0.8778 |     26.791 |     4.7
   79 |   0.5386 |     18.197 |   0.8871 |     26.231 |     4.8
   80 |   0.5440 |     18.324 |   0.8926 |     27.134 |     4.8
   81 |   0.5316 |     18.038 |   0.8781 |     26.822 |     4.9
   82 |   0.5201 |     17.465 |   0.8962 |     26.417 |     5.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 572,962

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4387 |     66.270 |   1.8537 |     49.595 |     0.0
    2 |   1.6472 |     48.012 |   1.5114 |     45.763 |     0.1
    3 |   1.4642 |     46.036 |   1.4360 |     45.763 |     0.1
    4 |   1.4183 |     46.135 |   1.4088 |     45.763 |     0.1
    5 |   1.3943 |     46.008 |   1.3901 |     45.701 |     0.2
    6 |   1.3790 |     45.771 |   1.3786 |     45.701 |     0.2
    7 |   1.3610 |     45.364 |   1.3573 |     44.704 |     0.2
    8 |   1.3425 |     44.852 |   1.3421 |     44.673 |     0.3
    9 |   1.3261 |     44.637 |   1.3273 |     44.330 |     0.3
   10 |   1.3120 |     44.241 |   1.3260 |     44.829 |     0.3
   11 |   1.2975 |     44.230 |   1.3028 |     44.081 |     0.4
   12 |   1.2802 |     44.020 |   1.2821 |     44.174 |     0.4
   13 |   1.2601 |     43.729 |   1.2640 |     43.551 |     0.4
   14 |   1.2415 |     43.189 |   1.2433 |     42.368 |     0.5
   15 |   1.2232 |     42.732 |   1.2334 |     42.243 |     0.5
   16 |   1.2091 |     42.159 |   1.2219 |     43.115 |     0.5
   17 |   1.1911 |     41.278 |   1.2144 |     41.713 |     0.6
   18 |   1.1766 |     41.053 |   1.2061 |     42.274 |     0.6
   19 |   1.1590 |     40.530 |   1.1856 |     40.654 |     0.6
   20 |   1.1391 |     40.018 |   1.1800 |     40.841 |     0.7
   21 |   1.1188 |     39.093 |   1.1690 |     41.371 |     0.7
   22 |   1.0976 |     38.636 |   1.1459 |     39.720 |     0.7
   23 |   1.0746 |     37.661 |   1.1383 |     40.779 |     0.8
   24 |   1.0517 |     36.830 |   1.1340 |     38.941 |     0.8
   25 |   1.0249 |     35.558 |   1.1191 |     38.660 |     0.9
   26 |   1.0068 |     35.156 |   1.1241 |     39.377 |     0.9
   27 |   0.9832 |     33.862 |   1.1024 |     37.882 |     0.9
   28 |   0.9525 |     32.821 |   1.0970 |     36.885 |     1.0
   29 |   0.9351 |     32.161 |   1.0907 |     37.040 |     1.0
   30 |   0.9104 |     31.445 |   1.0877 |     36.916 |     1.0
   31 |   0.8855 |     30.244 |   1.1196 |     36.667 |     1.1
   32 |   0.8725 |     29.776 |   1.0909 |     36.324 |     1.1
   33 |   0.8405 |     28.879 |   1.0652 |     34.611 |     1.1
   34 |   0.8169 |     27.651 |   1.0831 |     34.953 |     1.2
   35 |   0.7934 |     26.924 |   1.0623 |     34.174 |     1.2
   36 |   0.7667 |     25.460 |   1.0805 |     34.642 |     1.2
   37 |   0.7485 |     25.047 |   1.0716 |     33.333 |     1.3
   38 |   0.7218 |     23.659 |   1.0806 |     32.679 |     1.3
   39 |   0.7031 |     22.861 |   1.0713 |     31.682 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,178,978

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4988 |     66.694 |   1.9323 |     53.333 |     0.1
    2 |   1.6879 |     48.552 |   1.5250 |     45.763 |     0.1
    3 |   1.4679 |     46.019 |   1.4271 |     45.763 |     0.2
    4 |   1.3967 |     45.981 |   1.3715 |     45.763 |     0.2
    5 |   1.3524 |     45.590 |   1.3419 |     45.483 |     0.3
    6 |   1.3207 |     44.802 |   1.3178 |     44.953 |     0.4
    7 |   1.2965 |     44.411 |   1.2911 |     43.801 |     0.4
    8 |   1.2744 |     44.037 |   1.2704 |     43.458 |     0.5
    9 |   1.2546 |     43.866 |   1.2504 |     43.427 |     0.5
   10 |   1.2378 |     43.415 |   1.2363 |     43.583 |     0.6
   11 |   1.2196 |     43.189 |   1.2213 |     43.333 |     0.6
   12 |   1.2067 |     42.215 |   1.2079 |     42.897 |     0.7
   13 |   1.1954 |     41.994 |   1.1949 |     42.741 |     0.8
   14 |   1.1893 |     41.983 |   1.1915 |     42.336 |     0.8
   15 |   1.1774 |     41.521 |   1.1811 |     43.053 |     0.9
   16 |   1.1693 |     41.207 |   1.1926 |     42.991 |     0.9
   17 |   1.1605 |     41.091 |   1.1656 |     41.807 |     1.0
   18 |   1.1513 |     40.893 |   1.1596 |     40.903 |     1.1
   19 |   1.1447 |     40.519 |   1.1548 |     41.121 |     1.1
   20 |   1.1361 |     40.513 |   1.1485 |     39.938 |     1.2
   21 |   1.1291 |     40.133 |   1.1395 |     40.498 |     1.2
   22 |   1.1243 |     40.045 |   1.1427 |     40.530 |     1.3
   23 |   1.1167 |     39.539 |   1.1360 |     40.841 |     1.4
   24 |   1.1098 |     39.517 |   1.1234 |     40.280 |     1.4
   25 |   1.1027 |     39.049 |   1.1244 |     40.062 |     1.5
   26 |   1.0963 |     39.082 |   1.1228 |     40.561 |     1.5
   27 |   1.0884 |     38.729 |   1.1117 |     39.439 |     1.6
   28 |   1.0807 |     38.393 |   1.1041 |     38.474 |     1.7
   29 |   1.0763 |     38.146 |   1.0991 |     39.751 |     1.7
   30 |   1.0701 |     37.903 |   1.0965 |     38.349 |     1.8
   31 |   1.0638 |     37.749 |   1.0939 |     38.037 |     1.8
   32 |   1.0572 |     37.463 |   1.0885 |     37.757 |     1.9
   33 |   1.0514 |     37.287 |   1.0764 |     38.006 |     1.9
   34 |   1.0407 |     36.780 |   1.0798 |     37.944 |     2.0
   35 |   1.0398 |     36.797 |   1.0721 |     37.508 |     2.1
   36 |   1.0306 |     36.279 |   1.0687 |     37.664 |     2.1
   37 |   1.0227 |     35.932 |   1.0619 |     36.636 |     2.2
   38 |   1.0183 |     36.053 |   1.0637 |     37.352 |     2.2
   39 |   1.0142 |     36.042 |   1.0594 |     37.134 |     2.3
   40 |   1.0096 |     35.728 |   1.0630 |     36.947 |     2.4
   41 |   1.0032 |     35.767 |   1.0459 |     36.106 |     2.4
   42 |   0.9979 |     35.227 |   1.0482 |     36.106 |     2.5
   43 |   0.9897 |     35.013 |   1.0482 |     35.919 |     2.5
   44 |   0.9847 |     34.914 |   1.0635 |     36.916 |     2.6
   45 |   0.9786 |     34.682 |   1.0371 |     36.106 |     2.7
   46 |   0.9747 |     34.413 |   1.0414 |     35.670 |     2.7
   47 |   0.9648 |     33.933 |   1.0308 |     35.358 |     2.8
   48 |   0.9590 |     33.956 |   1.0287 |     35.639 |     2.8
   49 |   0.9547 |     33.647 |   1.0379 |     35.327 |     2.9
   50 |   0.9448 |     33.284 |   1.0264 |     35.140 |     2.9
   51 |   0.9432 |     33.383 |   1.0190 |     35.047 |     3.0
   52 |   0.9342 |     32.744 |   1.0198 |     34.891 |     3.1
   53 |   0.9296 |     32.832 |   1.0232 |     35.047 |     3.1
   54 |   0.9283 |     32.678 |   1.0215 |     34.860 |     3.2
   55 |   0.9241 |     32.750 |   1.0093 |     34.268 |     3.2
   56 |   0.9100 |     32.243 |   1.0147 |     34.735 |     3.3
   57 |   0.9045 |     31.566 |   1.0135 |     34.174 |     3.4
   58 |   0.8979 |     31.847 |   1.0234 |     34.174 |     3.4
   59 |   0.8901 |     31.252 |   1.0117 |     33.738 |     3.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 503,970

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3468 |     60.737 |   1.7853 |     48.972 |     0.0
    2 |   1.5895 |     46.669 |   1.4745 |     45.763 |     0.1
    3 |   1.4190 |     45.749 |   1.3880 |     45.732 |     0.1
    4 |   1.3557 |     45.111 |   1.3407 |     45.296 |     0.1
    5 |   1.3212 |     44.483 |   1.3123 |     44.579 |     0.2
    6 |   1.2966 |     44.246 |   1.2909 |     44.330 |     0.2
    7 |   1.2747 |     43.756 |   1.2700 |     43.956 |     0.2
    8 |   1.2553 |     43.327 |   1.2499 |     43.427 |     0.3
    9 |   1.2340 |     42.936 |   1.2322 |     42.960 |     0.3
   10 |   1.2143 |     42.528 |   1.2150 |     42.025 |     0.3
   11 |   1.1920 |     41.669 |   1.1926 |     41.900 |     0.3
   12 |   1.1687 |     40.998 |   1.1797 |     40.997 |     0.4
   13 |   1.1426 |     39.649 |   1.1635 |     40.654 |     0.4
   14 |   1.1191 |     38.977 |   1.1429 |     39.844 |     0.4
   15 |   1.0951 |     37.551 |   1.1292 |     39.128 |     0.5
   16 |   1.0611 |     36.532 |   1.1023 |     38.069 |     0.5
   17 |   1.0339 |     35.079 |   1.0878 |     37.539 |     0.5
   18 |   1.0013 |     33.967 |   1.0723 |     36.854 |     0.6
   19 |   0.9659 |     32.546 |   1.0732 |     36.137 |     0.6
   20 |   0.9334 |     31.252 |   1.0370 |     34.860 |     0.6
   21 |   0.8973 |     29.942 |   1.0264 |     34.922 |     0.7
   22 |   0.8626 |     28.466 |   1.0102 |     33.396 |     0.7
   23 |   0.8292 |     27.062 |   0.9955 |     33.551 |     0.7
   24 |   0.7932 |     25.818 |   0.9990 |     33.209 |     0.8
   25 |   0.7587 |     24.738 |   0.9738 |     32.025 |     0.8
   26 |   0.7188 |     23.164 |   0.9716 |     31.090 |     0.8
   27 |   0.6859 |     22.283 |   0.9668 |     30.654 |     0.9
   28 |   0.6572 |     21.038 |   0.9596 |     29.875 |     0.9
   29 |   0.6248 |     19.717 |   0.9805 |     29.377 |     0.9
   30 |   0.5919 |     18.809 |   0.9575 |     29.034 |     1.0
   31 |   0.5548 |     17.327 |   0.9674 |     28.318 |     1.0
   32 |   0.5315 |     16.468 |   0.9731 |     27.944 |     1.0
   33 |   0.5001 |     15.461 |   0.9809 |     28.006 |     1.0
   34 |   0.4709 |     14.294 |   0.9815 |     27.788 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 324,386

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4600 |     66.672 |   1.9004 |     54.829 |     0.0
    2 |   1.6869 |     48.354 |   1.5268 |     45.763 |     0.0
    3 |   1.4673 |     46.036 |   1.4389 |     45.701 |     0.0
    4 |   1.4114 |     45.926 |   1.4032 |     45.701 |     0.1
    5 |   1.3868 |     45.904 |   1.3794 |     45.763 |     0.1
    6 |   1.3612 |     45.810 |   1.3545 |     45.016 |     0.1
    7 |   1.3407 |     45.281 |   1.3364 |     45.078 |     0.1
    8 |   1.3215 |     45.034 |   1.3213 |     45.919 |     0.1
    9 |   1.3051 |     44.923 |   1.3001 |     44.268 |     0.2
   10 |   1.2926 |     44.764 |   1.2946 |     44.174 |     0.2
   11 |   1.2775 |     44.120 |   1.2784 |     44.019 |     0.2
   12 |   1.2631 |     43.690 |   1.2631 |     43.738 |     0.2
   13 |   1.2439 |     43.431 |   1.2464 |     43.427 |     0.2
   14 |   1.2226 |     43.051 |   1.2291 |     43.178 |     0.2
   15 |   1.2004 |     42.352 |   1.2080 |     42.617 |     0.3
   16 |   1.1806 |     41.223 |   1.1933 |     41.495 |     0.3
   17 |   1.1609 |     40.392 |   1.1830 |     40.841 |     0.3
   18 |   1.1404 |     39.737 |   1.1751 |     41.464 |     0.3
   19 |   1.1227 |     38.751 |   1.1564 |     40.156 |     0.3
   20 |   1.1063 |     38.190 |   1.1476 |     39.128 |     0.3
   21 |   1.0841 |     37.292 |   1.1407 |     40.623 |     0.4
   22 |   1.0690 |     36.747 |   1.1282 |     38.660 |     0.4
   23 |   1.0458 |     35.684 |   1.1210 |     37.882 |     0.4
   24 |   1.0276 |     35.029 |   1.1070 |     37.414 |     0.4
   25 |   1.0018 |     34.049 |   1.1143 |     37.165 |     0.4
   26 |   0.9855 |     33.251 |   1.1003 |     36.916 |     0.4
   27 |   0.9622 |     32.353 |   1.0976 |     36.822 |     0.5
   28 |   0.9406 |     31.803 |   1.0999 |     36.137 |     0.5
   29 |   0.9227 |     30.933 |   1.0949 |     36.293 |     0.5
   30 |   0.9061 |     30.382 |   1.0957 |     35.857 |     0.5
   31 |   0.8814 |     29.457 |   1.0782 |     34.953 |     0.5
   32 |   0.8600 |     28.527 |   1.0992 |     35.826 |     0.5
   33 |   0.8423 |     28.235 |   1.0965 |     35.576 |     0.6
   34 |   0.8283 |     27.640 |   1.0782 |     34.798 |     0.6
   35 |   0.8116 |     27.271 |   1.0991 |     35.078 |     0.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 785,186

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4987 |     65.461 |   1.9333 |     54.393 |     0.0
    2 |   1.7173 |     48.910 |   1.5551 |     45.763 |     0.1
    3 |   1.4822 |     45.964 |   1.4376 |     45.701 |     0.1
    4 |   1.4131 |     45.832 |   1.4067 |     46.667 |     0.2
    5 |   1.3839 |     45.854 |   1.3725 |     45.607 |     0.2
    6 |   1.3560 |     45.546 |   1.3529 |     44.143 |     0.2
    7 |   1.3339 |     44.940 |   1.3228 |     44.673 |     0.3
    8 |   1.3103 |     44.654 |   1.3102 |     43.614 |     0.3
    9 |   1.2924 |     44.158 |   1.2877 |     43.551 |     0.3
   10 |   1.2724 |     43.916 |   1.2716 |     43.551 |     0.4
   11 |   1.2538 |     43.530 |   1.2516 |     43.458 |     0.4
   12 |   1.2374 |     43.272 |   1.2366 |     43.271 |     0.5
   13 |   1.2201 |     42.644 |   1.2255 |     42.586 |     0.5
   14 |   1.2050 |     42.253 |   1.2130 |     42.461 |     0.5
   15 |   1.1945 |     41.686 |   1.2056 |     42.555 |     0.6
   16 |   1.1839 |     41.312 |   1.1832 |     41.402 |     0.6
   17 |   1.1723 |     41.064 |   1.1771 |     41.059 |     0.6
   18 |   1.1639 |     40.607 |   1.1731 |     41.464 |     0.7
   19 |   1.1580 |     40.436 |   1.1624 |     42.025 |     0.7
   20 |   1.1476 |     40.100 |   1.1599 |     41.090 |     0.8
   21 |   1.1397 |     39.885 |   1.1521 |     40.810 |     0.8
   22 |   1.1337 |     39.665 |   1.1432 |     39.907 |     0.8
   23 |   1.1285 |     39.462 |   1.1481 |     41.526 |     0.9
   24 |   1.1233 |     39.599 |   1.1431 |     40.779 |     0.9
   25 |   1.1158 |     39.313 |   1.1286 |     40.405 |     1.0
   26 |   1.1102 |     39.219 |   1.1290 |     40.125 |     1.0
   27 |   1.1031 |     38.784 |   1.1193 |     39.190 |     1.0
   28 |   1.0983 |     38.459 |   1.1205 |     39.626 |     1.1
   29 |   1.0913 |     38.559 |   1.1222 |     40.187 |     1.1
   30 |   1.0872 |     38.735 |   1.1186 |     39.720 |     1.1
   31 |   1.0825 |     38.223 |   1.1118 |     39.221 |     1.2
   32 |   1.0771 |     38.151 |   1.1028 |     39.844 |     1.2
   33 |   1.0761 |     38.399 |   1.0995 |     39.439 |     1.3
   34 |   1.0691 |     37.837 |   1.0928 |     39.097 |     1.3
   35 |   1.0629 |     37.771 |   1.0907 |     39.128 |     1.3
   36 |   1.0588 |     37.650 |   1.0825 |     38.505 |     1.4
   37 |   1.0536 |     37.534 |   1.0892 |     38.255 |     1.4
   38 |   1.0483 |     37.358 |   1.0850 |     39.190 |     1.4
   39 |   1.0454 |     37.287 |   1.0729 |     39.470 |     1.5
   40 |   1.0382 |     37.199 |   1.0723 |     37.757 |     1.5
   41 |   1.0345 |     37.055 |   1.0779 |     38.318 |     1.6
   42 |   1.0303 |     36.609 |   1.0754 |     37.508 |     1.6
   43 |   1.0218 |     36.576 |   1.0648 |     37.695 |     1.6
   44 |   1.0171 |     36.235 |   1.0495 |     36.916 |     1.7
   45 |   1.0092 |     36.103 |   1.0530 |     37.134 |     1.7
   46 |   1.0036 |     35.547 |   1.0484 |     36.760 |     1.8
   47 |   0.9979 |     35.585 |   1.0539 |     36.947 |     1.8
   48 |   0.9894 |     35.216 |   1.0467 |     36.417 |     1.8
   49 |   0.9810 |     34.446 |   1.0415 |     37.072 |     1.9
   50 |   0.9734 |     34.561 |   1.0445 |     36.947 |     1.9
   51 |   0.9665 |     34.121 |   1.0353 |     35.483 |     1.9
   52 |   0.9624 |     33.939 |   1.0208 |     35.732 |     2.0
   53 |   0.9563 |     33.526 |   1.0217 |     35.732 |     2.0
   54 |   0.9491 |     33.476 |   1.0234 |     35.670 |     2.1
   55 |   0.9403 |     33.130 |   1.0121 |     35.016 |     2.1
   56 |   0.9348 |     32.832 |   1.0152 |     35.016 |     2.1
   57 |   0.9265 |     32.623 |   1.0076 |     34.299 |     2.2
   58 |   0.9206 |     32.166 |   1.0098 |     34.393 |     2.2
   59 |   0.9105 |     31.995 |   1.0098 |     34.673 |     2.3
   60 |   0.8992 |     31.335 |   1.0079 |     34.424 |     2.3
   61 |   0.8941 |     31.505 |   0.9923 |     34.112 |     2.3
   62 |   0.8899 |     31.103 |   0.9970 |     34.081 |     2.4
   63 |   0.8755 |     30.432 |   0.9997 |     35.016 |     2.4
   64 |   0.8718 |     30.344 |   0.9836 |     33.489 |     2.4
   65 |   0.8582 |     29.975 |   0.9880 |     33.801 |     2.5
   66 |   0.8526 |     29.567 |   0.9846 |     33.801 |     2.5
   67 |   0.8400 |     29.160 |   0.9790 |     33.240 |     2.6
   68 |   0.8287 |     28.851 |   0.9826 |     33.333 |     2.6
   69 |   0.8238 |     28.697 |   0.9721 |     33.489 |     2.6
   70 |   0.8138 |     28.235 |   0.9778 |     33.084 |     2.7
   71 |   0.8039 |     27.888 |   0.9609 |     32.991 |     2.7
   72 |   0.7947 |     27.734 |   0.9712 |     32.866 |     2.7
   73 |   0.7861 |     27.112 |   0.9602 |     33.022 |     2.8
   74 |   0.7771 |     26.842 |   0.9619 |     32.118 |     2.8
   75 |   0.7721 |     26.666 |   0.9543 |     32.212 |     2.9
   76 |   0.7587 |     26.005 |   0.9676 |     32.679 |     2.9
   77 |   0.7514 |     25.647 |   0.9476 |     31.931 |     2.9
   78 |   0.7384 |     25.531 |   0.9453 |     31.277 |     3.0
   79 |   0.7245 |     24.783 |   0.9607 |     31.495 |     3.0
   80 |   0.7145 |     24.226 |   0.9583 |     31.807 |     3.1
   81 |   0.7039 |     23.935 |   0.9472 |     30.966 |     3.1
   82 |   0.6987 |     23.742 |   0.9377 |     30.498 |     3.1
   83 |   0.6840 |     23.054 |   0.9474 |     30.467 |     3.2
   84 |   0.6696 |     22.376 |   0.9464 |     29.782 |     3.2
   85 |   0.6596 |     22.354 |   0.9294 |     28.972 |     3.3
   86 |   0.6533 |     21.930 |   0.9389 |     29.938 |     3.3
   87 |   0.6463 |     21.330 |   0.9316 |     29.252 |     3.3
   88 |   0.6281 |     20.620 |   0.9347 |     28.536 |     3.4
   89 |   0.6109 |     19.844 |   0.9383 |     29.159 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 750,946

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5330 |     67.526 |   1.9647 |     58.816 |     0.0
    2 |   1.7401 |     50.275 |   1.5623 |     48.785 |     0.1
    3 |   1.4982 |     46.201 |   1.4481 |     45.763 |     0.1
    4 |   1.4293 |     46.030 |   1.4104 |     45.763 |     0.2
    5 |   1.3947 |     46.003 |   1.3812 |     45.763 |     0.2
    6 |   1.3633 |     45.848 |   1.3524 |     45.265 |     0.2
    7 |   1.3370 |     45.254 |   1.3279 |     45.202 |     0.3
    8 |   1.3117 |     44.670 |   1.2979 |     43.894 |     0.3
    9 |   1.2860 |     44.433 |   1.2769 |     44.642 |     0.4
   10 |   1.2647 |     44.131 |   1.2575 |     43.458 |     0.4
   11 |   1.2485 |     43.833 |   1.2401 |     43.115 |     0.4
   12 |   1.2344 |     43.569 |   1.2284 |     43.520 |     0.5
   13 |   1.2219 |     43.586 |   1.2176 |     43.551 |     0.5
   14 |   1.2089 |     43.035 |   1.2129 |     44.486 |     0.5
   15 |   1.2005 |     42.418 |   1.1971 |     42.928 |     0.6
   16 |   1.1889 |     42.110 |   1.1881 |     43.240 |     0.6
   17 |   1.1814 |     42.044 |   1.1873 |     42.648 |     0.7
   18 |   1.1715 |     41.846 |   1.1768 |     42.523 |     0.7
   19 |   1.1667 |     41.267 |   1.1743 |     42.025 |     0.7
   20 |   1.1588 |     41.091 |   1.1669 |     41.682 |     0.8
   21 |   1.1512 |     40.899 |   1.1557 |     41.340 |     0.8
   22 |   1.1453 |     40.772 |   1.1507 |     41.121 |     0.9
   23 |   1.1383 |     40.376 |   1.1466 |     41.402 |     0.9
   24 |   1.1328 |     40.535 |   1.1459 |     40.312 |     0.9
   25 |   1.1259 |     39.737 |   1.1349 |     40.218 |     1.0
   26 |   1.1197 |     39.792 |   1.1299 |     40.125 |     1.0
   27 |   1.1153 |     39.671 |   1.1246 |     39.844 |     1.1
   28 |   1.1080 |     39.401 |   1.1268 |     39.782 |     1.1
   29 |   1.1061 |     39.087 |   1.1164 |     39.065 |     1.1
   30 |   1.0982 |     38.955 |   1.1090 |     38.910 |     1.2
   31 |   1.0904 |     38.768 |   1.1066 |     39.782 |     1.2
   32 |   1.0870 |     38.691 |   1.1100 |     39.221 |     1.3
   33 |   1.0842 |     38.581 |   1.0989 |     38.785 |     1.3
   34 |   1.0767 |     38.404 |   1.0946 |     37.975 |     1.3
   35 |   1.0751 |     37.942 |   1.0958 |     39.252 |     1.4
   36 |   1.0672 |     37.551 |   1.1030 |     38.692 |     1.4
   37 |   1.0654 |     37.683 |   1.0822 |     38.349 |     1.5
   38 |   1.0613 |     37.534 |   1.0787 |     37.414 |     1.5
   39 |   1.0520 |     37.110 |   1.0775 |     38.006 |     1.5
   40 |   1.0523 |     37.419 |   1.0742 |     38.910 |     1.6
   41 |   1.0463 |     37.055 |   1.0683 |     37.445 |     1.6
   42 |   1.0408 |     36.780 |   1.0720 |     38.723 |     1.7
   43 |   1.0358 |     36.664 |   1.0707 |     37.290 |     1.7
   44 |   1.0357 |     36.505 |   1.0709 |     37.788 |     1.7
   45 |   1.0258 |     36.268 |   1.0568 |     38.006 |     1.8
   46 |   1.0207 |     36.224 |   1.0556 |     36.324 |     1.8
   47 |   1.0165 |     36.097 |   1.0543 |     37.539 |     1.9
   48 |   1.0112 |     35.745 |   1.0441 |     37.103 |     1.9
   49 |   1.0077 |     35.558 |   1.0457 |     36.978 |     1.9
   50 |   1.0023 |     35.525 |   1.0472 |     36.604 |     2.0
   51 |   1.0000 |     35.271 |   1.0451 |     36.791 |     2.0
   52 |   0.9972 |     35.464 |   1.0412 |     36.449 |     2.1
   53 |   0.9930 |     35.040 |   1.0383 |     36.698 |     2.1
   54 |   0.9871 |     34.941 |   1.0378 |     36.293 |     2.1
   55 |   0.9838 |     34.952 |   1.0327 |     36.885 |     2.2
   56 |   0.9781 |     34.726 |   1.0375 |     36.231 |     2.2
   57 |   0.9739 |     34.732 |   1.0219 |     36.293 |     2.3
   58 |   0.9703 |     34.495 |   1.0193 |     35.794 |     2.3
   59 |   0.9671 |     34.302 |   1.0227 |     36.760 |     2.3
   60 |   0.9610 |     34.088 |   1.0281 |     36.760 |     2.4
   61 |   0.9580 |     33.895 |   1.0249 |     36.137 |     2.4
   62 |   0.9515 |     33.790 |   1.0188 |     36.168 |     2.5
   63 |   0.9517 |     33.900 |   1.0135 |     35.826 |     2.5
   64 |   0.9455 |     33.686 |   1.0160 |     35.826 |     2.5
   65 |   0.9392 |     33.410 |   1.0109 |     35.701 |     2.6
   66 |   0.9369 |     33.526 |   1.0123 |     35.421 |     2.6
   67 |   0.9305 |     32.937 |   1.0072 |     35.078 |     2.7
   68 |   0.9295 |     33.108 |   0.9972 |     35.483 |     2.7
   69 |   0.9249 |     33.190 |   0.9963 |     34.673 |     2.7
   70 |   0.9225 |     32.810 |   1.0004 |     34.766 |     2.8
   71 |   0.9160 |     32.397 |   1.0099 |     34.984 |     2.8
   72 |   0.9134 |     32.249 |   1.0035 |     34.829 |     2.9
   73 |   0.9055 |     31.929 |   0.9995 |     34.798 |     2.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 640,162

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0605 |     56.310 |   1.5103 |     45.763 |     0.0
    2 |   1.4250 |     45.981 |   1.3774 |     45.763 |     0.1
    3 |   1.3385 |     45.067 |   1.3167 |     44.829 |     0.1
    4 |   1.2932 |     44.312 |   1.2804 |     43.676 |     0.1
    5 |   1.2604 |     43.817 |   1.2481 |     43.769 |     0.1
    6 |   1.2304 |     43.095 |   1.2252 |     42.991 |     0.2
    7 |   1.2023 |     42.396 |   1.2033 |     42.586 |     0.2
    8 |   1.1736 |     41.598 |   1.1719 |     41.495 |     0.2
    9 |   1.1472 |     40.596 |   1.1572 |     41.059 |     0.3
   10 |   1.1181 |     39.417 |   1.1383 |     39.875 |     0.3
   11 |   1.0876 |     38.201 |   1.0926 |     38.380 |     0.3
   12 |   1.0452 |     36.698 |   1.0637 |     37.227 |     0.3
   13 |   1.0093 |     34.600 |   1.0407 |     35.857 |     0.4
   14 |   0.9648 |     32.843 |   1.0140 |     34.673 |     0.4
   15 |   0.9236 |     31.577 |   0.9938 |     33.676 |     0.4
   16 |   0.8762 |     29.369 |   0.9639 |     32.305 |     0.5
   17 |   0.8344 |     28.031 |   0.9450 |     31.558 |     0.5
   18 |   0.7877 |     26.142 |   0.9358 |     30.779 |     0.5
   19 |   0.7436 |     24.557 |   0.9267 |     29.688 |     0.5
   20 |   0.7056 |     22.999 |   0.9438 |     29.751 |     0.6
   21 |   0.6696 |     21.650 |   0.9070 |     28.972 |     0.6
   22 |   0.6282 |     20.257 |   0.9141 |     28.816 |     0.6
   23 |   0.5934 |     19.326 |   0.9106 |     28.847 |     0.7
   24 |   0.5591 |     18.049 |   0.9199 |     28.069 |     0.7
   25 |   0.5469 |     17.867 |   0.9070 |     27.352 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 639,394

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2264 |     61.205 |   1.6001 |     45.763 |     0.0
    2 |   1.4722 |     46.074 |   1.4129 |     45.763 |     0.0
    3 |   1.3859 |     45.959 |   1.3648 |     45.701 |     0.1
    4 |   1.3401 |     45.380 |   1.3219 |     44.268 |     0.1
    5 |   1.3062 |     44.731 |   1.3013 |     44.642 |     0.1
    6 |   1.2793 |     44.488 |   1.2736 |     44.517 |     0.1
    7 |   1.2574 |     44.026 |   1.2535 |     43.551 |     0.1
    8 |   1.2425 |     43.894 |   1.2373 |     43.489 |     0.1
    9 |   1.2233 |     43.503 |   1.2190 |     44.579 |     0.2
   10 |   1.2060 |     43.068 |   1.2045 |     44.112 |     0.2
   11 |   1.1915 |     42.424 |   1.1918 |     42.118 |     0.2
   12 |   1.1749 |     41.559 |   1.1795 |     42.056 |     0.2
   13 |   1.1618 |     41.135 |   1.1694 |     41.526 |     0.2
   14 |   1.1520 |     40.899 |   1.1578 |     41.745 |     0.2
   15 |   1.1427 |     40.546 |   1.1472 |     40.498 |     0.3
   16 |   1.1294 |     40.166 |   1.1375 |     41.090 |     0.3
   17 |   1.1196 |     39.919 |   1.1247 |     40.249 |     0.3
   18 |   1.1087 |     39.682 |   1.1235 |     40.810 |     0.3
   19 |   1.1017 |     39.423 |   1.1342 |     41.963 |     0.3
   20 |   1.0946 |     39.346 |   1.1078 |     39.470 |     0.4
   21 |   1.0853 |     39.109 |   1.0958 |     39.003 |     0.4
   22 |   1.0798 |     38.718 |   1.0968 |     39.533 |     0.4
   23 |   1.0721 |     38.492 |   1.0862 |     39.564 |     0.4
   24 |   1.0627 |     37.953 |   1.0775 |     38.069 |     0.4
   25 |   1.0520 |     37.353 |   1.0705 |     37.570 |     0.4
   26 |   1.0456 |     37.292 |   1.0637 |     37.383 |     0.5
   27 |   1.0340 |     37.017 |   1.0633 |     37.570 |     0.5
   28 |   1.0301 |     36.813 |   1.0538 |     36.916 |     0.5
   29 |   1.0208 |     36.642 |   1.0496 |     36.916 |     0.5
   30 |   1.0183 |     36.279 |   1.0410 |     37.134 |     0.5
   31 |   1.0068 |     35.883 |   1.0394 |     37.009 |     0.6
   32 |   1.0031 |     35.789 |   1.0276 |     36.636 |     0.6
   33 |   0.9938 |     35.739 |   1.0196 |     35.171 |     0.6
   34 |   0.9820 |     35.442 |   1.0176 |     35.389 |     0.6
   35 |   0.9752 |     34.886 |   1.0185 |     35.296 |     0.6
   36 |   0.9657 |     34.545 |   1.0110 |     34.704 |     0.6
   37 |   0.9609 |     34.154 |   1.0111 |     34.953 |     0.7
   38 |   0.9501 |     33.944 |   0.9947 |     34.922 |     0.7
   39 |   0.9429 |     33.537 |   0.9911 |     34.268 |     0.7
   40 |   0.9409 |     33.240 |   0.9930 |     34.330 |     0.7
   41 |   0.9298 |     32.992 |   0.9955 |     33.863 |     0.7
   42 |   0.9225 |     32.689 |   0.9948 |     33.801 |     0.8
   43 |   0.9124 |     32.194 |   0.9763 |     33.894 |     0.8
   44 |   0.9053 |     32.177 |   0.9796 |     33.458 |     0.8
   45 |   0.9020 |     32.023 |   0.9698 |     32.991 |     0.8
   46 |   0.8897 |     31.544 |   0.9688 |     32.555 |     0.8
   47 |   0.8805 |     31.054 |   0.9685 |     32.399 |     0.8
   48 |   0.8769 |     30.993 |   0.9670 |     32.430 |     0.9
   49 |   0.8709 |     30.740 |   0.9496 |     32.212 |     0.9
   50 |   0.8558 |     30.200 |   0.9463 |     31.682 |     0.9
   51 |   0.8591 |     30.410 |   0.9424 |     31.495 |     0.9
   52 |   0.8361 |     29.364 |   0.9337 |     31.558 |     0.9
   53 |   0.8287 |     29.055 |   0.9389 |     31.277 |     1.0
   54 |   0.8266 |     29.116 |   0.9276 |     30.935 |     1.0
   55 |   0.8124 |     28.450 |   0.9244 |     30.748 |     1.0
   56 |   0.8040 |     28.180 |   0.9299 |     31.121 |     1.0
   57 |   0.7958 |     27.948 |   0.9146 |     30.623 |     1.0
   58 |   0.7883 |     27.712 |   0.9254 |     30.810 |     1.0
   59 |   0.7842 |     27.646 |   0.9032 |     29.688 |     1.1
   60 |   0.7759 |     27.068 |   0.9082 |     29.875 |     1.1
   61 |   0.7625 |     26.699 |   0.9122 |     29.875 |     1.1
   62 |   0.7494 |     25.685 |   0.9011 |     28.910 |     1.1
   63 |   0.7472 |     25.768 |   0.8948 |     29.844 |     1.1
   64 |   0.7362 |     25.614 |   0.8929 |     29.626 |     1.2
   65 |   0.7319 |     25.465 |   0.8886 |     29.003 |     1.2
   66 |   0.7224 |     25.201 |   0.8916 |     28.754 |     1.2
   67 |   0.7158 |     24.843 |   0.8830 |     28.536 |     1.2
   68 |   0.6988 |     24.116 |   0.8913 |     28.847 |     1.2
   69 |   0.6931 |     23.725 |   0.8945 |     28.287 |     1.2
   70 |   0.6852 |     23.648 |   0.8890 |     28.785 |     1.3
   71 |   0.6828 |     23.389 |   0.9029 |     28.910 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,263,138

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5259 |     68.131 |   2.0145 |     59.159 |     0.1
    2 |   1.7718 |     50.363 |   1.5873 |     45.763 |     0.1
    3 |   1.5090 |     46.157 |   1.4579 |     45.763 |     0.2
    4 |   1.4378 |     46.036 |   1.4202 |     45.763 |     0.2
    5 |   1.4127 |     46.091 |   1.4029 |     45.763 |     0.3
    6 |   1.3983 |     46.036 |   1.3926 |     45.763 |     0.3
    7 |   1.3818 |     46.041 |   1.3788 |     45.701 |     0.4
    8 |   1.3624 |     45.507 |   1.3538 |     44.486 |     0.5
    9 |   1.3325 |     44.659 |   1.3261 |     43.769 |     0.5
   10 |   1.3060 |     44.494 |   1.2987 |     43.520 |     0.6
   11 |   1.2788 |     44.015 |   1.2790 |     44.050 |     0.6
   12 |   1.2565 |     43.982 |   1.2570 |     43.520 |     0.7
   13 |   1.2377 |     43.608 |   1.2432 |     43.240 |     0.8
   14 |   1.2207 |     43.211 |   1.2220 |     42.305 |     0.8
   15 |   1.2004 |     42.044 |   1.2035 |     41.900 |     0.9
   16 |   1.1822 |     41.190 |   1.1940 |     42.025 |     0.9
   17 |   1.1648 |     40.678 |   1.1822 |     41.963 |     1.0
   18 |   1.1468 |     40.067 |   1.1655 |     41.121 |     1.0
   19 |   1.1260 |     39.104 |   1.1604 |     40.125 |     1.1
   20 |   1.1047 |     38.272 |   1.1355 |     39.720 |     1.2
   21 |   1.0836 |     37.254 |   1.1307 |     39.283 |     1.2
   22 |   1.0607 |     36.389 |   1.1125 |     38.349 |     1.3
   23 |   1.0386 |     35.448 |   1.1058 |     37.913 |     1.3
   24 |   1.0153 |     34.545 |   1.0977 |     37.508 |     1.4
   25 |   0.9912 |     33.862 |   1.0931 |     36.231 |     1.4
   26 |   0.9637 |     32.794 |   1.0714 |     35.763 |     1.5
   27 |   0.9386 |     31.929 |   1.0818 |     36.012 |     1.6
   28 |   0.9120 |     30.905 |   1.0648 |     35.670 |     1.6
   29 |   0.8805 |     29.876 |   1.0508 |     34.735 |     1.7
   30 |   0.8443 |     28.345 |   1.0482 |     33.146 |     1.7
   31 |   0.8145 |     27.178 |   1.0478 |     33.707 |     1.8
   32 |   0.7842 |     25.856 |   1.0371 |     33.022 |     1.9
   33 |   0.7572 |     24.887 |   1.0407 |     32.056 |     1.9
   34 |   0.7239 |     23.725 |   1.0603 |     32.523 |     2.0
   35 |   0.6953 |     22.173 |   1.0699 |     31.526 |     2.0
   36 |   0.6640 |     21.440 |   1.0550 |     31.526 |     2.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,098,722

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1746 |     59.443 |   1.5884 |     45.763 |     0.0
    2 |   1.4696 |     46.102 |   1.4210 |     46.231 |     0.1
    3 |   1.3875 |     46.019 |   1.3724 |     45.607 |     0.1
    4 |   1.3494 |     45.573 |   1.3426 |     45.888 |     0.2
    5 |   1.3249 |     45.254 |   1.3174 |     44.611 |     0.2
    6 |   1.3040 |     45.034 |   1.3037 |     44.112 |     0.2
    7 |   1.2814 |     44.191 |   1.2864 |     44.237 |     0.3
    8 |   1.2637 |     44.433 |   1.2671 |     44.361 |     0.3
    9 |   1.2467 |     44.065 |   1.2443 |     43.458 |     0.4
   10 |   1.2279 |     43.492 |   1.2307 |     43.209 |     0.4
   11 |   1.2075 |     42.897 |   1.2228 |     43.022 |     0.4
   12 |   1.1894 |     42.402 |   1.1955 |     43.458 |     0.5
   13 |   1.1725 |     41.813 |   1.1780 |     41.558 |     0.5
   14 |   1.1538 |     41.102 |   1.1563 |     40.903 |     0.6
   15 |   1.1323 |     40.150 |   1.1366 |     41.246 |     0.6
   16 |   1.1153 |     39.952 |   1.1244 |     39.657 |     0.6
   17 |   1.0991 |     39.412 |   1.1136 |     39.283 |     0.7
   18 |   1.0810 |     38.531 |   1.1050 |     39.969 |     0.7
   19 |   1.0653 |     37.837 |   1.0877 |     39.065 |     0.8
   20 |   1.0456 |     37.105 |   1.0805 |     37.601 |     0.8
   21 |   1.0253 |     36.097 |   1.0607 |     35.919 |     0.8
   22 |   1.0058 |     34.980 |   1.0493 |     36.293 |     0.9
   23 |   0.9819 |     34.154 |   1.0290 |     35.794 |     0.9
   24 |   0.9587 |     32.964 |   1.0152 |     34.050 |     1.0
   25 |   0.9314 |     32.089 |   1.0148 |     33.925 |     1.0
   26 |   0.9143 |     31.406 |   0.9989 |     33.427 |     1.1
   27 |   0.8872 |     30.520 |   0.9758 |     33.178 |     1.1
   28 |   0.8599 |     29.193 |   0.9712 |     33.053 |     1.1
   29 |   0.8389 |     28.648 |   0.9759 |     32.804 |     1.2
   30 |   0.8108 |     27.574 |   0.9764 |     33.115 |     1.2
   31 |   0.7919 |     26.902 |   0.9557 |     31.153 |     1.3
   32 |   0.7684 |     26.038 |   0.9530 |     31.558 |     1.3
   33 |   0.7460 |     25.217 |   0.9409 |     30.935 |     1.3
   34 |   0.7176 |     23.946 |   0.9364 |     30.000 |     1.4
   35 |   0.6930 |     23.015 |   0.9322 |     30.592 |     1.4
   36 |   0.6654 |     22.382 |   0.9137 |     29.533 |     1.5
   37 |   0.6414 |     21.341 |   0.9318 |     28.972 |     1.5
   38 |   0.6222 |     20.636 |   0.9244 |     28.567 |     1.5
   39 |   0.6024 |     20.146 |   0.9245 |     28.567 |     1.6
   40 |   0.5795 |     19.232 |   0.9329 |     28.255 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 572,962

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5591 |     68.814 |   2.0234 |     59.159 |     0.0
    2 |   1.7922 |     50.936 |   1.5913 |     45.763 |     0.1
    3 |   1.5067 |     46.041 |   1.4457 |     45.763 |     0.1
    4 |   1.4313 |     46.140 |   1.4131 |     45.732 |     0.1
    5 |   1.4075 |     45.986 |   1.3984 |     46.791 |     0.2
    6 |   1.3892 |     46.019 |   1.3788 |     45.576 |     0.2
    7 |   1.3729 |     45.744 |   1.3651 |     45.732 |     0.3
    8 |   1.3512 |     45.243 |   1.3422 |     44.019 |     0.3
    9 |   1.3339 |     44.912 |   1.3259 |     44.330 |     0.3
   10 |   1.3116 |     44.742 |   1.3032 |     44.143 |     0.4
   11 |   1.2925 |     44.351 |   1.2859 |     43.707 |     0.4
   12 |   1.2772 |     43.987 |   1.2729 |     43.645 |     0.4
   13 |   1.2614 |     44.026 |   1.2591 |     43.614 |     0.5
   14 |   1.2480 |     44.026 |   1.2481 |     43.396 |     0.5
   15 |   1.2374 |     43.806 |   1.2375 |     43.458 |     0.5
   16 |   1.2267 |     43.608 |   1.2246 |     43.520 |     0.6
   17 |   1.2182 |     43.409 |   1.2197 |     43.676 |     0.6
   18 |   1.2115 |     43.222 |   1.2073 |     42.866 |     0.6
   19 |   1.2062 |     43.018 |   1.2092 |     42.804 |     0.7
   20 |   1.1968 |     42.578 |   1.1966 |     42.741 |     0.7
   21 |   1.1882 |     42.435 |   1.1888 |     42.118 |     0.8
   22 |   1.1815 |     42.038 |   1.1820 |     42.025 |     0.8
   23 |   1.1745 |     41.680 |   1.1769 |     42.087 |     0.8
   24 |   1.1693 |     41.625 |   1.1711 |     41.651 |     0.9
   25 |   1.1638 |     41.356 |   1.1614 |     41.807 |     0.9
   26 |   1.1570 |     41.301 |   1.1588 |     41.651 |     0.9
   27 |   1.1480 |     40.860 |   1.1512 |     40.841 |     1.0
   28 |   1.1432 |     40.750 |   1.1451 |     41.215 |     1.0
   29 |   1.1375 |     40.453 |   1.1454 |     41.153 |     1.0
   30 |   1.1321 |     40.414 |   1.1411 |     41.246 |     1.1
   31 |   1.1313 |     40.766 |   1.1349 |     40.592 |     1.1
   32 |   1.1248 |     40.331 |   1.1345 |     40.872 |     1.1
   33 |   1.1194 |     40.067 |   1.1271 |     40.343 |     1.2
   34 |   1.1140 |     39.924 |   1.1213 |     40.467 |     1.2
   35 |   1.1104 |     39.715 |   1.1136 |     39.657 |     1.3
   36 |   1.1037 |     39.786 |   1.1121 |     40.374 |     1.3
   37 |   1.1026 |     39.412 |   1.1111 |     40.062 |     1.3
   38 |   1.0976 |     39.390 |   1.1053 |     40.031 |     1.4
   39 |   1.0934 |     39.280 |   1.1067 |     39.907 |     1.4
   40 |   1.0909 |     39.082 |   1.1034 |     40.156 |     1.4
   41 |   1.0845 |     39.021 |   1.0934 |     39.626 |     1.5
   42 |   1.0831 |     38.944 |   1.0945 |     39.782 |     1.5
   43 |   1.0812 |     38.845 |   1.0938 |     39.688 |     1.5
   44 |   1.0750 |     38.641 |   1.0933 |     39.844 |     1.6
   45 |   1.0711 |     38.592 |   1.0947 |     39.315 |     1.6
   46 |   1.0694 |     38.404 |   1.0928 |     39.907 |     1.6
   47 |   1.0662 |     38.322 |   1.0837 |     39.315 |     1.7
   48 |   1.0572 |     37.848 |   1.0823 |     38.847 |     1.7
   49 |   1.0614 |     38.294 |   1.0814 |     39.065 |     1.7
   50 |   1.0587 |     38.316 |   1.0765 |     39.782 |     1.8
   51 |   1.0515 |     37.859 |   1.0702 |     38.879 |     1.8
   52 |   1.0446 |     37.617 |   1.0666 |     38.536 |     1.9
   53 |   1.0415 |     37.358 |   1.0644 |     38.442 |     1.9
   54 |   1.0372 |     37.254 |   1.0658 |     39.034 |     1.9
   55 |   1.0306 |     37.204 |   1.0624 |     37.882 |     2.0
   56 |   1.0318 |     37.066 |   1.0554 |     38.505 |     2.0
   57 |   1.0232 |     36.758 |   1.0481 |     37.632 |     2.0
   58 |   1.0153 |     36.373 |   1.0438 |     37.040 |     2.1
   59 |   1.0133 |     36.301 |   1.0422 |     37.290 |     2.1
   60 |   1.0089 |     35.949 |   1.0349 |     36.573 |     2.1
   61 |   0.9995 |     35.585 |   1.0305 |     37.290 |     2.2
   62 |   0.9952 |     35.668 |   1.0375 |     37.196 |     2.2
   63 |   0.9898 |     34.996 |   1.0400 |     36.293 |     2.2
   64 |   0.9880 |     35.024 |   1.0416 |     36.791 |     2.3
   65 |   0.9870 |     35.062 |   1.0337 |     37.072 |     2.3
   66 |   0.9770 |     34.523 |   1.0204 |     36.199 |     2.4
   67 |   0.9791 |     34.820 |   1.0237 |     36.480 |     2.4
   68 |   0.9660 |     34.170 |   1.0130 |     36.168 |     2.4
   69 |   0.9581 |     33.642 |   1.0087 |     35.670 |     2.5
   70 |   0.9538 |     33.344 |   1.0093 |     35.545 |     2.5
   71 |   0.9460 |     33.196 |   1.0039 |     35.701 |     2.5
   72 |   0.9418 |     33.130 |   0.9987 |     34.953 |     2.6
   73 |   0.9374 |     32.931 |   0.9907 |     34.891 |     2.6
   74 |   0.9297 |     32.436 |   0.9982 |     35.171 |     2.6
   75 |   0.9198 |     32.100 |   0.9857 |     34.642 |     2.7
   76 |   0.9177 |     31.924 |   0.9831 |     34.486 |     2.7
   77 |   0.9075 |     31.687 |   0.9804 |     34.579 |     2.7
   78 |   0.9035 |     31.544 |   0.9801 |     33.956 |     2.8
   79 |   0.8985 |     31.285 |   0.9753 |     33.956 |     2.8
   80 |   0.8905 |     31.010 |   0.9752 |     34.112 |     2.8
   81 |   0.8882 |     30.536 |   0.9650 |     34.330 |     2.9
   82 |   0.8817 |     30.586 |   0.9673 |     32.897 |     2.9
   83 |   0.8742 |     30.371 |   0.9659 |     33.583 |     3.0
   84 |   0.8686 |     30.035 |   0.9638 |     33.925 |     3.0
   85 |   0.8749 |     30.305 |   0.9652 |     33.364 |     3.0
   86 |   0.8625 |     29.677 |   0.9581 |     32.523 |     3.1
   87 |   0.8565 |     29.534 |   0.9678 |     33.458 |     3.1
   88 |   0.8495 |     29.408 |   0.9592 |     32.866 |     3.1
   89 |   0.8443 |     29.193 |   0.9519 |     32.555 |     3.2
   90 |   0.8332 |     28.670 |   0.9753 |     33.146 |     3.2
   91 |   0.8363 |     28.934 |   0.9648 |     33.115 |     3.2
   92 |   0.8269 |     28.802 |   0.9572 |     32.336 |     3.3
   93 |   0.8197 |     28.125 |   0.9452 |     32.212 |     3.3
   94 |   0.8097 |     27.849 |   0.9469 |     32.679 |     3.3
   95 |   0.8057 |     27.580 |   0.9437 |     31.931 |     3.4
   96 |   0.8006 |     27.403 |   0.9449 |     31.931 |     3.4
   97 |   0.7961 |     27.436 |   0.9474 |     32.399 |     3.5
   98 |   0.7904 |     27.178 |   0.9429 |     31.900 |     3.5
   99 |   0.7874 |     26.875 |   0.9536 |     31.682 |     3.5
  100 |   0.7724 |     26.462 |   0.9362 |     31.090 |     3.6
  101 |   0.7665 |     26.060 |   0.9396 |     31.340 |     3.6
  102 |   0.7642 |     26.071 |   0.9472 |     31.121 |     3.6
  103 |   0.7580 |     25.862 |   0.9436 |     30.436 |     3.7
  104 |   0.7472 |     25.476 |   0.9362 |     31.246 |     3.7
  105 |   0.7439 |     25.113 |   0.9367 |     31.651 |     3.7
  106 |   0.7382 |     24.915 |   0.9407 |     31.402 |     3.8
  107 |   0.7389 |     25.234 |   0.9568 |     31.402 |     3.8
  108 |   0.7359 |     24.898 |   0.9413 |     30.654 |     3.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,622,690

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5107 |     68.407 |   1.9484 |     59.159 |     0.1
    2 |   1.6975 |     48.111 |   1.5270 |     45.763 |     0.2
    3 |   1.4705 |     46.190 |   1.4370 |     45.763 |     0.2
    4 |   1.4191 |     46.030 |   1.4064 |     45.763 |     0.3
    5 |   1.3951 |     46.008 |   1.3892 |     45.701 |     0.4
    6 |   1.3766 |     45.777 |   1.3733 |     44.922 |     0.5
    7 |   1.3591 |     45.331 |   1.3543 |     44.860 |     0.5
    8 |   1.3395 |     45.215 |   1.3379 |     44.143 |     0.6
    9 |   1.3213 |     44.720 |   1.3223 |     45.358 |     0.7
   10 |   1.3022 |     44.411 |   1.3001 |     44.330 |     0.8
   11 |   1.2825 |     44.323 |   1.2838 |     44.112 |     0.8
   12 |   1.2648 |     43.899 |   1.2621 |     43.489 |     0.9
   13 |   1.2469 |     43.448 |   1.2548 |     43.240 |     1.0
   14 |   1.2311 |     42.914 |   1.2332 |     43.396 |     1.1
   15 |   1.2146 |     42.523 |   1.2206 |     42.741 |     1.1
   16 |   1.1997 |     41.752 |   1.2076 |     42.025 |     1.2
   17 |   1.1803 |     41.488 |   1.1922 |     41.807 |     1.3
   18 |   1.1640 |     41.009 |   1.1782 |     41.526 |     1.4
   19 |   1.1467 |     40.364 |   1.1685 |     40.280 |     1.4
   20 |   1.1252 |     39.616 |   1.1555 |     39.938 |     1.5
   21 |   1.1069 |     38.883 |   1.1419 |     39.346 |     1.6
   22 |   1.0864 |     37.672 |   1.1300 |     39.065 |     1.7
   23 |   1.0630 |     36.934 |   1.1250 |     38.037 |     1.7
   24 |   1.0431 |     35.795 |   1.1036 |     37.632 |     1.8
   25 |   1.0185 |     35.002 |   1.1062 |     38.069 |     1.9
   26 |   0.9959 |     34.313 |   1.1019 |     37.227 |     2.0
   27 |   0.9711 |     33.273 |   1.0864 |     36.542 |     2.0
   28 |   0.9453 |     32.243 |   1.0766 |     36.760 |     2.1
   29 |   0.9151 |     30.889 |   1.0816 |     36.012 |     2.2
   30 |   0.8938 |     29.909 |   1.0770 |     36.324 |     2.3
   31 |   0.8672 |     28.747 |   1.0669 |     35.171 |     2.3
   32 |   0.8420 |     27.893 |   1.0595 |     34.611 |     2.4
   33 |   0.8152 |     26.710 |   1.0803 |     35.016 |     2.5
   34 |   0.7854 |     25.619 |   1.0755 |     33.988 |     2.6
   35 |   0.7606 |     24.772 |   1.0797 |     33.925 |     2.6
   36 |   0.7329 |     23.802 |   1.0871 |     33.364 |     2.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,588,450

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4916 |     66.969 |   1.9645 |     59.159 |     0.1
    2 |   1.7564 |     50.418 |   1.5671 |     45.763 |     0.2
    3 |   1.5009 |     46.113 |   1.4503 |     45.763 |     0.2
    4 |   1.4344 |     46.157 |   1.4163 |     45.763 |     0.3
    5 |   1.4075 |     46.074 |   1.3903 |     45.763 |     0.4
    6 |   1.3811 |     45.953 |   1.3689 |     45.794 |     0.5
    7 |   1.3576 |     45.749 |   1.3539 |     45.452 |     0.6
    8 |   1.3374 |     45.617 |   1.3296 |     45.701 |     0.6
    9 |   1.3172 |     45.232 |   1.3136 |     45.234 |     0.7
   10 |   1.2966 |     45.045 |   1.2938 |     44.798 |     0.8
   11 |   1.2791 |     44.533 |   1.2802 |     44.299 |     0.9
   12 |   1.2597 |     44.219 |   1.2607 |     43.801 |     1.0
   13 |   1.2450 |     43.993 |   1.2502 |     43.489 |     1.0
   14 |   1.2312 |     43.448 |   1.2373 |     43.302 |     1.1
   15 |   1.2206 |     42.517 |   1.2263 |     42.430 |     1.2
   16 |   1.2106 |     41.961 |   1.2124 |     42.430 |     1.3
   17 |   1.1974 |     41.780 |   1.2078 |     42.586 |     1.4
   18 |   1.1868 |     41.267 |   1.2017 |     42.150 |     1.4
   19 |   1.1772 |     41.229 |   1.1847 |     41.433 |     1.5
   20 |   1.1696 |     41.003 |   1.1836 |     41.838 |     1.6
   21 |   1.1600 |     40.662 |   1.1735 |     40.966 |     1.7
   22 |   1.1489 |     40.491 |   1.1621 |     40.436 |     1.8
   23 |   1.1427 |     40.144 |   1.1608 |     40.685 |     1.8
   24 |   1.1326 |     39.781 |   1.1506 |     41.308 |     1.9
   25 |   1.1196 |     39.544 |   1.1358 |     40.031 |     2.0
   26 |   1.1091 |     38.994 |   1.1350 |     40.530 |     2.1
   27 |   1.1006 |     38.630 |   1.1288 |     39.564 |     2.2
   28 |   1.0932 |     38.426 |   1.1165 |     39.315 |     2.2
   29 |   1.0835 |     37.887 |   1.1082 |     38.629 |     2.3
   30 |   1.0754 |     37.683 |   1.0992 |     37.227 |     2.4
   31 |   1.0641 |     37.314 |   1.1015 |     38.318 |     2.5
   32 |   1.0548 |     36.753 |   1.0946 |     37.477 |     2.6
   33 |   1.0492 |     36.642 |   1.0900 |     37.726 |     2.6
   34 |   1.0346 |     36.257 |   1.0819 |     37.072 |     2.7
   35 |   1.0270 |     35.872 |   1.0817 |     37.414 |     2.8
   36 |   1.0131 |     35.580 |   1.0676 |     36.729 |     2.9
   37 |   1.0060 |     35.244 |   1.0664 |     36.854 |     3.0
   38 |   0.9933 |     34.424 |   1.0637 |     36.480 |     3.0
   39 |   0.9827 |     34.082 |   1.0543 |     35.296 |     3.1
   40 |   0.9744 |     33.631 |   1.0574 |     36.075 |     3.2
   41 |   0.9587 |     33.251 |   1.0371 |     34.984 |     3.3
   42 |   0.9495 |     32.673 |   1.0381 |     35.140 |     3.4
   43 |   0.9320 |     31.759 |   1.0398 |     34.548 |     3.4
   44 |   0.9206 |     31.533 |   1.0348 |     33.863 |     3.5
   45 |   0.9094 |     30.834 |   1.0328 |     33.956 |     3.6
   46 |   0.8987 |     30.470 |   1.0451 |     34.143 |     3.7
   47 |   0.8906 |     30.244 |   1.0363 |     33.801 |     3.7
   48 |   0.8724 |     29.419 |   1.0229 |     33.738 |     3.8
   49 |   0.8601 |     28.895 |   1.0305 |     33.707 |     3.9
   50 |   0.8505 |     28.466 |   1.0233 |     33.146 |     4.0
   51 |   0.8326 |     27.778 |   1.0203 |     32.617 |     4.1
   52 |   0.8238 |     27.464 |   1.0136 |     32.648 |     4.1
   53 |   0.8119 |     27.018 |   1.0153 |     32.025 |     4.2
   54 |   0.7968 |     26.506 |   0.9981 |     31.776 |     4.3
   55 |   0.7764 |     25.575 |   0.9978 |     32.150 |     4.4
   56 |   0.7646 |     25.262 |   1.0140 |     32.492 |     4.5
   57 |   0.7545 |     24.876 |   1.0075 |     31.807 |     4.5
   58 |   0.7359 |     24.127 |   1.0396 |     32.928 |     4.6
   59 |   0.7260 |     23.990 |   1.0291 |     31.713 |     4.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 639,394

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0004 |     55.071 |   1.4878 |     45.763 |     0.0
    2 |   1.3885 |     45.171 |   1.3407 |     44.268 |     0.0
    3 |   1.3090 |     44.488 |   1.2914 |     43.832 |     0.1
    4 |   1.2665 |     44.020 |   1.2555 |     44.486 |     0.1
    5 |   1.2299 |     42.875 |   1.2287 |     43.146 |     0.1
    6 |   1.1958 |     42.198 |   1.2000 |     42.274 |     0.1
    7 |   1.1650 |     41.130 |   1.1690 |     40.966 |     0.2
    8 |   1.1354 |     40.067 |   1.1457 |     40.530 |     0.2
    9 |   1.1068 |     39.076 |   1.1227 |     39.190 |     0.2
   10 |   1.0788 |     38.035 |   1.0924 |     37.632 |     0.2
   11 |   1.0416 |     36.477 |   1.0750 |     37.570 |     0.3
   12 |   1.0082 |     35.238 |   1.0488 |     36.885 |     0.3
   13 |   0.9787 |     33.989 |   1.0176 |     35.794 |     0.3
   14 |   0.9475 |     32.667 |   1.0047 |     34.330 |     0.3
   15 |   0.9081 |     31.021 |   0.9954 |     34.548 |     0.4
   16 |   0.8790 |     29.909 |   0.9787 |     33.551 |     0.4
   17 |   0.8447 |     28.483 |   0.9446 |     32.274 |     0.4
   18 |   0.8110 |     27.299 |   0.9493 |     31.869 |     0.4
   19 |   0.7760 |     25.818 |   0.9254 |     31.090 |     0.5
   20 |   0.7349 |     24.397 |   0.9193 |     30.405 |     0.5
   21 |   0.7012 |     23.401 |   0.8987 |     29.533 |     0.5
   22 |   0.6672 |     21.974 |   0.9108 |     29.813 |     0.5
   23 |   0.6411 |     21.143 |   0.8966 |     29.065 |     0.6
   24 |   0.6081 |     20.064 |   0.8851 |     28.037 |     0.6
   25 |   0.5741 |     18.786 |   0.8969 |     28.660 |     0.6
   26 |   0.5422 |     17.729 |   0.8849 |     27.103 |     0.6
   27 |   0.5209 |     16.887 |   0.8857 |     26.729 |     0.7
   28 |   0.4938 |     16.033 |   0.8985 |     27.445 |     0.7
   29 |   0.4656 |     15.086 |   0.8942 |     26.760 |     0.7
   30 |   0.4360 |     14.029 |   0.8866 |     26.480 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 717,666

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3738 |     63.275 |   1.8096 |     48.785 |     0.0
    2 |   1.5849 |     46.862 |   1.4613 |     45.763 |     0.1
    3 |   1.4096 |     45.727 |   1.3803 |     45.701 |     0.1
    4 |   1.3494 |     45.292 |   1.3318 |     44.361 |     0.2
    5 |   1.3130 |     44.610 |   1.3067 |     44.299 |     0.2
    6 |   1.2908 |     44.043 |   1.2881 |     43.894 |     0.2
    7 |   1.2707 |     43.910 |   1.2771 |     45.140 |     0.3
    8 |   1.2521 |     43.365 |   1.2543 |     42.835 |     0.3
    9 |   1.2332 |     42.892 |   1.2342 |     42.461 |     0.3
   10 |   1.2147 |     42.237 |   1.2252 |     42.368 |     0.4
   11 |   1.1958 |     41.532 |   1.2050 |     42.150 |     0.4
   12 |   1.1748 |     40.893 |   1.1842 |     42.523 |     0.5
   13 |   1.1561 |     40.447 |   1.1745 |     40.623 |     0.5
   14 |   1.1372 |     39.362 |   1.1493 |     39.439 |     0.5
   15 |   1.1209 |     39.054 |   1.1319 |     39.346 |     0.6
   16 |   1.1017 |     38.355 |   1.1085 |     37.944 |     0.6
   17 |   1.0772 |     37.226 |   1.0909 |     37.134 |     0.7
   18 |   1.0550 |     36.301 |   1.0858 |     38.069 |     0.7
   19 |   1.0293 |     35.365 |   1.0641 |     36.386 |     0.7
   20 |   1.0056 |     34.126 |   1.0719 |     36.760 |     0.8
   21 |   0.9877 |     33.449 |   1.0348 |     34.860 |     0.8
   22 |   0.9619 |     32.518 |   1.0211 |     34.891 |     0.8
   23 |   0.9344 |     31.505 |   1.0257 |     35.421 |     0.9
   24 |   0.9105 |     30.239 |   0.9904 |     32.056 |     0.9
   25 |   0.8809 |     29.176 |   0.9808 |     32.368 |     1.0
   26 |   0.8476 |     27.899 |   0.9650 |     31.028 |     1.0
   27 |   0.8206 |     26.677 |   0.9523 |     30.810 |     1.0
   28 |   0.7880 |     25.498 |   0.9511 |     30.561 |     1.1
   29 |   0.7575 |     24.292 |   0.9428 |     30.125 |     1.1
   30 |   0.7206 |     22.762 |   0.9368 |     30.125 |     1.2
   31 |   0.6945 |     22.156 |   0.9271 |     29.315 |     1.2
   32 |   0.6570 |     20.703 |   0.9321 |     29.720 |     1.2
   33 |   0.6337 |     19.871 |   0.9278 |     28.723 |     1.3
   34 |   0.6016 |     18.660 |   0.9319 |     28.318 |     1.3
   35 |   0.5698 |     17.702 |   0.9239 |     28.006 |     1.3
   36 |   0.5441 |     16.568 |   0.9385 |     28.162 |     1.4
   37 |   0.5233 |     16.248 |   0.9416 |     27.570 |     1.4
   38 |   0.4905 |     14.855 |   0.9270 |     27.009 |     1.5
   39 |   0.4651 |     13.886 |   0.9565 |     27.165 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,495,202

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0994 |     58.017 |   1.5459 |     45.763 |     0.1
    2 |   1.4507 |     46.289 |   1.4058 |     45.639 |     0.1
    3 |   1.3796 |     45.915 |   1.3590 |     46.604 |     0.2
    4 |   1.3310 |     45.226 |   1.3174 |     44.393 |     0.2
    5 |   1.3007 |     44.659 |   1.2931 |     44.237 |     0.3
    6 |   1.2773 |     44.345 |   1.2667 |     44.673 |     0.4
    7 |   1.2543 |     43.949 |   1.2514 |     43.614 |     0.4
    8 |   1.2357 |     43.349 |   1.2374 |     43.520 |     0.5
    9 |   1.2157 |     42.765 |   1.2124 |     41.900 |     0.5
   10 |   1.1956 |     41.846 |   1.1980 |     42.617 |     0.6
   11 |   1.1778 |     41.675 |   1.1830 |     41.433 |     0.7
   12 |   1.1595 |     41.223 |   1.1669 |     41.215 |     0.7
   13 |   1.1404 |     40.464 |   1.1552 |     41.059 |     0.8
   14 |   1.1297 |     39.770 |   1.1273 |     40.530 |     0.9
   15 |   1.1071 |     39.115 |   1.1184 |     40.467 |     0.9
   16 |   1.0903 |     38.537 |   1.1021 |     38.941 |     1.0
   17 |   1.0705 |     37.634 |   1.0894 |     38.037 |     1.0
   18 |   1.0522 |     36.973 |   1.0721 |     38.224 |     1.1
   19 |   1.0321 |     36.125 |   1.0517 |     36.667 |     1.2
   20 |   1.0114 |     35.448 |   1.0465 |     36.075 |     1.2
   21 |   0.9992 |     35.051 |   1.0227 |     34.984 |     1.3
   22 |   0.9787 |     34.313 |   1.0113 |     35.234 |     1.3
   23 |   0.9575 |     33.130 |   0.9973 |     34.112 |     1.4
   24 |   0.9344 |     32.507 |   0.9873 |     33.925 |     1.5
   25 |   0.9191 |     31.566 |   0.9842 |     33.583 |     1.5
   26 |   0.8975 |     30.679 |   0.9643 |     32.773 |     1.6
   27 |   0.8689 |     29.876 |   0.9641 |     32.679 |     1.6
   28 |   0.8575 |     29.347 |   0.9377 |     31.433 |     1.7
   29 |   0.8307 |     28.009 |   0.9232 |     30.498 |     1.8
   30 |   0.8102 |     27.574 |   0.9341 |     31.308 |     1.8
   31 |   0.7976 |     27.062 |   0.9092 |     30.218 |     1.9
   32 |   0.7730 |     26.297 |   0.9042 |     29.751 |     1.9
   33 |   0.7476 |     25.096 |   0.8921 |     29.221 |     2.0
   34 |   0.7337 |     24.595 |   0.9295 |     30.498 |     2.1
   35 |   0.7314 |     24.430 |   0.8728 |     28.910 |     2.1
   36 |   0.7003 |     23.362 |   0.8810 |     29.221 |     2.2
   37 |   0.6756 |     22.277 |   0.8668 |     28.318 |     2.3
   38 |   0.6622 |     21.985 |   0.8629 |     28.318 |     2.3
   39 |   0.6410 |     21.182 |   0.8567 |     27.788 |     2.4
   40 |   0.6245 |     20.659 |   0.8533 |     26.760 |     2.4
   41 |   0.6076 |     19.981 |   0.8351 |     26.698 |     2.5
   42 |   0.5894 |     19.183 |   0.8438 |     26.386 |     2.6
   43 |   0.5742 |     18.875 |   0.8617 |     27.009 |     2.6
   44 |   0.5610 |     18.418 |   0.8530 |     26.885 |     2.7
   45 |   0.5357 |     17.074 |   0.8484 |     26.262 |     2.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,887,522

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1344 |     57.808 |   1.5633 |     45.763 |     0.1
    2 |   1.4546 |     46.041 |   1.3938 |     45.794 |     0.2
    3 |   1.3712 |     45.496 |   1.3490 |     44.579 |     0.2
    4 |   1.3285 |     44.731 |   1.3217 |     44.330 |     0.3
    5 |   1.2942 |     44.356 |   1.2878 |     43.894 |     0.4
    6 |   1.2698 |     43.833 |   1.2625 |     43.520 |     0.5
    7 |   1.2476 |     43.239 |   1.2416 |     43.240 |     0.6
    8 |   1.2239 |     42.787 |   1.2202 |     43.645 |     0.7
    9 |   1.2009 |     41.967 |   1.1977 |     42.679 |     0.7
   10 |   1.1737 |     40.888 |   1.1783 |     40.374 |     0.8
   11 |   1.1439 |     39.792 |   1.1485 |     40.156 |     0.9
   12 |   1.1119 |     38.404 |   1.1233 |     38.879 |     1.0
   13 |   1.0777 |     37.430 |   1.0878 |     37.850 |     1.1
   14 |   1.0372 |     35.624 |   1.0676 |     36.916 |     1.1
   15 |   1.0037 |     34.600 |   1.0397 |     36.355 |     1.2
   16 |   0.9621 |     32.783 |   1.0274 |     35.078 |     1.3
   17 |   0.9259 |     31.990 |   0.9896 |     34.424 |     1.4
   18 |   0.8842 |     30.046 |   0.9662 |     32.243 |     1.5
   19 |   0.8445 |     28.554 |   0.9490 |     31.713 |     1.6
   20 |   0.8045 |     27.117 |   0.9486 |     31.059 |     1.6
   21 |   0.7639 |     25.328 |   0.9094 |     30.436 |     1.7
   22 |   0.7247 |     24.001 |   0.8985 |     29.159 |     1.8
   23 |   0.6875 |     22.542 |   0.8902 |     28.879 |     1.9
   24 |   0.6444 |     21.082 |   0.8882 |     28.100 |     2.0
   25 |   0.5984 |     19.337 |   0.8856 |     27.726 |     2.0
   26 |   0.5669 |     18.368 |   0.8824 |     28.162 |     2.1
   27 |   0.5241 |     16.678 |   0.8764 |     26.636 |     2.2
   28 |   0.4958 |     15.890 |   0.9084 |     27.570 |     2.3
   29 |   0.4663 |     14.773 |   0.8862 |     26.199 |     2.4
   30 |   0.4272 |     13.633 |   0.8856 |     26.231 |     2.5
   31 |   0.3979 |     12.411 |   0.9231 |     25.950 |     2.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 507,298

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0186 |     53.992 |   1.4630 |     45.763 |     0.0
    2 |   1.3858 |     45.298 |   1.3341 |     44.206 |     0.0
    3 |   1.3079 |     44.279 |   1.2884 |     43.894 |     0.1
    4 |   1.2694 |     44.070 |   1.2606 |     43.489 |     0.1
    5 |   1.2354 |     43.068 |   1.2235 |     43.053 |     0.1
    6 |   1.2048 |     42.600 |   1.1955 |     42.399 |     0.1
    7 |   1.1718 |     41.141 |   1.1623 |     41.558 |     0.1
    8 |   1.1460 |     40.629 |   1.1466 |     41.059 |     0.1
    9 |   1.1189 |     39.318 |   1.1288 |     40.062 |     0.2
   10 |   1.0937 |     38.630 |   1.1169 |     39.190 |     0.2
   11 |   1.0648 |     37.452 |   1.0800 |     38.037 |     0.2
   12 |   1.0336 |     36.318 |   1.0491 |     36.417 |     0.2
   13 |   1.0026 |     35.051 |   1.0369 |     35.545 |     0.2
   14 |   0.9688 |     33.427 |   1.0000 |     34.548 |     0.2
   15 |   0.9380 |     32.585 |   0.9846 |     33.209 |     0.3
   16 |   0.9043 |     31.004 |   0.9715 |     32.835 |     0.3
   17 |   0.8748 |     29.776 |   0.9513 |     31.526 |     0.3
   18 |   0.8435 |     28.521 |   0.9521 |     31.963 |     0.3
   19 |   0.8139 |     27.717 |   0.9338 |     31.402 |     0.3
   20 |   0.7848 |     26.528 |   0.9219 |     29.782 |     0.4
   21 |   0.7553 |     25.311 |   0.9244 |     30.436 |     0.4
   22 |   0.7271 |     24.502 |   0.9130 |     29.969 |     0.4
   23 |   0.7030 |     23.725 |   0.9056 |     28.318 |     0.4
   24 |   0.6770 |     22.701 |   0.8980 |     29.283 |     0.4
   25 |   0.6570 |     21.688 |   0.8844 |     28.598 |     0.4
   26 |   0.6275 |     20.961 |   0.8985 |     27.975 |     0.5
   27 |   0.6124 |     20.389 |   0.8951 |     28.598 |     0.5
   28 |   0.5870 |     19.409 |   0.8740 |     27.040 |     0.5
   29 |   0.5682 |     18.875 |   0.8927 |     27.508 |     0.5
   30 |   0.5541 |     18.434 |   0.8962 |     27.227 |     0.5
   31 |   0.5339 |     17.564 |   0.8819 |     26.355 |     0.6
   32 |   0.5168 |     16.793 |   0.9041 |     26.044 |     0.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 766,626

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5274 |     67.256 |   1.9703 |     59.159 |     0.0
    2 |   1.7256 |     49.224 |   1.5497 |     45.763 |     0.1
    3 |   1.4804 |     46.036 |   1.4417 |     45.763 |     0.1
    4 |   1.4183 |     46.173 |   1.4132 |     45.763 |     0.2
    5 |   1.3942 |     46.036 |   1.3981 |     45.701 |     0.2
    6 |   1.3726 |     45.942 |   1.3707 |     45.763 |     0.2
    7 |   1.3542 |     45.408 |   1.3495 |     44.984 |     0.3
    8 |   1.3368 |     45.100 |   1.3355 |     44.704 |     0.3
    9 |   1.3226 |     44.874 |   1.3276 |     44.237 |     0.3
   10 |   1.3120 |     44.918 |   1.3170 |     45.109 |     0.4
   11 |   1.3022 |     44.516 |   1.3129 |     44.268 |     0.4
   12 |   1.2945 |     44.142 |   1.3077 |     45.140 |     0.5
   13 |   1.2857 |     44.092 |   1.3102 |     44.735 |     0.5
   14 |   1.2739 |     43.899 |   1.2820 |     43.956 |     0.5
   15 |   1.2644 |     43.712 |   1.2697 |     43.364 |     0.6
   16 |   1.2478 |     43.178 |   1.2637 |     43.146 |     0.6
   17 |   1.2329 |     42.732 |   1.2410 |     42.555 |     0.6
   18 |   1.2230 |     42.468 |   1.2382 |     43.084 |     0.7
   19 |   1.2140 |     42.341 |   1.2265 |     42.679 |     0.7
   20 |   1.2054 |     42.099 |   1.2166 |     43.178 |     0.8
   21 |   1.1983 |     42.198 |   1.2111 |     42.804 |     0.8
   22 |   1.1901 |     41.752 |   1.2051 |     42.555 |     0.8
   23 |   1.1844 |     41.713 |   1.2013 |     43.583 |     0.9
   24 |   1.1755 |     41.780 |   1.1895 |     41.838 |     0.9
   25 |   1.1723 |     41.515 |   1.1882 |     42.866 |     1.0
   26 |   1.1645 |     41.009 |   1.1829 |     41.900 |     1.0
   27 |   1.1554 |     40.772 |   1.1755 |     42.212 |     1.0
   28 |   1.1486 |     40.888 |   1.1690 |     41.246 |     1.1
   29 |   1.1421 |     40.585 |   1.1690 |     41.651 |     1.1
   30 |   1.1438 |     40.959 |   1.1721 |     41.963 |     1.1
   31 |   1.1335 |     40.315 |   1.1535 |     41.433 |     1.2
   32 |   1.1213 |     39.935 |   1.1488 |     41.215 |     1.2
   33 |   1.1144 |     39.781 |   1.1485 |     40.872 |     1.3
   34 |   1.1070 |     39.660 |   1.1342 |     40.779 |     1.3
   35 |   1.1021 |     39.428 |   1.1330 |     40.405 |     1.3
   36 |   1.0940 |     39.197 |   1.1281 |     40.685 |     1.4
   37 |   1.0883 |     38.861 |   1.1128 |     39.470 |     1.4
   38 |   1.0775 |     38.575 |   1.1175 |     39.533 |     1.5
   39 |   1.0716 |     37.980 |   1.1081 |     38.972 |     1.5
   40 |   1.0586 |     37.997 |   1.0966 |     39.533 |     1.5
   41 |   1.0518 |     37.501 |   1.0847 |     37.227 |     1.6
   42 |   1.0420 |     36.951 |   1.0942 |     38.598 |     1.6
   43 |   1.0359 |     36.802 |   1.0810 |     37.757 |     1.6
   44 |   1.0266 |     36.477 |   1.0812 |     37.975 |     1.7
   45 |   1.0180 |     36.202 |   1.0742 |     37.944 |     1.7
   46 |   1.0081 |     36.064 |   1.0694 |     37.414 |     1.8
   47 |   1.0063 |     35.717 |   1.0657 |     36.978 |     1.8
   48 |   0.9967 |     35.497 |   1.0597 |     37.103 |     1.8
   49 |   0.9882 |     35.035 |   1.0526 |     37.103 |     1.9
   50 |   0.9804 |     34.770 |   1.0533 |     36.449 |     1.9
   51 |   0.9856 |     34.936 |   1.0529 |     36.511 |     2.0
   52 |   0.9741 |     34.622 |   1.0398 |     36.355 |     2.0
   53 |   0.9630 |     34.236 |   1.0384 |     36.480 |     2.0
   54 |   0.9488 |     33.521 |   1.0447 |     35.171 |     2.1
   55 |   0.9442 |     33.223 |   1.0309 |     34.642 |     2.1
   56 |   0.9476 |     33.421 |   1.0316 |     35.576 |     2.1
   57 |   0.9306 |     32.700 |   1.0206 |     34.704 |     2.2
   58 |   0.9275 |     32.441 |   1.0305 |     34.735 |     2.2
   59 |   0.9148 |     32.172 |   1.0173 |     34.517 |     2.3
   60 |   0.9152 |     32.056 |   1.0370 |     35.047 |     2.3
   61 |   0.9110 |     31.781 |   1.0094 |     34.517 |     2.3
   62 |   0.8987 |     31.236 |   1.0153 |     34.424 |     2.4
   63 |   0.8913 |     31.021 |   1.0042 |     34.299 |     2.4
   64 |   0.8825 |     30.663 |   1.0147 |     34.673 |     2.5
   65 |   0.8753 |     30.244 |   1.0022 |     33.988 |     2.5
   66 |   0.8739 |     30.217 |   0.9968 |     34.174 |     2.5
   67 |   0.8672 |     29.931 |   1.0024 |     33.364 |     2.6
   68 |   0.8568 |     29.341 |   0.9982 |     33.863 |     2.6
   69 |   0.8465 |     29.110 |   1.0015 |     34.112 |     2.7
   70 |   0.8373 |     28.912 |   0.9877 |     33.209 |     2.7
   71 |   0.8264 |     28.317 |   0.9855 |     32.555 |     2.7
   72 |   0.8169 |     28.026 |   0.9896 |     33.115 |     2.8
   73 |   0.8089 |     27.491 |   0.9816 |     32.586 |     2.8
   74 |   0.7981 |     27.023 |   0.9808 |     32.430 |     2.8
   75 |   0.7945 |     27.101 |   0.9826 |     32.274 |     2.9
   76 |   0.7837 |     26.511 |   0.9823 |     32.212 |     2.9
   77 |   0.7735 |     26.330 |   0.9887 |     32.181 |     3.0
   78 |   0.7641 |     25.735 |   0.9691 |     32.087 |     3.0
   79 |   0.7500 |     25.184 |   0.9607 |     30.748 |     3.0
   80 |   0.7440 |     25.041 |   0.9701 |     31.713 |     3.1
   81 |   0.8064 |     27.574 |   1.0022 |     33.240 |     3.1
   82 |   0.7513 |     25.460 |   0.9719 |     30.685 |     3.2
   83 |   0.7246 |     24.243 |   0.9807 |     31.994 |     3.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,559,522

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2237 |     59.503 |   1.6226 |     48.785 |     0.1
    2 |   1.4881 |     46.195 |   1.4200 |     45.763 |     0.1
    3 |   1.4028 |     46.019 |   1.3872 |     45.701 |     0.2
    4 |   1.3718 |     45.887 |   1.3710 |     45.763 |     0.2
    5 |   1.3503 |     45.535 |   1.3399 |     46.199 |     0.3
    6 |   1.3191 |     45.072 |   1.3085 |     46.075 |     0.4
    7 |   1.2965 |     44.973 |   1.2828 |     44.517 |     0.4
    8 |   1.2719 |     44.340 |   1.2800 |     44.299 |     0.5
    9 |   1.2515 |     44.180 |   1.2469 |     43.832 |     0.5
   10 |   1.2319 |     43.729 |   1.2361 |     44.330 |     0.6
   11 |   1.2105 |     42.914 |   1.2071 |     43.427 |     0.7
   12 |   1.1894 |     42.583 |   1.1885 |     43.801 |     0.7
   13 |   1.1720 |     41.631 |   1.1793 |     42.087 |     0.8
   14 |   1.1523 |     40.954 |   1.1581 |     42.399 |     0.9
   15 |   1.1372 |     40.293 |   1.1364 |     41.807 |     0.9
   16 |   1.1185 |     39.830 |   1.1249 |     39.564 |     1.0
   17 |   1.0988 |     39.236 |   1.1148 |     39.782 |     1.0
   18 |   1.0799 |     38.201 |   1.0912 |     38.380 |     1.1
   19 |   1.0573 |     37.276 |   1.0820 |     38.006 |     1.2
   20 |   1.0424 |     36.670 |   1.0637 |     37.321 |     1.2
   21 |   1.0192 |     35.806 |   1.0554 |     36.978 |     1.3
   22 |   0.9988 |     34.853 |   1.0375 |     35.950 |     1.3
   23 |   0.9755 |     34.099 |   1.0323 |     35.514 |     1.4
   24 |   0.9494 |     33.163 |   0.9985 |     34.548 |     1.5
   25 |   0.9338 |     32.326 |   0.9990 |     34.361 |     1.5
   26 |   0.9240 |     31.929 |   0.9918 |     34.050 |     1.6
   27 |   0.8903 |     30.850 |   0.9689 |     32.399 |     1.7
   28 |   0.8654 |     29.633 |   0.9705 |     32.804 |     1.7
   29 |   0.8436 |     28.918 |   0.9656 |     32.118 |     1.8
   30 |   0.8127 |     27.646 |   0.9512 |     31.994 |     1.8
   31 |   0.8000 |     27.007 |   0.9610 |     32.056 |     1.9
   32 |   0.7762 |     26.159 |   0.9481 |     30.623 |     2.0
   33 |   0.7482 |     25.251 |   0.9467 |     31.340 |     2.0
   34 |   0.7357 |     24.584 |   0.9324 |     30.187 |     2.1
   35 |   0.7162 |     23.902 |   0.9725 |     31.713 |     2.1
   36 |   0.7239 |     24.188 |   0.9071 |     29.097 |     2.2
   37 |   0.6692 |     22.118 |   0.9221 |     29.221 |     2.3
   38 |   0.6469 |     21.341 |   0.9214 |     28.660 |     2.3
   39 |   0.6370 |     20.895 |   0.9002 |     28.255 |     2.4
   40 |   0.6064 |     19.822 |   0.9173 |     28.442 |     2.5
   41 |   0.5779 |     19.034 |   0.9313 |     28.318 |     2.5
   42 |   0.5688 |     18.610 |   0.9515 |     28.349 |     2.6
   43 |   0.6358 |     21.314 |   0.9159 |     27.570 |     2.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,000,866

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0217 |     55.666 |   1.4886 |     45.701 |     0.0
    2 |   1.3863 |     45.122 |   1.3386 |     44.393 |     0.1
    3 |   1.3038 |     44.147 |   1.2799 |     44.922 |     0.1
    4 |   1.2598 |     43.910 |   1.2540 |     44.642 |     0.2
    5 |   1.2232 |     42.611 |   1.2207 |     42.212 |     0.2
    6 |   1.1935 |     41.658 |   1.1901 |     41.745 |     0.2
    7 |   1.1618 |     40.722 |   1.1544 |     39.813 |     0.3
    8 |   1.1321 |     39.742 |   1.1434 |     40.592 |     0.3
    9 |   1.1002 |     38.267 |   1.1120 |     39.938 |     0.3
   10 |   1.0631 |     37.138 |   1.0826 |     38.723 |     0.4
   11 |   1.0288 |     35.519 |   1.0579 |     36.355 |     0.4
   12 |   0.9887 |     33.889 |   1.0320 |     35.639 |     0.5
   13 |   0.9535 |     32.370 |   1.0025 |     33.769 |     0.5
   14 |   0.9212 |     30.993 |   0.9939 |     34.050 |     0.5
   15 |   0.8750 |     29.650 |   0.9749 |     32.928 |     0.6
   16 |   0.8302 |     27.602 |   0.9492 |     31.807 |     0.6
   17 |   0.7817 |     25.851 |   0.9179 |     31.121 |     0.7
   18 |   0.7343 |     24.089 |   0.9108 |     29.439 |     0.7
   19 |   0.6963 |     22.767 |   0.8995 |     29.470 |     0.7
   20 |   0.6481 |     21.082 |   0.8820 |     28.131 |     0.8
   21 |   0.6066 |     19.789 |   0.8889 |     28.131 |     0.8
   22 |   0.5705 |     18.109 |   0.8852 |     28.069 |     0.9
   23 |   0.5270 |     16.826 |   0.8798 |     27.539 |     0.9
   24 |   0.4936 |     15.725 |   0.8893 |     26.262 |     0.9
   25 |   0.4599 |     14.503 |   0.8958 |     25.981 |     1.0
   26 |   0.4295 |     13.561 |   0.9080 |     26.449 |     1.0
   27 |   0.3965 |     12.322 |   0.9290 |     26.573 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 503,970

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4021 |     63.550 |   1.8731 |     48.785 |     0.0
    2 |   1.6500 |     47.467 |   1.5022 |     45.763 |     0.1
    3 |   1.4428 |     45.771 |   1.4025 |     45.763 |     0.1
    4 |   1.3742 |     45.672 |   1.3592 |     46.791 |     0.1
    5 |   1.3370 |     45.292 |   1.3284 |     44.424 |     0.2
    6 |   1.3112 |     44.593 |   1.3080 |     44.299 |     0.2
    7 |   1.2915 |     44.599 |   1.2898 |     44.174 |     0.2
    8 |   1.2771 |     44.219 |   1.2812 |     43.925 |     0.3
    9 |   1.2644 |     43.833 |   1.2648 |     44.019 |     0.3
   10 |   1.2506 |     43.608 |   1.2522 |     43.427 |     0.3
   11 |   1.2347 |     43.305 |   1.2386 |     43.427 |     0.4
   12 |   1.2227 |     43.079 |   1.2231 |     42.492 |     0.4
   13 |   1.2104 |     42.969 |   1.2144 |     42.492 |     0.4
   14 |   1.1959 |     42.413 |   1.1981 |     42.648 |     0.5
   15 |   1.1819 |     41.769 |   1.1833 |     43.364 |     0.5
   16 |   1.1688 |     41.730 |   1.1755 |     42.212 |     0.5
   17 |   1.1586 |     41.262 |   1.1664 |     41.963 |     0.6
   18 |   1.1448 |     40.475 |   1.1606 |     42.960 |     0.6
   19 |   1.1350 |     40.403 |   1.1395 |     40.436 |     0.6
   20 |   1.1238 |     39.737 |   1.1252 |     39.408 |     0.7
   21 |   1.1079 |     39.274 |   1.1187 |     39.844 |     0.7
   22 |   1.0971 |     38.867 |   1.1098 |     39.097 |     0.7
   23 |   1.0840 |     38.206 |   1.0968 |     38.380 |     0.8
   24 |   1.0740 |     37.788 |   1.0958 |     38.349 |     0.8
   25 |   1.0592 |     37.237 |   1.0815 |     37.975 |     0.8
   26 |   1.0461 |     36.731 |   1.0818 |     37.632 |     0.9
   27 |   1.0316 |     35.954 |   1.0569 |     36.231 |     0.9
   28 |   1.0164 |     35.172 |   1.0591 |     36.106 |     0.9
   29 |   1.0036 |     34.341 |   1.0443 |     36.324 |     1.0
   30 |   0.9894 |     33.917 |   1.0405 |     36.012 |     1.0
   31 |   0.9840 |     34.005 |   1.0283 |     35.670 |     1.0
   32 |   0.9666 |     33.031 |   1.0228 |     35.670 |     1.1
   33 |   0.9574 |     32.937 |   1.0052 |     34.798 |     1.1
   34 |   0.9418 |     32.320 |   0.9961 |     34.019 |     1.1
   35 |   0.9283 |     31.704 |   1.0034 |     34.174 |     1.2
   36 |   0.9309 |     31.946 |   0.9959 |     33.956 |     1.2
   37 |   0.9084 |     31.153 |   0.9817 |     33.146 |     1.2
   38 |   0.9006 |     30.575 |   0.9768 |     33.520 |     1.3
   39 |   0.8843 |     30.311 |   0.9789 |     33.427 |     1.3
   40 |   0.8706 |     29.821 |   0.9764 |     33.333 |     1.3
   41 |   0.8571 |     29.176 |   0.9662 |     32.928 |     1.4
   42 |   0.8449 |     28.703 |   0.9610 |     33.053 |     1.4
   43 |   0.8351 |     28.725 |   0.9609 |     32.679 |     1.4
   44 |   0.8181 |     27.910 |   0.9473 |     31.994 |     1.5
   45 |   0.8009 |     26.891 |   0.9540 |     32.243 |     1.5
   46 |   0.7914 |     26.616 |   0.9493 |     31.308 |     1.5
   47 |   0.7819 |     26.611 |   0.9424 |     31.308 |     1.6
   48 |   0.7582 |     25.383 |   0.9254 |     30.841 |     1.6
   49 |   0.7511 |     25.228 |   0.9322 |     30.654 |     1.6
   50 |   0.7299 |     24.259 |   0.9255 |     30.592 |     1.7
   51 |   0.7166 |     23.929 |   0.9171 |     30.218 |     1.7
   52 |   0.7011 |     23.274 |   0.9239 |     30.249 |     1.7
   53 |   0.7114 |     23.725 |   0.9196 |     30.374 |     1.8
   54 |   0.6899 |     22.855 |   0.9140 |     29.159 |     1.8
   55 |   0.6598 |     21.528 |   0.9135 |     29.813 |     1.8
   56 |   0.6455 |     21.066 |   0.9039 |     29.128 |     1.9
   57 |   0.6366 |     20.747 |   0.9127 |     29.283 |     1.9
   58 |   0.6306 |     20.681 |   0.9185 |     29.626 |     1.9
   59 |   0.6119 |     20.091 |   0.9074 |     29.003 |     2.0
   60 |   0.5991 |     19.502 |   0.8993 |     28.287 |     2.0
   61 |   0.5797 |     18.583 |   0.9096 |     28.193 |     2.0
   62 |   0.5639 |     18.418 |   0.9206 |     28.536 |     2.1
   63 |   0.5551 |     17.900 |   0.9138 |     28.037 |     2.1
   64 |   0.5435 |     17.795 |   0.9265 |     28.006 |     2.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,985,378

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2031 |     60.825 |   1.6193 |     45.763 |     0.1
    2 |   1.4878 |     46.069 |   1.4217 |     45.763 |     0.1
    3 |   1.4049 |     46.036 |   1.3867 |     45.763 |     0.2
    4 |   1.3706 |     45.777 |   1.3550 |     44.704 |     0.3
    5 |   1.3406 |     45.777 |   1.3295 |     45.389 |     0.4
    6 |   1.3083 |     45.023 |   1.3082 |     44.984 |     0.5
    7 |   1.2848 |     44.934 |   1.2842 |     44.019 |     0.5
    8 |   1.2653 |     44.643 |   1.2687 |     44.050 |     0.6
    9 |   1.2499 |     44.241 |   1.2537 |     43.956 |     0.7
   10 |   1.2354 |     43.591 |   1.2410 |     43.084 |     0.8
   11 |   1.2204 |     43.145 |   1.2312 |     43.084 |     0.8
   12 |   1.2092 |     43.084 |   1.2116 |     42.586 |     0.9
   13 |   1.1954 |     42.369 |   1.1993 |     42.181 |     1.0
   14 |   1.1849 |     41.818 |   1.1985 |     43.520 |     1.1
   15 |   1.1732 |     41.824 |   1.1828 |     41.713 |     1.1
   16 |   1.1624 |     41.411 |   1.1671 |     41.028 |     1.2
   17 |   1.1535 |     41.267 |   1.1614 |     41.495 |     1.3
   18 |   1.1455 |     41.009 |   1.1460 |     41.651 |     1.4
   19 |   1.1313 |     40.579 |   1.1447 |     40.935 |     1.4
   20 |   1.1232 |     40.172 |   1.1364 |     41.994 |     1.5
   21 |   1.1154 |     39.880 |   1.1241 |     40.000 |     1.6
   22 |   1.1068 |     39.819 |   1.1222 |     42.025 |     1.7
   23 |   1.1007 |     39.616 |   1.1082 |     40.810 |     1.7
   24 |   1.0897 |     38.944 |   1.1030 |     39.252 |     1.8
   25 |   1.0824 |     38.894 |   1.1029 |     39.221 |     1.9
   26 |   1.0746 |     38.355 |   1.0928 |     39.128 |     2.0
   27 |   1.0663 |     38.035 |   1.0905 |     39.190 |     2.0
   28 |   1.0605 |     37.854 |   1.0817 |     38.505 |     2.1
   29 |   1.0549 |     37.876 |   1.0904 |     39.159 |     2.2
   30 |   1.0474 |     37.507 |   1.0726 |     38.723 |     2.3
   31 |   1.0372 |     36.923 |   1.0651 |     38.162 |     2.3
   32 |   1.0335 |     36.885 |   1.0626 |     37.882 |     2.4
   33 |   1.0309 |     37.006 |   1.0785 |     37.913 |     2.5
   34 |   1.0222 |     36.780 |   1.0616 |     37.570 |     2.5
   35 |   1.0201 |     36.681 |   1.0594 |     37.850 |     2.6
   36 |   1.0096 |     36.114 |   1.0552 |     37.664 |     2.7
   37 |   1.0044 |     35.965 |   1.0501 |     37.383 |     2.8
   38 |   0.9974 |     35.662 |   1.0508 |     37.383 |     2.8
   39 |   0.9956 |     35.651 |   1.0513 |     37.508 |     2.9
   40 |   0.9856 |     35.266 |   1.0481 |     36.791 |     3.0
   41 |   0.9827 |     35.266 |   1.0361 |     36.667 |     3.1
   42 |   0.9753 |     34.919 |   1.0331 |     36.449 |     3.1
   43 |   0.9700 |     34.809 |   1.0313 |     36.542 |     3.2
   44 |   0.9634 |     34.512 |   1.0224 |     36.480 |     3.3
   45 |   0.9560 |     34.280 |   1.0276 |     36.012 |     3.4
   46 |   0.9486 |     34.115 |   1.0265 |     36.075 |     3.4
   47 |   0.9414 |     33.669 |   1.0230 |     36.137 |     3.5
   48 |   0.9368 |     33.377 |   1.0176 |     36.231 |     3.6
   49 |   0.9273 |     33.141 |   1.0105 |     35.888 |     3.7
   50 |   0.9191 |     32.777 |   1.0106 |     35.545 |     3.7
   51 |   0.9126 |     32.601 |   0.9984 |     35.047 |     3.8
   52 |   0.8984 |     32.083 |   0.9911 |     35.171 |     3.9
   53 |   0.8909 |     31.726 |   0.9940 |     35.888 |     4.0
   54 |   0.8848 |     31.797 |   0.9873 |     34.330 |     4.0
   55 |   0.8782 |     31.395 |   0.9786 |     34.393 |     4.1
   56 |   0.8681 |     30.823 |   0.9704 |     34.050 |     4.2
   57 |   0.8584 |     30.707 |   0.9673 |     33.240 |     4.3
   58 |   0.8461 |     30.013 |   0.9746 |     33.489 |     4.3
   59 |   0.8481 |     30.013 |   0.9821 |     34.143 |     4.4
   60 |   0.8353 |     29.694 |   0.9554 |     32.305 |     4.5
   61 |   0.8312 |     29.567 |   0.9551 |     33.364 |     4.6
   62 |   0.8182 |     28.835 |   0.9552 |     33.022 |     4.6
   63 |   0.8130 |     28.879 |   0.9601 |     33.146 |     4.7
   64 |   0.8029 |     28.240 |   0.9487 |     33.614 |     4.8
   65 |   0.8000 |     28.125 |   0.9489 |     32.617 |     4.9
   66 |   0.7913 |     27.915 |   0.9411 |     32.648 |     4.9
   67 |   0.7811 |     27.552 |   0.9472 |     32.461 |     5.0
   68 |   0.7770 |     27.525 |   0.9360 |     32.461 |     5.1
   69 |   0.7630 |     26.930 |   0.9368 |     31.807 |     5.2
   70 |   0.7580 |     26.990 |   0.9490 |     32.679 |     5.2
   71 |   0.7468 |     26.275 |   0.9390 |     31.215 |     5.3
   72 |   0.7406 |     26.082 |   0.9445 |     31.713 |     5.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,593,762

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1964 |     60.406 |   1.6026 |     45.763 |     0.1
    2 |   1.4781 |     46.113 |   1.4233 |     45.763 |     0.1
    3 |   1.3987 |     46.206 |   1.3791 |     45.047 |     0.2
    4 |   1.3602 |     45.458 |   1.3543 |     44.922 |     0.2
    5 |   1.3345 |     45.298 |   1.3239 |     45.919 |     0.3
    6 |   1.3134 |     45.133 |   1.3142 |     45.047 |     0.4
    7 |   1.2918 |     44.615 |   1.2921 |     44.486 |     0.4
    8 |   1.2745 |     44.610 |   1.2751 |     44.237 |     0.5
    9 |   1.2580 |     44.345 |   1.2632 |     44.143 |     0.5
   10 |   1.2397 |     43.795 |   1.2499 |     44.330 |     0.6
   11 |   1.2243 |     43.536 |   1.2308 |     42.866 |     0.7
   12 |   1.2064 |     42.705 |   1.2099 |     42.897 |     0.7
   13 |   1.1816 |     41.851 |   1.1922 |     41.807 |     0.8
   14 |   1.1632 |     41.003 |   1.1702 |     41.402 |     0.8
   15 |   1.1433 |     40.579 |   1.1575 |     40.717 |     0.9
   16 |   1.1262 |     40.232 |   1.1361 |     40.530 |     1.0
   17 |   1.1018 |     39.109 |   1.1168 |     39.159 |     1.0
   18 |   1.0792 |     37.887 |   1.1059 |     39.377 |     1.1
   19 |   1.0605 |     37.611 |   1.0872 |     38.193 |     1.2
   20 |   1.0387 |     36.543 |   1.0620 |     37.695 |     1.2
   21 |   1.0131 |     35.112 |   1.0552 |     36.511 |     1.3
   22 |   0.9965 |     34.633 |   1.0350 |     35.826 |     1.3
   23 |   0.9751 |     34.071 |   1.0375 |     37.134 |     1.4
   24 |   0.9435 |     32.750 |   1.0115 |     34.984 |     1.5
   25 |   0.9229 |     32.265 |   0.9983 |     34.704 |     1.5
   26 |   0.9010 |     30.938 |   0.9804 |     33.769 |     1.6
   27 |   0.8673 |     29.727 |   0.9784 |     33.302 |     1.6
   28 |   0.8468 |     29.165 |   0.9677 |     32.835 |     1.7
   29 |   0.8220 |     27.937 |   0.9532 |     32.617 |     1.8
   30 |   0.7968 |     26.798 |   0.9451 |     31.308 |     1.8
   31 |   0.7676 |     25.724 |   0.9348 |     30.685 |     1.9
   32 |   0.7364 |     24.215 |   0.9299 |     30.125 |     1.9
   33 |   0.7107 |     23.511 |   0.9108 |     29.408 |     2.0
   34 |   0.6844 |     22.784 |   0.9088 |     28.318 |     2.1
   35 |   0.6557 |     21.886 |   0.9313 |     29.657 |     2.1
   36 |   0.6334 |     21.121 |   0.9048 |     28.536 |     2.2
   37 |   0.6004 |     19.629 |   0.9234 |     28.660 |     2.2
   38 |   0.5734 |     18.720 |   0.9137 |     27.913 |     2.3
   39 |   0.5495 |     17.784 |   0.9135 |     27.414 |     2.4
   40 |   0.5242 |     16.936 |   0.9193 |     26.604 |     2.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,194,658

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5074 |     67.184 |   1.9764 |     54.393 |     0.1
    2 |   1.7435 |     49.191 |   1.5740 |     45.763 |     0.1
    3 |   1.5001 |     46.036 |   1.4511 |     45.763 |     0.2
    4 |   1.4292 |     46.096 |   1.4188 |     45.763 |     0.2
    5 |   1.4022 |     46.058 |   1.3952 |     45.763 |     0.3
    6 |   1.3792 |     45.926 |   1.3714 |     45.701 |     0.3
    7 |   1.3522 |     45.920 |   1.3447 |     45.389 |     0.4
    8 |   1.3329 |     45.320 |   1.3304 |     46.044 |     0.5
    9 |   1.3187 |     45.056 |   1.3128 |     44.673 |     0.5
   10 |   1.3043 |     44.901 |   1.3007 |     44.579 |     0.6
   11 |   1.2880 |     44.461 |   1.2861 |     45.327 |     0.6
   12 |   1.2753 |     44.373 |   1.2761 |     44.081 |     0.7
   13 |   1.2623 |     44.120 |   1.2643 |     44.050 |     0.7
   14 |   1.2528 |     44.081 |   1.2554 |     44.486 |     0.8
   15 |   1.2431 |     44.020 |   1.2433 |     43.489 |     0.8
   16 |   1.2333 |     43.547 |   1.2360 |     43.458 |     0.9
   17 |   1.2240 |     43.563 |   1.2239 |     43.583 |     1.0
   18 |   1.2136 |     43.184 |   1.2255 |     43.520 |     1.0
   19 |   1.2062 |     43.057 |   1.2169 |     43.240 |     1.1
   20 |   1.2038 |     43.007 |   1.2074 |     43.738 |     1.1
   21 |   1.1949 |     42.743 |   1.2002 |     42.461 |     1.2
   22 |   1.1886 |     42.396 |   1.1956 |     44.050 |     1.2
   23 |   1.1807 |     42.446 |   1.1924 |     44.237 |     1.3
   24 |   1.1736 |     42.027 |   1.1789 |     42.274 |     1.3
   25 |   1.1657 |     41.741 |   1.1690 |     42.025 |     1.4
   26 |   1.1571 |     41.273 |   1.1747 |     41.340 |     1.5
   27 |   1.1510 |     40.954 |   1.1680 |     41.526 |     1.5
   28 |   1.1526 |     41.273 |   1.1671 |     41.090 |     1.6
   29 |   1.1427 |     40.717 |   1.1536 |     40.872 |     1.6
   30 |   1.1344 |     40.331 |   1.1516 |     40.717 |     1.7
   31 |   1.1298 |     40.249 |   1.1385 |     40.561 |     1.7
   32 |   1.1224 |     39.996 |   1.1431 |     40.592 |     1.8
   33 |   1.1173 |     39.896 |   1.1422 |     40.436 |     1.9
   34 |   1.1135 |     39.759 |   1.1307 |     39.844 |     1.9
   35 |   1.1093 |     39.533 |   1.1290 |     39.875 |     2.0
   36 |   1.1020 |     39.329 |   1.1285 |     39.844 |     2.0
   37 |   1.0980 |     39.109 |   1.1170 |     39.252 |     2.1
   38 |   1.0951 |     39.263 |   1.1190 |     39.813 |     2.1
   39 |   1.0919 |     38.790 |   1.1168 |     40.125 |     2.2
   40 |   1.0864 |     38.713 |   1.1160 |     39.844 |     2.2
   41 |   1.0836 |     38.707 |   1.1075 |     39.159 |     2.3
   42 |   1.0785 |     38.773 |   1.1068 |     39.190 |     2.4
   43 |   1.0747 |     38.481 |   1.1002 |     39.470 |     2.4
   44 |   1.0696 |     38.426 |   1.0995 |     40.125 |     2.5
   45 |   1.0711 |     38.828 |   1.0964 |     39.346 |     2.5
   46 |   1.0667 |     38.256 |   1.0909 |     39.626 |     2.6
   47 |   1.0623 |     38.344 |   1.0939 |     39.377 |     2.6
   48 |   1.0574 |     38.008 |   1.0831 |     38.723 |     2.7
   49 |   1.0544 |     37.942 |   1.0879 |     40.156 |     2.8
   50 |   1.0518 |     38.206 |   1.0952 |     40.717 |     2.8
   51 |   1.0512 |     38.344 |   1.0752 |     38.785 |     2.9
   52 |   1.0433 |     37.832 |   1.0824 |     38.785 |     2.9
   53 |   1.0406 |     37.474 |   1.0753 |     38.567 |     3.0
   54 |   1.0375 |     37.435 |   1.0766 |     39.252 |     3.0
   55 |   1.0348 |     37.143 |   1.0731 |     38.879 |     3.1
   56 |   1.0339 |     37.254 |   1.0697 |     38.629 |     3.1
   57 |   1.0259 |     37.171 |   1.0648 |     38.193 |     3.2
   58 |   1.0229 |     36.753 |   1.0595 |     37.975 |     3.3
   59 |   1.0214 |     36.841 |   1.0603 |     38.505 |     3.3
   60 |   1.0194 |     36.852 |   1.0622 |     38.442 |     3.4
   61 |   1.0152 |     36.670 |   1.0567 |     38.536 |     3.4
   62 |   1.0097 |     36.356 |   1.0549 |     38.816 |     3.5
   63 |   1.0064 |     36.235 |   1.0522 |     37.445 |     3.5
   64 |   1.0023 |     36.108 |   1.0425 |     38.100 |     3.6
   65 |   0.9940 |     35.866 |   1.0496 |     37.539 |     3.6
   66 |   0.9925 |     35.701 |   1.0558 |     37.695 |     3.7
   67 |   0.9871 |     35.717 |   1.0349 |     37.227 |     3.8
   68 |   0.9784 |     35.530 |   1.0288 |     37.352 |     3.8
   69 |   0.9749 |     34.974 |   1.0315 |     37.352 |     3.9
   70 |   0.9691 |     34.914 |   1.0191 |     36.511 |     3.9
   71 |   0.9623 |     34.280 |   1.0222 |     36.822 |     4.0
   72 |   0.9567 |     34.308 |   1.0297 |     37.165 |     4.0
   73 |   0.9530 |     34.192 |   1.0155 |     35.981 |     4.1
   74 |   0.9408 |     33.543 |   1.0099 |     35.670 |     4.2
   75 |   0.9352 |     33.251 |   1.0125 |     35.732 |     4.2
   76 |   0.9283 |     32.876 |   1.0100 |     35.483 |     4.3
   77 |   0.9259 |     32.816 |   1.0057 |     34.984 |     4.3
   78 |   0.9196 |     32.557 |   1.0127 |     35.296 |     4.4
   79 |   0.9058 |     31.968 |   1.0045 |     34.984 |     4.4
   80 |   0.8989 |     31.907 |   1.0049 |     35.327 |     4.5
   81 |   0.8884 |     31.208 |   0.9901 |     33.925 |     4.5
   82 |   0.8887 |     31.175 |   0.9929 |     34.611 |     4.6
   83 |   0.8803 |     30.900 |   0.9910 |     34.548 |     4.7
   84 |   0.8700 |     30.377 |   0.9927 |     34.206 |     4.7
   85 |   0.8655 |     30.228 |   0.9891 |     34.393 |     4.8
   86 |   0.8563 |     29.826 |   0.9904 |     33.707 |     4.8
   87 |   0.8491 |     29.479 |   0.9938 |     33.988 |     4.9
   88 |   0.8394 |     29.204 |   0.9777 |     33.769 |     4.9
   89 |   0.8369 |     29.066 |   0.9775 |     33.333 |     5.0
   90 |   0.8264 |     28.549 |   0.9832 |     33.676 |     5.0
   91 |   0.8180 |     28.103 |   0.9707 |     33.645 |     5.1
   92 |   0.8089 |     27.844 |   1.0434 |     35.234 |     5.2
   93 |   0.8117 |     27.871 |   0.9696 |     32.835 |     5.2
   94 |   0.7916 |     27.200 |   0.9693 |     32.150 |     5.3
   95 |   0.7811 |     26.671 |   0.9556 |     32.430 |     5.3
   96 |   0.7775 |     26.489 |   0.9658 |     32.710 |     5.4
   97 |   0.7758 |     26.517 |   0.9691 |     32.773 |     5.4
   98 |   0.8359 |     28.978 |   0.9770 |     32.804 |     5.5
   99 |   0.7832 |     26.875 |   0.9627 |     31.713 |     5.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,530,146

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0104 |     55.798 |   1.4769 |     45.732 |     0.1
    2 |   1.3972 |     45.705 |   1.3477 |     45.483 |     0.1
    3 |   1.3232 |     44.968 |   1.3076 |     44.299 |     0.2
    4 |   1.2807 |     44.037 |   1.2758 |     43.863 |     0.2
    5 |   1.2472 |     43.899 |   1.2329 |     43.115 |     0.3
    6 |   1.2177 |     42.985 |   1.2174 |     42.461 |     0.4
    7 |   1.1968 |     42.292 |   1.1957 |     41.994 |     0.4
    8 |   1.1706 |     41.499 |   1.1634 |     42.243 |     0.5
    9 |   1.1512 |     40.821 |   1.1518 |     40.062 |     0.5
   10 |   1.1319 |     40.414 |   1.1319 |     40.218 |     0.6
   11 |   1.1131 |     39.830 |   1.1179 |     39.813 |     0.7
   12 |   1.0934 |     39.027 |   1.1045 |     39.408 |     0.7
   13 |   1.0730 |     38.245 |   1.0860 |     38.692 |     0.8
   14 |   1.0549 |     37.303 |   1.0641 |     36.947 |     0.9
   15 |   1.0287 |     36.103 |   1.0480 |     37.321 |     0.9
   16 |   1.0043 |     35.437 |   1.0288 |     35.452 |     1.0
   17 |   0.9825 |     34.220 |   1.0184 |     35.327 |     1.0
   18 |   0.9585 |     33.075 |   1.0030 |     34.611 |     1.1
   19 |   0.9337 |     31.984 |   0.9876 |     33.645 |     1.2
   20 |   0.9165 |     31.236 |   0.9729 |     32.960 |     1.2
   21 |   0.8925 |     30.696 |   0.9551 |     32.399 |     1.3
   22 |   0.8657 |     29.297 |   0.9466 |     32.368 |     1.3
   23 |   0.8455 |     28.802 |   0.9395 |     32.399 |     1.4
   24 |   0.8217 |     27.739 |   0.9232 |     31.215 |     1.5
   25 |   0.7998 |     27.073 |   0.9078 |     30.654 |     1.5
   26 |   0.7760 |     26.484 |   0.9093 |     30.748 |     1.6
   27 |   0.7538 |     25.482 |   0.8955 |     29.751 |     1.6
   28 |   0.7269 |     24.353 |   0.8943 |     29.502 |     1.7
   29 |   0.7063 |     23.588 |   0.8835 |     29.128 |     1.8
   30 |   0.6781 |     22.542 |   0.8731 |     29.346 |     1.8
   31 |   0.6623 |     22.024 |   0.8748 |     27.944 |     1.9
   32 |   0.6438 |     21.347 |   0.8611 |     27.788 |     1.9
   33 |   0.6201 |     20.345 |   0.8733 |     27.290 |     2.0
   34 |   0.6003 |     19.794 |   0.8876 |     27.570 |     2.1
   35 |   0.5781 |     19.111 |   0.8775 |     27.539 |     2.1
   36 |   0.5592 |     18.269 |   0.8937 |     27.383 |     2.2
   37 |   0.5392 |     17.685 |   0.8507 |     25.576 |     2.2
   38 |   0.5251 |     17.217 |   0.8785 |     27.072 |     2.3
   39 |   0.5103 |     16.903 |   0.8803 |     27.352 |     2.4
   40 |   0.4789 |     15.422 |   0.8795 |     26.293 |     2.4
   41 |   0.4670 |     15.301 |   0.8865 |     26.729 |     2.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,691,170

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4434 |     63.523 |   1.8892 |     53.396 |     0.1
    2 |   1.6893 |     48.877 |   1.5279 |     45.763 |     0.2
    3 |   1.4767 |     46.052 |   1.4346 |     45.763 |     0.2
    4 |   1.4195 |     46.118 |   1.4010 |     45.763 |     0.3
    5 |   1.3924 |     45.959 |   1.3828 |     45.701 |     0.4
    6 |   1.3691 |     45.771 |   1.3542 |     45.452 |     0.5
    7 |   1.3410 |     45.441 |   1.3287 |     45.109 |     0.6
    8 |   1.3162 |     45.144 |   1.3082 |     45.389 |     0.7
    9 |   1.2939 |     44.753 |   1.2912 |     43.956 |     0.7
   10 |   1.2766 |     44.235 |   1.2748 |     43.645 |     0.8
   11 |   1.2535 |     43.646 |   1.2574 |     43.209 |     0.9
   12 |   1.2362 |     43.387 |   1.2356 |     42.773 |     1.0
   13 |   1.2191 |     42.655 |   1.2237 |     43.427 |     1.1
   14 |   1.2012 |     42.016 |   1.2075 |     42.648 |     1.1
   15 |   1.1857 |     41.620 |   1.1931 |     41.246 |     1.2
   16 |   1.1719 |     41.064 |   1.1855 |     41.402 |     1.3
   17 |   1.1527 |     40.530 |   1.1743 |     41.495 |     1.4
   18 |   1.1371 |     39.935 |   1.1614 |     40.592 |     1.5
   19 |   1.1210 |     39.208 |   1.1507 |     40.561 |     1.5
   20 |   1.1050 |     38.608 |   1.1348 |     39.533 |     1.6
   21 |   1.0860 |     38.168 |   1.1219 |     39.844 |     1.7
   22 |   1.0695 |     37.166 |   1.1191 |     38.754 |     1.8
   23 |   1.0543 |     36.962 |   1.1040 |     38.442 |     1.9
   24 |   1.0359 |     36.147 |   1.0985 |     37.757 |     2.0
   25 |   1.0224 |     35.651 |   1.0981 |     37.445 |     2.0
   26 |   1.0061 |     34.892 |   1.0901 |     37.321 |     2.1
   27 |   0.9857 |     34.330 |   1.0930 |     36.885 |     2.2
   28 |   0.9679 |     33.515 |   1.0730 |     36.012 |     2.3
   29 |   0.9515 |     33.185 |   1.0713 |     36.199 |     2.4
   30 |   0.9322 |     32.161 |   1.0677 |     35.576 |     2.4
   31 |   0.9159 |     31.582 |   1.0602 |     35.701 |     2.5
   32 |   0.8933 |     30.371 |   1.0662 |     35.202 |     2.6
   33 |   0.8759 |     29.854 |   1.0581 |     34.611 |     2.7
   34 |   0.8564 |     29.006 |   1.0461 |     34.112 |     2.8
   35 |   0.8366 |     28.180 |   1.0588 |     34.081 |     2.9
   36 |   0.8179 |     27.679 |   1.0514 |     34.112 |     2.9
   37 |   0.8003 |     26.666 |   1.0406 |     33.146 |     3.0
   38 |   0.7772 |     25.950 |   1.0356 |     32.617 |     3.1
   39 |   0.7539 |     25.047 |   1.0288 |     31.931 |     3.2
   40 |   0.7411 |     24.727 |   1.0422 |     31.651 |     3.3
   41 |   0.7188 |     23.808 |   1.0506 |     31.433 |     3.3
   42 |   0.7026 |     23.367 |   1.0317 |     31.277 |     3.4
   43 |   0.6822 |     22.575 |   1.0451 |     30.997 |     3.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 489,442

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0368 |     55.820 |   1.4707 |     45.919 |     0.0
    2 |   1.3832 |     45.485 |   1.3337 |     45.358 |     0.0
    3 |   1.2978 |     44.511 |   1.2725 |     43.832 |     0.1
    4 |   1.2558 |     43.679 |   1.2471 |     44.361 |     0.1
    5 |   1.2281 |     42.892 |   1.2263 |     42.492 |     0.1
    6 |   1.2002 |     41.983 |   1.2012 |     41.651 |     0.1
    7 |   1.1735 |     41.306 |   1.1732 |     41.028 |     0.1
    8 |   1.1460 |     40.596 |   1.1432 |     40.810 |     0.1
    9 |   1.1220 |     39.693 |   1.1247 |     39.315 |     0.2
   10 |   1.0998 |     38.773 |   1.1163 |     39.564 |     0.2
   11 |   1.0757 |     37.771 |   1.0883 |     38.069 |     0.2
   12 |   1.0540 |     36.780 |   1.0767 |     37.508 |     0.2
   13 |   1.0286 |     35.938 |   1.0477 |     36.449 |     0.2
   14 |   1.0021 |     34.781 |   1.0274 |     35.950 |     0.2
   15 |   0.9757 |     33.807 |   1.0218 |     35.919 |     0.3
   16 |   0.9487 |     32.722 |   0.9911 |     34.548 |     0.3
   17 |   0.9255 |     31.984 |   0.9859 |     33.707 |     0.3
   18 |   0.9068 |     31.401 |   0.9722 |     32.835 |     0.3
   19 |   0.8797 |     30.068 |   0.9580 |     33.053 |     0.3
   20 |   0.8588 |     29.837 |   0.9581 |     33.209 |     0.3
   21 |   0.8401 |     29.006 |   0.9399 |     31.558 |     0.4
   22 |   0.8268 |     28.251 |   0.9415 |     31.589 |     0.4
   23 |   0.8026 |     27.547 |   0.9261 |     31.558 |     0.4
   24 |   0.7760 |     26.489 |   0.9265 |     30.561 |     0.4
   25 |   0.7610 |     26.065 |   0.9055 |     29.720 |     0.4
   26 |   0.7438 |     25.361 |   0.9425 |     30.467 |     0.5
   27 |   0.7323 |     24.937 |   0.9046 |     29.813 |     0.5
   28 |   0.7055 |     23.984 |   0.8904 |     28.785 |     0.5
   29 |   0.6910 |     23.230 |   0.8813 |     28.318 |     0.5
   30 |   0.6648 |     22.492 |   0.8993 |     28.069 |     0.5
   31 |   0.6491 |     21.864 |   0.8769 |     28.442 |     0.5
   32 |   0.6336 |     21.330 |   0.8693 |     27.290 |     0.6
   33 |   0.6178 |     20.510 |   0.8798 |     28.255 |     0.6
   34 |   0.6078 |     20.510 |   0.8636 |     27.788 |     0.6
   35 |   0.5829 |     19.590 |   0.8725 |     27.508 |     0.6
   36 |   0.5678 |     19.089 |   0.8761 |     27.757 |     0.6
   37 |   0.5516 |     18.274 |   0.8718 |     27.819 |     0.6
   38 |   0.5442 |     18.170 |   0.8710 |     26.760 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,662,242

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1760 |     60.087 |   1.5886 |     45.763 |     0.1
    2 |   1.4679 |     46.080 |   1.4128 |     46.791 |     0.1
    3 |   1.3884 |     46.151 |   1.3653 |     46.044 |     0.2
    4 |   1.3450 |     45.474 |   1.3290 |     44.860 |     0.2
    5 |   1.3105 |     44.984 |   1.3051 |     43.738 |     0.3
    6 |   1.2821 |     44.153 |   1.2813 |     44.984 |     0.4
    7 |   1.2612 |     44.180 |   1.2603 |     43.863 |     0.4
    8 |   1.2417 |     43.993 |   1.2343 |     43.178 |     0.5
    9 |   1.2213 |     43.084 |   1.2205 |     43.022 |     0.5
   10 |   1.2027 |     42.842 |   1.2000 |     42.773 |     0.6
   11 |   1.1844 |     41.873 |   1.1872 |     42.181 |     0.7
   12 |   1.1713 |     41.499 |   1.1729 |     42.586 |     0.7
   13 |   1.1554 |     40.959 |   1.1585 |     42.741 |     0.8
   14 |   1.1448 |     40.777 |   1.1479 |     40.903 |     0.9
   15 |   1.1303 |     40.293 |   1.1415 |     40.280 |     0.9
   16 |   1.1192 |     39.902 |   1.1202 |     40.810 |     1.0
   17 |   1.1061 |     39.561 |   1.1144 |     39.875 |     1.0
   18 |   1.0947 |     38.927 |   1.1028 |     39.408 |     1.1
   19 |   1.0811 |     38.421 |   1.0965 |     38.754 |     1.2
   20 |   1.0662 |     38.052 |   1.0835 |     38.100 |     1.2
   21 |   1.0540 |     37.132 |   1.0711 |     37.882 |     1.3
   22 |   1.0434 |     36.808 |   1.0784 |     38.224 |     1.3
   23 |   1.0337 |     36.604 |   1.0598 |     36.885 |     1.4
   24 |   1.0220 |     35.976 |   1.0469 |     36.947 |     1.5
   25 |   1.0113 |     35.585 |   1.0361 |     35.888 |     1.5
   26 |   1.0009 |     35.387 |   1.0302 |     36.106 |     1.6
   27 |   0.9862 |     34.655 |   1.0196 |     35.514 |     1.6
   28 |   0.9845 |     34.781 |   1.0178 |     35.857 |     1.7
   29 |   0.9600 |     33.862 |   0.9997 |     34.486 |     1.8
   30 |   0.9505 |     33.521 |   1.0113 |     35.607 |     1.8
   31 |   0.9373 |     32.931 |   0.9978 |     34.486 |     1.9
   32 |   0.9273 |     32.711 |   0.9835 |     33.925 |     1.9
   33 |   0.9145 |     32.419 |   0.9840 |     33.925 |     2.0
   34 |   0.9070 |     32.050 |   0.9729 |     33.115 |     2.1
   35 |   0.8884 |     30.922 |   0.9660 |     33.894 |     2.1
   36 |   0.8758 |     30.878 |   0.9664 |     32.274 |     2.2
   37 |   0.8631 |     30.112 |   0.9421 |     32.025 |     2.3
   38 |   0.8495 |     29.683 |   0.9633 |     32.648 |     2.3
   39 |   0.8350 |     28.857 |   0.9537 |     32.150 |     2.4
   40 |   0.8233 |     28.339 |   0.9314 |     30.405 |     2.4
   41 |   0.8151 |     28.411 |   0.9314 |     30.717 |     2.5
   42 |   0.7951 |     27.370 |   0.9224 |     30.685 |     2.6
   43 |   0.7830 |     27.189 |   0.9316 |     30.872 |     2.6
   44 |   0.7753 |     26.572 |   0.9281 |     30.561 |     2.7
   45 |   0.7589 |     26.142 |   0.9122 |     29.657 |     2.7
   46 |   0.7465 |     25.658 |   0.9213 |     30.343 |     2.8
   47 |   0.7358 |     25.405 |   0.9184 |     29.938 |     2.9
   48 |   0.7180 |     24.744 |   0.9130 |     29.751 |     2.9
   49 |   0.7132 |     24.540 |   0.9020 |     28.910 |     3.0
   50 |   0.7093 |     24.116 |   0.9017 |     29.720 |     3.0
   51 |   0.6920 |     23.478 |   0.9101 |     29.470 |     3.1
   52 |   0.6802 |     22.938 |   0.9008 |     28.505 |     3.2
   53 |   0.6706 |     22.756 |   0.8943 |     27.944 |     3.2
   54 |   0.6517 |     22.013 |   0.8940 |     28.006 |     3.3
   55 |   0.6435 |     21.688 |   0.9064 |     29.034 |     3.4
   56 |   0.6332 |     21.633 |   0.9143 |     27.601 |     3.4
   57 |   0.6249 |     21.121 |   0.9261 |     28.660 |     3.5
   58 |   0.6141 |     20.736 |   0.9076 |     27.570 |     3.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 835,106

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5296 |     67.619 |   1.9672 |     59.159 |     0.0
    2 |   1.7342 |     49.912 |   1.5614 |     45.763 |     0.1
    3 |   1.4959 |     46.030 |   1.4499 |     45.763 |     0.1
    4 |   1.4263 |     46.080 |   1.4082 |     45.763 |     0.2
    5 |   1.3963 |     46.041 |   1.3927 |     45.701 |     0.2
    6 |   1.3773 |     45.870 |   1.3665 |     45.670 |     0.2
    7 |   1.3531 |     45.491 |   1.3504 |     44.922 |     0.3
    8 |   1.3335 |     45.221 |   1.3360 |     44.735 |     0.3
    9 |   1.3177 |     45.221 |   1.3173 |     44.829 |     0.4
   10 |   1.3003 |     44.791 |   1.2999 |     43.583 |     0.4
   11 |   1.2860 |     44.147 |   1.2880 |     42.866 |     0.4
   12 |   1.2744 |     43.795 |   1.2782 |     44.237 |     0.5
   13 |   1.2605 |     43.442 |   1.2581 |     43.489 |     0.5
   14 |   1.2517 |     43.178 |   1.2495 |     42.710 |     0.6
   15 |   1.2374 |     43.266 |   1.2413 |     43.022 |     0.6
   16 |   1.2253 |     42.672 |   1.2272 |     42.243 |     0.7
   17 |   1.2121 |     42.176 |   1.2194 |     42.399 |     0.7
   18 |   1.1989 |     41.785 |   1.2067 |     41.807 |     0.7
   19 |   1.1882 |     41.796 |   1.1965 |     41.620 |     0.8
   20 |   1.1724 |     41.218 |   1.1874 |     41.246 |     0.8
   21 |   1.1584 |     40.866 |   1.1712 |     40.187 |     0.9
   22 |   1.1458 |     40.238 |   1.1737 |     40.872 |     0.9
   23 |   1.1363 |     39.913 |   1.1540 |     39.782 |     0.9
   24 |   1.1177 |     39.082 |   1.1489 |     39.190 |     1.0
   25 |   1.1041 |     38.289 |   1.1350 |     38.193 |     1.0
   26 |   1.0903 |     37.760 |   1.1303 |     39.034 |     1.1
   27 |   1.0698 |     36.830 |   1.1252 |     38.255 |     1.1
   28 |   1.0520 |     36.307 |   1.1079 |     37.383 |     1.1
   29 |   1.0365 |     35.310 |   1.1031 |     37.788 |     1.2
   30 |   1.0176 |     34.831 |   1.1048 |     38.037 |     1.2
   31 |   1.0002 |     33.851 |   1.0857 |     36.885 |     1.3
   32 |   0.9820 |     33.229 |   1.0763 |     36.729 |     1.3
   33 |   0.9591 |     32.430 |   1.0751 |     36.044 |     1.4
   34 |   0.9366 |     31.704 |   1.0675 |     35.202 |     1.4
   35 |   0.9120 |     30.674 |   1.0566 |     34.922 |     1.4
   36 |   0.8860 |     29.622 |   1.0423 |     34.361 |     1.5
   37 |   0.8716 |     28.995 |   1.0482 |     34.330 |     1.5
   38 |   0.8453 |     28.053 |   1.0328 |     33.832 |     1.6
   39 |   0.8159 |     26.682 |   1.0318 |     33.832 |     1.6
   40 |   0.8071 |     26.688 |   1.0187 |     32.773 |     1.6
   41 |   0.7698 |     25.317 |   1.0140 |     31.963 |     1.7
   42 |   0.7413 |     24.023 |   1.0215 |     31.620 |     1.7
   43 |   0.7281 |     23.918 |   1.0303 |     31.838 |     1.8
   44 |   0.7024 |     22.597 |   0.9982 |     30.810 |     1.8
   45 |   0.6871 |     22.195 |   1.0211 |     31.651 |     1.9
   46 |   0.6576 |     21.127 |   1.0140 |     30.280 |     1.9
   47 |   0.6344 |     20.526 |   1.0157 |     30.187 |     1.9
   48 |   0.6158 |     19.778 |   1.0353 |     30.062 |     2.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 555,810

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4997 |     67.129 |   1.9489 |     57.664 |     0.0
    2 |   1.7334 |     49.884 |   1.5664 |     45.763 |     0.1
    3 |   1.4933 |     46.036 |   1.4482 |     45.763 |     0.1
    4 |   1.4173 |     46.069 |   1.3986 |     45.763 |     0.1
    5 |   1.3812 |     45.870 |   1.3737 |     45.794 |     0.2
    6 |   1.3498 |     45.727 |   1.3474 |     46.667 |     0.2
    7 |   1.3207 |     45.331 |   1.3167 |     45.234 |     0.2
    8 |   1.2964 |     44.626 |   1.2908 |     44.081 |     0.3
    9 |   1.2750 |     44.224 |   1.2699 |     43.551 |     0.3
   10 |   1.2590 |     43.701 |   1.2554 |     43.738 |     0.3
   11 |   1.2433 |     43.613 |   1.2416 |     43.956 |     0.4
   12 |   1.2283 |     43.018 |   1.2280 |     42.928 |     0.4
   13 |   1.2151 |     42.809 |   1.2167 |     42.928 |     0.4
   14 |   1.2043 |     42.594 |   1.2117 |     42.741 |     0.5
   15 |   1.1927 |     42.181 |   1.1941 |     43.925 |     0.5
   16 |   1.1838 |     41.829 |   1.1866 |     42.118 |     0.5
   17 |   1.1723 |     41.245 |   1.1839 |     42.181 |     0.6
   18 |   1.1636 |     41.003 |   1.1763 |     42.243 |     0.6
   19 |   1.1595 |     40.882 |   1.1675 |     41.402 |     0.6
   20 |   1.1505 |     40.337 |   1.1611 |     41.184 |     0.7
   21 |   1.1449 |     40.618 |   1.1552 |     40.903 |     0.7
   22 |   1.1352 |     39.963 |   1.1550 |     41.184 |     0.7
   23 |   1.1325 |     40.073 |   1.1462 |     40.623 |     0.8
   24 |   1.1268 |     39.819 |   1.1465 |     41.558 |     0.8
   25 |   1.1232 |     39.913 |   1.1410 |     41.340 |     0.8
   26 |   1.1240 |     39.852 |   1.1434 |     42.025 |     0.9
   27 |   1.1168 |     39.858 |   1.1298 |     40.592 |     0.9
   28 |   1.1078 |     39.368 |   1.1282 |     40.249 |     0.9
   29 |   1.1033 |     39.236 |   1.1215 |     39.907 |     1.0
   30 |   1.1008 |     39.247 |   1.1209 |     40.997 |     1.0
   31 |   1.0968 |     39.362 |   1.1143 |     39.751 |     1.0
   32 |   1.0918 |     39.093 |   1.1110 |     39.782 |     1.1
   33 |   1.0875 |     38.806 |   1.1123 |     39.564 |     1.1
   34 |   1.0831 |     38.459 |   1.1027 |     39.470 |     1.1
   35 |   1.0787 |     38.581 |   1.1090 |     39.813 |     1.2
   36 |   1.0773 |     38.581 |   1.1001 |     40.312 |     1.2
   37 |   1.0724 |     38.625 |   1.0938 |     39.159 |     1.2
   38 |   1.0707 |     38.625 |   1.0952 |     39.439 |     1.3
   39 |   1.0644 |     38.283 |   1.0871 |     39.003 |     1.3
   40 |   1.0579 |     38.157 |   1.0915 |     39.315 |     1.3
   41 |   1.0559 |     37.870 |   1.0809 |     38.567 |     1.4
   42 |   1.0546 |     37.810 |   1.0883 |     39.097 |     1.4
   43 |   1.0509 |     37.848 |   1.0778 |     38.692 |     1.4
   44 |   1.0437 |     37.573 |   1.0988 |     39.782 |     1.5
   45 |   1.0410 |     37.534 |   1.0730 |     38.411 |     1.5
   46 |   1.0370 |     37.595 |   1.0766 |     38.692 |     1.5
   47 |   1.0313 |     36.989 |   1.0619 |     38.567 |     1.6
   48 |   1.0256 |     37.166 |   1.0658 |     37.757 |     1.6
   49 |   1.0237 |     37.033 |   1.0617 |     38.723 |     1.6
   50 |   1.0172 |     36.439 |   1.0643 |     38.287 |     1.7
   51 |   1.0154 |     36.631 |   1.0578 |     37.913 |     1.7
   52 |   1.0120 |     36.334 |   1.0484 |     38.162 |     1.7
   53 |   1.0049 |     35.982 |   1.0487 |     36.604 |     1.8
   54 |   1.0005 |     35.932 |   1.0461 |     37.290 |     1.8
   55 |   0.9977 |     35.712 |   1.0504 |     36.698 |     1.8
   56 |   0.9968 |     35.668 |   1.0438 |     37.259 |     1.9
   57 |   0.9934 |     35.690 |   1.0421 |     37.196 |     1.9
   58 |   0.9901 |     35.310 |   1.0351 |     36.480 |     1.9
   59 |   0.9809 |     34.947 |   1.0393 |     36.791 |     2.0
   60 |   0.9762 |     34.704 |   1.0348 |     36.231 |     2.0
   61 |   0.9747 |     34.561 |   1.0340 |     36.791 |     2.0
   62 |   0.9692 |     34.490 |   1.0352 |     36.231 |     2.1
   63 |   0.9670 |     34.192 |   1.0251 |     35.265 |     2.1
   64 |   0.9586 |     34.049 |   1.0302 |     35.358 |     2.1
   65 |   0.9599 |     34.104 |   1.0344 |     35.981 |     2.2
   66 |   0.9540 |     33.790 |   1.0135 |     35.639 |     2.2
   67 |   0.9525 |     33.548 |   1.0275 |     35.140 |     2.2
   68 |   0.9434 |     33.394 |   1.0206 |     35.234 |     2.3
   69 |   0.9396 |     33.223 |   1.0045 |     34.112 |     2.3
   70 |   0.9304 |     32.761 |   1.0131 |     34.486 |     2.3
   71 |   0.9301 |     32.898 |   1.0094 |     34.517 |     2.4
   72 |   0.9243 |     32.585 |   1.0058 |     34.361 |     2.4
   73 |   0.9201 |     32.375 |   0.9940 |     34.237 |     2.4
   74 |   0.9143 |     32.205 |   0.9985 |     34.206 |     2.5
   75 |   0.9044 |     31.632 |   0.9982 |     33.863 |     2.5
   76 |   0.9009 |     31.648 |   0.9985 |     34.143 |     2.5
   77 |   0.8957 |     31.494 |   0.9948 |     33.769 |     2.6
   78 |   0.8913 |     30.988 |   0.9936 |     32.928 |     2.6
   79 |   0.8851 |     30.889 |   0.9833 |     33.302 |     2.6
   80 |   0.8808 |     31.059 |   0.9746 |     33.271 |     2.7
   81 |   0.8771 |     30.591 |   0.9872 |     33.240 |     2.7
   82 |   0.8720 |     30.311 |   0.9748 |     32.586 |     2.7
   83 |   0.8600 |     29.765 |   0.9658 |     32.586 |     2.8
   84 |   0.8504 |     29.352 |   0.9721 |     32.461 |     2.8
   85 |   0.8545 |     29.611 |   0.9684 |     32.555 |     2.8
   86 |   0.8384 |     28.995 |   0.9725 |     33.053 |     2.9
   87 |   0.8289 |     28.538 |   0.9616 |     32.150 |     2.9
   88 |   0.8219 |     28.284 |   0.9507 |     32.430 |     2.9
   89 |   0.8301 |     28.350 |   0.9692 |     33.146 |     3.0
   90 |   0.8139 |     27.789 |   0.9465 |     32.118 |     3.0
   91 |   0.8100 |     27.574 |   0.9438 |     31.277 |     3.0
   92 |   0.7996 |     27.238 |   0.9581 |     31.931 |     3.1
   93 |   0.7900 |     26.963 |   0.9457 |     31.745 |     3.1
   94 |   0.7816 |     26.511 |   0.9380 |     31.340 |     3.1
   95 |   0.7733 |     26.390 |   0.9412 |     31.526 |     3.2
   96 |   0.7678 |     25.950 |   0.9477 |     31.433 |     3.2
   97 |   0.7601 |     25.774 |   0.9392 |     31.838 |     3.2
   98 |   0.7536 |     25.482 |   0.9303 |     31.153 |     3.3
   99 |   0.7487 |     25.553 |   0.9430 |     31.184 |     3.3
  100 |   0.7432 |     25.168 |   0.9485 |     31.807 |     3.3
  101 |   0.7361 |     24.871 |   0.9370 |     30.997 |     3.4
  102 |   0.7323 |     24.667 |   0.9507 |     31.869 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 403,682

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5020 |     66.590 |   1.9351 |     53.271 |     0.0
    2 |   1.7094 |     48.805 |   1.5409 |     45.763 |     0.1
    3 |   1.4797 |     46.014 |   1.4359 |     45.763 |     0.1
    4 |   1.4134 |     45.920 |   1.3952 |     45.639 |     0.1
    5 |   1.3714 |     45.628 |   1.3685 |     44.673 |     0.1
    6 |   1.3442 |     45.050 |   1.3388 |     45.265 |     0.2
    7 |   1.3216 |     45.105 |   1.3219 |     45.109 |     0.2
    8 |   1.3045 |     44.758 |   1.3043 |     44.455 |     0.2
    9 |   1.2901 |     44.384 |   1.2935 |     44.361 |     0.2
   10 |   1.2778 |     44.400 |   1.2849 |     44.673 |     0.3
   11 |   1.2649 |     44.395 |   1.2650 |     43.925 |     0.3
   12 |   1.2552 |     44.120 |   1.2567 |     43.676 |     0.3
   13 |   1.2452 |     43.872 |   1.2469 |     43.956 |     0.3
   14 |   1.2367 |     43.652 |   1.2415 |     42.866 |     0.4
   15 |   1.2265 |     43.382 |   1.2307 |     44.237 |     0.4
   16 |   1.2169 |     42.996 |   1.2188 |     44.393 |     0.4
   17 |   1.2059 |     42.815 |   1.2133 |     43.396 |     0.4
   18 |   1.1956 |     42.424 |   1.2009 |     42.991 |     0.5
   19 |   1.1857 |     41.895 |   1.1891 |     43.115 |     0.5
   20 |   1.1777 |     41.675 |   1.1796 |     41.931 |     0.5
   21 |   1.1697 |     41.301 |   1.1742 |     41.526 |     0.5
   22 |   1.1618 |     41.152 |   1.1644 |     41.121 |     0.6
   23 |   1.1551 |     40.838 |   1.1567 |     40.872 |     0.6
   24 |   1.1472 |     40.728 |   1.1578 |     40.997 |     0.6
   25 |   1.1414 |     40.469 |   1.1554 |     41.651 |     0.6
   26 |   1.1360 |     40.623 |   1.1468 |     40.841 |     0.7
   27 |   1.1304 |     40.645 |   1.1411 |     40.810 |     0.7
   28 |   1.1214 |     39.786 |   1.1313 |     41.371 |     0.7
   29 |   1.1161 |     39.709 |   1.1301 |     39.907 |     0.7
   30 |   1.1092 |     39.577 |   1.1275 |     39.595 |     0.8
   31 |   1.1036 |     39.517 |   1.1170 |     40.031 |     0.8
   32 |   1.0960 |     39.010 |   1.1096 |     39.190 |     0.8
   33 |   1.0918 |     39.186 |   1.1019 |     39.252 |     0.8
   34 |   1.0843 |     38.878 |   1.1008 |     39.346 |     0.9
   35 |   1.0796 |     38.592 |   1.0936 |     38.567 |     0.9
   36 |   1.0719 |     38.267 |   1.0948 |     38.442 |     0.9
   37 |   1.0658 |     37.870 |   1.0894 |     38.692 |     0.9
   38 |   1.0590 |     37.639 |   1.0801 |     37.726 |     1.0
   39 |   1.0465 |     37.160 |   1.0796 |     37.726 |     1.0
   40 |   1.0449 |     37.099 |   1.0654 |     36.604 |     1.0
   41 |   1.0308 |     36.367 |   1.0680 |     37.975 |     1.0
   42 |   1.0286 |     36.356 |   1.0640 |     37.539 |     1.1
   43 |   1.0201 |     35.828 |   1.0506 |     36.012 |     1.1
   44 |   1.0115 |     35.861 |   1.0422 |     36.417 |     1.1
   45 |   1.0041 |     35.299 |   1.0389 |     36.106 |     1.1
   46 |   0.9940 |     35.002 |   1.0374 |     35.670 |     1.2
   47 |   0.9952 |     35.018 |   1.0370 |     36.573 |     1.2
   48 |   0.9870 |     35.128 |   1.0274 |     35.857 |     1.2
   49 |   0.9752 |     34.539 |   1.0283 |     36.542 |     1.2
   50 |   0.9672 |     34.016 |   1.0142 |     34.798 |     1.3
   51 |   0.9581 |     33.603 |   1.0130 |     35.421 |     1.3
   52 |   0.9490 |     33.515 |   1.0151 |     35.576 |     1.3
   53 |   0.9461 |     33.410 |   1.0014 |     34.922 |     1.3
   54 |   0.9416 |     33.185 |   0.9915 |     33.988 |     1.4
   55 |   0.9325 |     32.909 |   1.0083 |     34.673 |     1.4
   56 |   0.9190 |     32.111 |   0.9976 |     34.143 |     1.4
   57 |   0.9644 |     33.559 |   1.0220 |     35.888 |     1.4
   58 |   0.9437 |     33.196 |   1.0108 |     35.888 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 772,258

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2034 |     59.448 |   1.6296 |     48.785 |     0.0
    2 |   1.4865 |     46.146 |   1.4213 |     45.763 |     0.1
    3 |   1.3985 |     46.025 |   1.3867 |     46.729 |     0.1
    4 |   1.3651 |     45.738 |   1.3567 |     45.358 |     0.1
    5 |   1.3338 |     45.551 |   1.3269 |     44.548 |     0.1
    6 |   1.3090 |     45.171 |   1.3045 |     45.421 |     0.2
    7 |   1.2905 |     44.901 |   1.2916 |     44.891 |     0.2
    8 |   1.2689 |     44.301 |   1.2659 |     43.614 |     0.2
    9 |   1.2465 |     43.745 |   1.2473 |     43.396 |     0.2
   10 |   1.2275 |     43.106 |   1.2340 |     42.866 |     0.3
   11 |   1.2115 |     42.815 |   1.2349 |     44.112 |     0.3
   12 |   1.1983 |     42.336 |   1.2068 |     43.832 |     0.3
   13 |   1.1832 |     42.077 |   1.2043 |     43.520 |     0.3
   14 |   1.1708 |     41.444 |   1.1806 |     41.153 |     0.4
   15 |   1.1601 |     41.141 |   1.1772 |     41.340 |     0.4
   16 |   1.1474 |     40.799 |   1.1578 |     42.586 |     0.4
   17 |   1.1363 |     40.387 |   1.1629 |     41.340 |     0.4
   18 |   1.1282 |     40.260 |   1.1525 |     40.997 |     0.5
   19 |   1.1215 |     40.326 |   1.1428 |     40.249 |     0.5
   20 |   1.1080 |     39.621 |   1.1272 |     39.907 |     0.5
   21 |   1.0934 |     39.280 |   1.1196 |     39.128 |     0.6
   22 |   1.0817 |     38.696 |   1.1112 |     39.159 |     0.6
   23 |   1.0757 |     38.454 |   1.1079 |     38.660 |     0.6
   24 |   1.0617 |     37.925 |   1.0881 |     38.629 |     0.6
   25 |   1.0514 |     37.771 |   1.0816 |     38.754 |     0.7
   26 |   1.0387 |     37.545 |   1.0856 |     38.037 |     0.7
   27 |   1.0266 |     36.819 |   1.0823 |     37.882 |     0.7
   28 |   1.0170 |     36.626 |   1.0669 |     37.445 |     0.7
   29 |   1.0062 |     36.163 |   1.0689 |     38.193 |     0.8
   30 |   0.9957 |     35.662 |   1.0507 |     37.196 |     0.8
   31 |   0.9819 |     35.079 |   1.0375 |     36.947 |     0.8
   32 |   0.9665 |     34.715 |   1.0364 |     36.199 |     0.8
   33 |   0.9528 |     33.779 |   1.0348 |     35.265 |     0.9
   34 |   0.9430 |     33.686 |   1.0252 |     35.171 |     0.9
   35 |   0.9221 |     32.623 |   1.0148 |     34.704 |     0.9
   36 |   0.9072 |     32.001 |   1.0275 |     35.389 |     0.9
   37 |   0.8938 |     31.577 |   1.0170 |     34.268 |     1.0
   38 |   0.8762 |     30.922 |   0.9948 |     34.393 |     1.0
   39 |   0.8593 |     30.035 |   0.9838 |     33.333 |     1.0
   40 |   0.8431 |     29.479 |   0.9937 |     33.707 |     1.1
   41 |   0.8319 |     28.890 |   0.9818 |     32.835 |     1.1
   42 |   0.8121 |     28.262 |   0.9718 |     32.523 |     1.1
   43 |   0.8004 |     27.882 |   0.9573 |     31.931 |     1.1
   44 |   0.7787 |     26.847 |   0.9610 |     31.838 |     1.2
   45 |   0.7610 |     26.093 |   0.9487 |     31.308 |     1.2
   46 |   0.7469 |     25.328 |   0.9518 |     31.495 |     1.2
   47 |   0.7262 |     24.634 |   0.9459 |     30.498 |     1.2
   48 |   0.7091 |     23.907 |   0.9329 |     30.093 |     1.3
   49 |   0.6957 |     23.489 |   0.9277 |     29.938 |     1.3
   50 |   0.6792 |     22.894 |   0.9367 |     29.533 |     1.3
   51 |   0.6845 |     23.010 |   0.9265 |     29.564 |     1.3
   52 |   0.6464 |     21.699 |   0.9347 |     29.688 |     1.4
   53 |   0.6345 |     21.071 |   0.9300 |     30.343 |     1.4
   54 |   0.6154 |     20.378 |   0.9287 |     29.034 |     1.4
   55 |   0.6010 |     19.965 |   0.9367 |     29.221 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 750,946

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5499 |     68.842 |   1.9784 |     58.941 |     0.0
    2 |   1.7522 |     50.160 |   1.5690 |     45.763 |     0.1
    3 |   1.5006 |     46.063 |   1.4500 |     45.763 |     0.1
    4 |   1.4279 |     46.113 |   1.4144 |     45.763 |     0.2
    5 |   1.3946 |     45.992 |   1.3800 |     45.763 |     0.2
    6 |   1.3752 |     45.788 |   1.3666 |     45.047 |     0.2
    7 |   1.3567 |     45.810 |   1.3504 |     45.016 |     0.3
    8 |   1.3341 |     45.535 |   1.3284 |     45.234 |     0.3
    9 |   1.3128 |     45.281 |   1.3094 |     45.981 |     0.4
   10 |   1.2979 |     44.934 |   1.2980 |     44.081 |     0.4
   11 |   1.2836 |     44.411 |   1.2759 |     43.863 |     0.4
   12 |   1.2653 |     43.822 |   1.2634 |     42.586 |     0.5
   13 |   1.2532 |     43.200 |   1.2466 |     44.081 |     0.5
   14 |   1.2377 |     43.057 |   1.2393 |     42.586 |     0.6
   15 |   1.2230 |     42.457 |   1.2250 |     43.769 |     0.6
   16 |   1.2082 |     42.204 |   1.2114 |     41.994 |     0.6
   17 |   1.1975 |     41.824 |   1.1951 |     41.963 |     0.7
   18 |   1.1867 |     41.493 |   1.1881 |     41.838 |     0.7
   19 |   1.1894 |     41.851 |   1.1882 |     41.776 |     0.8
   20 |   1.1721 |     40.998 |   1.1707 |     41.277 |     0.8
   21 |   1.1673 |     41.064 |   1.1646 |     40.872 |     0.8
   22 |   1.1546 |     40.563 |   1.1597 |     41.028 |     0.9
   23 |   1.1436 |     40.111 |   1.1496 |     40.530 |     0.9
   24 |   1.1340 |     39.797 |   1.1477 |     40.156 |     1.0
   25 |   1.1227 |     39.572 |   1.1341 |     40.000 |     1.0
   26 |   1.1170 |     39.252 |   1.1410 |     39.875 |     1.0
   27 |   1.1119 |     39.021 |   1.1172 |     39.252 |     1.1
   28 |   1.0939 |     38.300 |   1.1148 |     40.156 |     1.1
   29 |   1.0863 |     38.437 |   1.1049 |     39.470 |     1.2
   30 |   1.0785 |     37.925 |   1.1379 |     39.782 |     1.2
   31 |   1.0908 |     38.592 |   1.0953 |     38.567 |     1.2
   32 |   1.0699 |     37.578 |   1.0923 |     38.255 |     1.3
   33 |   1.0582 |     37.099 |   1.0835 |     37.975 |     1.3
   34 |   1.0450 |     36.549 |   1.0763 |     37.259 |     1.4
   35 |   1.0329 |     36.307 |   1.0769 |     36.729 |     1.4
   36 |   1.0253 |     35.772 |   1.0719 |     37.539 |     1.4
   37 |   1.0173 |     35.993 |   1.0670 |     36.885 |     1.5
   38 |   1.0147 |     35.541 |   1.0618 |     37.009 |     1.5
   39 |   0.9951 |     34.732 |   1.0599 |     36.885 |     1.6
   40 |   0.9933 |     34.858 |   1.0674 |     37.227 |     1.6
   41 |   0.9797 |     34.401 |   1.0511 |     36.044 |     1.7
   42 |   0.9676 |     33.895 |   1.0456 |     36.137 |     1.7
   43 |   0.9544 |     33.454 |   1.0463 |     35.763 |     1.7
   44 |   0.9466 |     33.075 |   1.0486 |     35.857 |     1.8
   45 |   0.9332 |     32.860 |   1.0470 |     36.012 |     1.8
   46 |   0.9256 |     32.568 |   1.0349 |     35.483 |     1.9
   47 |   0.9123 |     32.045 |   1.0356 |     35.202 |     1.9
   48 |   0.9002 |     31.274 |   1.0329 |     35.047 |     1.9
   49 |   0.8923 |     31.522 |   1.0248 |     35.109 |     2.0
   50 |   0.8837 |     30.905 |   1.0286 |     34.829 |     2.0
   51 |   0.8761 |     30.547 |   1.0250 |     33.956 |     2.1
   52 |   0.8672 |     30.322 |   1.0268 |     33.925 |     2.1
   53 |   0.8463 |     29.479 |   1.0257 |     33.832 |     2.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 324,386

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5438 |     69.210 |   1.9376 |     48.847 |     0.0
    2 |   1.7284 |     48.833 |   1.5523 |     45.763 |     0.0
    3 |   1.4889 |     46.041 |   1.4410 |     45.763 |     0.1
    4 |   1.4173 |     46.036 |   1.3960 |     45.763 |     0.1
    5 |   1.3771 |     46.030 |   1.3607 |     45.670 |     0.1
    6 |   1.3441 |     45.535 |   1.3305 |     46.106 |     0.1
    7 |   1.3178 |     45.050 |   1.3108 |     45.109 |     0.1
    8 |   1.2978 |     44.951 |   1.2915 |     43.801 |     0.1
    9 |   1.2809 |     44.351 |   1.2759 |     43.551 |     0.2
   10 |   1.2619 |     43.998 |   1.2599 |     45.047 |     0.2
   11 |   1.2486 |     43.866 |   1.2412 |     43.551 |     0.2
   12 |   1.2323 |     43.239 |   1.2279 |     44.330 |     0.2
   13 |   1.2188 |     42.842 |   1.2166 |     42.835 |     0.2
   14 |   1.2062 |     42.248 |   1.2040 |     43.271 |     0.2
   15 |   1.1953 |     42.192 |   1.1982 |     42.056 |     0.3
   16 |   1.1870 |     41.851 |   1.1930 |     42.991 |     0.3
   17 |   1.1794 |     41.686 |   1.1807 |     40.935 |     0.3
   18 |   1.1725 |     41.581 |   1.1741 |     41.308 |     0.3
   19 |   1.1631 |     41.009 |   1.1694 |     40.654 |     0.3
   20 |   1.1572 |     41.108 |   1.1628 |     40.841 |     0.3
   21 |   1.1488 |     40.596 |   1.1564 |     40.685 |     0.4
   22 |   1.1470 |     40.634 |   1.1580 |     41.776 |     0.4
   23 |   1.1420 |     40.684 |   1.1476 |     40.717 |     0.4
   24 |   1.1325 |     40.320 |   1.1445 |     40.685 |     0.4
   25 |   1.1277 |     39.880 |   1.1461 |     40.187 |     0.4
   26 |   1.1260 |     40.293 |   1.1432 |     40.530 |     0.5
   27 |   1.1201 |     39.968 |   1.1344 |     40.280 |     0.5
   28 |   1.1117 |     39.731 |   1.1228 |     39.938 |     0.5
   29 |   1.1093 |     39.451 |   1.1235 |     39.657 |     0.5
   30 |   1.1056 |     39.153 |   1.1229 |     40.685 |     0.5
   31 |   1.0988 |     39.170 |   1.1126 |     39.377 |     0.5
   32 |   1.0946 |     39.269 |   1.1079 |     38.723 |     0.6
   33 |   1.0891 |     38.724 |   1.1057 |     38.692 |     0.6
   34 |   1.0861 |     38.498 |   1.1035 |     38.629 |     0.6
   35 |   1.0829 |     38.377 |   1.0984 |     38.536 |     0.6
   36 |   1.0752 |     38.366 |   1.0911 |     38.255 |     0.6
   37 |   1.0720 |     38.135 |   1.0925 |     38.287 |     0.7
   38 |   1.0638 |     37.958 |   1.0915 |     38.474 |     0.7
   39 |   1.0604 |     37.870 |   1.0807 |     38.037 |     0.7
   40 |   1.0546 |     37.595 |   1.0754 |     38.505 |     0.7
   41 |   1.0488 |     37.358 |   1.0748 |     38.349 |     0.7
   42 |   1.0492 |     37.259 |   1.0730 |     38.287 |     0.7
   43 |   1.0443 |     37.099 |   1.0707 |     37.539 |     0.8
   44 |   1.0378 |     36.709 |   1.0651 |     36.978 |     0.8
   45 |   1.0348 |     37.132 |   1.0805 |     37.819 |     0.8
   46 |   1.0313 |     36.549 |   1.0597 |     36.667 |     0.8
   47 |   1.0257 |     36.571 |   1.0591 |     38.287 |     0.8
   48 |   1.0200 |     36.373 |   1.0540 |     36.542 |     0.8
   49 |   1.0182 |     36.389 |   1.0529 |     37.009 |     0.9
   50 |   1.0141 |     36.191 |   1.0497 |     36.262 |     0.9
   51 |   1.0101 |     35.839 |   1.0470 |     36.854 |     0.9
   52 |   1.0048 |     35.844 |   1.0462 |     36.573 |     0.9
   53 |   1.0031 |     35.393 |   1.0435 |     36.106 |     0.9
   54 |   0.9972 |     35.315 |   1.0357 |     35.607 |     1.0
   55 |   0.9921 |     35.222 |   1.0308 |     35.639 |     1.0
   56 |   0.9834 |     34.803 |   1.0330 |     35.576 |     1.0
   57 |   0.9857 |     34.980 |   1.0277 |     36.012 |     1.0
   58 |   0.9765 |     34.495 |   1.0175 |     35.234 |     1.0
   59 |   0.9702 |     34.011 |   1.0255 |     35.607 |     1.0
   60 |   0.9643 |     33.944 |   1.0187 |     35.047 |     1.1
   61 |   0.9606 |     33.686 |   1.0173 |     35.358 |     1.1
   62 |   0.9581 |     33.708 |   1.0092 |     34.673 |     1.1
   63 |   0.9530 |     33.565 |   1.0116 |     34.642 |     1.1
   64 |   0.9439 |     33.086 |   1.0008 |     34.361 |     1.1
   65 |   0.9427 |     32.981 |   0.9959 |     34.548 |     1.1
   66 |   0.9356 |     32.695 |   0.9938 |     34.611 |     1.2
   67 |   0.9354 |     33.042 |   1.0010 |     33.707 |     1.2
   68 |   0.9255 |     32.585 |   0.9963 |     34.237 |     1.2
   69 |   0.9195 |     32.419 |   0.9892 |     33.925 |     1.2
   70 |   0.9175 |     32.298 |   0.9953 |     34.050 |     1.2
   71 |   0.9090 |     31.946 |   0.9921 |     33.583 |     1.3
   72 |   0.9062 |     31.698 |   0.9817 |     33.676 |     1.3
   73 |   0.9001 |     31.390 |   0.9871 |     33.676 |     1.3
   74 |   0.8936 |     31.362 |   0.9713 |     32.804 |     1.3
   75 |   0.8876 |     30.982 |   0.9767 |     32.991 |     1.3
   76 |   0.8867 |     30.867 |   0.9751 |     32.617 |     1.3
   77 |   0.8757 |     30.542 |   0.9815 |     32.617 |     1.4
   78 |   0.8766 |     30.575 |   0.9780 |     33.053 |     1.4
   79 |   0.8656 |     30.107 |   0.9710 |     32.243 |     1.4
   80 |   0.8573 |     29.721 |   0.9628 |     32.928 |     1.4
   81 |   0.8576 |     29.666 |   0.9674 |     32.274 |     1.4
   82 |   0.8504 |     29.573 |   0.9710 |     32.399 |     1.5
   83 |   0.8499 |     29.182 |   0.9593 |     32.648 |     1.5
   84 |   0.8296 |     28.560 |   0.9685 |     32.399 |     1.5
   85 |   0.8248 |     28.411 |   0.9725 |     32.461 |     1.5
   86 |   0.8235 |     28.427 |   0.9630 |     32.617 |     1.5
   87 |   0.8183 |     27.987 |   0.9585 |     32.305 |     1.5
   88 |   0.8128 |     28.224 |   0.9581 |     32.617 |     1.6
   89 |   0.7998 |     27.640 |   0.9637 |     31.963 |     1.6
   90 |   0.7980 |     27.541 |   0.9587 |     31.838 |     1.6
   91 |   0.7925 |     27.178 |   0.9468 |     32.617 |     1.6
   92 |   0.7825 |     26.825 |   0.9416 |     31.776 |     1.6
   93 |   0.7791 |     26.732 |   0.9489 |     32.025 |     1.6
   94 |   0.7705 |     26.495 |   0.9310 |     31.277 |     1.7
   95 |   0.7629 |     26.109 |   0.9368 |     31.121 |     1.7
   96 |   0.7585 |     26.043 |   0.9344 |     31.215 |     1.7
   97 |   0.7517 |     25.542 |   0.9475 |     30.997 |     1.7
   98 |   0.7482 |     25.603 |   0.9522 |     31.713 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,985,378

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2515 |     61.287 |   1.6440 |     45.763 |     0.1
    2 |   1.4998 |     46.107 |   1.4273 |     45.763 |     0.2
    3 |   1.4113 |     46.256 |   1.3999 |     45.763 |     0.2
    4 |   1.3796 |     46.019 |   1.3645 |     45.732 |     0.3
    5 |   1.3405 |     45.705 |   1.3390 |     45.047 |     0.4
    6 |   1.3153 |     45.325 |   1.3156 |     44.704 |     0.5
    7 |   1.2932 |     44.857 |   1.2850 |     43.894 |     0.6
    8 |   1.2707 |     44.384 |   1.2700 |     43.988 |     0.6
    9 |   1.2548 |     43.784 |   1.2613 |     44.081 |     0.7
   10 |   1.2433 |     43.552 |   1.2438 |     43.364 |     0.8
   11 |   1.2349 |     43.525 |   1.2344 |     43.458 |     0.9
   12 |   1.2235 |     43.211 |   1.2324 |     43.925 |     1.0
   13 |   1.2176 |     43.123 |   1.2280 |     43.396 |     1.0
   14 |   1.2082 |     42.853 |   1.2135 |     42.523 |     1.1
   15 |   1.2015 |     42.683 |   1.2097 |     42.991 |     1.2
   16 |   1.1941 |     42.446 |   1.1936 |     42.430 |     1.3
   17 |   1.1871 |     42.027 |   1.1857 |     42.430 |     1.4
   18 |   1.1815 |     42.066 |   1.1835 |     42.243 |     1.4
   19 |   1.1733 |     41.868 |   1.1757 |     41.994 |     1.5
   20 |   1.1646 |     41.868 |   1.1609 |     41.745 |     1.6
   21 |   1.1568 |     41.510 |   1.1622 |     41.371 |     1.7
   22 |   1.1498 |     40.888 |   1.1742 |     42.710 |     1.8
   23 |   1.1392 |     40.678 |   1.1509 |     41.371 |     1.8
   24 |   1.1335 |     40.816 |   1.1569 |     41.433 |     1.9
   25 |   1.1266 |     40.326 |   1.1434 |     40.903 |     2.0
   26 |   1.1209 |     40.304 |   1.1302 |     40.561 |     2.1
   27 |   1.1141 |     40.166 |   1.1329 |     41.308 |     2.2
   28 |   1.1085 |     40.304 |   1.1204 |     40.312 |     2.2
   29 |   1.0994 |     39.803 |   1.1173 |     40.592 |     2.3
   30 |   1.0948 |     39.638 |   1.1154 |     40.841 |     2.4
   31 |   1.0874 |     39.495 |   1.1165 |     41.028 |     2.5
   32 |   1.0845 |     39.351 |   1.1081 |     40.903 |     2.6
   33 |   1.0766 |     38.994 |   1.1030 |     40.156 |     2.6
   34 |   1.0721 |     38.911 |   1.0967 |     39.595 |     2.7
   35 |   1.0693 |     38.702 |   1.0963 |     39.720 |     2.8
   36 |   1.0617 |     38.636 |   1.0854 |     39.221 |     2.9
   37 |   1.0582 |     38.514 |   1.0861 |     40.623 |     3.0
   38 |   1.0535 |     38.498 |   1.0924 |     40.093 |     3.0
   39 |   1.0512 |     38.426 |   1.0817 |     39.408 |     3.1
   40 |   1.0477 |     38.234 |   1.0851 |     39.533 |     3.2
   41 |   1.0459 |     38.289 |   1.0731 |     39.533 |     3.3
   42 |   1.0394 |     37.826 |   1.0739 |     38.785 |     3.4
   43 |   1.0426 |     38.294 |   1.0783 |     39.408 |     3.4
   44 |   1.0399 |     38.046 |   1.0697 |     39.346 |     3.5
   45 |   1.0334 |     38.041 |   1.0629 |     38.847 |     3.6
   46 |   1.0329 |     37.887 |   1.0626 |     39.283 |     3.7
   47 |   1.0308 |     37.848 |   1.0678 |     39.595 |     3.8
   48 |   1.0268 |     37.848 |   1.0540 |     38.380 |     3.8
   49 |   1.0217 |     37.611 |   1.0706 |     39.346 |     3.9
   50 |   1.0212 |     37.419 |   1.0670 |     39.657 |     4.0
   51 |   1.0193 |     37.441 |   1.0499 |     38.847 |     4.1
   52 |   1.0129 |     37.309 |   1.0522 |     38.255 |     4.2
   53 |   1.0101 |     37.160 |   1.0505 |     38.442 |     4.2
   54 |   1.0098 |     37.358 |   1.0441 |     38.318 |     4.3
   55 |   1.0066 |     37.050 |   1.0493 |     38.785 |     4.4
   56 |   1.0045 |     36.786 |   1.0589 |     39.003 |     4.5
   57 |   1.0027 |     36.912 |   1.0415 |     38.037 |     4.6
   58 |   0.9997 |     36.874 |   1.0376 |     38.193 |     4.6
   59 |   1.0026 |     37.276 |   1.0547 |     38.567 |     4.7
   60 |   1.0056 |     36.786 |   1.1016 |     40.436 |     4.8
   61 |   1.0102 |     37.457 |   1.0298 |     37.882 |     4.9
   62 |   0.9909 |     36.554 |   1.0312 |     38.193 |     5.0
   63 |   0.9892 |     36.808 |   1.0285 |     37.570 |     5.0
   64 |   0.9828 |     36.296 |   1.0309 |     38.193 |     5.1
   65 |   0.9833 |     36.235 |   1.0280 |     37.259 |     5.2
   66 |   0.9760 |     35.706 |   1.0287 |     38.629 |     5.3
   67 |   0.9765 |     36.252 |   1.0190 |     36.916 |     5.4
   68 |   0.9731 |     36.301 |   1.0236 |     37.445 |     5.4
   69 |   0.9734 |     35.800 |   1.0155 |     37.352 |     5.5
   70 |   0.9676 |     35.459 |   1.0144 |     37.103 |     5.6
   71 |   0.9628 |     35.227 |   1.0135 |     37.103 |     5.7
   72 |   0.9576 |     35.095 |   1.0113 |     37.165 |     5.8
   73 |   0.9527 |     35.007 |   1.0121 |     36.168 |     5.8
   74 |   0.9480 |     34.600 |   1.0144 |     36.511 |     5.9
   75 |   0.9424 |     34.269 |   1.0103 |     36.480 |     6.0
   76 |   0.9434 |     34.335 |   1.0053 |     36.636 |     6.1
   77 |   0.9346 |     34.148 |   1.0006 |     35.483 |     6.2
   78 |   0.9332 |     33.944 |   0.9946 |     35.701 |     6.2
   79 |   0.9225 |     33.196 |   0.9894 |     34.891 |     6.3
   80 |   0.9190 |     33.240 |   0.9900 |     35.607 |     6.4
   81 |   0.9490 |     34.600 |   0.9940 |     35.421 |     6.5
   82 |   0.9205 |     33.086 |   0.9879 |     35.576 |     6.6
   83 |   0.9602 |     34.958 |   1.0099 |     36.604 |     6.6
   84 |   0.9298 |     33.928 |   0.9932 |     35.545 |     6.7
   85 |   0.9161 |     32.865 |   0.9880 |     35.389 |     6.8
   86 |   0.9099 |     33.102 |   0.9792 |     35.016 |     6.9
   87 |   0.8990 |     32.061 |   0.9849 |     35.607 |     7.0
   88 |   0.8942 |     32.188 |   0.9746 |     35.514 |     7.0
   89 |   0.8847 |     31.836 |   0.9608 |     33.551 |     7.1
   90 |   0.8797 |     31.500 |   0.9734 |     33.863 |     7.2
   91 |   0.8773 |     31.357 |   0.9678 |     34.174 |     7.3
   92 |   0.8714 |     31.191 |   0.9662 |     34.206 |     7.4
   93 |   0.8637 |     30.685 |   0.9701 |     33.956 |     7.4
   94 |   0.8576 |     30.338 |   0.9607 |     33.925 |     7.5
   95 |   0.8471 |     29.892 |   0.9668 |     33.146 |     7.6
   96 |   0.8427 |     29.843 |   0.9504 |     33.240 |     7.7
   97 |   0.8423 |     29.837 |   0.9467 |     33.146 |     7.8
   98 |   0.8458 |     29.876 |   0.9764 |     34.143 |     7.8
   99 |   0.8439 |     29.798 |   0.9483 |     33.396 |     7.9
  100 |   0.8317 |     29.408 |   0.9737 |     34.393 |     8.0
  101 |   0.8257 |     29.193 |   0.9461 |     32.866 |     8.1
  102 |   0.8151 |     28.554 |   0.9562 |     33.053 |     8.2
  103 |   0.8081 |     28.273 |   0.9405 |     32.710 |     8.2
  104 |   0.7995 |     28.262 |   0.9423 |     32.243 |     8.3
  105 |   0.7969 |     27.915 |   0.9411 |     32.555 |     8.4
  106 |   0.7848 |     27.442 |   0.9368 |     32.835 |     8.5
  107 |   0.7872 |     27.602 |   0.9271 |     31.931 |     8.6
  108 |   0.7809 |     27.337 |   0.9397 |     33.302 |     8.6
  109 |   0.7728 |     27.001 |   0.9210 |     31.900 |     8.7
  110 |   0.7627 |     26.737 |   0.9345 |     32.056 |     8.8
  111 |   0.7564 |     26.319 |   0.9255 |     31.838 |     8.9
  112 |   0.7504 |     25.928 |   0.9192 |     31.963 |     9.0
  113 |   0.7499 |     26.198 |   0.9357 |     32.181 |     9.0
  114 |   0.7471 |     25.922 |   0.9240 |     31.620 |     9.1
  115 |   0.7333 |     25.339 |   0.9345 |     31.308 |     9.2
  116 |   0.7303 |     25.388 |   0.9201 |     31.651 |     9.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,063,778

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2066 |     60.594 |   1.6134 |     45.763 |     0.0
    2 |   1.4817 |     46.124 |   1.4250 |     45.763 |     0.1
    3 |   1.3993 |     46.085 |   1.4032 |     47.975 |     0.1
    4 |   1.3718 |     45.705 |   1.3625 |     44.611 |     0.2
    5 |   1.3398 |     45.353 |   1.3303 |     44.922 |     0.2
    6 |   1.3119 |     45.342 |   1.3177 |     44.922 |     0.2
    7 |   1.2927 |     45.006 |   1.2984 |     44.984 |     0.3
    8 |   1.2806 |     44.808 |   1.2838 |     44.206 |     0.3
    9 |   1.2685 |     44.494 |   1.2696 |     43.925 |     0.3
   10 |   1.2514 |     44.213 |   1.2581 |     43.676 |     0.4
   11 |   1.2360 |     43.872 |   1.2384 |     43.832 |     0.4
   12 |   1.2224 |     43.415 |   1.2313 |     43.520 |     0.5
   13 |   1.2101 |     43.051 |   1.2208 |     43.178 |     0.5
   14 |   1.1986 |     42.424 |   1.2046 |     42.243 |     0.5
   15 |   1.1890 |     42.215 |   1.2069 |     42.212 |     0.6
   16 |   1.1790 |     42.121 |   1.1859 |     41.651 |     0.6
   17 |   1.1702 |     41.642 |   1.1863 |     42.212 |     0.6
   18 |   1.1617 |     41.356 |   1.1810 |     42.368 |     0.7
   19 |   1.1510 |     41.075 |   1.1668 |     42.212 |     0.7
   20 |   1.1406 |     40.937 |   1.1575 |     41.402 |     0.8
   21 |   1.1360 |     40.926 |   1.1516 |     41.090 |     0.8
   22 |   1.1237 |     40.442 |   1.1500 |     41.277 |     0.8
   23 |   1.1146 |     40.320 |   1.1387 |     42.056 |     0.9
   24 |   1.1078 |     40.122 |   1.1242 |     39.657 |     0.9
   25 |   1.0957 |     39.764 |   1.1304 |     40.467 |     1.0
   26 |   1.0870 |     39.263 |   1.1181 |     40.031 |     1.0
   27 |   1.0776 |     38.790 |   1.1067 |     39.595 |     1.0
   28 |   1.0686 |     38.432 |   1.1074 |     39.470 |     1.1
   29 |   1.0598 |     38.245 |   1.0887 |     38.879 |     1.1
   30 |   1.0504 |     38.002 |   1.0961 |     38.318 |     1.2
   31 |   1.0385 |     36.929 |   1.0756 |     38.224 |     1.2
   32 |   1.0307 |     36.653 |   1.0758 |     38.536 |     1.2
   33 |   1.0181 |     36.125 |   1.0635 |     38.193 |     1.3
   34 |   1.0098 |     36.059 |   1.0633 |     38.598 |     1.3
   35 |   0.9984 |     35.244 |   1.0503 |     37.165 |     1.3
   36 |   0.9918 |     34.991 |   1.0455 |     37.601 |     1.4
   37 |   0.9811 |     34.798 |   1.0359 |     36.885 |     1.4
   38 |   0.9700 |     34.181 |   1.0277 |     36.791 |     1.5
   39 |   0.9586 |     33.834 |   1.0371 |     36.386 |     1.5
   40 |   0.9517 |     33.713 |   1.0190 |     36.542 |     1.5
   41 |   0.9462 |     33.532 |   1.0218 |     36.199 |     1.6
   42 |   0.9379 |     33.174 |   1.0143 |     35.639 |     1.6
   43 |   0.9236 |     32.689 |   1.0026 |     35.545 |     1.7
   44 |   0.9102 |     32.144 |   0.9992 |     35.016 |     1.7
   45 |   0.9015 |     31.819 |   1.0078 |     35.265 |     1.7
   46 |   0.8923 |     31.329 |   0.9949 |     34.455 |     1.8
   47 |   0.8871 |     31.329 |   0.9844 |     34.486 |     1.8
   48 |   0.8705 |     30.690 |   0.9857 |     34.486 |     1.9
   49 |   0.8594 |     29.881 |   0.9961 |     34.798 |     1.9
   50 |   0.8552 |     30.052 |   0.9830 |     33.832 |     1.9
   51 |   0.8464 |     29.771 |   0.9742 |     33.738 |     2.0
   52 |   0.8390 |     29.507 |   0.9594 |     33.084 |     2.0
   53 |   0.8226 |     28.879 |   0.9659 |     33.364 |     2.1
   54 |   0.8139 |     28.499 |   0.9804 |     33.209 |     2.1
   55 |   0.8076 |     28.202 |   0.9740 |     33.022 |     2.1
   56 |   0.7985 |     27.767 |   0.9734 |     32.773 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 771,490

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1059 |     57.686 |   1.5328 |     45.763 |     0.0
    2 |   1.4347 |     46.069 |   1.3902 |     45.763 |     0.0
    3 |   1.3602 |     45.700 |   1.3500 |     45.047 |     0.1
    4 |   1.3295 |     45.391 |   1.3232 |     45.919 |     0.1
    5 |   1.3044 |     44.758 |   1.3006 |     43.956 |     0.1
    6 |   1.2834 |     44.433 |   1.2813 |     43.988 |     0.1
    7 |   1.2626 |     43.932 |   1.2610 |     43.084 |     0.2
    8 |   1.2387 |     43.586 |   1.2519 |     43.801 |     0.2
    9 |   1.2147 |     42.820 |   1.2213 |     43.676 |     0.2
   10 |   1.1929 |     42.077 |   1.1980 |     42.399 |     0.2
   11 |   1.1667 |     41.278 |   1.1693 |     42.804 |     0.3
   12 |   1.1435 |     40.535 |   1.1526 |     41.340 |     0.3
   13 |   1.1154 |     39.610 |   1.1271 |     39.564 |     0.3
   14 |   1.0931 |     38.795 |   1.1168 |     40.249 |     0.3
   15 |   1.0614 |     37.551 |   1.0832 |     37.757 |     0.4
   16 |   1.0348 |     36.307 |   1.0650 |     37.383 |     0.4
   17 |   1.0095 |     35.349 |   1.0455 |     36.231 |     0.4
   18 |   0.9794 |     34.071 |   1.0506 |     36.449 |     0.4
   19 |   0.9503 |     33.311 |   1.0235 |     35.171 |     0.5
   20 |   0.9190 |     31.819 |   1.0054 |     34.611 |     0.5
   21 |   0.8902 |     30.707 |   0.9996 |     34.704 |     0.5
   22 |   0.8579 |     29.556 |   0.9885 |     33.458 |     0.5
   23 |   0.8222 |     27.789 |   0.9904 |     33.022 |     0.6
   24 |   0.7907 |     26.627 |   0.9737 |     31.776 |     0.6
   25 |   0.7603 |     25.625 |   0.9738 |     31.963 |     0.6
   26 |   0.7299 |     24.254 |   0.9807 |     31.371 |     0.6
   27 |   0.6929 |     22.910 |   0.9471 |     29.346 |     0.7
   28 |   0.6624 |     21.407 |   0.9615 |     29.782 |     0.7
   29 |   0.6325 |     20.367 |   0.9425 |     28.692 |     0.7
   30 |   0.6065 |     19.717 |   0.9386 |     28.442 |     0.7
   31 |   0.5790 |     18.786 |   0.9336 |     27.445 |     0.8
   32 |   0.5623 |     18.247 |   0.9660 |     28.224 |     0.8
   33 |   0.5326 |     16.969 |   0.9524 |     27.352 |     0.8
   34 |   0.5040 |     15.934 |   0.9625 |     27.757 |     0.8
   35 |   0.4926 |     15.791 |   0.9578 |     27.290 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 699,106

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4207 |     64.861 |   1.8874 |     53.178 |     0.0
    2 |   1.6682 |     47.831 |   1.5200 |     45.701 |     0.1
    3 |   1.4580 |     45.859 |   1.4215 |     45.763 |     0.1
    4 |   1.3982 |     45.705 |   1.3873 |     45.763 |     0.2
    5 |   1.3696 |     45.485 |   1.3616 |     45.327 |     0.2
    6 |   1.3513 |     45.402 |   1.3476 |     45.047 |     0.2
    7 |   1.3348 |     45.160 |   1.3339 |     44.922 |     0.3
    8 |   1.3169 |     44.934 |   1.3130 |     45.950 |     0.3
    9 |   1.2965 |     44.896 |   1.3006 |     44.860 |     0.4
   10 |   1.2794 |     44.973 |   1.2763 |     44.268 |     0.4
   11 |   1.2616 |     44.076 |   1.2657 |     43.988 |     0.4
   12 |   1.2448 |     43.674 |   1.2573 |     44.424 |     0.5
   13 |   1.2306 |     43.233 |   1.2342 |     43.707 |     0.5
   14 |   1.2141 |     42.897 |   1.2236 |     43.364 |     0.6
   15 |   1.1989 |     42.517 |   1.2080 |     42.897 |     0.6
   16 |   1.1900 |     42.286 |   1.1949 |     42.056 |     0.6
   17 |   1.1701 |     41.174 |   1.1789 |     42.960 |     0.7
   18 |   1.1545 |     40.331 |   1.1722 |     41.340 |     0.7
   19 |   1.1385 |     39.996 |   1.1507 |     40.592 |     0.8
   20 |   1.1201 |     39.071 |   1.1323 |     39.720 |     0.8
   21 |   1.1053 |     38.597 |   1.1217 |     38.131 |     0.8
   22 |   1.0852 |     37.617 |   1.1050 |     38.006 |     0.9
   23 |   1.0633 |     36.422 |   1.0948 |     37.788 |     0.9
   24 |   1.0429 |     35.839 |   1.0989 |     37.757 |     1.0
   25 |   1.0308 |     35.260 |   1.0676 |     36.168 |     1.0
   26 |   1.0133 |     34.875 |   1.0556 |     35.607 |     1.0
   27 |   0.9895 |     33.620 |   1.0371 |     34.424 |     1.1
   28 |   0.9663 |     32.634 |   1.0245 |     34.019 |     1.1
   29 |   0.9480 |     31.874 |   1.0252 |     34.611 |     1.2
   30 |   0.9232 |     31.015 |   1.0134 |     34.112 |     1.2
   31 |   0.9032 |     30.399 |   0.9947 |     33.520 |     1.2
   32 |   0.8782 |     29.077 |   0.9817 |     32.305 |     1.3
   33 |   0.8569 |     28.565 |   0.9713 |     31.526 |     1.3
   34 |   0.8353 |     27.497 |   0.9644 |     31.869 |     1.4
   35 |   0.8078 |     26.599 |   0.9415 |     30.561 |     1.4
   36 |   0.7810 |     25.366 |   0.9378 |     30.374 |     1.4
   37 |   0.7607 |     24.810 |   0.9312 |     29.595 |     1.5
   38 |   0.7322 |     23.615 |   0.9283 |     29.190 |     1.5
   39 |   0.7087 |     23.213 |   0.9298 |     29.564 |     1.6
   40 |   0.6834 |     22.107 |   0.9190 |     28.224 |     1.6
   41 |   0.6597 |     21.132 |   0.9289 |     29.190 |     1.6
   42 |   0.6366 |     20.499 |   0.9230 |     28.037 |     1.7
   43 |   0.6189 |     19.833 |   0.9264 |     28.442 |     1.7
   44 |   0.6025 |     19.409 |   0.9188 |     28.442 |     1.8
   45 |   0.5748 |     18.412 |   0.9282 |     28.162 |     1.8
   46 |   0.5536 |     17.768 |   0.9205 |     27.975 |     1.8
   47 |   0.5287 |     16.788 |   0.9401 |     28.131 |     1.9
   48 |   0.5153 |     16.689 |   0.9477 |     28.318 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 868,642

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2156 |     60.263 |   1.6183 |     45.763 |     0.0
    2 |   1.4790 |     46.041 |   1.4164 |     45.763 |     0.1
    3 |   1.4007 |     46.052 |   1.3878 |     45.701 |     0.1
    4 |   1.3716 |     45.859 |   1.3579 |     45.358 |     0.1
    5 |   1.3361 |     45.226 |   1.3276 |     44.891 |     0.2
    6 |   1.3097 |     45.171 |   1.3100 |     44.579 |     0.2
    7 |   1.2914 |     44.835 |   1.2892 |     44.268 |     0.2
    8 |   1.2749 |     44.786 |   1.2728 |     44.486 |     0.3
    9 |   1.2549 |     44.450 |   1.2484 |     43.894 |     0.3
   10 |   1.2338 |     43.299 |   1.2256 |     43.022 |     0.3
   11 |   1.2153 |     43.294 |   1.2119 |     42.368 |     0.4
   12 |   1.1966 |     42.567 |   1.1974 |     41.776 |     0.4
   13 |   1.1766 |     41.994 |   1.1802 |     41.589 |     0.5
   14 |   1.1590 |     41.179 |   1.1635 |     41.246 |     0.5
   15 |   1.1384 |     40.866 |   1.1660 |     41.433 |     0.5
   16 |   1.1167 |     39.715 |   1.1295 |     40.093 |     0.6
   17 |   1.0946 |     38.911 |   1.1217 |     39.875 |     0.6
   18 |   1.0763 |     37.876 |   1.1004 |     38.692 |     0.6
   19 |   1.0551 |     37.221 |   1.0891 |     37.819 |     0.7
   20 |   1.0359 |     36.389 |   1.0773 |     36.760 |     0.7
   21 |   1.0073 |     35.156 |   1.0674 |     36.511 |     0.7
   22 |   0.9896 |     34.049 |   1.0390 |     35.639 |     0.8
   23 |   0.9715 |     33.471 |   1.0394 |     35.826 |     0.8
   24 |   0.9426 |     32.276 |   1.0369 |     33.988 |     0.8
   25 |   0.9145 |     31.687 |   1.0220 |     33.676 |     0.9
   26 |   0.8957 |     30.520 |   1.0108 |     34.143 |     0.9
   27 |   0.8719 |     29.308 |   1.0027 |     32.991 |     0.9
   28 |   0.8353 |     27.981 |   0.9894 |     31.651 |     1.0
   29 |   0.8045 |     26.666 |   0.9683 |     30.748 |     1.0
   30 |   0.7741 |     25.719 |   0.9726 |     30.966 |     1.0
   31 |   0.7530 |     25.025 |   0.9704 |     31.215 |     1.1
   32 |   0.7194 |     23.505 |   0.9828 |     30.966 |     1.1
   33 |   0.6969 |     22.932 |   0.9552 |     29.252 |     1.1
   34 |   0.6691 |     21.908 |   0.9522 |     28.754 |     1.2
   35 |   0.6457 |     21.044 |   0.9533 |     28.941 |     1.2
   36 |   0.6227 |     20.174 |   0.9615 |     29.065 |     1.3
   37 |   0.6050 |     19.684 |   0.9667 |     28.131 |     1.3
   38 |   0.5777 |     18.654 |   0.9547 |     28.723 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,558,818

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2131 |     60.208 |   1.6482 |     48.785 |     0.1
    2 |   1.5019 |     46.173 |   1.4295 |     45.763 |     0.1
    3 |   1.4140 |     46.085 |   1.3983 |     45.763 |     0.2
    4 |   1.3873 |     46.052 |   1.3790 |     45.763 |     0.2
    5 |   1.3470 |     45.755 |   1.3311 |     45.607 |     0.3
    6 |   1.3118 |     45.419 |   1.3095 |     44.766 |     0.4
    7 |   1.2878 |     45.012 |   1.2791 |     44.050 |     0.4
    8 |   1.2676 |     44.235 |   1.2596 |     43.458 |     0.5
    9 |   1.2507 |     44.103 |   1.2503 |     43.925 |     0.5
   10 |   1.2377 |     43.690 |   1.2374 |     43.614 |     0.6
   11 |   1.2235 |     43.464 |   1.2201 |     43.583 |     0.7
   12 |   1.2091 |     42.996 |   1.2096 |     42.835 |     0.7
   13 |   1.1986 |     42.804 |   1.2015 |     43.333 |     0.8
   14 |   1.1898 |     42.616 |   1.1894 |     43.053 |     0.8
   15 |   1.1811 |     42.418 |   1.1865 |     43.209 |     0.9
   16 |   1.1749 |     42.237 |   1.1795 |     42.025 |     1.0
   17 |   1.1687 |     41.873 |   1.1773 |     42.617 |     1.0
   18 |   1.1602 |     41.658 |   1.1686 |     42.181 |     1.1
   19 |   1.1520 |     41.460 |   1.1688 |     41.869 |     1.1
   20 |   1.1458 |     41.080 |   1.1500 |     40.810 |     1.2
   21 |   1.1386 |     40.728 |   1.1458 |     41.931 |     1.3
   22 |   1.1312 |     40.546 |   1.1475 |     40.966 |     1.3
   23 |   1.1239 |     40.238 |   1.1327 |     41.184 |     1.4
   24 |   1.1163 |     40.001 |   1.1322 |     41.526 |     1.4
   25 |   1.1087 |     39.764 |   1.1213 |     40.467 |     1.5
   26 |   1.1012 |     39.423 |   1.1133 |     39.190 |     1.6
   27 |   1.0957 |     39.478 |   1.1056 |     39.720 |     1.6
   28 |   1.0883 |     39.296 |   1.1025 |     38.941 |     1.7
   29 |   1.0788 |     38.801 |   1.0950 |     38.879 |     1.7
   30 |   1.0739 |     38.663 |   1.0911 |     39.003 |     1.8
   31 |   1.0669 |     38.448 |   1.0858 |     38.660 |     1.9
   32 |   1.0599 |     38.212 |   1.0882 |     38.069 |     1.9
   33 |   1.0538 |     37.578 |   1.0754 |     38.255 |     2.0
   34 |   1.0433 |     37.380 |   1.0719 |     37.601 |     2.0
   35 |   1.0373 |     37.248 |   1.0635 |     37.632 |     2.1
   36 |   1.0285 |     36.890 |   1.0631 |     37.788 |     2.2
   37 |   1.0237 |     36.709 |   1.0400 |     36.729 |     2.2
   38 |   1.0064 |     35.905 |   1.0484 |     37.040 |     2.3
   39 |   0.9980 |     35.514 |   1.0408 |     37.165 |     2.3
   40 |   0.9934 |     35.404 |   1.0308 |     35.888 |     2.4
   41 |   0.9896 |     35.382 |   1.0259 |     35.919 |     2.5
   42 |   0.9800 |     35.018 |   1.0185 |     35.888 |     2.5
   43 |   0.9673 |     34.341 |   1.0187 |     35.826 |     2.6
   44 |   0.9558 |     34.099 |   1.0039 |     35.265 |     2.6
   45 |   0.9531 |     33.939 |   1.0081 |     35.234 |     2.7
   46 |   0.9464 |     33.801 |   0.9969 |     34.642 |     2.8
   47 |   0.9337 |     33.130 |   0.9963 |     34.424 |     2.8
   48 |   0.9264 |     32.997 |   0.9896 |     34.019 |     2.9
   49 |   0.9208 |     32.821 |   0.9920 |     34.798 |     2.9
   50 |   0.9153 |     32.733 |   0.9963 |     34.579 |     3.0
   51 |   0.9117 |     32.596 |   0.9823 |     34.019 |     3.1
   52 |   0.9066 |     32.474 |   0.9758 |     34.237 |     3.1
   53 |   0.8968 |     31.797 |   0.9823 |     33.676 |     3.2
   54 |   0.8968 |     32.078 |   0.9812 |     33.769 |     3.2
   55 |   0.8829 |     31.329 |   0.9646 |     32.960 |     3.3
   56 |   0.8884 |     31.582 |   1.0236 |     35.732 |     3.4
   57 |   0.9467 |     33.823 |   0.9958 |     34.642 |     3.4
   58 |   0.9005 |     32.392 |   0.9814 |     33.645 |     3.5
   59 |   0.8875 |     31.434 |   0.9857 |     34.579 |     3.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 931,682

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1162 |     57.675 |   1.5316 |     45.763 |     0.0
    2 |   1.4344 |     45.992 |   1.3777 |     45.389 |     0.1
    3 |   1.3492 |     45.034 |   1.3279 |     44.735 |     0.1
    4 |   1.3067 |     44.764 |   1.2900 |     44.268 |     0.2
    5 |   1.2783 |     44.109 |   1.2696 |     43.489 |     0.2
    6 |   1.2524 |     43.899 |   1.2526 |     43.832 |     0.2
    7 |   1.2291 |     43.195 |   1.2213 |     42.835 |     0.3
    8 |   1.2049 |     42.358 |   1.2006 |     41.713 |     0.3
    9 |   1.1821 |     41.895 |   1.1828 |     41.464 |     0.4
   10 |   1.1523 |     40.590 |   1.1607 |     40.654 |     0.4
   11 |   1.1279 |     39.588 |   1.1383 |     39.782 |     0.4
   12 |   1.1026 |     38.531 |   1.1083 |     38.847 |     0.5
   13 |   1.0726 |     37.408 |   1.0935 |     37.664 |     0.5
   14 |   1.0429 |     36.125 |   1.0739 |     37.819 |     0.6
   15 |   1.0104 |     34.611 |   1.0472 |     35.514 |     0.6
   16 |   0.9791 |     33.322 |   1.0145 |     34.237 |     0.6
   17 |   0.9439 |     32.067 |   1.0030 |     34.112 |     0.7
   18 |   0.9037 |     30.679 |   0.9843 |     32.928 |     0.7
   19 |   0.8807 |     30.046 |   0.9759 |     32.586 |     0.8
   20 |   0.8349 |     28.356 |   0.9454 |     31.745 |     0.8
   21 |   0.7993 |     26.952 |   0.9458 |     30.935 |     0.8
   22 |   0.7690 |     25.873 |   0.9071 |     29.907 |     0.9
   23 |   0.7390 |     24.595 |   0.8988 |     29.657 |     0.9
   24 |   0.7019 |     23.307 |   0.8993 |     28.941 |     1.0
   25 |   0.6713 |     22.272 |   0.8841 |     28.505 |     1.0
   26 |   0.6430 |     21.402 |   0.9011 |     28.287 |     1.0
   27 |   0.6049 |     19.568 |   0.9029 |     28.287 |     1.1
   28 |   0.5766 |     18.561 |   0.8790 |     27.259 |     1.1
   29 |   0.5506 |     17.817 |   0.9002 |     28.037 |     1.2
   30 |   0.5441 |     17.647 |   0.8939 |     27.445 |     1.2
   31 |   0.5017 |     15.962 |   0.9000 |     26.791 |     1.3
   32 |   0.4806 |     15.439 |   0.9039 |     26.573 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 442,530

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3717 |     63.220 |   1.7743 |     50.717 |     0.0
    2 |   1.5709 |     46.283 |   1.4591 |     45.701 |     0.1
    3 |   1.4034 |     45.551 |   1.3708 |     45.140 |     0.1
    4 |   1.3413 |     44.929 |   1.3258 |     45.514 |     0.1
    5 |   1.3026 |     44.031 |   1.2963 |     43.364 |     0.1
    6 |   1.2673 |     43.398 |   1.2576 |     43.925 |     0.2
    7 |   1.2382 |     43.106 |   1.2372 |     42.960 |     0.2
    8 |   1.2154 |     42.374 |   1.2268 |     42.928 |     0.2
    9 |   1.1918 |     41.471 |   1.1965 |     42.056 |     0.2
   10 |   1.1717 |     40.772 |   1.1836 |     41.807 |     0.3
   11 |   1.1481 |     40.001 |   1.1622 |     40.498 |     0.3
   12 |   1.1300 |     39.456 |   1.1518 |     40.312 |     0.3
   13 |   1.1108 |     38.459 |   1.1384 |     39.688 |     0.3
   14 |   1.0917 |     37.837 |   1.1203 |     39.439 |     0.4
   15 |   1.0738 |     37.232 |   1.0978 |     38.536 |     0.4
   16 |   1.0531 |     36.378 |   1.0896 |     37.882 |     0.4
   17 |   1.0298 |     35.574 |   1.0692 |     36.947 |     0.4
   18 |   1.0091 |     34.820 |   1.0594 |     36.262 |     0.5
   19 |   0.9824 |     33.482 |   1.0548 |     35.888 |     0.5
   20 |   0.9611 |     32.755 |   1.0296 |     35.576 |     0.5
   21 |   0.9322 |     31.593 |   1.0098 |     34.019 |     0.5
   22 |   0.9113 |     30.498 |   0.9946 |     33.614 |     0.6
   23 |   0.8779 |     29.336 |   0.9907 |     32.897 |     0.6
   24 |   0.8520 |     28.191 |   0.9648 |     32.866 |     0.6
   25 |   0.8212 |     27.167 |   0.9500 |     31.776 |     0.7
   26 |   0.7920 |     25.950 |   0.9452 |     31.433 |     0.7
   27 |   0.7703 |     25.289 |   0.9264 |     31.246 |     0.7
   28 |   0.7373 |     23.835 |   0.9348 |     30.779 |     0.7
   29 |   0.7162 |     23.026 |   0.9266 |     30.374 |     0.8
   30 |   0.6900 |     21.908 |   0.9286 |     30.405 |     0.8
   31 |   0.6645 |     21.270 |   0.9154 |     28.754 |     0.8
   32 |   0.6424 |     20.273 |   0.9398 |     30.312 |     0.8
   33 |   0.6591 |     21.402 |   0.9011 |     28.287 |     0.9
   34 |   0.6175 |     19.502 |   0.9128 |     28.660 |     0.9
   35 |   0.5750 |     17.867 |   0.8955 |     28.474 |     0.9
   36 |   0.5512 |     17.096 |   0.9039 |     27.227 |     0.9
   37 |   0.5359 |     16.859 |   0.9205 |     27.944 |     1.0
   38 |   0.5156 |     16.122 |   0.9176 |     27.664 |     1.0
   39 |   0.4951 |     15.334 |   0.9371 |     27.601 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,281,698

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4887 |     67.586 |   1.9397 |     53.333 |     0.1
    2 |   1.7153 |     48.431 |   1.5452 |     45.763 |     0.1
    3 |   1.4843 |     46.003 |   1.4411 |     45.763 |     0.2
    4 |   1.4158 |     45.904 |   1.3962 |     45.701 |     0.2
    5 |   1.3800 |     45.639 |   1.3748 |     45.047 |     0.3
    6 |   1.3434 |     45.287 |   1.3265 |     44.829 |     0.4
    7 |   1.3133 |     44.852 |   1.3043 |     44.393 |     0.4
    8 |   1.2890 |     44.290 |   1.2808 |     43.551 |     0.5
    9 |   1.2717 |     43.833 |   1.2624 |     44.455 |     0.5
   10 |   1.2554 |     43.960 |   1.2515 |     43.333 |     0.6
   11 |   1.2381 |     43.619 |   1.2411 |     43.520 |     0.7
   12 |   1.2247 |     43.503 |   1.2216 |     43.551 |     0.7
   13 |   1.2083 |     42.897 |   1.2142 |     43.209 |     0.8
   14 |   1.1959 |     42.363 |   1.2009 |     42.336 |     0.8
   15 |   1.1893 |     41.735 |   1.1988 |     42.866 |     0.9
   16 |   1.1783 |     41.466 |   1.1857 |     41.651 |     1.0
   17 |   1.1676 |     41.075 |   1.1779 |     42.243 |     1.0
   18 |   1.1609 |     40.783 |   1.1765 |     41.184 |     1.1
   19 |   1.1508 |     40.530 |   1.1614 |     40.717 |     1.2
   20 |   1.1412 |     40.309 |   1.1631 |     40.717 |     1.2
   21 |   1.1357 |     40.161 |   1.1549 |     41.776 |     1.3
   22 |   1.1268 |     39.803 |   1.1525 |     40.561 |     1.3
   23 |   1.1190 |     39.599 |   1.1449 |     40.654 |     1.4
   24 |   1.1100 |     39.506 |   1.1356 |     40.031 |     1.5
   25 |   1.1075 |     39.093 |   1.1306 |     40.062 |     1.5
   26 |   1.0987 |     38.762 |   1.1259 |     39.813 |     1.6
   27 |   1.0964 |     39.087 |   1.1231 |     40.312 |     1.6
   28 |   1.0891 |     38.768 |   1.1207 |     39.875 |     1.7
   29 |   1.0810 |     38.366 |   1.1072 |     39.439 |     1.8
   30 |   1.0725 |     37.969 |   1.1048 |     38.816 |     1.8
   31 |   1.0714 |     38.035 |   1.0999 |     39.470 |     1.9
   32 |   1.0624 |     37.826 |   1.1008 |     39.408 |     1.9
   33 |   1.0564 |     37.617 |   1.0920 |     39.907 |     2.0
   34 |   1.0492 |     37.259 |   1.0849 |     38.567 |     2.1
   35 |   1.0454 |     37.188 |   1.0939 |     38.723 |     2.1
   36 |   1.0453 |     37.226 |   1.0754 |     38.162 |     2.2
   37 |   1.0324 |     36.742 |   1.0740 |     38.162 |     2.2
   38 |   1.0259 |     36.775 |   1.0688 |     37.632 |     2.3
   39 |   1.0242 |     36.417 |   1.0720 |     38.100 |     2.4
   40 |   1.0186 |     35.954 |   1.0622 |     38.474 |     2.4
   41 |   1.0052 |     35.673 |   1.0682 |     37.726 |     2.5
   42 |   1.0052 |     35.640 |   1.0563 |     37.726 |     2.5
   43 |   0.9976 |     35.365 |   1.0561 |     37.850 |     2.6
   44 |   0.9889 |     34.941 |   1.0446 |     37.072 |     2.7
   45 |   0.9831 |     34.958 |   1.0436 |     36.542 |     2.7
   46 |   0.9773 |     34.451 |   1.0417 |     35.981 |     2.8
   47 |   0.9652 |     34.022 |   1.0391 |     35.919 |     2.9
   48 |   0.9621 |     33.801 |   1.0371 |     35.701 |     2.9
   49 |   0.9555 |     33.532 |   1.0282 |     35.826 |     3.0
   50 |   0.9497 |     33.421 |   1.0311 |     35.639 |     3.0
   51 |   0.9412 |     33.102 |   1.0191 |     35.607 |     3.1
   52 |   0.9321 |     32.788 |   1.0127 |     35.296 |     3.2
   53 |   0.9285 |     32.551 |   1.0169 |     35.327 |     3.2
   54 |   0.9176 |     32.094 |   1.0214 |     35.047 |     3.3
   55 |   0.9136 |     31.990 |   1.0068 |     35.016 |     3.3
   56 |   0.9086 |     31.577 |   1.0041 |     34.050 |     3.4
   57 |   0.8995 |     31.610 |   0.9926 |     33.738 |     3.5
   58 |   0.8868 |     30.922 |   0.9910 |     34.050 |     3.5
   59 |   0.8820 |     30.619 |   0.9923 |     33.551 |     3.6
   60 |   0.8782 |     30.426 |   0.9865 |     33.364 |     3.6
   61 |   0.8694 |     30.222 |   0.9699 |     32.897 |     3.7
   62 |   0.8592 |     29.595 |   0.9752 |     32.866 |     3.8
   63 |   0.8542 |     29.507 |   0.9696 |     32.523 |     3.8
   64 |   0.8467 |     29.193 |   0.9800 |     32.461 |     3.9
   65 |   0.8371 |     28.758 |   0.9638 |     31.589 |     4.0
   66 |   0.8313 |     28.257 |   0.9777 |     32.087 |     4.0
   67 |   0.8211 |     28.240 |   0.9675 |     31.931 |     4.1
   68 |   0.8174 |     28.103 |   0.9837 |     32.928 |     4.1
   69 |   0.8073 |     27.475 |   0.9538 |     31.558 |     4.2
   70 |   0.8000 |     27.343 |   0.9635 |     31.776 |     4.3
   71 |   0.7931 |     27.139 |   0.9600 |     31.090 |     4.3
   72 |   0.7834 |     26.423 |   0.9535 |     30.997 |     4.4
   73 |   0.7770 |     26.544 |   0.9530 |     31.433 |     4.4
   74 |   0.7633 |     25.658 |   0.9478 |     30.623 |     4.5
   75 |   0.7568 |     25.520 |   0.9513 |     30.498 |     4.6
   76 |   0.7539 |     25.592 |   0.9510 |     30.810 |     4.6
   77 |   0.7467 |     25.190 |   0.9452 |     30.467 |     4.7
   78 |   0.7366 |     24.727 |   0.9493 |     30.498 |     4.7
   79 |   0.7306 |     24.474 |   0.9576 |     30.561 |     4.8
   80 |   0.7231 |     24.386 |   0.9401 |     29.595 |     4.9
   81 |   0.7101 |     24.188 |   0.9562 |     30.374 |     4.9
   82 |   0.7009 |     23.780 |   0.9413 |     29.782 |     5.0
   83 |   0.6929 |     23.362 |   0.9570 |     30.654 |     5.1
   84 |   0.6848 |     23.114 |   0.9364 |     29.875 |     5.1
   85 |   0.6783 |     22.762 |   0.9504 |     30.000 |     5.2
   86 |   0.6654 |     22.475 |   0.9406 |     29.439 |     5.2
   87 |   0.6553 |     21.936 |   0.9350 |     28.941 |     5.3
   88 |   0.6471 |     21.936 |   0.9348 |     29.688 |     5.4
   89 |   0.6388 |     21.374 |   0.9479 |     29.034 |     5.4
   90 |   0.6327 |     21.330 |   0.9507 |     29.751 |     5.5
   91 |   0.6248 |     20.879 |   0.9472 |     29.097 |     5.5
   92 |   0.6125 |     20.444 |   0.9441 |     29.128 |     5.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 558,242

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3782 |     60.968 |   1.8158 |     48.910 |     0.0
    2 |   1.6071 |     47.269 |   1.4730 |     45.763 |     0.1
    3 |   1.4137 |     45.364 |   1.3725 |     45.047 |     0.1
    4 |   1.3446 |     44.549 |   1.3283 |     43.894 |     0.1
    5 |   1.3077 |     44.433 |   1.3002 |     43.489 |     0.2
    6 |   1.2842 |     43.910 |   1.2786 |     43.707 |     0.2
    7 |   1.2587 |     43.332 |   1.2548 |     43.178 |     0.3
    8 |   1.2393 |     42.974 |   1.2392 |     42.679 |     0.3
    9 |   1.2216 |     42.600 |   1.2307 |     42.835 |     0.3
   10 |   1.2017 |     41.983 |   1.2074 |     42.991 |     0.4
   11 |   1.1825 |     41.163 |   1.1908 |     41.963 |     0.4
   12 |   1.1604 |     40.293 |   1.1696 |     41.153 |     0.4
   13 |   1.1390 |     39.412 |   1.1561 |     39.844 |     0.5
   14 |   1.1136 |     38.151 |   1.1382 |     38.941 |     0.5
   15 |   1.0898 |     37.199 |   1.1173 |     38.629 |     0.6
   16 |   1.0603 |     36.461 |   1.0933 |     37.290 |     0.6
   17 |   1.0304 |     35.024 |   1.0769 |     37.103 |     0.6
   18 |   1.0030 |     33.878 |   1.0694 |     36.760 |     0.7
   19 |   0.9790 |     32.766 |   1.0528 |     35.452 |     0.7
   20 |   0.9466 |     31.715 |   1.0378 |     34.922 |     0.7
   21 |   0.9163 |     30.564 |   1.0114 |     32.773 |     0.8
   22 |   0.8855 |     29.286 |   1.0080 |     33.520 |     0.8
   23 |   0.8590 |     28.125 |   1.0002 |     32.866 |     0.9
   24 |   0.8245 |     26.924 |   0.9915 |     31.807 |     0.9
   25 |   0.7952 |     25.625 |   0.9696 |     30.997 |     0.9
   26 |   0.7697 |     24.887 |   0.9717 |     31.340 |     1.0
   27 |   0.7362 |     23.456 |   0.9658 |     31.620 |     1.0
   28 |   0.7062 |     22.569 |   0.9431 |     30.062 |     1.0
   29 |   0.6811 |     21.996 |   0.9549 |     29.470 |     1.1
   30 |   0.6622 |     21.127 |   0.9521 |     29.502 |     1.1
   31 |   0.6353 |     20.273 |   0.9455 |     28.816 |     1.1
   32 |   0.6174 |     19.987 |   0.9504 |     28.660 |     1.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 306,530

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5714 |     69.392 |   2.0206 |     59.159 |     0.0
    2 |   1.7632 |     49.923 |   1.5662 |     45.763 |     0.0
    3 |   1.4930 |     46.052 |   1.4436 |     45.763 |     0.1
    4 |   1.4154 |     46.036 |   1.3905 |     45.763 |     0.1
    5 |   1.3721 |     45.733 |   1.3598 |     45.016 |     0.1
    6 |   1.3405 |     45.325 |   1.3329 |     44.798 |     0.1
    7 |   1.3165 |     45.039 |   1.3112 |     45.109 |     0.1
    8 |   1.3013 |     44.676 |   1.2981 |     44.642 |     0.1
    9 |   1.2803 |     44.340 |   1.2761 |     44.299 |     0.2
   10 |   1.2632 |     43.828 |   1.2641 |     43.614 |     0.2
   11 |   1.2463 |     43.233 |   1.2475 |     42.928 |     0.2
   12 |   1.2336 |     42.787 |   1.2358 |     43.178 |     0.2
   13 |   1.2218 |     42.380 |   1.2300 |     42.835 |     0.2
   14 |   1.2123 |     42.126 |   1.2175 |     42.617 |     0.2
   15 |   1.2030 |     42.044 |   1.2132 |     42.336 |     0.3
   16 |   1.1939 |     41.576 |   1.1982 |     42.087 |     0.3
   17 |   1.1851 |     41.328 |   1.1947 |     41.464 |     0.3
   18 |   1.1776 |     41.179 |   1.1863 |     41.433 |     0.3
   19 |   1.1692 |     41.240 |   1.1781 |     42.150 |     0.3
   20 |   1.1658 |     41.179 |   1.1717 |     41.776 |     0.3
   21 |   1.1578 |     41.042 |   1.1720 |     41.246 |     0.4
   22 |   1.1499 |     40.612 |   1.1621 |     41.589 |     0.4
   23 |   1.1438 |     40.530 |   1.1594 |     41.371 |     0.4
   24 |   1.1386 |     40.392 |   1.1557 |     42.305 |     0.4
   25 |   1.1304 |     40.304 |   1.1444 |     41.153 |     0.4
   26 |   1.1283 |     40.095 |   1.1453 |     40.997 |     0.4
   27 |   1.1237 |     40.166 |   1.1345 |     39.969 |     0.5
   28 |   1.1125 |     39.467 |   1.1346 |     40.498 |     0.5
   29 |   1.1097 |     39.561 |   1.1308 |     40.280 |     0.5
   30 |   1.1053 |     39.495 |   1.1262 |     39.844 |     0.5
   31 |   1.1020 |     39.302 |   1.1219 |     39.720 |     0.5
   32 |   1.0949 |     38.977 |   1.1174 |     39.907 |     0.6
   33 |   1.0892 |     38.988 |   1.1107 |     39.657 |     0.6
   34 |   1.0858 |     38.641 |   1.1172 |     40.218 |     0.6
   35 |   1.0812 |     38.922 |   1.1133 |     39.626 |     0.6
   36 |   1.0772 |     38.520 |   1.0995 |     39.657 |     0.6
   37 |   1.0716 |     38.283 |   1.0950 |     39.283 |     0.6
   38 |   1.0676 |     38.338 |   1.0979 |     40.280 |     0.7
   39 |   1.0645 |     37.991 |   1.0925 |     39.283 |     0.7
   40 |   1.0545 |     37.672 |   1.0844 |     38.972 |     0.7
   41 |   1.0524 |     37.413 |   1.0852 |     38.847 |     0.7
   42 |   1.0511 |     37.534 |   1.0826 |     38.224 |     0.7
   43 |   1.0457 |     37.077 |   1.0775 |     37.726 |     0.7
   44 |   1.0398 |     37.017 |   1.0761 |     37.788 |     0.8
   45 |   1.0377 |     37.099 |   1.0766 |     38.847 |     0.8
   46 |   1.0337 |     36.918 |   1.0661 |     37.882 |     0.8
   47 |   1.0289 |     36.631 |   1.0738 |     38.006 |     0.8
   48 |   1.0279 |     36.764 |   1.0648 |     37.695 |     0.8
   49 |   1.0216 |     36.626 |   1.0604 |     37.445 |     0.8
   50 |   1.0218 |     36.769 |   1.0598 |     37.040 |     0.9
   51 |   1.0188 |     36.560 |   1.0579 |     37.757 |     0.9
   52 |   1.0096 |     36.086 |   1.0533 |     37.259 |     0.9
   53 |   1.0102 |     36.428 |   1.0521 |     37.321 |     0.9
   54 |   1.0035 |     36.241 |   1.0504 |     37.601 |     0.9
   55 |   1.0044 |     36.224 |   1.0541 |     37.570 |     1.0
   56 |   1.0097 |     36.241 |   1.0540 |     37.134 |     1.0
   57 |   0.9940 |     35.629 |   1.0432 |     36.760 |     1.0
   58 |   0.9939 |     35.635 |   1.0344 |     37.196 |     1.0
   59 |   0.9947 |     35.580 |   1.0329 |     36.293 |     1.0
   60 |   0.9856 |     35.304 |   1.0354 |     36.324 |     1.0
   61 |   0.9841 |     35.420 |   1.0269 |     36.106 |     1.1
   62 |   0.9801 |     35.035 |   1.0253 |     36.012 |     1.1
   63 |   0.9847 |     35.150 |   1.0313 |     36.916 |     1.1
   64 |   0.9851 |     35.332 |   1.0275 |     36.449 |     1.1
   65 |   0.9749 |     35.051 |   1.0225 |     35.514 |     1.1
   66 |   0.9679 |     34.352 |   1.0258 |     36.480 |     1.1
   67 |   0.9734 |     35.194 |   1.0388 |     36.231 |     1.2
   68 |   0.9724 |     35.106 |   1.0193 |     36.012 |     1.2
   69 |   0.9666 |     34.925 |   1.0168 |     35.826 |     1.2
   70 |   0.9583 |     34.550 |   1.0059 |     35.888 |     1.2
   71 |   0.9548 |     34.286 |   1.0065 |     35.576 |     1.2
   72 |   0.9502 |     34.269 |   1.0089 |     36.480 |     1.2
   73 |   0.9470 |     34.027 |   1.0101 |     35.794 |     1.3
   74 |   0.9439 |     33.664 |   1.0060 |     35.639 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 675,106

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0428 |     55.875 |   1.4769 |     45.732 |     0.0
    2 |   1.3959 |     45.705 |   1.3451 |     44.548 |     0.1
    3 |   1.3131 |     44.582 |   1.2919 |     43.988 |     0.1
    4 |   1.2699 |     43.987 |   1.2518 |     42.648 |     0.1
    5 |   1.2348 |     43.123 |   1.2287 |     43.427 |     0.1
    6 |   1.2072 |     42.501 |   1.2103 |     42.461 |     0.2
    7 |   1.1860 |     41.658 |   1.1824 |     41.620 |     0.2
    8 |   1.1629 |     40.888 |   1.1670 |     43.302 |     0.2
    9 |   1.1466 |     40.844 |   1.1587 |     41.246 |     0.3
   10 |   1.1314 |     40.188 |   1.1428 |     41.526 |     0.3
   11 |   1.1130 |     39.919 |   1.1246 |     39.907 |     0.3
   12 |   1.1014 |     39.269 |   1.1085 |     39.907 |     0.3
   13 |   1.0825 |     38.548 |   1.0885 |     38.723 |     0.4
   14 |   1.0615 |     37.678 |   1.0814 |     37.850 |     0.4
   15 |   1.0417 |     36.896 |   1.0576 |     37.383 |     0.4
   16 |   1.0222 |     36.279 |   1.0532 |     37.352 |     0.4
   17 |   1.0036 |     35.178 |   1.0336 |     35.919 |     0.5
   18 |   0.9761 |     34.115 |   1.0159 |     35.421 |     0.5
   19 |   0.9565 |     33.471 |   1.0061 |     35.234 |     0.5
   20 |   0.9332 |     32.645 |   0.9792 |     33.614 |     0.6
   21 |   0.9163 |     32.034 |   0.9664 |     33.084 |     0.6
   22 |   0.8879 |     30.773 |   0.9663 |     33.520 |     0.6
   23 |   0.8670 |     30.096 |   0.9386 |     31.900 |     0.6
   24 |   0.8484 |     29.160 |   0.9444 |     31.931 |     0.7
   25 |   0.8294 |     28.042 |   0.9282 |     31.121 |     0.7
   26 |   0.8057 |     27.668 |   0.9068 |     30.748 |     0.7
   27 |   0.7883 |     27.068 |   0.9081 |     30.717 |     0.8
   28 |   0.7709 |     26.352 |   0.8948 |     29.751 |     0.8
   29 |   0.7456 |     25.394 |   0.8839 |     29.221 |     0.8
   30 |   0.7355 |     24.865 |   0.8764 |     29.315 |     0.8
   31 |   0.7094 |     23.940 |   0.8791 |     28.941 |     0.9
   32 |   0.6900 |     22.988 |   0.8830 |     28.069 |     0.9
   33 |   0.6793 |     22.784 |   0.8713 |     27.539 |     0.9
   34 |   0.6626 |     22.167 |   0.8668 |     28.536 |     0.9
   35 |   0.6401 |     21.435 |   0.8631 |     27.882 |     1.0
   36 |   0.6247 |     20.862 |   0.8641 |     27.383 |     1.0
   37 |   0.6065 |     20.185 |   0.8774 |     27.570 |     1.0
   38 |   0.5977 |     19.882 |   0.8641 |     27.040 |     1.1
   39 |   0.5761 |     18.990 |   0.8631 |     27.072 |     1.1
   40 |   0.5688 |     18.594 |   0.8581 |     26.262 |     1.1
   41 |   0.5470 |     17.955 |   0.8662 |     27.352 |     1.1
   42 |   0.5317 |     17.619 |   0.8602 |     26.386 |     1.2
   43 |   0.5158 |     16.766 |   0.8632 |     26.293 |     1.2
   44 |   0.5064 |     16.639 |   0.8695 |     26.168 |     1.2
   45 |   0.4872 |     15.929 |   0.8565 |     25.265 |     1.3
   46 |   0.4856 |     15.857 |   0.8701 |     25.763 |     1.3
   47 |   0.4723 |     15.615 |   0.8674 |     25.296 |     1.3
   48 |   0.4601 |     15.042 |   0.8805 |     25.950 |     1.3
   49 |   0.4411 |     14.525 |   0.8786 |     25.389 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,248,418

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3842 |     61.943 |   1.8210 |     48.847 |     0.1
    2 |   1.6053 |     47.109 |   1.4746 |     45.701 |     0.1
    3 |   1.4273 |     45.777 |   1.3964 |     45.234 |     0.2
    4 |   1.3680 |     45.458 |   1.3455 |     45.171 |     0.2
    5 |   1.3292 |     45.056 |   1.3257 |     45.639 |     0.3
    6 |   1.3032 |     44.527 |   1.2977 |     44.330 |     0.4
    7 |   1.2796 |     43.811 |   1.2777 |     44.174 |     0.4
    8 |   1.2594 |     43.024 |   1.2567 |     43.925 |     0.5
    9 |   1.2381 |     42.594 |   1.2334 |     42.243 |     0.6
   10 |   1.2144 |     41.934 |   1.2176 |     41.776 |     0.6
   11 |   1.1976 |     41.510 |   1.2005 |     41.589 |     0.7
   12 |   1.1728 |     40.475 |   1.1841 |     40.467 |     0.7
   13 |   1.1494 |     39.280 |   1.1635 |     39.813 |     0.8
   14 |   1.1231 |     38.333 |   1.1393 |     38.879 |     0.9
   15 |   1.0972 |     37.171 |   1.1212 |     38.474 |     0.9
   16 |   1.0677 |     36.020 |   1.1044 |     37.414 |     1.0
   17 |   1.0361 |     34.594 |   1.0892 |     36.324 |     1.1
   18 |   1.0023 |     33.344 |   1.0793 |     36.168 |     1.1
   19 |   0.9671 |     31.940 |   1.0500 |     35.296 |     1.2
   20 |   0.9316 |     30.404 |   1.0287 |     33.614 |     1.2
   21 |   0.8942 |     29.430 |   1.0223 |     34.050 |     1.3
   22 |   0.8600 |     28.180 |   1.0098 |     33.084 |     1.4
   23 |   0.8227 |     26.836 |   0.9730 |     32.118 |     1.4
   24 |   0.7857 |     25.636 |   0.9673 |     31.495 |     1.5
   25 |   0.7463 |     23.907 |   0.9511 |     30.592 |     1.5
   26 |   0.7046 |     22.233 |   0.9421 |     30.125 |     1.6
   27 |   0.6718 |     21.209 |   0.9488 |     29.938 |     1.7
   28 |   0.6367 |     19.877 |   0.9500 |     29.346 |     1.7
   29 |   0.6053 |     19.034 |   0.9393 |     28.692 |     1.8
   30 |   0.5682 |     17.608 |   0.9452 |     28.536 |     1.9
   31 |   0.5431 |     16.821 |   0.9386 |     27.321 |     1.9
   32 |   0.5114 |     15.736 |   0.9690 |     28.442 |     2.0
   33 |   0.4895 |     15.175 |   0.9510 |     27.632 |     2.0
   34 |   0.4653 |     14.194 |   0.9607 |     27.383 |     2.1
   35 |   0.4413 |     13.506 |   0.9679 |     27.819 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,132,962

Training started
X_train.shape: torch.Size([3027, 702])
Y_train.shape: torch.Size([3027, 7])
X_dev.shape: torch.Size([535, 258])
Y_dev.shape: torch.Size([535, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1956 |     60.291 |   1.6041 |     45.763 |     0.0
    2 |   1.4777 |     46.234 |   1.4166 |     46.791 |     0.1
    3 |   1.3891 |     46.129 |   1.3686 |     45.016 |     0.1
    4 |   1.3480 |     45.595 |   1.3409 |     44.735 |     0.2
    5 |   1.3227 |     45.265 |   1.3242 |     45.732 |     0.2
    6 |   1.3056 |     45.160 |   1.3089 |     45.171 |     0.2
    7 |   1.2888 |     44.901 |   1.2910 |     45.202 |     0.3
    8 |   1.2759 |     44.582 |   1.2772 |     45.514 |     0.3
    9 |   1.2632 |     44.444 |   1.2708 |     44.143 |     0.4
   10 |   1.2501 |     43.954 |   1.2563 |     44.486 |     0.4
   11 |   1.2344 |     43.657 |   1.2347 |     42.991 |     0.4
   12 |   1.2171 |     43.073 |   1.2212 |     43.240 |     0.5
   13 |   1.1916 |     42.380 |   1.2072 |     43.707 |     0.5
   14 |   1.1714 |     41.587 |   1.1895 |     41.900 |     0.6
   15 |   1.1527 |     40.706 |   1.1561 |     41.402 |     0.6
   16 |   1.1310 |     40.271 |   1.1353 |     40.125 |     0.6
   17 |   1.1097 |     39.159 |   1.1226 |     39.751 |     0.7
   18 |   1.0897 |     38.790 |   1.1051 |     38.193 |     0.7
   19 |   1.0708 |     37.551 |   1.0935 |     37.819 |     0.8
   20 |   1.0440 |     36.934 |   1.0690 |     36.636 |     0.8
   21 |   1.0220 |     35.695 |   1.0537 |     36.542 |     0.9
   22 |   0.9959 |     34.881 |   1.0400 |     36.199 |     0.9
   23 |   0.9711 |     33.669 |   1.0229 |     35.016 |     0.9
   24 |   0.9497 |     32.882 |   1.0111 |     34.112 |     1.0
   25 |   0.9429 |     32.744 |   1.0049 |     35.078 |     1.0
   26 |   0.8991 |     30.817 |   0.9914 |     34.050 |     1.1
   27 |   0.8764 |     29.859 |   0.9745 |     32.835 |     1.1
   28 |   0.8489 |     29.044 |   0.9642 |     32.305 |     1.1
   29 |   0.8146 |     27.662 |   0.9572 |     32.150 |     1.2
   30 |   0.7962 |     26.886 |   0.9569 |     31.745 |     1.2
   31 |   0.7713 |     25.928 |   0.9422 |     30.748 |     1.3
   32 |   0.7418 |     24.794 |   0.9432 |     30.280 |     1.3
   33 |   0.7152 |     23.670 |   0.9373 |     29.969 |     1.3
   34 |   0.6937 |     22.883 |   0.9410 |     28.660 |     1.4
   35 |   0.6592 |     21.754 |   0.9425 |     29.470 |     1.4
   36 |   0.6374 |     20.758 |   0.9372 |     28.879 |     1.5
   37 |   0.6156 |     20.086 |   0.9370 |     28.255 |     1.5
   38 |   0.5915 |     19.321 |   0.9446 |     27.788 |     1.5
   39 |   0.5722 |     18.660 |   0.9409 |     27.850 |     1.6
   40 |   0.5425 |     17.669 |   0.9480 |     27.726 |     1.6
   41 |   0.5233 |     17.129 |   0.9463 |     27.414 |     1.7
Early stopping

