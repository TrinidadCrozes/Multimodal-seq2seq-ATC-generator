Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 421,410

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5369 |     68.344 |   2.0122 |     59.550 |     0.0
    2 |   1.7904 |     50.596 |   1.5867 |     45.903 |     0.1
    3 |   1.5065 |     46.044 |   1.4513 |     46.426 |     0.1
    4 |   1.4328 |     46.017 |   1.4140 |     45.903 |     0.1
    5 |   1.3999 |     46.011 |   1.3782 |     45.903 |     0.1
    6 |   1.3776 |     45.873 |   1.3655 |     45.903 |     0.2
    7 |   1.3645 |     45.642 |   1.3533 |     44.855 |     0.2
    8 |   1.3505 |     45.283 |   1.3417 |     45.964 |     0.2
    9 |   1.3423 |     45.515 |   1.3293 |     44.763 |     0.2
   10 |   1.3293 |     45.200 |   1.3210 |     44.794 |     0.3
   11 |   1.3178 |     44.924 |   1.3141 |     44.732 |     0.3
   12 |   1.3056 |     44.770 |   1.2985 |     44.331 |     0.3
   13 |   1.2910 |     44.406 |   1.2877 |     44.023 |     0.3
   14 |   1.2767 |     43.981 |   1.2727 |     43.715 |     0.4
   15 |   1.2649 |     43.909 |   1.2690 |     43.777 |     0.4
   16 |   1.2525 |     43.611 |   1.2532 |     43.623 |     0.4
   17 |   1.2388 |     43.402 |   1.2418 |     43.068 |     0.4
   18 |   1.2284 |     43.021 |   1.2312 |     43.315 |     0.5
   19 |   1.2184 |     42.817 |   1.2194 |     42.329 |     0.5
   20 |   1.2036 |     42.464 |   1.2157 |     42.637 |     0.5
   21 |   1.1892 |     41.730 |   1.1979 |     42.113 |     0.5
   22 |   1.1771 |     41.432 |   1.1911 |     41.590 |     0.6
   23 |   1.1633 |     41.140 |   1.1817 |     41.929 |     0.6
   24 |   1.1505 |     40.406 |   1.1757 |     41.497 |     0.6
   25 |   1.1375 |     40.070 |   1.1621 |     41.220 |     0.6
   26 |   1.1217 |     39.556 |   1.1643 |     41.220 |     0.7
   27 |   1.1127 |     38.961 |   1.1531 |     40.419 |     0.7
   28 |   1.0984 |     38.188 |   1.1397 |     39.895 |     0.7
   29 |   1.0824 |     37.504 |   1.1382 |     40.635 |     0.7
   30 |   1.0715 |     37.559 |   1.1270 |     39.834 |     0.8
   31 |   1.0573 |     37.057 |   1.1174 |     38.971 |     0.8
   32 |   1.0423 |     36.357 |   1.1177 |     39.649 |     0.8
   33 |   1.0352 |     35.926 |   1.1152 |     38.940 |     0.8
   34 |   1.0186 |     35.496 |   1.1183 |     38.817 |     0.9
   35 |   1.0044 |     34.817 |   1.1116 |     38.232 |     0.9
   36 |   0.9948 |     34.238 |   1.1106 |     38.509 |     0.9
   37 |   0.9898 |     34.382 |   1.1179 |     38.417 |     0.9
   38 |   0.9738 |     33.565 |   1.1160 |     38.232 |     1.0
   39 |   0.9584 |     33.074 |   1.1031 |     37.431 |     1.0
   40 |   0.9479 |     32.611 |   1.1061 |     37.523 |     1.0
   41 |   0.9355 |     32.092 |   1.0942 |     37.061 |     1.1
   42 |   0.9240 |     31.540 |   1.0950 |     36.476 |     1.1
   43 |   0.9108 |     30.884 |   1.1022 |     37.246 |     1.1
   44 |   0.8960 |     30.801 |   1.0881 |     35.890 |     1.1
   45 |   0.8867 |     30.316 |   1.0854 |     36.291 |     1.2
   46 |   0.8718 |     29.802 |   1.0705 |     34.935 |     1.2
   47 |   0.8561 |     29.069 |   1.0769 |     35.798 |     1.2
   48 |   0.8457 |     28.732 |   1.0878 |     35.490 |     1.2
   49 |   0.8281 |     28.098 |   1.0739 |     34.627 |     1.3
   50 |   0.8161 |     27.585 |   1.0811 |     34.381 |     1.3
   51 |   0.8126 |     27.149 |   1.0667 |     34.011 |     1.3
   52 |   0.7998 |     26.746 |   1.0692 |     33.426 |     1.3
   53 |   0.7812 |     25.814 |   1.0796 |     34.042 |     1.4
   54 |   0.7780 |     26.090 |   1.0530 |     33.210 |     1.4
   55 |   0.7634 |     25.345 |   1.0610 |     32.964 |     1.4
   56 |   0.7508 |     25.041 |   1.0615 |     32.779 |     1.4
   57 |   0.7323 |     24.313 |   1.0699 |     32.656 |     1.5
   58 |   0.7215 |     24.015 |   1.0748 |     32.840 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 504,610

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3742 |     62.275 |   1.7850 |     49.476 |     0.0
    2 |   1.5755 |     47.004 |   1.4499 |     45.718 |     0.1
    3 |   1.4022 |     45.449 |   1.3732 |     45.379 |     0.1
    4 |   1.3404 |     45.195 |   1.3285 |     44.886 |     0.1
    5 |   1.3051 |     44.444 |   1.3001 |     44.578 |     0.2
    6 |   1.2772 |     43.816 |   1.2847 |     43.654 |     0.2
    7 |   1.2564 |     43.600 |   1.2558 |     43.561 |     0.2
    8 |   1.2313 |     43.032 |   1.2362 |     42.822 |     0.2
    9 |   1.2126 |     42.822 |   1.2194 |     42.822 |     0.3
   10 |   1.1992 |     42.276 |   1.2095 |     43.099 |     0.3
   11 |   1.1795 |     41.349 |   1.1941 |     41.990 |     0.3
   12 |   1.1662 |     41.018 |   1.1748 |     40.881 |     0.4
   13 |   1.1550 |     40.533 |   1.1796 |     41.497 |     0.4
   14 |   1.1475 |     40.467 |   1.1622 |     42.113 |     0.4
   15 |   1.1331 |     40.241 |   1.1598 |     41.713 |     0.5
   16 |   1.1234 |     39.645 |   1.1429 |     40.696 |     0.5
   17 |   1.1096 |     38.966 |   1.1377 |     40.049 |     0.5
   18 |   1.1024 |     39.126 |   1.1242 |     39.649 |     0.5
   19 |   1.0874 |     38.150 |   1.1225 |     39.495 |     0.6
   20 |   1.0744 |     37.890 |   1.1127 |     40.203 |     0.6
   21 |   1.0614 |     37.278 |   1.1035 |     38.540 |     0.6
   22 |   1.0465 |     36.726 |   1.0962 |     37.862 |     0.7
   23 |   1.0367 |     36.130 |   1.0871 |     37.677 |     0.7
   24 |   1.0633 |     37.697 |   1.1143 |     40.511 |     0.7
   25 |   1.0487 |     37.184 |   1.0686 |     36.999 |     0.8
   26 |   1.0185 |     35.579 |   1.0710 |     37.400 |     0.8
   27 |   0.9998 |     34.977 |   1.0566 |     36.753 |     0.8
   28 |   0.9904 |     35.055 |   1.0501 |     37.030 |     0.8
   29 |   0.9798 |     34.481 |   1.0529 |     36.753 |     0.9
   30 |   1.0029 |     35.601 |   1.0656 |     37.215 |     0.9
   31 |   0.9785 |     34.575 |   1.0381 |     36.168 |     0.9
   32 |   0.9560 |     33.758 |   1.0353 |     36.014 |     1.0
   33 |   0.9404 |     32.782 |   1.0328 |     35.767 |     1.0
   34 |   0.9292 |     32.318 |   1.0199 |     34.843 |     1.0
   35 |   0.9180 |     31.987 |   1.0158 |     35.059 |     1.1
   36 |   0.9052 |     31.480 |   1.0188 |     35.367 |     1.1
   37 |   0.8957 |     31.209 |   0.9996 |     34.288 |     1.1
   38 |   0.8813 |     30.156 |   0.9948 |     33.919 |     1.1
   39 |   0.8698 |     30.040 |   0.9841 |     32.933 |     1.2
   40 |   0.8507 |     29.118 |   0.9782 |     32.933 |     1.2
   41 |   0.8397 |     28.269 |   0.9846 |     33.333 |     1.2
   42 |   0.8272 |     28.153 |   0.9759 |     33.087 |     1.3
   43 |   0.8149 |     27.739 |   0.9907 |     33.549 |     1.3
   44 |   0.8117 |     27.750 |   0.9687 |     32.163 |     1.3
   45 |   0.7865 |     26.619 |   0.9541 |     31.731 |     1.4
   46 |   0.7697 |     25.858 |   0.9642 |     31.793 |     1.4
   47 |   0.7531 |     25.268 |   0.9545 |     31.331 |     1.4
   48 |   0.7341 |     24.600 |   0.9468 |     31.146 |     1.5
   49 |   0.7179 |     23.756 |   0.9451 |     31.054 |     1.5
   50 |   0.7065 |     23.414 |   0.9562 |     31.547 |     1.5
   51 |   0.7228 |     24.137 |   0.9681 |     31.392 |     1.5
   52 |   0.6943 |     23.072 |   0.9552 |     31.547 |     1.6
   53 |   0.6607 |     21.726 |   0.9395 |     30.561 |     1.6
   54 |   0.6389 |     20.843 |   0.9569 |     31.084 |     1.6
   55 |   0.6208 |     20.291 |   0.9584 |     30.129 |     1.7
   56 |   0.6125 |     19.927 |   0.9523 |     30.099 |     1.7
   57 |   0.6001 |     19.547 |   0.9354 |     29.359 |     1.7
   58 |   0.5738 |     18.504 |   0.9628 |     30.591 |     1.8
   59 |   0.5713 |     18.410 |   0.9483 |     29.791 |     1.8
   60 |   0.5398 |     17.174 |   0.9535 |     29.421 |     1.8
   61 |   0.5188 |     16.336 |   0.9709 |     29.852 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,529,890

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0188 |     55.865 |   1.4841 |     45.687 |     0.1
    2 |   1.3961 |     45.658 |   1.3507 |     45.625 |     0.1
    3 |   1.3157 |     44.720 |   1.3006 |     44.270 |     0.2
    4 |   1.2749 |     43.904 |   1.2724 |     43.962 |     0.2
    5 |   1.2420 |     43.313 |   1.2362 |     42.175 |     0.3
    6 |   1.2125 |     42.547 |   1.2201 |     42.421 |     0.3
    7 |   1.1833 |     41.851 |   1.1925 |     41.898 |     0.4
    8 |   1.1594 |     40.715 |   1.1720 |     42.113 |     0.5
    9 |   1.1291 |     39.772 |   1.1509 |     41.035 |     0.5
   10 |   1.1061 |     38.845 |   1.1439 |     40.327 |     0.6
   11 |   1.0713 |     37.339 |   1.1019 |     38.478 |     0.6
   12 |   1.0456 |     36.329 |   1.0992 |     38.879 |     0.7
   13 |   1.0148 |     35.347 |   1.0656 |     37.646 |     0.7
   14 |   0.9891 |     34.332 |   1.0498 |     35.459 |     0.8
   15 |   0.9574 |     33.035 |   1.0434 |     35.675 |     0.9
   16 |   0.9272 |     31.800 |   1.0274 |     34.258 |     0.9
   17 |   0.8961 |     30.602 |   1.0061 |     33.703 |     1.0
   18 |   0.8672 |     29.179 |   0.9866 |     33.148 |     1.0
   19 |   0.8381 |     28.247 |   0.9747 |     32.255 |     1.1
   20 |   0.8107 |     27.491 |   0.9554 |     31.454 |     1.1
   21 |   0.7776 |     25.830 |   0.9472 |     31.177 |     1.2
   22 |   0.7468 |     24.710 |   0.9478 |     31.300 |     1.3
   23 |   0.7113 |     23.370 |   0.9274 |     30.222 |     1.3
   24 |   0.6744 |     21.913 |   0.9240 |     29.020 |     1.4
   25 |   0.6499 |     21.240 |   0.9310 |     29.452 |     1.4
   26 |   0.6152 |     19.740 |   0.9377 |     30.037 |     1.5
   27 |   0.5795 |     18.857 |   0.9298 |     28.805 |     1.5
   28 |   0.5529 |     17.765 |   0.9367 |     28.835 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 439,970

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5624 |     68.708 |   1.9901 |     59.057 |     0.0
    2 |   1.7564 |     50.044 |   1.5641 |     45.903 |     0.1
    3 |   1.4952 |     46.011 |   1.4454 |     45.903 |     0.1
    4 |   1.4247 |     46.028 |   1.4031 |     45.903 |     0.1
    5 |   1.3923 |     45.774 |   1.3746 |     45.071 |     0.1
    6 |   1.3686 |     45.421 |   1.3596 |     45.194 |     0.2
    7 |   1.3490 |     45.454 |   1.3414 |     45.163 |     0.2
    8 |   1.3310 |     45.134 |   1.3244 |     44.824 |     0.2
    9 |   1.3128 |     44.941 |   1.3092 |     45.071 |     0.2
   10 |   1.3000 |     44.593 |   1.2974 |     44.331 |     0.3
   11 |   1.2832 |     44.301 |   1.2849 |     44.547 |     0.3
   12 |   1.2648 |     43.689 |   1.2651 |     43.654 |     0.3
   13 |   1.2486 |     43.385 |   1.2517 |     43.068 |     0.3
   14 |   1.2307 |     42.613 |   1.2423 |     43.346 |     0.4
   15 |   1.2169 |     42.210 |   1.2208 |     42.822 |     0.4
   16 |   1.2023 |     42.017 |   1.2071 |     41.929 |     0.4
   17 |   1.1833 |     41.195 |   1.1949 |     41.774 |     0.4
   18 |   1.1712 |     40.914 |   1.1896 |     41.374 |     0.5
   19 |   1.1549 |     40.401 |   1.1799 |     41.713 |     0.5
   20 |   1.1394 |     40.103 |   1.1707 |     40.388 |     0.5
   21 |   1.1196 |     39.165 |   1.1613 |     41.066 |     0.5
   22 |   1.1051 |     38.470 |   1.1539 |     40.296 |     0.6
   23 |   1.0960 |     37.957 |   1.1468 |     39.741 |     0.6
   24 |   1.0770 |     37.670 |   1.1276 |     39.217 |     0.6
   25 |   1.0570 |     36.859 |   1.1236 |     39.464 |     0.6
   26 |   1.0405 |     35.998 |   1.1175 |     38.694 |     0.7
   27 |   1.0264 |     35.364 |   1.1213 |     39.002 |     0.7
   28 |   1.0101 |     34.784 |   1.1051 |     38.262 |     0.7
   29 |   0.9897 |     34.172 |   1.0988 |     37.092 |     0.7
   30 |   0.9764 |     33.593 |   1.0992 |     37.523 |     0.8
   31 |   0.9576 |     32.633 |   1.0932 |     37.061 |     0.8
   32 |   0.9417 |     31.899 |   1.0838 |     35.860 |     0.8
   33 |   0.9233 |     31.430 |   1.0852 |     35.551 |     0.9
   34 |   0.9104 |     30.900 |   1.0845 |     35.459 |     0.9
   35 |   0.8889 |     30.426 |   1.0797 |     35.397 |     0.9
   36 |   0.8785 |     29.924 |   1.0811 |     35.028 |     0.9
   37 |   0.8606 |     29.107 |   1.0749 |     34.134 |     1.0
   38 |   0.8409 |     28.335 |   1.0747 |     34.134 |     1.0
   39 |   0.8332 |     28.147 |   1.0780 |     34.258 |     1.0
   40 |   0.8124 |     27.259 |   1.0760 |     34.381 |     1.0
   41 |   0.7961 |     26.851 |   1.0851 |     33.457 |     1.1
   42 |   0.7866 |     26.360 |   1.0762 |     33.857 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,555,106

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4083 |     65.017 |   1.9206 |     53.697 |     0.1
    2 |   1.6882 |     47.804 |   1.5264 |     45.933 |     0.1
    3 |   1.4605 |     45.940 |   1.4194 |     45.779 |     0.2
    4 |   1.3969 |     45.846 |   1.3883 |     45.656 |     0.3
    5 |   1.3592 |     45.669 |   1.3437 |     45.595 |     0.4
    6 |   1.3291 |     45.057 |   1.3240 |     45.410 |     0.4
    7 |   1.3063 |     44.489 |   1.3050 |     44.301 |     0.5
    8 |   1.2873 |     43.865 |   1.2905 |     44.239 |     0.6
    9 |   1.2694 |     43.898 |   1.2759 |     43.931 |     0.7
   10 |   1.2530 |     43.871 |   1.2579 |     43.685 |     0.7
   11 |   1.2364 |     43.418 |   1.2481 |     44.547 |     0.8
   12 |   1.2200 |     43.187 |   1.2356 |     43.869 |     0.9
   13 |   1.2085 |     42.690 |   1.2248 |     42.699 |     1.0
   14 |   1.1933 |     42.293 |   1.2140 |     42.267 |     1.0
   15 |   1.1813 |     41.653 |   1.2028 |     42.052 |     1.1
   16 |   1.1701 |     41.184 |   1.1972 |     42.668 |     1.2
   17 |   1.1596 |     40.996 |   1.1904 |     41.651 |     1.3
   18 |   1.1461 |     40.334 |   1.1738 |     41.559 |     1.3
   19 |   1.1350 |     39.783 |   1.1681 |     40.912 |     1.4
   20 |   1.1245 |     39.639 |   1.1542 |     40.727 |     1.5
   21 |   1.1137 |     38.861 |   1.1482 |     40.974 |     1.6
   22 |   1.1022 |     38.784 |   1.1462 |     41.097 |     1.6
   23 |   1.0912 |     38.403 |   1.1354 |     40.173 |     1.7
   24 |   1.0789 |     37.846 |   1.1338 |     40.974 |     1.8
   25 |   1.0695 |     37.686 |   1.1200 |     40.080 |     1.8
   26 |   1.0573 |     37.146 |   1.1241 |     39.526 |     1.9
   27 |   1.0447 |     36.743 |   1.1073 |     39.649 |     2.0
   28 |   1.0366 |     36.588 |   1.1026 |     39.156 |     2.1
   29 |   1.0262 |     36.075 |   1.0925 |     38.694 |     2.1
   30 |   1.0161 |     35.766 |   1.0845 |     38.817 |     2.2
   31 |   1.0037 |     35.187 |   1.0855 |     37.862 |     2.3
   32 |   0.9916 |     34.668 |   1.0737 |     37.985 |     2.4
   33 |   0.9808 |     34.393 |   1.0672 |     37.338 |     2.4
   34 |   0.9671 |     33.664 |   1.0576 |     36.568 |     2.5
   35 |   0.9547 |     33.124 |   1.0557 |     36.137 |     2.6
   36 |   0.9456 |     32.682 |   1.0494 |     35.613 |     2.7
   37 |   0.9287 |     32.087 |   1.0382 |     35.428 |     2.7
   38 |   0.9164 |     31.436 |   1.0382 |     35.644 |     2.8
   39 |   0.8993 |     30.575 |   1.0284 |     34.997 |     2.9
   40 |   0.8864 |     30.067 |   1.0252 |     34.504 |     2.9
   41 |   0.8655 |     29.163 |   1.0255 |     33.980 |     3.0
   42 |   0.8559 |     28.699 |   1.0126 |     33.457 |     3.1
   43 |   0.8350 |     27.783 |   1.0131 |     33.765 |     3.2
   44 |   0.8197 |     27.143 |   1.0064 |     32.933 |     3.2
   45 |   0.8033 |     26.735 |   1.0010 |     32.101 |     3.3
   46 |   0.7853 |     25.885 |   0.9866 |     32.039 |     3.4
   47 |   0.7665 |     25.025 |   0.9885 |     31.516 |     3.5
   48 |   0.7530 |     24.865 |   0.9885 |     31.608 |     3.5
   49 |   0.7336 |     24.048 |   0.9773 |     31.331 |     3.6
   50 |   0.7162 |     23.381 |   0.9698 |     30.807 |     3.7
   51 |   0.6977 |     22.664 |   0.9665 |     31.516 |     3.8
   52 |   0.6772 |     22.128 |   0.9633 |     30.253 |     3.8
   53 |   0.6570 |     21.053 |   0.9704 |     30.376 |     3.9
   54 |   0.6358 |     20.567 |   0.9630 |     30.191 |     4.0
   55 |   0.6167 |     19.541 |   0.9652 |     29.883 |     4.1
   56 |   0.6024 |     19.458 |   0.9610 |     29.298 |     4.1
   57 |   0.5914 |     18.758 |   0.9717 |     30.407 |     4.2
   58 |   0.5679 |     18.018 |   0.9893 |     29.482 |     4.3
   59 |   0.5428 |     17.158 |   0.9813 |     29.174 |     4.4
   60 |   0.5485 |     17.472 |   0.9725 |     29.236 |     4.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,524,514

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2059 |     60.609 |   1.6081 |     45.903 |     0.1
    2 |   1.4777 |     46.094 |   1.4172 |     45.903 |     0.1
    3 |   1.4032 |     45.945 |   1.3815 |     45.872 |     0.2
    4 |   1.3710 |     45.829 |   1.3525 |     45.348 |     0.2
    5 |   1.3506 |     45.498 |   1.3334 |     45.718 |     0.3
    6 |   1.3267 |     45.471 |   1.3132 |     44.147 |     0.3
    7 |   1.3097 |     45.211 |   1.3106 |     45.225 |     0.4
    8 |   1.2933 |     45.040 |   1.2902 |     44.670 |     0.5
    9 |   1.2771 |     44.786 |   1.2802 |     44.455 |     0.5
   10 |   1.2643 |     44.571 |   1.2634 |     43.900 |     0.6
   11 |   1.2490 |     44.053 |   1.2505 |     43.500 |     0.6
   12 |   1.2334 |     43.633 |   1.2312 |     43.161 |     0.7
   13 |   1.2179 |     43.319 |   1.2200 |     42.452 |     0.7
   14 |   1.2054 |     42.514 |   1.2066 |     41.744 |     0.8
   15 |   1.1882 |     42.023 |   1.1908 |     41.898 |     0.9
   16 |   1.1718 |     41.747 |   1.1820 |     42.267 |     0.9
   17 |   1.1527 |     41.206 |   1.1625 |     42.267 |     1.0
   18 |   1.1314 |     40.434 |   1.1547 |     40.696 |     1.0
   19 |   1.1133 |     39.678 |   1.1407 |     40.696 |     1.1
   20 |   1.0948 |     38.928 |   1.1304 |     40.912 |     1.1
   21 |   1.0731 |     38.326 |   1.1206 |     39.556 |     1.2
   22 |   1.0580 |     37.730 |   1.1031 |     39.094 |     1.3
   23 |   1.0370 |     36.952 |   1.0941 |     38.262 |     1.3
   24 |   1.0145 |     35.805 |   1.0791 |     37.523 |     1.4
   25 |   0.9919 |     35.071 |   1.0614 |     37.338 |     1.4
   26 |   0.9772 |     34.475 |   1.0514 |     37.153 |     1.5
   27 |   0.9493 |     33.460 |   1.0300 |     36.322 |     1.5
   28 |   0.9197 |     31.976 |   1.0291 |     35.428 |     1.6
   29 |   0.8948 |     30.746 |   1.0056 |     33.857 |     1.7
   30 |   0.8733 |     29.836 |   1.0010 |     33.703 |     1.7
   31 |   0.8479 |     28.964 |   0.9791 |     32.409 |     1.8
   32 |   0.8142 |     27.568 |   0.9860 |     31.916 |     1.8
   33 |   0.7844 |     26.255 |   0.9668 |     31.547 |     1.9
   34 |   0.7556 |     25.317 |   0.9644 |     30.900 |     1.9
   35 |   0.7239 |     23.971 |   0.9647 |     31.824 |     2.0
   36 |   0.6994 |     23.138 |   0.9482 |     29.975 |     2.1
   37 |   0.6713 |     21.930 |   0.9776 |     31.423 |     2.1
   38 |   0.6438 |     21.224 |   0.9708 |     30.099 |     2.2
   39 |   0.6072 |     19.618 |   0.9725 |     29.945 |     2.2
   40 |   0.5769 |     18.531 |   0.9547 |     29.174 |     2.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,132,834

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1955 |     59.853 |   1.6006 |     45.903 |     0.0
    2 |   1.4776 |     46.116 |   1.4133 |     45.903 |     0.1
    3 |   1.3946 |     46.055 |   1.3643 |     45.625 |     0.1
    4 |   1.3629 |     45.597 |   1.3517 |     45.441 |     0.2
    5 |   1.3384 |     45.355 |   1.3273 |     45.287 |     0.2
    6 |   1.3160 |     45.079 |   1.3119 |     44.824 |     0.2
    7 |   1.2954 |     44.930 |   1.2834 |     44.239 |     0.3
    8 |   1.2790 |     44.455 |   1.2766 |     44.609 |     0.3
    9 |   1.2627 |     44.621 |   1.2595 |     43.900 |     0.4
   10 |   1.2418 |     43.987 |   1.2433 |     43.438 |     0.4
   11 |   1.2252 |     43.667 |   1.2274 |     43.715 |     0.4
   12 |   1.2071 |     43.021 |   1.2058 |     42.144 |     0.5
   13 |   1.1868 |     42.249 |   1.1947 |     42.052 |     0.5
   14 |   1.1642 |     41.272 |   1.1751 |     42.483 |     0.6
   15 |   1.1427 |     40.732 |   1.1581 |     41.097 |     0.6
   16 |   1.1230 |     39.954 |   1.1379 |     40.265 |     0.6
   17 |   1.1037 |     38.955 |   1.1263 |     40.819 |     0.7
   18 |   1.0784 |     37.934 |   1.1086 |     39.372 |     0.7
   19 |   1.0595 |     37.201 |   1.0940 |     39.033 |     0.8
   20 |   1.0346 |     36.312 |   1.0761 |     38.694 |     0.8
   21 |   1.0154 |     35.650 |   1.0637 |     37.646 |     0.8
   22 |   0.9929 |     34.795 |   1.0592 |     37.307 |     0.9
   23 |   0.9718 |     33.951 |   1.0551 |     37.400 |     0.9
   24 |   0.9588 |     33.433 |   1.0396 |     36.352 |     1.0
   25 |   0.9309 |     32.671 |   1.0320 |     36.044 |     1.0
   26 |   0.9115 |     31.833 |   1.0092 |     35.644 |     1.0
   27 |   0.8898 |     31.276 |   1.0140 |     34.997 |     1.1
   28 |   0.8709 |     30.084 |   1.0086 |     33.795 |     1.1
   29 |   0.8492 |     29.102 |   0.9984 |     33.826 |     1.2
   30 |   0.8267 |     28.186 |   0.9819 |     32.902 |     1.2
   31 |   0.8000 |     26.934 |   0.9835 |     32.532 |     1.2
   32 |   0.7820 |     26.669 |   0.9949 |     32.009 |     1.3
   33 |   0.7665 |     25.830 |   0.9846 |     32.471 |     1.3
   34 |   0.7379 |     25.008 |   0.9725 |     31.362 |     1.4
   35 |   0.7213 |     24.175 |   0.9801 |     30.961 |     1.4
   36 |   0.7017 |     23.563 |   0.9738 |     30.776 |     1.4
   37 |   0.6845 |     22.741 |   0.9625 |     30.499 |     1.5
   38 |   0.6645 |     22.139 |   0.9816 |     30.499 |     1.5
   39 |   0.6411 |     21.218 |   0.9579 |     29.945 |     1.6
   40 |   0.6182 |     20.369 |   0.9694 |     29.636 |     1.6
   41 |   0.6021 |     19.916 |   0.9595 |     29.082 |     1.6
   42 |   0.5896 |     19.381 |   0.9721 |     28.990 |     1.7
   43 |   0.5682 |     18.614 |   0.9897 |     29.945 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 717,602

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3603 |     61.337 |   1.8180 |     48.768 |     0.0
    2 |   1.6001 |     47.021 |   1.4679 |     45.687 |     0.1
    3 |   1.4206 |     45.509 |   1.3913 |     45.595 |     0.1
    4 |   1.3635 |     45.487 |   1.3487 |     45.317 |     0.2
    5 |   1.3229 |     44.500 |   1.3236 |     44.640 |     0.2
    6 |   1.2949 |     43.738 |   1.3017 |     43.962 |     0.2
    7 |   1.2726 |     43.606 |   1.2811 |     43.561 |     0.3
    8 |   1.2545 |     43.291 |   1.2581 |     43.561 |     0.3
    9 |   1.2319 |     42.762 |   1.2506 |     43.469 |     0.3
   10 |   1.2136 |     42.475 |   1.2215 |     42.421 |     0.4
   11 |   1.1943 |     42.034 |   1.2090 |     41.774 |     0.4
   12 |   1.1782 |     41.300 |   1.1951 |     41.682 |     0.5
   13 |   1.1619 |     40.770 |   1.1803 |     40.696 |     0.5
   14 |   1.1444 |     39.777 |   1.1680 |     40.481 |     0.5
   15 |   1.1340 |     39.545 |   1.1644 |     40.481 |     0.6
   16 |   1.1172 |     38.569 |   1.1517 |     40.450 |     0.6
   17 |   1.1040 |     38.194 |   1.1381 |     39.279 |     0.6
   18 |   1.0900 |     37.714 |   1.1310 |     39.217 |     0.7
   19 |   1.0758 |     37.416 |   1.1177 |     38.540 |     0.7
   20 |   1.0631 |     36.787 |   1.1146 |     39.125 |     0.8
   21 |   1.0494 |     36.285 |   1.1134 |     38.694 |     0.8
   22 |   1.0380 |     36.070 |   1.0976 |     38.663 |     0.8
   23 |   1.0265 |     35.761 |   1.0808 |     37.338 |     0.9
   24 |   1.0137 |     35.148 |   1.0814 |     37.677 |     0.9
   25 |   1.0013 |     34.735 |   1.0669 |     37.277 |     0.9
   26 |   0.9879 |     34.078 |   1.0572 |     36.630 |     1.0
   27 |   0.9747 |     33.753 |   1.0566 |     36.075 |     1.0
   28 |   0.9557 |     32.831 |   1.0377 |     35.613 |     1.1
   29 |   0.9401 |     32.462 |   1.0278 |     35.367 |     1.1
   30 |   0.9263 |     31.717 |   1.0260 |     35.243 |     1.1
   31 |   0.9238 |     31.436 |   1.0539 |     35.613 |     1.2
   32 |   0.9119 |     31.309 |   1.0134 |     34.288 |     1.2
   33 |   0.8841 |     29.946 |   1.0102 |     34.227 |     1.2
   34 |   0.8675 |     29.284 |   0.9953 |     33.395 |     1.3
   35 |   0.8510 |     28.578 |   0.9932 |     33.303 |     1.3
   36 |   0.8375 |     27.844 |   0.9788 |     32.563 |     1.4
   37 |   0.8329 |     27.993 |   0.9864 |     32.532 |     1.4
   38 |   0.8076 |     26.961 |   0.9773 |     31.793 |     1.4
   39 |   0.7897 |     26.095 |   0.9783 |     33.056 |     1.5
   40 |   0.7931 |     26.470 |   0.9606 |     31.885 |     1.5
   41 |   0.7594 |     25.141 |   0.9766 |     31.916 |     1.6
   42 |   0.7381 |     24.374 |   0.9686 |     31.023 |     1.6
   43 |   0.7221 |     23.568 |   0.9566 |     30.900 |     1.6
   44 |   0.7047 |     22.961 |   0.9663 |     30.499 |     1.7
   45 |   0.6896 |     22.680 |   0.9605 |     30.376 |     1.7
   46 |   0.6657 |     21.693 |   0.9622 |     30.160 |     1.7
   47 |   0.6467 |     20.926 |   0.9672 |     30.160 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,627,042

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1975 |     59.588 |   1.6123 |     49.076 |     0.1
    2 |   1.4809 |     46.204 |   1.4230 |     45.903 |     0.1
    3 |   1.4027 |     46.083 |   1.3874 |     45.502 |     0.2
    4 |   1.3705 |     45.697 |   1.3571 |     45.287 |     0.2
    5 |   1.3381 |     45.333 |   1.3267 |     45.163 |     0.3
    6 |   1.3070 |     45.178 |   1.3010 |     45.009 |     0.4
    7 |   1.2836 |     44.726 |   1.2759 |     44.177 |     0.4
    8 |   1.2648 |     44.384 |   1.2663 |     44.424 |     0.5
    9 |   1.2468 |     43.920 |   1.2457 |     44.085 |     0.5
   10 |   1.2340 |     43.678 |   1.2365 |     42.760 |     0.6
   11 |   1.2217 |     43.231 |   1.2174 |     42.606 |     0.7
   12 |   1.2057 |     42.690 |   1.2403 |     42.853 |     0.7
   13 |   1.1970 |     42.905 |   1.2075 |     43.068 |     0.8
   14 |   1.1873 |     42.216 |   1.2033 |     42.267 |     0.8
   15 |   1.1765 |     42.078 |   1.1915 |     42.575 |     0.9
   16 |   1.1644 |     41.515 |   1.1823 |     42.083 |     1.0
   17 |   1.1508 |     41.173 |   1.1771 |     42.391 |     1.0
   18 |   1.1421 |     40.925 |   1.1689 |     41.528 |     1.1
   19 |   1.1307 |     40.627 |   1.1583 |     41.251 |     1.1
   20 |   1.1156 |     40.224 |   1.1476 |     40.974 |     1.2
   21 |   1.1076 |     39.876 |   1.1426 |     40.789 |     1.3
   22 |   1.0972 |     39.590 |   1.1281 |     40.481 |     1.3
   23 |   1.0864 |     39.027 |   1.1197 |     40.018 |     1.4
   24 |   1.0685 |     38.608 |   1.1455 |     40.481 |     1.4
   25 |   1.0634 |     38.442 |   1.1153 |     40.450 |     1.5
   26 |   1.0476 |     37.355 |   1.0990 |     39.464 |     1.6
   27 |   1.0369 |     37.272 |   1.1106 |     40.234 |     1.6
   28 |   1.0236 |     36.660 |   1.0773 |     38.262 |     1.7
   29 |   1.0108 |     36.191 |   1.0643 |     37.739 |     1.8
   30 |   0.9980 |     35.617 |   1.0656 |     37.554 |     1.8
   31 |   0.9865 |     35.126 |   1.0569 |     36.969 |     1.9
   32 |   0.9693 |     34.233 |   1.0487 |     36.599 |     1.9
   33 |   0.9615 |     34.078 |   1.0422 |     35.736 |     2.0
   34 |   0.9494 |     33.377 |   1.0330 |     36.260 |     2.1
   35 |   0.9401 |     33.245 |   1.0266 |     34.904 |     2.1
   36 |   0.9204 |     32.230 |   1.0182 |     35.274 |     2.2
   37 |   0.9084 |     31.711 |   1.0158 |     34.874 |     2.2
   38 |   0.9009 |     31.551 |   1.0139 |     35.336 |     2.3
   39 |   0.8889 |     31.000 |   1.0059 |     34.165 |     2.4
   40 |   0.8771 |     30.691 |   0.9909 |     33.611 |     2.4
   41 |   0.8654 |     30.040 |   0.9870 |     33.457 |     2.5
   42 |   0.8498 |     29.311 |   0.9840 |     32.902 |     2.5
   43 |   0.8405 |     28.892 |   0.9895 |     33.580 |     2.6
   44 |   0.8249 |     28.401 |   0.9893 |     33.611 |     2.7
   45 |   0.8199 |     28.153 |   0.9789 |     32.224 |     2.7
   46 |   0.8029 |     27.419 |   0.9669 |     32.009 |     2.8
   47 |   0.7875 |     26.763 |   0.9718 |     32.871 |     2.8
   48 |   0.7782 |     26.498 |   0.9855 |     32.471 |     2.9
   49 |   0.7664 |     26.310 |   0.9814 |     32.286 |     3.0
   50 |   0.7595 |     25.687 |   0.9674 |     31.947 |     3.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 718,626

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1965 |     59.633 |   1.5970 |     45.903 |     0.0
    2 |   1.4747 |     46.066 |   1.4172 |     45.903 |     0.0
    3 |   1.4067 |     46.155 |   1.3917 |     45.903 |     0.1
    4 |   1.3760 |     45.575 |   1.3588 |     45.410 |     0.1
    5 |   1.3496 |     45.454 |   1.3327 |     45.410 |     0.1
    6 |   1.3256 |     45.272 |   1.3151 |     45.071 |     0.1
    7 |   1.3072 |     44.996 |   1.2988 |     44.424 |     0.2
    8 |   1.2896 |     44.615 |   1.2835 |     43.592 |     0.2
    9 |   1.2746 |     44.329 |   1.2791 |     44.455 |     0.2
   10 |   1.2583 |     44.196 |   1.2669 |     43.962 |     0.2
   11 |   1.2448 |     44.086 |   1.2544 |     44.640 |     0.3
   12 |   1.2256 |     43.402 |   1.2351 |     44.054 |     0.3
   13 |   1.2131 |     43.010 |   1.2222 |     42.699 |     0.3
   14 |   1.1973 |     42.563 |   1.2116 |     42.545 |     0.3
   15 |   1.1826 |     42.000 |   1.1975 |     42.144 |     0.4
   16 |   1.1656 |     41.421 |   1.1863 |     41.651 |     0.4
   17 |   1.1483 |     40.759 |   1.1779 |     42.083 |     0.4
   18 |   1.1288 |     40.086 |   1.1566 |     40.850 |     0.4
   19 |   1.1045 |     39.242 |   1.1322 |     40.234 |     0.5
   20 |   1.0829 |     38.199 |   1.1269 |     39.618 |     0.5
   21 |   1.0580 |     37.328 |   1.1120 |     39.279 |     0.5
   22 |   1.0330 |     36.257 |   1.0919 |     39.217 |     0.5
   23 |   1.0058 |     35.082 |   1.0754 |     37.277 |     0.6
   24 |   0.9839 |     34.674 |   1.0621 |     37.523 |     0.6
   25 |   0.9598 |     33.245 |   1.0569 |     36.969 |     0.6
   26 |   0.9397 |     32.859 |   1.0335 |     36.014 |     0.6
   27 |   0.9079 |     31.491 |   1.0148 |     34.196 |     0.7
   28 |   0.8811 |     30.238 |   1.0178 |     33.857 |     0.7
   29 |   0.8600 |     29.731 |   1.0007 |     33.611 |     0.7
   30 |   0.8331 |     28.462 |   0.9954 |     33.395 |     0.7
   31 |   0.8080 |     27.408 |   0.9882 |     32.378 |     0.8
   32 |   0.7851 |     26.437 |   0.9777 |     32.286 |     0.8
   33 |   0.7618 |     25.676 |   0.9764 |     31.300 |     0.8
   34 |   0.7355 |     24.754 |   0.9793 |     31.639 |     0.8
   35 |   0.7070 |     23.640 |   0.9799 |     31.701 |     0.9
   36 |   0.6883 |     23.221 |   0.9792 |     31.454 |     0.9
   37 |   0.6625 |     22.162 |   0.9880 |     30.961 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,529,890

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9856 |     54.927 |   1.4761 |     45.625 |     0.1
    2 |   1.4004 |     45.609 |   1.3626 |     44.578 |     0.1
    3 |   1.3216 |     44.748 |   1.3016 |     44.455 |     0.2
    4 |   1.2792 |     44.091 |   1.2700 |     44.116 |     0.2
    5 |   1.2469 |     43.694 |   1.2352 |     43.530 |     0.3
    6 |   1.2135 |     42.751 |   1.2197 |     42.976 |     0.4
    7 |   1.1884 |     42.122 |   1.1928 |     42.267 |     0.4
    8 |   1.1620 |     41.267 |   1.1732 |     41.682 |     0.5
    9 |   1.1376 |     40.323 |   1.1513 |     40.696 |     0.5
   10 |   1.1114 |     39.027 |   1.1330 |     39.556 |     0.6
   11 |   1.0939 |     38.431 |   1.1130 |     39.495 |     0.7
   12 |   1.0668 |     37.443 |   1.1125 |     39.464 |     0.7
   13 |   1.0465 |     36.815 |   1.0817 |     37.985 |     0.8
   14 |   1.0165 |     35.391 |   1.0550 |     36.075 |     0.8
   15 |   0.9914 |     34.547 |   1.0461 |     36.106 |     0.9
   16 |   0.9659 |     33.880 |   1.0302 |     35.428 |     1.0
   17 |   0.9381 |     32.434 |   1.0240 |     35.120 |     1.0
   18 |   0.9102 |     31.253 |   0.9867 |     33.457 |     1.1
   19 |   0.8826 |     30.089 |   0.9696 |     32.502 |     1.1
   20 |   0.8581 |     29.129 |   0.9619 |     31.670 |     1.2
   21 |   0.8274 |     27.949 |   0.9372 |     31.269 |     1.3
   22 |   0.7970 |     26.807 |   0.9475 |     31.855 |     1.3
   23 |   0.7759 |     26.112 |   0.9292 |     31.208 |     1.4
   24 |   0.7427 |     24.848 |   0.9276 |     30.930 |     1.4
   25 |   0.7151 |     23.734 |   0.9089 |     29.883 |     1.5
   26 |   0.6879 |     22.614 |   0.9123 |     29.452 |     1.6
   27 |   0.6645 |     21.979 |   0.9120 |     29.482 |     1.6
   28 |   0.6381 |     21.036 |   0.9125 |     28.712 |     1.7
   29 |   0.6115 |     20.170 |   0.9024 |     28.158 |     1.8
   30 |   0.5837 |     19.028 |   0.8994 |     27.911 |     1.8
   31 |   0.5592 |     18.189 |   0.9109 |     27.973 |     1.9
   32 |   0.5354 |     17.472 |   0.9062 |     28.466 |     1.9
   33 |   0.5115 |     16.771 |   0.9138 |     27.295 |     2.0
   34 |   0.4915 |     16.032 |   0.9024 |     27.480 |     2.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,063,714

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1892 |     59.743 |   1.6126 |     45.903 |     0.0
    2 |   1.4812 |     46.155 |   1.4183 |     45.903 |     0.1
    3 |   1.4043 |     46.022 |   1.3895 |     45.903 |     0.1
    4 |   1.3779 |     45.895 |   1.3637 |     45.903 |     0.2
    5 |   1.3518 |     44.836 |   1.3409 |     45.379 |     0.2
    6 |   1.3310 |     44.902 |   1.3184 |     44.609 |     0.2
    7 |   1.3117 |     44.809 |   1.3062 |     44.701 |     0.3
    8 |   1.2934 |     44.566 |   1.2905 |     44.917 |     0.3
    9 |   1.2725 |     44.229 |   1.2772 |     44.670 |     0.4
   10 |   1.2561 |     43.893 |   1.2508 |     44.177 |     0.4
   11 |   1.2352 |     43.948 |   1.2367 |     44.701 |     0.4
   12 |   1.2209 |     43.518 |   1.2156 |     43.161 |     0.5
   13 |   1.2050 |     43.093 |   1.2055 |     43.376 |     0.5
   14 |   1.1855 |     42.205 |   1.1841 |     42.360 |     0.6
   15 |   1.1719 |     41.780 |   1.1721 |     41.343 |     0.6
   16 |   1.1562 |     41.311 |   1.1634 |     40.758 |     0.6
   17 |   1.1420 |     40.649 |   1.1549 |     41.251 |     0.7
   18 |   1.1267 |     40.180 |   1.1409 |     40.203 |     0.7
   19 |   1.1116 |     39.849 |   1.1251 |     40.727 |     0.8
   20 |   1.0945 |     39.016 |   1.1170 |     40.265 |     0.8
   21 |   1.0799 |     38.161 |   1.0951 |     39.125 |     0.8
   22 |   1.0655 |     37.841 |   1.0860 |     38.571 |     0.9
   23 |   1.0512 |     37.300 |   1.0742 |     37.893 |     0.9
   24 |   1.0324 |     36.428 |   1.0656 |     38.848 |     1.0
   25 |   1.0159 |     36.042 |   1.0512 |     37.739 |     1.0
   26 |   1.0001 |     35.364 |   1.0465 |     36.630 |     1.0
   27 |   0.9803 |     34.702 |   1.0286 |     36.414 |     1.1
   28 |   0.9654 |     33.995 |   1.0338 |     36.229 |     1.1
   29 |   0.9494 |     33.504 |   1.0180 |     34.997 |     1.2
   30 |   0.9305 |     32.583 |   1.0305 |     35.767 |     1.2
   31 |   0.9183 |     32.031 |   0.9960 |     34.104 |     1.3
   32 |   0.8932 |     31.132 |   0.9806 |     33.087 |     1.3
   33 |   0.8716 |     30.012 |   0.9774 |     33.395 |     1.3
   34 |   0.8564 |     29.416 |   0.9653 |     33.025 |     1.4
   35 |   0.8308 |     28.545 |   0.9519 |     31.701 |     1.4
   36 |   0.8141 |     27.717 |   0.9596 |     31.947 |     1.5
   37 |   0.8121 |     27.734 |   0.9759 |     32.348 |     1.5
   38 |   0.7895 |     26.834 |   0.9490 |     31.855 |     1.5
   39 |   0.7643 |     25.941 |   0.9526 |     31.701 |     1.6
   40 |   0.7464 |     25.284 |   0.9509 |     30.715 |     1.6
   41 |   0.7195 |     24.241 |   0.9597 |     30.591 |     1.7
   42 |   0.7073 |     23.817 |   0.9373 |     30.037 |     1.7
   43 |   0.6850 |     22.912 |   0.9358 |     29.482 |     1.7
   44 |   0.6613 |     21.930 |   0.9267 |     29.513 |     1.8
   45 |   0.6465 |     21.577 |   0.9356 |     29.390 |     1.8
   46 |   0.6221 |     20.771 |   0.9355 |     28.404 |     1.9
   47 |   0.6000 |     19.960 |   0.9305 |     28.404 |     1.9
   48 |   0.5843 |     19.497 |   0.9437 |     28.343 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 732,322

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5767 |     69.999 |   2.0420 |     59.550 |     0.0
    2 |   1.7812 |     50.474 |   1.5749 |     45.903 |     0.1
    3 |   1.4924 |     46.105 |   1.4421 |     45.903 |     0.1
    4 |   1.4241 |     46.094 |   1.4097 |     46.426 |     0.2
    5 |   1.3975 |     46.083 |   1.3884 |     45.903 |     0.2
    6 |   1.3777 |     46.011 |   1.3674 |     45.687 |     0.2
    7 |   1.3545 |     45.636 |   1.3443 |     45.132 |     0.3
    8 |   1.3272 |     45.029 |   1.3235 |     44.455 |     0.3
    9 |   1.3077 |     44.505 |   1.3032 |     44.547 |     0.3
   10 |   1.2874 |     44.235 |   1.2859 |     43.931 |     0.4
   11 |   1.2688 |     43.981 |   1.2727 |     44.732 |     0.4
   12 |   1.2535 |     43.782 |   1.2577 |     43.931 |     0.5
   13 |   1.2389 |     43.689 |   1.2430 |     43.962 |     0.5
   14 |   1.2200 |     42.845 |   1.2294 |     43.438 |     0.5
   15 |   1.2081 |     42.640 |   1.2215 |     43.130 |     0.6
   16 |   1.1958 |     42.111 |   1.2118 |     42.791 |     0.6
   17 |   1.1819 |     41.532 |   1.1973 |     42.945 |     0.6
   18 |   1.1696 |     41.272 |   1.1881 |     41.528 |     0.7
   19 |   1.1555 |     40.709 |   1.1765 |     40.635 |     0.7
   20 |   1.1424 |     40.147 |   1.1675 |     40.727 |     0.8
   21 |   1.1237 |     39.314 |   1.1596 |     40.604 |     0.8
   22 |   1.1119 |     39.214 |   1.1513 |     40.419 |     0.8
   23 |   1.0940 |     38.337 |   1.1386 |     40.142 |     0.9
   24 |   1.0797 |     37.703 |   1.1364 |     40.912 |     0.9
   25 |   1.0686 |     37.427 |   1.1329 |     39.464 |     1.0
   26 |   1.0542 |     36.737 |   1.1151 |     39.495 |     1.0
   27 |   1.0351 |     36.081 |   1.1062 |     39.063 |     1.0
   28 |   1.0204 |     35.507 |   1.0963 |     38.232 |     1.1
   29 |   0.9967 |     34.376 |   1.0943 |     37.461 |     1.1
   30 |   0.9746 |     33.510 |   1.0891 |     37.708 |     1.1
   31 |   0.9567 |     32.313 |   1.0765 |     36.445 |     1.2
   32 |   0.9353 |     31.590 |   1.0633 |     35.551 |     1.2
   33 |   0.9078 |     30.354 |   1.0528 |     35.028 |     1.3
   34 |   0.8818 |     29.367 |   1.0504 |     34.874 |     1.3
   35 |   0.8661 |     28.870 |   1.0466 |     34.350 |     1.3
   36 |   0.8400 |     27.623 |   1.0503 |     34.319 |     1.4
   37 |   0.8149 |     26.680 |   1.0248 |     32.840 |     1.4
   38 |   0.7905 |     26.095 |   1.0243 |     32.563 |     1.5
   39 |   0.7574 |     24.595 |   1.0407 |     32.994 |     1.5
   40 |   0.7392 |     24.197 |   1.0356 |     32.563 |     1.5
   41 |   0.7086 |     22.973 |   1.0095 |     31.947 |     1.6
   42 |   0.6940 |     22.333 |   1.0370 |     32.409 |     1.6
   43 |   0.7335 |     23.993 |   1.0261 |     31.701 |     1.6
   44 |   0.6659 |     21.516 |   1.0149 |     31.824 |     1.7
   45 |   0.6344 |     20.335 |   1.0425 |     31.639 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,262,882

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5199 |     67.781 |   1.9728 |     54.529 |     0.1
    2 |   1.7512 |     49.415 |   1.5713 |     45.903 |     0.1
    3 |   1.5029 |     46.039 |   1.4560 |     45.903 |     0.2
    4 |   1.4371 |     46.006 |   1.4206 |     45.903 |     0.2
    5 |   1.4082 |     46.077 |   1.3985 |     45.933 |     0.3
    6 |   1.3856 |     45.917 |   1.3661 |     45.194 |     0.4
    7 |   1.3566 |     45.437 |   1.3502 |     45.471 |     0.4
    8 |   1.3316 |     45.200 |   1.3193 |     45.502 |     0.5
    9 |   1.3138 |     45.051 |   1.3133 |     44.978 |     0.5
   10 |   1.2971 |     44.693 |   1.2985 |     44.578 |     0.6
   11 |   1.2817 |     44.362 |   1.2842 |     44.455 |     0.7
   12 |   1.2704 |     43.854 |   1.2736 |     44.177 |     0.7
   13 |   1.2589 |     43.727 |   1.2671 |     43.685 |     0.8
   14 |   1.2496 |     43.611 |   1.2585 |     43.746 |     0.8
   15 |   1.2388 |     43.043 |   1.2415 |     42.699 |     0.9
   16 |   1.2299 |     42.833 |   1.2359 |     43.130 |     1.0
   17 |   1.2206 |     42.668 |   1.2300 |     42.976 |     1.0
   18 |   1.2128 |     42.519 |   1.2219 |     42.237 |     1.1
   19 |   1.2046 |     42.183 |   1.2136 |     43.068 |     1.1
   20 |   1.2001 |     42.425 |   1.2063 |     41.959 |     1.2
   21 |   1.1928 |     42.127 |   1.2094 |     43.315 |     1.3
   22 |   1.1873 |     42.056 |   1.1986 |     43.038 |     1.3
   23 |   1.1820 |     41.874 |   1.1918 |     42.206 |     1.4
   24 |   1.1757 |     41.802 |   1.1913 |     42.391 |     1.4
   25 |   1.1698 |     41.377 |   1.1857 |     41.744 |     1.5
   26 |   1.1650 |     41.278 |   1.1797 |     42.144 |     1.6
   27 |   1.1628 |     41.200 |   1.1769 |     41.990 |     1.6
   28 |   1.1555 |     40.930 |   1.1741 |     41.805 |     1.7
   29 |   1.1524 |     40.963 |   1.1663 |     41.189 |     1.7
   30 |   1.1458 |     40.704 |   1.1671 |     41.929 |     1.8
   31 |   1.1413 |     40.643 |   1.1639 |     41.158 |     1.9
   32 |   1.1364 |     40.384 |   1.1573 |     41.436 |     1.9
   33 |   1.1338 |     40.395 |   1.1715 |     41.774 |     2.0
   34 |   1.1285 |     40.378 |   1.1519 |     41.158 |     2.0
   35 |   1.1247 |     40.345 |   1.1515 |     41.774 |     2.1
   36 |   1.1214 |     40.136 |   1.1482 |     41.374 |     2.2
   37 |   1.1157 |     40.036 |   1.1424 |     41.035 |     2.2
   38 |   1.1103 |     39.926 |   1.1399 |     41.497 |     2.3
   39 |   1.1091 |     39.612 |   1.1407 |     41.128 |     2.3
   40 |   1.1068 |     39.634 |   1.1419 |     41.282 |     2.4
   41 |   1.1028 |     39.518 |   1.1324 |     40.665 |     2.5
   42 |   1.0969 |     39.209 |   1.1403 |     41.282 |     2.5
   43 |   1.0934 |     39.236 |   1.1278 |     40.850 |     2.6
   44 |   1.0917 |     39.209 |   1.1355 |     40.881 |     2.6
   45 |   1.0883 |     38.994 |   1.1233 |     40.018 |     2.7
   46 |   1.0823 |     38.994 |   1.1247 |     40.388 |     2.8
   47 |   1.0805 |     38.928 |   1.1264 |     39.556 |     2.8
   48 |   1.0824 |     38.889 |   1.1233 |     40.419 |     2.9
   49 |   1.0735 |     38.387 |   1.1177 |     40.357 |     2.9
   50 |   1.0731 |     38.619 |   1.1233 |     40.049 |     3.0
   51 |   1.0707 |     38.630 |   1.1176 |     40.049 |     3.1
   52 |   1.0653 |     38.431 |   1.1074 |     39.279 |     3.1
   53 |   1.0599 |     38.381 |   1.1119 |     40.018 |     3.2
   54 |   1.0579 |     38.083 |   1.1112 |     40.142 |     3.2
   55 |   1.0539 |     38.039 |   1.1058 |     39.710 |     3.3
   56 |   1.0494 |     37.901 |   1.1001 |     39.495 |     3.4
   57 |   1.0474 |     37.708 |   1.1015 |     39.063 |     3.4
   58 |   1.0453 |     37.708 |   1.0969 |     38.601 |     3.5
   59 |   1.0392 |     37.377 |   1.0999 |     39.464 |     3.5
   60 |   1.0355 |     37.322 |   1.0954 |     38.386 |     3.6
   61 |   1.0300 |     36.925 |   1.0931 |     39.895 |     3.7
   62 |   1.0271 |     36.875 |   1.0941 |     39.649 |     3.7
   63 |   1.0247 |     36.737 |   1.0878 |     37.924 |     3.8
   64 |   1.0255 |     36.792 |   1.0913 |     38.817 |     3.8
   65 |   1.0190 |     36.671 |   1.0789 |     38.725 |     3.9
   66 |   1.0158 |     36.544 |   1.0910 |     38.663 |     4.0
   67 |   1.0122 |     36.059 |   1.0857 |     38.232 |     4.0
   68 |   1.0067 |     36.026 |   1.0784 |     37.739 |     4.1
   69 |   1.0057 |     35.910 |   1.0747 |     37.831 |     4.1
   70 |   1.0007 |     35.634 |   1.0851 |     38.293 |     4.2
   71 |   0.9980 |     35.838 |   1.0696 |     37.246 |     4.3
   72 |   0.9912 |     35.066 |   1.0753 |     37.954 |     4.3
   73 |   0.9909 |     35.226 |   1.0694 |     37.831 |     4.4
   74 |   0.9883 |     35.187 |   1.0660 |     37.184 |     4.4
   75 |   0.9866 |     35.126 |   1.0701 |     37.616 |     4.5
   76 |   0.9797 |     35.055 |   1.0630 |     37.985 |     4.6
   77 |   0.9734 |     34.547 |   1.0664 |     37.924 |     4.6
   78 |   0.9765 |     34.448 |   1.0505 |     36.691 |     4.7
   79 |   0.9677 |     34.040 |   1.0491 |     37.523 |     4.7
   80 |   0.9617 |     34.172 |   1.0630 |     37.400 |     4.8
   81 |   0.9622 |     34.216 |   1.0426 |     36.229 |     4.9
   82 |   0.9558 |     33.626 |   1.0470 |     36.938 |     4.9
   83 |   0.9490 |     33.631 |   1.0374 |     36.476 |     5.0
   84 |   0.9436 |     33.372 |   1.0341 |     36.229 |     5.0
   85 |   0.9425 |     33.339 |   1.0312 |     35.860 |     5.1
   86 |   0.9326 |     32.925 |   1.0316 |     35.644 |     5.2
   87 |   0.9284 |     32.677 |   1.0273 |     35.952 |     5.2
   88 |   0.9196 |     32.478 |   1.0411 |     36.044 |     5.3
   89 |   0.9183 |     32.224 |   1.0149 |     35.644 |     5.3
   90 |   0.9156 |     31.993 |   1.0129 |     34.874 |     5.4
   91 |   0.9028 |     31.353 |   1.0137 |     35.305 |     5.5
   92 |   0.9006 |     31.270 |   1.0124 |     34.658 |     5.5
   93 |   0.8949 |     31.099 |   1.0138 |     34.535 |     5.6
   94 |   0.8944 |     31.171 |   1.0093 |     34.196 |     5.6
   95 |   0.8866 |     30.840 |   0.9972 |     34.689 |     5.7
   96 |   0.8767 |     30.321 |   1.0006 |     33.980 |     5.8
   97 |   0.8709 |     30.134 |   0.9992 |     33.703 |     5.8
   98 |   0.8648 |     30.012 |   1.0081 |     34.442 |     5.9
   99 |   0.8665 |     30.018 |   0.9841 |     33.272 |     5.9
  100 |   0.8521 |     29.659 |   0.9909 |     34.134 |     6.0
  101 |   0.8516 |     29.367 |   0.9788 |     33.703 |     6.1
  102 |   0.8503 |     29.218 |   0.9805 |     33.179 |     6.1
  103 |   0.8443 |     29.118 |   0.9849 |     33.426 |     6.2
  104 |   0.8321 |     28.793 |   0.9896 |     33.426 |     6.2
  105 |   0.8251 |     28.396 |   0.9920 |     33.087 |     6.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,426,594

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1079 |     57.503 |   1.5542 |     45.903 |     0.1
    2 |   1.4474 |     46.011 |   1.3967 |     45.903 |     0.1
    3 |   1.3632 |     45.366 |   1.3462 |     45.533 |     0.2
    4 |   1.3167 |     44.875 |   1.3176 |     44.547 |     0.2
    5 |   1.2895 |     44.284 |   1.2842 |     43.715 |     0.3
    6 |   1.2653 |     43.976 |   1.2691 |     43.962 |     0.3
    7 |   1.2416 |     43.104 |   1.2423 |     43.407 |     0.4
    8 |   1.2107 |     42.690 |   1.2187 |     42.360 |     0.5
    9 |   1.1807 |     41.305 |   1.1981 |     42.144 |     0.5
   10 |   1.1523 |     40.252 |   1.1660 |     40.604 |     0.6
   11 |   1.1176 |     39.005 |   1.1409 |     40.296 |     0.6
   12 |   1.0799 |     37.333 |   1.1134 |     38.509 |     0.7
   13 |   1.0360 |     35.606 |   1.1074 |     37.893 |     0.7
   14 |   0.9934 |     33.857 |   1.0883 |     36.938 |     0.8
   15 |   0.9485 |     32.009 |   1.0288 |     34.504 |     0.9
   16 |   0.8996 |     29.582 |   1.0218 |     34.258 |     0.9
   17 |   0.8518 |     27.844 |   0.9899 |     32.471 |     1.0
   18 |   0.7938 |     25.565 |   0.9762 |     32.132 |     1.0
   19 |   0.7420 |     23.425 |   0.9711 |     32.070 |     1.1
   20 |   0.6875 |     21.720 |   0.9647 |     31.331 |     1.1
   21 |   0.6296 |     19.662 |   0.9375 |     30.006 |     1.2
   22 |   0.5789 |     18.145 |   0.9283 |     28.651 |     1.3
   23 |   0.5320 |     16.440 |   0.9655 |     29.883 |     1.3
   24 |   0.4765 |     14.835 |   0.9701 |     29.144 |     1.4
   25 |   0.4337 |     13.456 |   0.9809 |     28.558 |     1.4
   26 |   0.3917 |     12.071 |   1.0251 |     29.544 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 537,890

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5149 |     67.362 |   1.9526 |     56.439 |     0.0
    2 |   1.7385 |     49.161 |   1.5630 |     45.903 |     0.1
    3 |   1.4954 |     46.072 |   1.4464 |     45.903 |     0.1
    4 |   1.4268 |     46.116 |   1.4038 |     45.903 |     0.1
    5 |   1.3907 |     45.951 |   1.3671 |     45.533 |     0.2
    6 |   1.3549 |     45.813 |   1.3423 |     45.718 |     0.2
    7 |   1.3316 |     45.548 |   1.3205 |     45.040 |     0.2
    8 |   1.3077 |     44.748 |   1.3008 |     43.962 |     0.3
    9 |   1.2876 |     44.467 |   1.2783 |     43.993 |     0.3
   10 |   1.2684 |     44.135 |   1.2659 |     43.962 |     0.3
   11 |   1.2512 |     43.998 |   1.2507 |     43.993 |     0.4
   12 |   1.2372 |     43.644 |   1.2334 |     43.130 |     0.4
   13 |   1.2258 |     43.584 |   1.2296 |     43.777 |     0.4
   14 |   1.2142 |     43.529 |   1.2194 |     43.222 |     0.5
   15 |   1.2049 |     43.153 |   1.2071 |     42.976 |     0.5
   16 |   1.1964 |     42.977 |   1.2059 |     43.284 |     0.5
   17 |   1.1886 |     43.087 |   1.1960 |     42.514 |     0.6
   18 |   1.1834 |     42.227 |   1.1922 |     42.421 |     0.6
   19 |   1.1780 |     42.409 |   1.1853 |     42.421 |     0.6
   20 |   1.1707 |     42.420 |   1.1771 |     42.514 |     0.7
   21 |   1.1644 |     41.691 |   1.1760 |     42.483 |     0.7
   22 |   1.1597 |     41.669 |   1.1721 |     42.360 |     0.7
   23 |   1.1524 |     41.432 |   1.1631 |     42.175 |     0.8
   24 |   1.1480 |     41.079 |   1.1645 |     41.312 |     0.8
   25 |   1.1440 |     41.360 |   1.1589 |     42.699 |     0.8
   26 |   1.1382 |     40.930 |   1.1599 |     41.682 |     0.8
   27 |   1.1332 |     40.561 |   1.1528 |     41.744 |     0.9
   28 |   1.1289 |     40.450 |   1.1483 |     41.651 |     0.9
   29 |   1.1229 |     40.141 |   1.1438 |     40.943 |     0.9
   30 |   1.1182 |     40.075 |   1.1472 |     41.035 |     1.0
   31 |   1.1179 |     40.136 |   1.1356 |     40.511 |     1.0
   32 |   1.1106 |     39.987 |   1.1367 |     40.604 |     1.0
   33 |   1.1058 |     39.573 |   1.1292 |     40.912 |     1.1
   34 |   1.1016 |     39.601 |   1.1293 |     41.097 |     1.1
   35 |   1.0972 |     39.441 |   1.1215 |     40.296 |     1.1
   36 |   1.0948 |     39.214 |   1.1201 |     39.957 |     1.2
   37 |   1.0897 |     39.198 |   1.1237 |     40.234 |     1.2
   38 |   1.0865 |     39.220 |   1.1157 |     39.926 |     1.2
   39 |   1.0839 |     39.137 |   1.1150 |     39.772 |     1.3
   40 |   1.0830 |     39.099 |   1.1084 |     39.094 |     1.3
   41 |   1.0756 |     38.784 |   1.1120 |     39.464 |     1.3
   42 |   1.0746 |     38.679 |   1.1103 |     40.049 |     1.4
   43 |   1.0707 |     38.585 |   1.1039 |     39.772 |     1.4
   44 |   1.0673 |     38.403 |   1.1091 |     39.895 |     1.4
   45 |   1.0657 |     38.310 |   1.1055 |     39.556 |     1.5
   46 |   1.0646 |     38.155 |   1.0957 |     39.895 |     1.5
   47 |   1.0606 |     38.293 |   1.0958 |     39.926 |     1.5
   48 |   1.0567 |     38.144 |   1.0931 |     39.649 |     1.6
   49 |   1.0550 |     38.133 |   1.0927 |     39.618 |     1.6
   50 |   1.0528 |     37.957 |   1.0908 |     39.125 |     1.6
   51 |   1.0458 |     37.769 |   1.0913 |     38.540 |     1.7
   52 |   1.0482 |     38.100 |   1.0892 |     38.909 |     1.7
   53 |   1.0410 |     37.603 |   1.0848 |     39.372 |     1.7
   54 |   1.0387 |     37.576 |   1.0898 |     39.495 |     1.8
   55 |   1.0385 |     37.466 |   1.0837 |     39.372 |     1.8
   56 |   1.0329 |     37.085 |   1.0800 |     38.909 |     1.8
   57 |   1.0336 |     37.565 |   1.0733 |     38.571 |     1.9
   58 |   1.0287 |     36.969 |   1.0788 |     38.694 |     1.9
   59 |   1.0253 |     37.190 |   1.0682 |     38.447 |     1.9
   60 |   1.0200 |     36.781 |   1.0714 |     38.909 |     2.0
   61 |   1.0187 |     36.677 |   1.0660 |     38.355 |     2.0
   62 |   1.0145 |     36.406 |   1.0702 |     38.324 |     2.0
   63 |   1.0132 |     36.770 |   1.0667 |     37.739 |     2.1
   64 |   1.0120 |     36.599 |   1.0731 |     38.139 |     2.1
   65 |   1.0112 |     36.566 |   1.0624 |     37.862 |     2.1
   66 |   1.0051 |     36.075 |   1.0657 |     37.954 |     2.2
   67 |   1.0051 |     36.252 |   1.0618 |     37.123 |     2.2
   68 |   1.0027 |     36.213 |   1.0568 |     37.954 |     2.2
   69 |   0.9967 |     35.844 |   1.0605 |     37.461 |     2.3
   70 |   1.0016 |     36.335 |   1.0576 |     37.307 |     2.3
   71 |   0.9965 |     35.904 |   1.0521 |     37.862 |     2.3
   72 |   0.9938 |     35.656 |   1.0517 |     36.999 |     2.4
   73 |   0.9909 |     35.772 |   1.0504 |     36.691 |     2.4
   74 |   0.9885 |     35.551 |   1.0547 |     37.554 |     2.4
   75 |   0.9869 |     35.921 |   1.0499 |     37.277 |     2.5
   76 |   0.9818 |     35.490 |   1.0465 |     36.568 |     2.5
   77 |   0.9831 |     35.375 |   1.0431 |     36.938 |     2.5
   78 |   0.9776 |     35.264 |   1.0409 |     37.307 |     2.6
   79 |   0.9771 |     35.375 |   1.0439 |     37.246 |     2.6
   80 |   0.9747 |     35.209 |   1.0359 |     36.691 |     2.6
   81 |   0.9708 |     34.928 |   1.0431 |     37.215 |     2.7
   82 |   0.9718 |     35.049 |   1.0490 |     37.585 |     2.7
   83 |   0.9664 |     34.646 |   1.0357 |     36.044 |     2.7
   84 |   0.9634 |     34.839 |   1.0370 |     37.277 |     2.8
   85 |   0.9621 |     34.834 |   1.0375 |     37.061 |     2.8
   86 |   0.9610 |     34.707 |   1.0377 |     37.431 |     2.8
   87 |   0.9561 |     34.420 |   1.0392 |     36.938 |     2.9
   88 |   0.9519 |     34.431 |   1.0202 |     36.445 |     2.9
   89 |   0.9536 |     34.194 |   1.0280 |     36.476 |     2.9
   90 |   0.9473 |     33.951 |   1.0226 |     36.876 |     3.0
   91 |   0.9435 |     33.962 |   1.0282 |     37.616 |     3.0
   92 |   0.9413 |     33.692 |   1.0131 |     35.921 |     3.0
   93 |   0.9373 |     33.957 |   1.0313 |     37.400 |     3.0
   94 |   0.9351 |     33.350 |   1.0237 |     36.137 |     3.1
   95 |   0.9345 |     33.295 |   1.0168 |     35.644 |     3.1
   96 |   0.9257 |     32.953 |   1.0152 |     34.997 |     3.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,087,842

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2328 |     60.686 |   1.6530 |     45.933 |     0.1
    2 |   1.4959 |     46.177 |   1.4318 |     45.933 |     0.2
    3 |   1.4164 |     46.204 |   1.4043 |     45.933 |     0.2
    4 |   1.3920 |     46.077 |   1.3824 |     45.718 |     0.3
    5 |   1.3611 |     45.890 |   1.3366 |     45.964 |     0.4
    6 |   1.3231 |     45.553 |   1.3157 |     45.071 |     0.5
    7 |   1.2997 |     45.140 |   1.2936 |     45.317 |     0.5
    8 |   1.2771 |     44.913 |   1.2953 |     45.749 |     0.6
    9 |   1.2602 |     44.345 |   1.2603 |     44.732 |     0.7
   10 |   1.2433 |     44.147 |   1.2410 |     43.068 |     0.8
   11 |   1.2321 |     43.937 |   1.2299 |     42.914 |     0.8
   12 |   1.2184 |     43.490 |   1.2215 |     43.222 |     0.9
   13 |   1.2087 |     43.402 |   1.2110 |     43.068 |     1.0
   14 |   1.1950 |     43.027 |   1.2008 |     43.099 |     1.0
   15 |   1.1863 |     42.806 |   1.1941 |     42.575 |     1.1
   16 |   1.1763 |     42.199 |   1.1890 |     42.237 |     1.2
   17 |   1.1679 |     42.034 |   1.1835 |     42.175 |     1.3
   18 |   1.1603 |     41.956 |   1.1724 |     41.990 |     1.3
   19 |   1.1528 |     41.427 |   1.1774 |     42.391 |     1.4
   20 |   1.1484 |     41.416 |   1.1674 |     42.329 |     1.5
   21 |   1.1377 |     40.919 |   1.1684 |     41.590 |     1.6
   22 |   1.1337 |     40.809 |   1.1617 |     40.943 |     1.6
   23 |   1.1254 |     40.241 |   1.1578 |     41.097 |     1.7
   24 |   1.1169 |     39.943 |   1.1460 |     41.035 |     1.8
   25 |   1.1086 |     39.750 |   1.1453 |     41.128 |     1.9
   26 |   1.0973 |     39.137 |   1.1307 |     39.618 |     1.9
   27 |   1.0884 |     38.784 |   1.1264 |     40.881 |     2.0
   28 |   1.0825 |     38.856 |   1.1161 |     39.957 |     2.1
   29 |   1.0705 |     37.912 |   1.1127 |     39.957 |     2.2
   30 |   1.0634 |     37.763 |   1.0983 |     39.803 |     2.2
   31 |   1.0532 |     37.289 |   1.1007 |     39.063 |     2.3
   32 |   1.0464 |     37.322 |   1.0848 |     38.755 |     2.4
   33 |   1.0350 |     36.743 |   1.0923 |     39.310 |     2.5
   34 |   1.0296 |     36.682 |   1.0883 |     38.601 |     2.5
   35 |   1.0200 |     36.147 |   1.0781 |     39.002 |     2.6
   36 |   1.0119 |     36.108 |   1.0628 |     37.523 |     2.7
   37 |   1.0008 |     35.628 |   1.0554 |     37.831 |     2.8
   38 |   0.9884 |     35.292 |   1.0507 |     37.153 |     2.8
   39 |   0.9779 |     34.751 |   1.0401 |     36.969 |     2.9
   40 |   0.9739 |     34.613 |   1.0375 |     37.184 |     3.0
   41 |   0.9659 |     34.437 |   1.0439 |     36.969 |     3.1
   42 |   0.9573 |     34.155 |   1.0349 |     36.876 |     3.2
   43 |   0.9444 |     33.504 |   1.0132 |     36.106 |     3.2
   44 |   0.9297 |     33.091 |   1.0163 |     36.537 |     3.3
   45 |   0.9258 |     32.842 |   1.0227 |     36.168 |     3.4
   46 |   0.9156 |     32.511 |   1.0045 |     35.336 |     3.5
   47 |   0.9074 |     32.103 |   1.0119 |     35.551 |     3.5
   48 |   0.9003 |     31.871 |   1.0084 |     35.367 |     3.6
   49 |   0.8927 |     31.739 |   1.0024 |     35.921 |     3.7
   50 |   0.8809 |     31.066 |   0.9771 |     34.288 |     3.8
   51 |   0.8689 |     30.520 |   0.9978 |     35.120 |     3.8
   52 |   0.8608 |     30.277 |   0.9910 |     34.504 |     3.9
   53 |   0.8634 |     30.470 |   0.9790 |     34.781 |     4.0
   54 |   0.8477 |     29.687 |   0.9715 |     33.765 |     4.1
   55 |   0.8370 |     29.367 |   0.9791 |     34.288 |     4.1
   56 |   0.8336 |     29.014 |   0.9915 |     34.473 |     4.2
   57 |   0.8243 |     28.694 |   0.9719 |     33.641 |     4.3
   58 |   0.8118 |     28.418 |   0.9623 |     33.148 |     4.4
   59 |   0.7990 |     27.579 |   0.9630 |     33.580 |     4.4
   60 |   0.7960 |     27.849 |   0.9676 |     33.611 |     4.5
   61 |   0.7812 |     26.867 |   0.9672 |     33.148 |     4.6
   62 |   0.7750 |     26.890 |   0.9560 |     33.333 |     4.7
   63 |   0.7664 |     26.531 |   0.9692 |     32.748 |     4.7
   64 |   0.7589 |     26.399 |   0.9557 |     32.409 |     4.8
   65 |   0.7486 |     25.885 |   0.9566 |     32.009 |     4.9
   66 |   0.7463 |     25.643 |   0.9607 |     32.625 |     5.0
   67 |   0.7421 |     25.725 |   0.9433 |     31.701 |     5.0
   68 |   0.7191 |     24.721 |   0.9541 |     32.193 |     5.1
   69 |   0.7062 |     24.313 |   0.9423 |     31.885 |     5.2
   70 |   0.6998 |     24.059 |   0.9545 |     32.009 |     5.3
   71 |   0.6952 |     24.203 |   0.9413 |     31.701 |     5.3
   72 |   0.6824 |     23.668 |   0.9609 |     31.547 |     5.4
   73 |   0.6741 |     23.270 |   0.9573 |     31.146 |     5.5
   74 |   0.6678 |     22.939 |   0.9553 |     31.577 |     5.6
   75 |   0.6531 |     22.338 |   0.9509 |     31.362 |     5.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,098,658

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1776 |     59.379 |   1.5744 |     45.903 |     0.0
    2 |   1.4681 |     46.254 |   1.4052 |     45.903 |     0.1
    3 |   1.3862 |     45.923 |   1.3899 |     45.471 |     0.1
    4 |   1.3483 |     45.735 |   1.3281 |     45.749 |     0.2
    5 |   1.3106 |     45.311 |   1.3016 |     44.917 |     0.2
    6 |   1.2890 |     44.511 |   1.2800 |     44.763 |     0.2
    7 |   1.2639 |     44.450 |   1.2595 |     43.993 |     0.3
    8 |   1.2426 |     43.998 |   1.2405 |     43.962 |     0.3
    9 |   1.2260 |     43.407 |   1.2247 |     42.914 |     0.4
   10 |   1.2108 |     43.109 |   1.2122 |     43.038 |     0.4
   11 |   1.1953 |     42.607 |   1.2047 |     43.192 |     0.4
   12 |   1.1785 |     41.951 |   1.1930 |     42.113 |     0.5
   13 |   1.1672 |     41.581 |   1.1903 |     42.267 |     0.5
   14 |   1.1558 |     41.278 |   1.1809 |     42.175 |     0.6
   15 |   1.1423 |     40.941 |   1.1640 |     41.959 |     0.6
   16 |   1.1378 |     40.594 |   1.1553 |     41.035 |     0.6
   17 |   1.1253 |     40.345 |   1.1448 |     41.312 |     0.7
   18 |   1.1124 |     39.650 |   1.1520 |     41.436 |     0.7
   19 |   1.1123 |     39.910 |   1.1460 |     40.234 |     0.8
   20 |   1.1077 |     39.750 |   1.1381 |     41.158 |     0.8
   21 |   1.0973 |     39.689 |   1.1138 |     39.957 |     0.8
   22 |   1.0833 |     39.176 |   1.1105 |     39.495 |     0.9
   23 |   1.0766 |     38.889 |   1.1125 |     40.173 |     0.9
   24 |   1.0688 |     38.398 |   1.0912 |     39.741 |     1.0
   25 |   1.0601 |     38.348 |   1.0958 |     39.217 |     1.0
   26 |   1.0527 |     37.846 |   1.1036 |     40.018 |     1.0
   27 |   1.0432 |     37.824 |   1.0869 |     39.526 |     1.1
   28 |   1.0395 |     37.272 |   1.0803 |     39.125 |     1.1
   29 |   1.0304 |     37.162 |   1.0743 |     38.571 |     1.2
   30 |   1.0196 |     36.798 |   1.0667 |     38.478 |     1.2
   31 |   1.0106 |     36.235 |   1.0514 |     36.876 |     1.2
   32 |   1.0005 |     36.031 |   1.0487 |     37.400 |     1.3
   33 |   0.9958 |     35.959 |   1.0443 |     37.585 |     1.3
   34 |   0.9877 |     35.827 |   1.0340 |     36.691 |     1.4
   35 |   0.9792 |     35.518 |   1.0332 |     37.369 |     1.4
   36 |   0.9707 |     34.878 |   1.0354 |     37.523 |     1.4
   37 |   0.9687 |     34.873 |   1.0193 |     36.383 |     1.5
   38 |   0.9558 |     34.674 |   1.0209 |     36.322 |     1.5
   39 |   0.9508 |     33.924 |   1.0226 |     36.999 |     1.6
   40 |   0.9473 |     34.238 |   1.0030 |     36.260 |     1.6
   41 |   0.9455 |     33.995 |   1.0050 |     35.367 |     1.6
   42 |   0.9417 |     33.780 |   0.9996 |     35.952 |     1.7
   43 |   0.9292 |     33.554 |   0.9901 |     35.305 |     1.7
   44 |   0.9164 |     32.969 |   1.0040 |     35.860 |     1.8
   45 |   0.9170 |     33.052 |   0.9978 |     35.428 |     1.8
   46 |   0.9125 |     32.280 |   0.9907 |     34.689 |     1.8
   47 |   0.8996 |     32.313 |   0.9836 |     34.843 |     1.9
   48 |   0.8912 |     31.794 |   0.9841 |     34.227 |     1.9
   49 |   0.8856 |     31.921 |   0.9911 |     34.997 |     2.0
   50 |   0.8792 |     31.358 |   0.9703 |     34.011 |     2.0
   51 |   0.8692 |     30.840 |   0.9837 |     33.888 |     2.0
   52 |   0.8672 |     30.834 |   0.9594 |     33.518 |     2.1
   53 |   0.8580 |     30.520 |   0.9637 |     32.625 |     2.1
   54 |   0.8515 |     30.249 |   0.9600 |     33.734 |     2.2
   55 |   0.8350 |     29.460 |   0.9766 |     33.487 |     2.2
   56 |   0.8343 |     29.471 |   0.9636 |     32.686 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,690,914

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5587 |     68.609 |   2.0573 |     59.550 |     0.1
    2 |   1.8033 |     50.171 |   1.6071 |     45.933 |     0.1
    3 |   1.5206 |     46.017 |   1.4694 |     45.903 |     0.2
    4 |   1.4417 |     46.039 |   1.4295 |     45.903 |     0.3
    5 |   1.4065 |     46.138 |   1.3877 |     45.903 |     0.4
    6 |   1.3686 |     46.077 |   1.3542 |     45.903 |     0.4
    7 |   1.3358 |     45.719 |   1.3297 |     45.502 |     0.5
    8 |   1.3137 |     45.151 |   1.3116 |     45.317 |     0.6
    9 |   1.2954 |     44.665 |   1.2999 |     43.931 |     0.7
   10 |   1.2821 |     44.373 |   1.2882 |     44.208 |     0.7
   11 |   1.2721 |     44.113 |   1.2792 |     44.486 |     0.8
   12 |   1.2612 |     43.860 |   1.2687 |     43.715 |     0.9
   13 |   1.2516 |     43.744 |   1.2571 |     43.407 |     1.0
   14 |   1.2399 |     43.291 |   1.2486 |     43.222 |     1.0
   15 |   1.2310 |     43.126 |   1.2423 |     43.315 |     1.1
   16 |   1.2245 |     42.905 |   1.2345 |     43.500 |     1.2
   17 |   1.2143 |     42.651 |   1.2359 |     43.007 |     1.3
   18 |   1.2057 |     42.320 |   1.2221 |     43.253 |     1.3
   19 |   1.2015 |     42.354 |   1.2191 |     42.483 |     1.4
   20 |   1.1918 |     42.188 |   1.2104 |     42.668 |     1.5
   21 |   1.1847 |     42.028 |   1.2020 |     42.206 |     1.6
   22 |   1.1791 |     41.631 |   1.1992 |     42.267 |     1.6
   23 |   1.1721 |     41.576 |   1.1985 |     42.083 |     1.7
   24 |   1.1678 |     41.609 |   1.2010 |     42.391 |     1.8
   25 |   1.1615 |     41.460 |   1.1873 |     42.421 |     1.9
   26 |   1.1550 |     41.355 |   1.1886 |     42.267 |     1.9
   27 |   1.1495 |     40.836 |   1.1826 |     41.805 |     2.0
   28 |   1.1444 |     40.803 |   1.1726 |     41.590 |     2.1
   29 |   1.1366 |     40.605 |   1.1800 |     41.898 |     2.2
   30 |   1.1281 |     40.296 |   1.1683 |     41.189 |     2.2
   31 |   1.1236 |     39.976 |   1.1701 |     41.713 |     2.3
   32 |   1.1209 |     40.031 |   1.1662 |     41.189 |     2.4
   33 |   1.1144 |     39.838 |   1.1589 |     42.206 |     2.5
   34 |   1.1100 |     39.926 |   1.1587 |     41.189 |     2.5
   35 |   1.1040 |     39.419 |   1.1528 |     41.713 |     2.6
   36 |   1.1004 |     39.225 |   1.1488 |     40.943 |     2.7
   37 |   1.0957 |     39.192 |   1.1490 |     41.282 |     2.8
   38 |   1.0939 |     39.187 |   1.1412 |     40.850 |     2.8
   39 |   1.0898 |     39.005 |   1.1417 |     40.481 |     2.9
   40 |   1.0816 |     38.403 |   1.1369 |     40.758 |     3.0
   41 |   1.0778 |     38.431 |   1.1309 |     40.727 |     3.1
   42 |   1.0726 |     38.194 |   1.1319 |     41.128 |     3.1
   43 |   1.0675 |     38.122 |   1.1333 |     40.850 |     3.2
   44 |   1.0623 |     37.868 |   1.1200 |     39.464 |     3.3
   45 |   1.0551 |     37.515 |   1.1147 |     40.604 |     3.4
   46 |   1.0466 |     37.085 |   1.1064 |     38.725 |     3.4
   47 |   1.0385 |     36.748 |   1.1068 |     39.341 |     3.5
   48 |   1.0308 |     36.263 |   1.1007 |     39.587 |     3.6
   49 |   1.0259 |     36.191 |   1.0921 |     39.033 |     3.7
   50 |   1.0161 |     35.827 |   1.0959 |     38.755 |     3.7
   51 |   1.0100 |     35.358 |   1.0889 |     38.201 |     3.8
   52 |   1.0024 |     35.226 |   1.0850 |     38.447 |     3.9
   53 |   0.9976 |     34.895 |   1.0806 |     37.924 |     4.0
   54 |   0.9900 |     34.668 |   1.0748 |     37.338 |     4.0
   55 |   0.9844 |     34.685 |   1.0798 |     38.078 |     4.1
   56 |   0.9799 |     34.133 |   1.0760 |     37.862 |     4.2
   57 |   0.9703 |     33.835 |   1.0617 |     36.630 |     4.3
   58 |   0.9633 |     33.587 |   1.0633 |     36.753 |     4.3
   59 |   0.9637 |     33.642 |   1.0548 |     37.061 |     4.4
   60 |   0.9497 |     33.212 |   1.0631 |     37.492 |     4.5
   61 |   0.9423 |     32.964 |   1.0545 |     37.184 |     4.6
   62 |   0.9331 |     32.418 |   1.0493 |     36.229 |     4.6
   63 |   0.9292 |     32.368 |   1.0489 |     36.845 |     4.7
   64 |   0.9203 |     32.164 |   1.0514 |     36.537 |     4.8
   65 |   0.9186 |     32.153 |   1.0419 |     35.367 |     4.9
   66 |   0.9070 |     31.629 |   1.0448 |     36.260 |     4.9
   67 |   0.8964 |     30.856 |   1.0411 |     36.291 |     5.0
   68 |   0.8865 |     30.586 |   1.0371 |     35.397 |     5.1
   69 |   0.8816 |     30.327 |   1.0533 |     35.890 |     5.2
   70 |   0.8741 |     29.962 |   1.0351 |     35.397 |     5.2
   71 |   0.8650 |     29.880 |   1.0291 |     34.750 |     5.3
   72 |   0.8540 |     29.295 |   1.0379 |     35.151 |     5.4
   73 |   0.8462 |     28.688 |   1.0245 |     34.812 |     5.5
   74 |   0.8345 |     28.175 |   1.0211 |     34.442 |     5.5
   75 |   0.8237 |     27.717 |   1.0374 |     34.258 |     5.6
   76 |   0.8179 |     27.888 |   1.0274 |     34.011 |     5.7
   77 |   0.8018 |     26.823 |   1.0182 |     33.364 |     5.8
   78 |   0.7938 |     26.536 |   1.0178 |     33.641 |     5.8
   79 |   0.7838 |     26.365 |   1.0267 |     34.073 |     5.9
   80 |   0.7734 |     25.985 |   1.0170 |     33.580 |     6.0
   81 |   0.7622 |     25.306 |   1.0371 |     33.611 |     6.1
   82 |   0.7563 |     25.350 |   1.0295 |     33.426 |     6.1
   83 |   0.7487 |     24.892 |   1.0279 |     33.426 |     6.2
   84 |   0.7343 |     24.517 |   1.0292 |     33.272 |     6.3
   85 |   0.7285 |     24.170 |   1.0168 |     32.994 |     6.4
   86 |   0.7124 |     23.585 |   1.0442 |     33.364 |     6.4
   87 |   0.7054 |     23.320 |   1.0441 |     32.748 |     6.5
   88 |   0.6941 |     22.790 |   1.0258 |     32.163 |     6.6
   89 |   0.6809 |     22.195 |   1.0333 |     31.978 |     6.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 753,570

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2688 |     61.089 |   1.6285 |     45.903 |     0.0
    2 |   1.4823 |     46.127 |   1.4130 |     45.903 |     0.1
    3 |   1.3838 |     46.083 |   1.3562 |     45.656 |     0.1
    4 |   1.3379 |     45.697 |   1.3269 |     44.886 |     0.1
    5 |   1.3134 |     45.410 |   1.3061 |     44.948 |     0.1
    6 |   1.2893 |     44.952 |   1.2822 |     44.701 |     0.2
    7 |   1.2627 |     44.268 |   1.2617 |     43.623 |     0.2
    8 |   1.2387 |     43.749 |   1.2353 |     43.561 |     0.2
    9 |   1.2181 |     43.043 |   1.2194 |     42.606 |     0.2
   10 |   1.2049 |     42.789 |   1.2144 |     42.791 |     0.3
   11 |   1.1895 |     42.122 |   1.1993 |     41.774 |     0.3
   12 |   1.1765 |     41.890 |   1.1923 |     41.374 |     0.3
   13 |   1.1655 |     41.052 |   1.1733 |     41.559 |     0.3
   14 |   1.1535 |     41.057 |   1.1604 |     41.035 |     0.4
   15 |   1.1438 |     40.930 |   1.1593 |     41.097 |     0.4
   16 |   1.1350 |     40.434 |   1.1500 |     41.774 |     0.4
   17 |   1.1279 |     40.279 |   1.1475 |     40.974 |     0.4
   18 |   1.1241 |     40.279 |   1.1342 |     41.405 |     0.5
   19 |   1.1115 |     39.777 |   1.1344 |     40.357 |     0.5
   20 |   1.1000 |     39.419 |   1.1240 |     41.497 |     0.5
   21 |   1.0912 |     39.424 |   1.1151 |     40.727 |     0.5
   22 |   1.0871 |     39.104 |   1.1107 |     40.665 |     0.6
   23 |   1.0760 |     38.872 |   1.1034 |     40.511 |     0.6
   24 |   1.0710 |     38.569 |   1.0997 |     39.988 |     0.6
   25 |   1.0634 |     38.359 |   1.0881 |     39.834 |     0.6
   26 |   1.0580 |     37.979 |   1.0932 |     39.033 |     0.7
   27 |   1.0502 |     37.962 |   1.0813 |     38.694 |     0.7
   28 |   1.0427 |     37.763 |   1.0797 |     39.772 |     0.7
   29 |   1.0364 |     37.460 |   1.0756 |     38.694 |     0.7
   30 |   1.0306 |     37.123 |   1.0780 |     39.002 |     0.8
   31 |   1.0242 |     37.184 |   1.0653 |     38.016 |     0.8
   32 |   1.0167 |     36.897 |   1.0675 |     38.755 |     0.8
   33 |   1.0164 |     36.588 |   1.0615 |     37.893 |     0.8
   34 |   1.0101 |     36.406 |   1.0594 |     38.047 |     0.9
   35 |   1.0030 |     36.384 |   1.0604 |     38.016 |     0.9
   36 |   0.9976 |     35.965 |   1.0483 |     37.924 |     0.9
   37 |   0.9878 |     35.628 |   1.0434 |     37.461 |     0.9
   38 |   0.9806 |     35.270 |   1.0423 |     37.646 |     1.0
   39 |   0.9768 |     34.999 |   1.0393 |     37.831 |     1.0
   40 |   0.9647 |     34.740 |   1.0313 |     36.291 |     1.0
   41 |   0.9553 |     34.211 |   1.0298 |     36.599 |     1.0
   42 |   0.9499 |     34.089 |   1.0166 |     35.890 |     1.1
   43 |   0.9476 |     33.907 |   1.0072 |     35.798 |     1.1
   44 |   0.9332 |     33.151 |   1.0125 |     35.705 |     1.1
   45 |   0.9317 |     33.300 |   1.0004 |     35.182 |     1.1
   46 |   0.9196 |     33.052 |   1.0063 |     35.798 |     1.2
   47 |   0.9146 |     32.335 |   1.0080 |     34.874 |     1.2
   48 |   0.9073 |     32.511 |   1.0011 |     35.089 |     1.2
   49 |   0.8999 |     32.164 |   0.9880 |     34.196 |     1.2
   50 |   0.8918 |     31.347 |   0.9843 |     34.319 |     1.3
   51 |   0.8819 |     31.022 |   0.9708 |     33.549 |     1.3
   52 |   0.8761 |     31.176 |   0.9720 |     33.672 |     1.3
   53 |   0.8644 |     30.393 |   0.9702 |     33.148 |     1.3
   54 |   0.8578 |     30.189 |   0.9603 |     32.933 |     1.4
   55 |   0.8502 |     29.582 |   0.9653 |     33.241 |     1.4
   56 |   0.8447 |     29.521 |   0.9833 |     33.179 |     1.4
   57 |   0.8365 |     29.356 |   0.9613 |     32.471 |     1.4
   58 |   0.8274 |     29.124 |   0.9560 |     32.532 |     1.5
   59 |   0.8138 |     28.379 |   0.9595 |     32.748 |     1.5
   60 |   0.8151 |     28.197 |   0.9589 |     32.779 |     1.5
   61 |   0.8043 |     28.203 |   0.9495 |     32.471 |     1.5
   62 |   0.7950 |     27.491 |   0.9390 |     31.793 |     1.6
   63 |   0.7871 |     27.381 |   0.9383 |     31.855 |     1.6
   64 |   0.7764 |     26.879 |   0.9508 |     31.454 |     1.6
   65 |   0.7681 |     26.365 |   0.9392 |     31.762 |     1.6
   66 |   0.7650 |     26.410 |   0.9481 |     31.731 |     1.7
   67 |   0.7584 |     26.277 |   0.9523 |     31.670 |     1.7
   68 |   0.7485 |     25.841 |   0.9289 |     30.345 |     1.7
   69 |   0.7504 |     25.863 |   0.9279 |     31.238 |     1.7
   70 |   0.7399 |     25.549 |   0.9362 |     30.591 |     1.8
   71 |   0.7309 |     25.499 |   0.9337 |     30.776 |     1.8
   72 |   0.7171 |     24.606 |   0.9344 |     30.776 |     1.8
   73 |   0.7118 |     24.407 |   0.9385 |     30.653 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 621,474

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0223 |     55.396 |   1.4648 |     45.625 |     0.0
    2 |   1.3893 |     45.581 |   1.3441 |     44.670 |     0.1
    3 |   1.3111 |     44.687 |   1.2955 |     44.362 |     0.1
    4 |   1.2731 |     44.053 |   1.2671 |     44.270 |     0.1
    5 |   1.2411 |     43.578 |   1.2397 |     44.455 |     0.1
    6 |   1.2121 |     42.696 |   1.2079 |     42.391 |     0.2
    7 |   1.1850 |     41.945 |   1.1859 |     41.744 |     0.2
    8 |   1.1587 |     40.886 |   1.1729 |     41.836 |     0.2
    9 |   1.1384 |     40.599 |   1.1446 |     40.450 |     0.2
   10 |   1.1074 |     39.474 |   1.1234 |     40.234 |     0.3
   11 |   1.0810 |     38.437 |   1.1007 |     39.710 |     0.3
   12 |   1.0604 |     37.631 |   1.0864 |     38.848 |     0.3
   13 |   1.0368 |     36.643 |   1.0578 |     36.969 |     0.3
   14 |   1.0049 |     35.347 |   1.0360 |     36.445 |     0.4
   15 |   0.9873 |     34.746 |   1.0255 |     35.829 |     0.4
   16 |   0.9597 |     33.571 |   1.0130 |     34.073 |     0.4
   17 |   0.9413 |     33.019 |   0.9932 |     33.980 |     0.4
   18 |   0.9149 |     31.689 |   0.9800 |     33.580 |     0.5
   19 |   0.8862 |     30.608 |   0.9701 |     33.118 |     0.5
   20 |   0.8652 |     29.836 |   0.9595 |     32.409 |     0.5
   21 |   0.8332 |     28.225 |   0.9410 |     31.608 |     0.5
   22 |   0.8186 |     27.805 |   0.9231 |     31.054 |     0.6
   23 |   0.7910 |     27.000 |   0.9130 |     29.945 |     0.6
   24 |   0.7668 |     25.737 |   0.9206 |     30.191 |     0.6
   25 |   0.7371 |     24.898 |   0.9111 |     29.482 |     0.6
   26 |   0.7138 |     23.955 |   0.9252 |     30.222 |     0.7
   27 |   0.6862 |     23.083 |   0.8974 |     29.144 |     0.7
   28 |   0.6721 |     22.592 |   0.8975 |     28.774 |     0.7
   29 |   0.6479 |     21.417 |   0.8857 |     28.435 |     0.7
   30 |   0.6208 |     20.517 |   0.9021 |     28.466 |     0.8
   31 |   0.6038 |     20.247 |   0.8859 |     27.850 |     0.8
   32 |   0.5835 |     19.171 |   0.9052 |     28.712 |     0.8
   33 |   0.5687 |     18.879 |   0.8980 |     28.096 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,160,354

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5468 |     66.452 |   2.0061 |     57.702 |     0.1
    2 |   1.7693 |     49.873 |   1.5785 |     45.903 |     0.1
    3 |   1.5007 |     46.055 |   1.4480 |     45.903 |     0.2
    4 |   1.4258 |     46.033 |   1.4044 |     46.426 |     0.2
    5 |   1.3952 |     46.199 |   1.3786 |     45.903 |     0.3
    6 |   1.3648 |     45.951 |   1.3546 |     45.595 |     0.4
    7 |   1.3410 |     45.675 |   1.3292 |     45.317 |     0.4
    8 |   1.3196 |     45.338 |   1.3195 |     45.441 |     0.5
    9 |   1.3062 |     45.228 |   1.3025 |     45.656 |     0.5
   10 |   1.2912 |     44.924 |   1.2894 |     43.993 |     0.6
   11 |   1.2763 |     44.141 |   1.2742 |     43.623 |     0.6
   12 |   1.2644 |     44.020 |   1.2663 |     43.438 |     0.7
   13 |   1.2548 |     43.788 |   1.2571 |     43.623 |     0.8
   14 |   1.2422 |     43.584 |   1.2461 |     43.715 |     0.8
   15 |   1.2339 |     43.733 |   1.2387 |     43.530 |     0.9
   16 |   1.2256 |     43.369 |   1.2288 |     43.222 |     0.9
   17 |   1.2167 |     43.468 |   1.2234 |     43.253 |     1.0
   18 |   1.2078 |     43.242 |   1.2181 |     43.469 |     1.1
   19 |   1.2005 |     42.982 |   1.2079 |     43.346 |     1.1
   20 |   1.1957 |     43.010 |   1.2004 |     42.760 |     1.2
   21 |   1.1886 |     43.005 |   1.1976 |     42.853 |     1.2
   22 |   1.1827 |     42.629 |   1.1936 |     42.391 |     1.3
   23 |   1.1789 |     42.425 |   1.1846 |     42.329 |     1.4
   24 |   1.1709 |     41.929 |   1.1840 |     42.144 |     1.4
   25 |   1.1683 |     42.078 |   1.1757 |     41.343 |     1.5
   26 |   1.1616 |     41.509 |   1.1729 |     41.805 |     1.5
   27 |   1.1579 |     41.625 |   1.1706 |     42.021 |     1.6
   28 |   1.1538 |     41.256 |   1.1675 |     41.744 |     1.6
   29 |   1.1493 |     40.941 |   1.1623 |     41.312 |     1.7
   30 |   1.1448 |     40.886 |   1.1634 |     41.528 |     1.8
   31 |   1.1432 |     40.693 |   1.1556 |     41.066 |     1.8
   32 |   1.1340 |     40.450 |   1.1508 |     40.511 |     1.9
   33 |   1.1330 |     40.594 |   1.1505 |     41.590 |     1.9
   34 |   1.1282 |     40.174 |   1.1558 |     40.912 |     2.0
   35 |   1.1232 |     39.970 |   1.1426 |     40.850 |     2.1
   36 |   1.1186 |     40.136 |   1.1416 |     40.327 |     2.1
   37 |   1.1129 |     39.716 |   1.1374 |     40.974 |     2.2
   38 |   1.1126 |     39.750 |   1.1362 |     40.665 |     2.2
   39 |   1.1065 |     39.617 |   1.1327 |     40.388 |     2.3
   40 |   1.1034 |     39.308 |   1.1283 |     40.203 |     2.4
   41 |   1.1003 |     39.523 |   1.1251 |     39.864 |     2.4
   42 |   1.0952 |     39.099 |   1.1250 |     40.234 |     2.5
   43 |   1.0932 |     39.126 |   1.1236 |     40.080 |     2.5
   44 |   1.0890 |     39.110 |   1.1231 |     40.727 |     2.6
   45 |   1.0881 |     38.685 |   1.1202 |     40.357 |     2.6
   46 |   1.0817 |     38.646 |   1.1168 |     39.803 |     2.7
   47 |   1.0791 |     38.497 |   1.1172 |     40.357 |     2.8
   48 |   1.0748 |     38.514 |   1.1198 |     40.049 |     2.8
   49 |   1.0705 |     38.343 |   1.1099 |     39.864 |     2.9
   50 |   1.0658 |     38.183 |   1.1053 |     39.618 |     2.9
   51 |   1.0643 |     38.117 |   1.1034 |     39.895 |     3.0
   52 |   1.0630 |     38.332 |   1.1076 |     39.772 |     3.1
   53 |   1.0568 |     37.890 |   1.1045 |     39.495 |     3.1
   54 |   1.0517 |     37.603 |   1.1029 |     39.526 |     3.2
   55 |   1.0507 |     37.543 |   1.0967 |     38.940 |     3.2
   56 |   1.0435 |     37.350 |   1.0946 |     39.002 |     3.3
   57 |   1.0448 |     37.234 |   1.0923 |     39.433 |     3.4
   58 |   1.0386 |     37.151 |   1.0965 |     39.156 |     3.4
   59 |   1.0349 |     36.963 |   1.0895 |     39.033 |     3.5
   60 |   1.0298 |     36.903 |   1.0888 |     39.341 |     3.5
   61 |   1.0288 |     36.842 |   1.0850 |     39.279 |     3.6
   62 |   1.0264 |     36.682 |   1.0815 |     39.063 |     3.7
   63 |   1.0199 |     36.649 |   1.0776 |     39.002 |     3.7
   64 |   1.0182 |     36.710 |   1.0818 |     37.800 |     3.8
   65 |   1.0133 |     35.844 |   1.0768 |     38.170 |     3.8
   66 |   1.0084 |     35.827 |   1.0645 |     38.139 |     3.9
   67 |   1.0066 |     35.788 |   1.0642 |     38.108 |     3.9
   68 |   1.0009 |     35.441 |   1.0642 |     38.108 |     4.0
   69 |   0.9943 |     35.088 |   1.0513 |     36.876 |     4.1
   70 |   0.9854 |     34.707 |   1.0527 |     37.800 |     4.1
   71 |   0.9875 |     34.961 |   1.0472 |     37.338 |     4.2
   72 |   0.9816 |     34.795 |   1.0485 |     37.215 |     4.2
   73 |   0.9771 |     34.481 |   1.0575 |     37.431 |     4.3
   74 |   0.9709 |     34.177 |   1.0454 |     37.770 |     4.4
   75 |   0.9699 |     34.122 |   1.0341 |     36.907 |     4.4
   76 |   0.9660 |     33.846 |   1.0332 |     36.599 |     4.5
   77 |   0.9607 |     34.089 |   1.0299 |     36.907 |     4.5
   78 |   0.9564 |     33.808 |   1.0278 |     36.322 |     4.6
   79 |   0.9451 |     33.400 |   1.0298 |     36.815 |     4.7
   80 |   0.9471 |     33.411 |   1.0195 |     36.044 |     4.7
   81 |   0.9420 |     32.997 |   1.0255 |     36.075 |     4.8
   82 |   0.9350 |     33.107 |   1.0206 |     35.860 |     4.8
   83 |   0.9343 |     33.024 |   1.0218 |     36.383 |     4.9
   84 |   0.9263 |     32.616 |   1.0176 |     36.044 |     5.0
   85 |   0.9319 |     33.091 |   1.0168 |     36.014 |     5.0
   86 |   0.9204 |     32.715 |   1.0194 |     36.322 |     5.1
   87 |   0.9342 |     33.019 |   1.0171 |     36.044 |     5.1
   88 |   0.9218 |     32.688 |   1.0225 |     35.798 |     5.2
   89 |   0.9173 |     32.406 |   1.0148 |     35.243 |     5.2
   90 |   0.9134 |     32.329 |   1.0156 |     35.213 |     5.3
   91 |   0.9099 |     32.037 |   1.0177 |     35.521 |     5.4
   92 |   0.9024 |     31.651 |   1.0066 |     35.028 |     5.4
   93 |   0.8996 |     31.557 |   1.0059 |     34.812 |     5.5
   94 |   0.8916 |     31.358 |   1.0042 |     35.367 |     5.5
   95 |   0.8887 |     30.972 |   1.0027 |     34.350 |     5.6
   96 |   0.8875 |     31.132 |   1.0010 |     34.566 |     5.7
   97 |   0.8826 |     30.773 |   1.0061 |     34.997 |     5.7
   98 |   0.8769 |     30.757 |   1.0041 |     35.243 |     5.8
   99 |   0.8713 |     30.509 |   1.0006 |     34.042 |     5.8
  100 |   0.8693 |     30.387 |   0.9953 |     34.627 |     5.9
  101 |   0.8625 |     30.233 |   0.9930 |     33.487 |     6.0
  102 |   0.8578 |     29.747 |   0.9951 |     33.857 |     6.0
  103 |   0.8564 |     29.587 |   0.9874 |     33.734 |     6.1
  104 |   0.8531 |     29.449 |   0.9984 |     34.258 |     6.1
  105 |   0.8441 |     29.163 |   0.9862 |     33.426 |     6.2
  106 |   0.8419 |     29.212 |   0.9914 |     33.949 |     6.3
  107 |   0.8373 |     28.936 |   0.9825 |     33.364 |     6.3
  108 |   0.8323 |     29.118 |   0.9861 |     33.611 |     6.4
  109 |   0.8311 |     28.622 |   0.9852 |     33.426 |     6.4
  110 |   0.8270 |     28.627 |   0.9806 |     33.364 |     6.5
  111 |   0.9203 |     31.706 |   1.1999 |     42.606 |     6.6
  112 |   1.1300 |     41.360 |   1.1274 |     43.315 |     6.6
  113 |   1.0967 |     40.853 |   1.1110 |     40.912 |     6.7
  114 |   1.0839 |     40.318 |   1.1080 |     41.405 |     6.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,461,538

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9865 |     55.120 |   1.4594 |     46.149 |     0.1
    2 |   1.3797 |     45.476 |   1.3321 |     44.023 |     0.1
    3 |   1.3026 |     43.970 |   1.2903 |     43.900 |     0.2
    4 |   1.2620 |     43.567 |   1.2525 |     43.685 |     0.2
    5 |   1.2304 |     42.817 |   1.2277 |     42.760 |     0.3
    6 |   1.2065 |     42.502 |   1.2086 |     41.774 |     0.3
    7 |   1.1822 |     41.785 |   1.1958 |     41.959 |     0.4
    8 |   1.1570 |     40.936 |   1.1723 |     40.727 |     0.5
    9 |   1.1337 |     39.738 |   1.1593 |     41.189 |     0.5
   10 |   1.1070 |     39.236 |   1.1288 |     39.803 |     0.6
   11 |   1.0849 |     38.100 |   1.1237 |     40.665 |     0.6
   12 |   1.0574 |     37.151 |   1.0896 |     38.879 |     0.7
   13 |   1.0376 |     36.456 |   1.0796 |     37.924 |     0.7
   14 |   1.0082 |     35.286 |   1.0673 |     37.030 |     0.8
   15 |   0.9831 |     34.271 |   1.0399 |     35.736 |     0.8
   16 |   0.9561 |     33.008 |   1.0261 |     35.860 |     0.9
   17 |   0.9380 |     32.864 |   1.0055 |     34.596 |     1.0
   18 |   0.9146 |     31.469 |   0.9956 |     33.857 |     1.0
   19 |   0.8874 |     30.327 |   0.9867 |     32.994 |     1.1
   20 |   0.8542 |     29.008 |   0.9827 |     32.902 |     1.1
   21 |   0.8234 |     27.739 |   0.9577 |     31.916 |     1.2
   22 |   0.8024 |     27.254 |   0.9655 |     32.163 |     1.2
   23 |   0.7726 |     25.825 |   0.9489 |     32.224 |     1.3
   24 |   0.7403 |     24.766 |   0.9423 |     31.023 |     1.4
   25 |   0.7129 |     23.949 |   0.9291 |     29.328 |     1.4
   26 |   0.6762 |     22.024 |   0.9126 |     29.482 |     1.5
   27 |   0.6485 |     21.400 |   0.9158 |     28.990 |     1.5
   28 |   0.6170 |     20.010 |   0.9372 |     29.513 |     1.6
   29 |   0.5930 |     19.348 |   0.9035 |     28.805 |     1.6
   30 |   0.5549 |     17.754 |   0.9388 |     28.990 |     1.7
   31 |   0.5377 |     17.185 |   0.9470 |     29.421 |     1.8
   32 |   0.5106 |     16.225 |   0.9265 |     28.866 |     1.8
   33 |   0.4815 |     15.525 |   0.9337 |     28.404 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,622,562

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5370 |     67.820 |   1.9905 |     59.550 |     0.1
    2 |   1.7436 |     50.022 |   1.5635 |     45.933 |     0.2
    3 |   1.4952 |     46.166 |   1.4451 |     45.933 |     0.2
    4 |   1.4255 |     46.039 |   1.4069 |     45.903 |     0.3
    5 |   1.3898 |     45.890 |   1.3747 |     45.194 |     0.4
    6 |   1.3623 |     45.476 |   1.3548 |     46.118 |     0.5
    7 |   1.3407 |     45.460 |   1.3334 |     45.379 |     0.6
    8 |   1.3198 |     45.311 |   1.3187 |     46.149 |     0.6
    9 |   1.3016 |     45.106 |   1.3030 |     44.670 |     0.7
   10 |   1.2887 |     44.604 |   1.2872 |     44.455 |     0.8
   11 |   1.2754 |     44.113 |   1.2788 |     43.685 |     0.9
   12 |   1.2622 |     43.898 |   1.2644 |     43.561 |     0.9
   13 |   1.2489 |     43.584 |   1.2458 |     43.161 |     1.0
   14 |   1.2368 |     43.060 |   1.2331 |     42.791 |     1.1
   15 |   1.2221 |     42.850 |   1.2284 |     43.130 |     1.2
   16 |   1.2154 |     42.718 |   1.2130 |     42.206 |     1.3
   17 |   1.2042 |     42.271 |   1.2143 |     42.760 |     1.4
   18 |   1.1952 |     41.995 |   1.2032 |     41.497 |     1.4
   19 |   1.1866 |     41.851 |   1.1941 |     42.052 |     1.5
   20 |   1.1802 |     41.614 |   1.1884 |     41.220 |     1.6
   21 |   1.1726 |     41.333 |   1.1803 |     41.620 |     1.7
   22 |   1.1653 |     41.322 |   1.1820 |     41.189 |     1.8
   23 |   1.1584 |     41.178 |   1.1738 |     41.158 |     1.8
   24 |   1.1524 |     40.715 |   1.1723 |     41.128 |     1.9
   25 |   1.1461 |     40.676 |   1.1640 |     41.220 |     2.0
   26 |   1.1429 |     40.687 |   1.1610 |     40.758 |     2.1
   27 |   1.1358 |     40.483 |   1.1621 |     40.943 |     2.2
   28 |   1.1298 |     40.097 |   1.1504 |     40.727 |     2.2
   29 |   1.1236 |     40.163 |   1.1494 |     40.974 |     2.3
   30 |   1.1176 |     39.744 |   1.1487 |     41.128 |     2.4
   31 |   1.1127 |     39.562 |   1.1423 |     40.789 |     2.5
   32 |   1.1063 |     39.452 |   1.1420 |     40.696 |     2.6
   33 |   1.1019 |     39.270 |   1.1411 |     40.111 |     2.6
   34 |   1.0982 |     39.121 |   1.1362 |     40.265 |     2.7
   35 |   1.0930 |     39.032 |   1.1302 |     40.542 |     2.8
   36 |   1.0851 |     39.099 |   1.1260 |     40.388 |     2.9
   37 |   1.0800 |     38.663 |   1.1234 |     41.158 |     3.0
   38 |   1.0764 |     38.574 |   1.1214 |     40.573 |     3.0
   39 |   1.0730 |     38.519 |   1.1245 |     40.665 |     3.1
   40 |   1.0700 |     38.332 |   1.1116 |     39.217 |     3.2
   41 |   1.0657 |     38.072 |   1.1093 |     40.234 |     3.3
   42 |   1.0567 |     37.725 |   1.1116 |     40.327 |     3.3
   43 |   1.0520 |     37.863 |   1.1048 |     39.864 |     3.4
   44 |   1.0465 |     37.135 |   1.1024 |     38.540 |     3.5
   45 |   1.0430 |     37.261 |   1.0965 |     38.417 |     3.6
   46 |   1.0383 |     36.941 |   1.0932 |     38.417 |     3.7
   47 |   1.0345 |     36.870 |   1.0887 |     38.386 |     3.7
   48 |   1.0260 |     36.208 |   1.0858 |     38.417 |     3.8
   49 |   1.0174 |     36.059 |   1.0793 |     37.646 |     3.9
   50 |   1.0212 |     35.772 |   1.1048 |     38.478 |     4.0
   51 |   1.0170 |     35.838 |   1.0762 |     37.431 |     4.1
   52 |   1.0076 |     35.711 |   1.0770 |     37.616 |     4.1
   53 |   1.0028 |     35.573 |   1.0795 |     37.215 |     4.2
   54 |   1.0005 |     35.242 |   1.0729 |     37.646 |     4.3
   55 |   0.9935 |     34.961 |   1.0709 |     37.307 |     4.4
   56 |   0.9869 |     34.724 |   1.0693 |     37.030 |     4.5
   57 |   0.9824 |     34.751 |   1.0722 |     37.369 |     4.5
   58 |   0.9797 |     34.155 |   1.0661 |     36.753 |     4.6
   59 |   0.9736 |     34.321 |   1.0564 |     36.476 |     4.7
   60 |   0.9691 |     34.089 |   1.0559 |     36.845 |     4.8
   61 |   0.9598 |     33.400 |   1.0527 |     36.137 |     4.8
   62 |   0.9588 |     33.427 |   1.0426 |     35.582 |     4.9
   63 |   0.9563 |     33.366 |   1.0474 |     35.798 |     5.0
   64 |   0.9407 |     32.914 |   1.0485 |     35.644 |     5.1
   65 |   0.9407 |     32.660 |   1.0335 |     34.904 |     5.2
   66 |   0.9360 |     32.357 |   1.0392 |     35.459 |     5.2
   67 |   0.9253 |     32.048 |   1.0299 |     35.305 |     5.3
   68 |   0.9224 |     31.971 |   1.0284 |     35.182 |     5.4
   69 |   0.9168 |     31.789 |   1.0261 |     34.658 |     5.5
   70 |   0.9015 |     31.005 |   1.0409 |     35.305 |     5.6
   71 |   0.9008 |     31.342 |   1.0112 |     33.611 |     5.6
   72 |   0.8901 |     30.420 |   1.0154 |     34.011 |     5.7
   73 |   0.8870 |     30.625 |   1.0065 |     34.350 |     5.8
   74 |   0.8769 |     30.134 |   1.0083 |     33.765 |     5.9
   75 |   0.8716 |     29.802 |   1.0140 |     34.350 |     6.0
   76 |   0.8658 |     29.709 |   1.0007 |     33.148 |     6.0
   77 |   0.8592 |     29.118 |   1.0007 |     33.272 |     6.1
   78 |   0.8525 |     29.400 |   0.9972 |     33.426 |     6.2
   79 |   0.8480 |     29.025 |   0.9944 |     32.964 |     6.3
   80 |   0.8366 |     28.539 |   0.9939 |     33.210 |     6.3
   81 |   0.8379 |     28.876 |   0.9812 |     32.317 |     6.4
   82 |   0.8224 |     27.949 |   0.9824 |     32.440 |     6.5
   83 |   0.8150 |     27.794 |   0.9840 |     32.348 |     6.6
   84 |   0.8099 |     27.717 |   0.9798 |     32.502 |     6.7
   85 |   0.8033 |     27.314 |   0.9745 |     31.947 |     6.7
   86 |   0.7965 |     27.160 |   0.9743 |     31.577 |     6.8
   87 |   0.7900 |     27.039 |   0.9764 |     31.885 |     6.9
   88 |   0.7808 |     26.757 |   0.9728 |     31.947 |     7.0
   89 |   0.7749 |     26.310 |   0.9772 |     31.885 |     7.1
   90 |   0.7640 |     26.150 |   0.9750 |     31.238 |     7.1
   91 |   0.7592 |     25.692 |   0.9755 |     31.423 |     7.2
   92 |   0.7536 |     25.389 |   0.9619 |     31.454 |     7.3
   93 |   0.7392 |     24.964 |   0.9814 |     31.701 |     7.4
   94 |   0.7354 |     24.788 |   0.9776 |     31.362 |     7.5
   95 |   0.7303 |     24.914 |   0.9774 |     31.084 |     7.5
   96 |   0.7297 |     24.705 |   0.9858 |     31.392 |     7.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 557,986

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4056 |     64.289 |   1.7931 |     49.661 |     0.0
    2 |   1.5989 |     47.126 |   1.4674 |     45.903 |     0.1
    3 |   1.4248 |     45.780 |   1.3910 |     45.533 |     0.1
    4 |   1.3711 |     45.421 |   1.3536 |     45.441 |     0.1
    5 |   1.3344 |     45.437 |   1.3218 |     44.763 |     0.2
    6 |   1.3051 |     44.753 |   1.2993 |     44.578 |     0.2
    7 |   1.2796 |     44.097 |   1.2791 |     44.054 |     0.3
    8 |   1.2566 |     43.716 |   1.2632 |     43.654 |     0.3
    9 |   1.2359 |     43.109 |   1.2395 |     43.192 |     0.3
   10 |   1.2139 |     42.188 |   1.2223 |     42.452 |     0.4
   11 |   1.1863 |     41.212 |   1.2041 |     41.436 |     0.4
   12 |   1.1617 |     40.114 |   1.1843 |     40.943 |     0.4
   13 |   1.1363 |     38.905 |   1.1714 |     41.497 |     0.5
   14 |   1.1107 |     37.968 |   1.1488 |     39.710 |     0.5
   15 |   1.0910 |     37.438 |   1.1331 |     39.495 |     0.5
   16 |   1.0584 |     35.661 |   1.1133 |     38.386 |     0.6
   17 |   1.0246 |     34.359 |   1.0726 |     36.260 |     0.6
   18 |   0.9942 |     33.493 |   1.0711 |     36.445 |     0.7
   19 |   0.9649 |     32.517 |   1.0449 |     34.874 |     0.7
   20 |   0.9379 |     31.391 |   1.0390 |     34.750 |     0.7
   21 |   0.9105 |     30.553 |   1.0252 |     34.381 |     0.8
   22 |   0.8834 |     29.383 |   1.0106 |     33.857 |     0.8
   23 |   0.8608 |     28.677 |   0.9951 |     33.087 |     0.8
   24 |   0.8344 |     27.557 |   0.9768 |     32.193 |     0.9
   25 |   0.8052 |     26.608 |   0.9801 |     32.009 |     0.9
   26 |   0.7832 |     25.786 |   0.9681 |     31.300 |     0.9
   27 |   0.7573 |     24.887 |   0.9741 |     31.392 |     1.0
   28 |   0.7350 |     24.059 |   0.9756 |     31.547 |     1.0
   29 |   0.7161 |     23.204 |   0.9755 |     30.807 |     1.1
   30 |   0.6921 |     22.493 |   0.9856 |     31.146 |     1.1
   31 |   0.6756 |     21.770 |   0.9585 |     30.160 |     1.1
   32 |   0.6506 |     21.202 |   0.9679 |     29.636 |     1.2
   33 |   0.6346 |     20.203 |   0.9760 |     30.129 |     1.2
   34 |   0.6195 |     20.120 |   0.9830 |     30.129 |     1.2
   35 |   0.6049 |     19.519 |   0.9880 |     30.283 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,063,714

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2422 |     60.852 |   1.6467 |     45.933 |     0.0
    2 |   1.4960 |     46.287 |   1.4233 |     45.903 |     0.1
    3 |   1.4117 |     46.111 |   1.3935 |     45.933 |     0.1
    4 |   1.3820 |     45.912 |   1.3547 |     45.163 |     0.2
    5 |   1.3417 |     45.432 |   1.3233 |     45.441 |     0.2
    6 |   1.3129 |     45.079 |   1.3109 |     45.533 |     0.2
    7 |   1.2966 |     44.897 |   1.2932 |     44.393 |     0.3
    8 |   1.2761 |     44.615 |   1.2705 |     44.054 |     0.3
    9 |   1.2589 |     44.571 |   1.2602 |     44.547 |     0.4
   10 |   1.2448 |     44.064 |   1.2421 |     43.561 |     0.4
   11 |   1.2336 |     44.036 |   1.2341 |     43.253 |     0.4
   12 |   1.2234 |     43.678 |   1.2254 |     43.407 |     0.5
   13 |   1.2138 |     43.369 |   1.2181 |     42.884 |     0.5
   14 |   1.2021 |     43.170 |   1.2065 |     42.175 |     0.6
   15 |   1.1934 |     42.718 |   1.1973 |     42.483 |     0.6
   16 |   1.1859 |     42.640 |   1.1885 |     42.267 |     0.6
   17 |   1.1755 |     42.072 |   1.1797 |     41.744 |     0.7
   18 |   1.1678 |     42.083 |   1.1867 |     42.113 |     0.7
   19 |   1.1608 |     41.548 |   1.1710 |     41.774 |     0.8
   20 |   1.1538 |     41.421 |   1.1639 |     41.867 |     0.8
   21 |   1.1447 |     41.029 |   1.1546 |     42.298 |     0.8
   22 |   1.1361 |     40.952 |   1.1542 |     41.128 |     0.9
   23 |   1.1300 |     40.627 |   1.1483 |     40.727 |     0.9
   24 |   1.1228 |     40.583 |   1.1455 |     41.158 |     1.0
   25 |   1.1126 |     39.921 |   1.1451 |     41.004 |     1.0
   26 |   1.1089 |     39.871 |   1.1306 |     40.388 |     1.0
   27 |   1.1009 |     39.512 |   1.1269 |     40.419 |     1.1
   28 |   1.0913 |     39.253 |   1.1211 |     39.957 |     1.1
   29 |   1.0824 |     38.928 |   1.1150 |     40.635 |     1.2
   30 |   1.0789 |     38.696 |   1.1146 |     39.433 |     1.2
   31 |   1.0678 |     38.117 |   1.1033 |     40.604 |     1.2
   32 |   1.0657 |     38.332 |   1.1034 |     39.834 |     1.3
   33 |   1.0566 |     38.089 |   1.0929 |     39.926 |     1.3
   34 |   1.0484 |     38.017 |   1.0870 |     39.033 |     1.4
   35 |   1.0417 |     37.637 |   1.0817 |     39.217 |     1.4
   36 |   1.0373 |     37.449 |   1.0822 |     39.187 |     1.4
   37 |   1.0312 |     37.283 |   1.0790 |     38.663 |     1.5
   38 |   1.0253 |     37.135 |   1.0664 |     38.447 |     1.5
   39 |   1.0176 |     36.848 |   1.0665 |     38.663 |     1.6
   40 |   1.0144 |     37.140 |   1.0621 |     38.478 |     1.6
   41 |   1.0068 |     36.776 |   1.0537 |     37.800 |     1.6
   42 |   1.0033 |     36.285 |   1.0668 |     38.601 |     1.7
   43 |   0.9962 |     36.004 |   1.0561 |     38.571 |     1.7
   44 |   0.9898 |     35.816 |   1.0582 |     38.386 |     1.8
   45 |   0.9869 |     35.700 |   1.0397 |     36.999 |     1.8
   46 |   0.9818 |     35.639 |   1.0388 |     37.431 |     1.8
   47 |   0.9702 |     34.823 |   1.0342 |     36.568 |     1.9
   48 |   0.9673 |     34.713 |   1.0432 |     37.770 |     1.9
   49 |   0.9611 |     34.464 |   1.0279 |     36.291 |     2.0
   50 |   0.9528 |     34.194 |   1.0392 |     37.431 |     2.0
   51 |   0.9496 |     33.813 |   1.0187 |     36.075 |     2.0
   52 |   0.9439 |     33.742 |   1.0222 |     36.815 |     2.1
   53 |   0.9384 |     33.052 |   1.0118 |     36.938 |     2.1
   54 |   0.9349 |     33.306 |   1.0081 |     35.798 |     2.2
   55 |   0.9235 |     32.771 |   0.9989 |     35.551 |     2.2
   56 |   0.9364 |     33.322 |   1.0134 |     35.490 |     2.2
   57 |   0.9195 |     32.980 |   0.9925 |     34.874 |     2.3
   58 |   0.9257 |     33.041 |   1.0089 |     35.798 |     2.3
   59 |   0.9058 |     32.296 |   0.9924 |     34.781 |     2.4
   60 |   0.9048 |     32.445 |   0.9828 |     34.134 |     2.4
   61 |   0.8901 |     31.634 |   0.9787 |     34.288 |     2.4
   62 |   0.8925 |     31.904 |   0.9898 |     34.196 |     2.5
   63 |   0.8781 |     31.215 |   0.9758 |     34.350 |     2.5
   64 |   0.8775 |     31.022 |   0.9671 |     34.134 |     2.6
   65 |   0.8678 |     30.503 |   0.9721 |     34.350 |     2.6
   66 |   0.8631 |     30.823 |   0.9603 |     33.672 |     2.6
   67 |   0.8637 |     30.514 |   0.9760 |     33.672 |     2.7
   68 |   0.8567 |     30.354 |   0.9703 |     33.641 |     2.7
   69 |   0.8495 |     30.216 |   0.9804 |     34.350 |     2.8
   70 |   0.8481 |     29.935 |   0.9461 |     32.193 |     2.8
   71 |   0.8417 |     29.643 |   0.9613 |     33.672 |     2.8
   72 |   0.8312 |     29.416 |   0.9544 |     32.471 |     2.9
   73 |   0.8240 |     29.008 |   0.9577 |     32.440 |     2.9
   74 |   0.8142 |     28.721 |   0.9388 |     31.947 |     3.0
   75 |   0.8103 |     28.820 |   0.9330 |     32.378 |     3.0
   76 |   0.8102 |     28.947 |   0.9446 |     31.855 |     3.0
   77 |   0.8054 |     28.341 |   0.9316 |     32.594 |     3.1
   78 |   0.8192 |     28.964 |   0.9456 |     32.070 |     3.1
   79 |   0.8052 |     28.164 |   0.9424 |     32.625 |     3.2
   80 |   0.8037 |     28.252 |   0.9313 |     31.454 |     3.2
   81 |   0.7931 |     28.037 |   0.9384 |     31.947 |     3.2
   82 |   0.7877 |     27.833 |   0.9434 |     32.224 |     3.3
   83 |   0.7770 |     27.370 |   0.9413 |     31.947 |     3.3
   84 |   0.7745 |     27.000 |   0.9300 |     31.023 |     3.4
   85 |   0.7630 |     26.807 |   0.9316 |     30.838 |     3.4
   86 |   0.7624 |     26.834 |   0.9276 |     30.961 |     3.5
   87 |   0.7558 |     26.685 |   0.9247 |     30.376 |     3.5
   88 |   0.7577 |     26.873 |   0.9330 |     31.392 |     3.5
   89 |   0.7520 |     26.316 |   1.0839 |     35.860 |     3.6
   90 |   0.8077 |     28.412 |   0.9278 |     30.561 |     3.6
   91 |   0.7516 |     26.393 |   0.9344 |     30.961 |     3.7
   92 |   0.7429 |     26.106 |   0.9211 |     30.129 |     3.7
   93 |   0.7311 |     25.786 |   0.9238 |     29.791 |     3.7
   94 |   0.7246 |     25.047 |   0.9202 |     30.191 |     3.8
   95 |   0.7192 |     25.207 |   0.9153 |     29.575 |     3.8
   96 |   0.7162 |     24.959 |   0.9231 |     30.468 |     3.9
   97 |   0.7152 |     24.986 |   0.9326 |     30.653 |     3.9
   98 |   0.7120 |     24.920 |   0.9132 |     30.037 |     3.9
   99 |   0.7023 |     24.368 |   0.9134 |     30.037 |     4.0
  100 |   0.6973 |     24.457 |   0.9243 |     29.636 |     4.0
  101 |   0.7221 |     25.179 |   0.9359 |     30.376 |     4.1
  102 |   0.6951 |     24.236 |   0.9293 |     29.791 |     4.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 733,218

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3821 |     63.836 |   1.8504 |     50.370 |     0.0
    2 |   1.6312 |     47.479 |   1.4968 |     45.903 |     0.1
    3 |   1.4504 |     46.127 |   1.4218 |     45.687 |     0.1
    4 |   1.3957 |     45.890 |   1.3836 |     45.441 |     0.2
    5 |   1.3650 |     45.366 |   1.3589 |     44.362 |     0.2
    6 |   1.3322 |     44.478 |   1.3293 |     44.763 |     0.2
    7 |   1.3101 |     44.240 |   1.3118 |     44.331 |     0.3
    8 |   1.2936 |     44.202 |   1.3029 |     44.208 |     0.3
    9 |   1.2800 |     43.617 |   1.2935 |     44.609 |     0.3
   10 |   1.2685 |     43.705 |   1.2769 |     44.239 |     0.4
   11 |   1.2555 |     43.407 |   1.2701 |     43.746 |     0.4
   12 |   1.2372 |     43.032 |   1.2621 |     43.376 |     0.5
   13 |   1.2232 |     42.839 |   1.2456 |     43.715 |     0.5
   14 |   1.2054 |     42.199 |   1.2333 |     42.545 |     0.5
   15 |   1.1865 |     41.526 |   1.2206 |     41.836 |     0.6
   16 |   1.1642 |     40.472 |   1.2079 |     40.881 |     0.6
   17 |   1.1418 |     39.799 |   1.1934 |     41.436 |     0.7
   18 |   1.1166 |     38.563 |   1.1748 |     39.680 |     0.7
   19 |   1.0887 |     37.289 |   1.1608 |     40.481 |     0.7
   20 |   1.0608 |     36.075 |   1.1434 |     39.063 |     0.8
   21 |   1.0224 |     34.679 |   1.1271 |     38.447 |     0.8
   22 |   0.9907 |     33.190 |   1.1044 |     35.705 |     0.8
   23 |   0.9519 |     31.540 |   1.0912 |     35.675 |     0.9
   24 |   0.9084 |     30.067 |   1.0817 |     35.213 |     0.9
   25 |   0.8679 |     28.164 |   1.0676 |     34.535 |     1.0
   26 |   0.8248 |     26.724 |   1.0554 |     34.073 |     1.0
   27 |   0.7824 |     25.190 |   1.0428 |     32.994 |     1.0
   28 |   0.7480 |     23.706 |   1.0544 |     33.487 |     1.1
   29 |   0.7057 |     22.233 |   1.0261 |     32.070 |     1.1
   30 |   0.6663 |     20.495 |   1.0490 |     31.762 |     1.2
   31 |   0.6263 |     19.282 |   1.0277 |     31.115 |     1.2
   32 |   0.5890 |     17.897 |   1.0536 |     31.731 |     1.2
   33 |   0.5530 |     16.683 |   1.0663 |     30.869 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 439,970

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5539 |     68.239 |   2.0093 |     57.794 |     0.0
    2 |   1.7702 |     50.177 |   1.5830 |     45.903 |     0.1
    3 |   1.5067 |     46.028 |   1.4467 |     46.426 |     0.1
    4 |   1.4166 |     46.083 |   1.3894 |     45.964 |     0.1
    5 |   1.3719 |     45.730 |   1.3564 |     45.225 |     0.1
    6 |   1.3422 |     45.349 |   1.3339 |     45.009 |     0.2
    7 |   1.3220 |     45.151 |   1.3201 |     44.948 |     0.2
    8 |   1.3074 |     44.814 |   1.3082 |     44.794 |     0.2
    9 |   1.2897 |     44.577 |   1.2898 |     44.331 |     0.2
   10 |   1.2697 |     44.500 |   1.2733 |     44.208 |     0.3
   11 |   1.2520 |     43.953 |   1.2539 |     43.993 |     0.3
   12 |   1.2363 |     43.733 |   1.2419 |     43.777 |     0.3
   13 |   1.2231 |     43.209 |   1.2285 |     43.068 |     0.3
   14 |   1.2117 |     43.093 |   1.2251 |     43.284 |     0.4
   15 |   1.2012 |     42.530 |   1.2113 |     42.699 |     0.4
   16 |   1.1898 |     42.034 |   1.2034 |     42.853 |     0.4
   17 |   1.1810 |     41.504 |   1.1996 |     42.391 |     0.4
   18 |   1.1708 |     41.449 |   1.1852 |     42.606 |     0.4
   19 |   1.1620 |     41.167 |   1.1783 |     41.590 |     0.5
   20 |   1.1539 |     40.594 |   1.1705 |     41.405 |     0.5
   21 |   1.1436 |     40.395 |   1.1647 |     41.497 |     0.5
   22 |   1.1353 |     40.119 |   1.1525 |     41.004 |     0.5
   23 |   1.1258 |     39.810 |   1.1542 |     40.789 |     0.6
   24 |   1.1199 |     39.430 |   1.1416 |     40.573 |     0.6
   25 |   1.1089 |     39.076 |   1.1403 |     40.327 |     0.6
   26 |   1.1037 |     39.259 |   1.1347 |     39.772 |     0.6
   27 |   1.0942 |     38.834 |   1.1290 |     40.080 |     0.7
   28 |   1.0899 |     38.729 |   1.1197 |     39.803 |     0.7
   29 |   1.0808 |     38.216 |   1.1183 |     39.926 |     0.7
   30 |   1.0773 |     38.337 |   1.1164 |     38.971 |     0.7
   31 |   1.0685 |     38.039 |   1.1168 |     39.926 |     0.8
   32 |   1.0630 |     37.890 |   1.1016 |     39.556 |     0.8
   33 |   1.0562 |     37.432 |   1.1017 |     39.372 |     0.8
   34 |   1.0510 |     37.179 |   1.0936 |     38.694 |     0.8
   35 |   1.0447 |     36.870 |   1.0920 |     38.509 |     0.9
   36 |   1.0371 |     36.875 |   1.0879 |     38.694 |     0.9
   37 |   1.0306 |     36.704 |   1.0852 |     38.817 |     0.9
   38 |   1.0266 |     36.467 |   1.0763 |     38.232 |     0.9
   39 |   1.0184 |     36.009 |   1.0792 |     38.170 |     1.0
   40 |   1.0113 |     35.777 |   1.0678 |     37.985 |     1.0
   41 |   1.0051 |     35.501 |   1.0790 |     38.355 |     1.0
   42 |   0.9966 |     35.148 |   1.0595 |     38.201 |     1.0
   43 |   0.9923 |     34.966 |   1.0626 |     37.585 |     1.1
   44 |   0.9883 |     34.900 |   1.0633 |     37.985 |     1.1
   45 |   0.9817 |     34.724 |   1.0578 |     37.862 |     1.1
   46 |   0.9732 |     34.497 |   1.0557 |     37.369 |     1.1
   47 |   0.9652 |     34.017 |   1.0554 |     37.215 |     1.2
   48 |   0.9581 |     33.841 |   1.0460 |     36.568 |     1.2
   49 |   0.9455 |     33.013 |   1.0378 |     36.969 |     1.2
   50 |   0.9432 |     33.262 |   1.0431 |     36.014 |     1.2
   51 |   0.9295 |     32.550 |   1.0295 |     36.198 |     1.3
   52 |   0.9230 |     32.169 |   1.0247 |     36.260 |     1.3
   53 |   0.9181 |     32.241 |   1.0336 |     35.490 |     1.3
   54 |   0.9079 |     31.893 |   1.0319 |     35.860 |     1.3
   55 |   0.9053 |     31.855 |   1.0259 |     35.675 |     1.4
   56 |   0.8948 |     31.353 |   1.0210 |     34.750 |     1.4
   57 |   0.8832 |     30.840 |   1.0164 |     35.274 |     1.4
   58 |   0.8782 |     30.387 |   1.0238 |     34.535 |     1.4
   59 |   0.8663 |     29.968 |   1.0130 |     34.812 |     1.5
   60 |   0.8625 |     29.471 |   1.0151 |     34.627 |     1.5
   61 |   0.8494 |     29.323 |   1.0008 |     34.473 |     1.5
   62 |   0.8393 |     28.920 |   1.0113 |     34.412 |     1.5
   63 |   0.8353 |     28.633 |   0.9883 |     33.333 |     1.6
   64 |   0.8289 |     28.832 |   0.9926 |     33.703 |     1.6
   65 |   0.8202 |     28.313 |   0.9925 |     33.580 |     1.6
   66 |   0.8111 |     27.971 |   0.9972 |     33.303 |     1.6
   67 |   0.8052 |     27.535 |   0.9958 |     34.258 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 621,474

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0048 |     55.142 |   1.4500 |     45.718 |     0.0
    2 |   1.3790 |     45.631 |   1.3328 |     46.180 |     0.1
    3 |   1.3049 |     44.621 |   1.2922 |     44.331 |     0.1
    4 |   1.2669 |     44.009 |   1.2563 |     43.623 |     0.1
    5 |   1.2367 |     43.418 |   1.2281 |     43.099 |     0.1
    6 |   1.2083 |     42.602 |   1.2152 |     43.161 |     0.2
    7 |   1.1838 |     42.061 |   1.1980 |     43.038 |     0.2
    8 |   1.1621 |     41.261 |   1.1695 |     40.789 |     0.2
    9 |   1.1411 |     40.467 |   1.1504 |     40.696 |     0.2
   10 |   1.1244 |     40.025 |   1.1323 |     40.696 |     0.3
   11 |   1.1052 |     39.264 |   1.1193 |     40.727 |     0.3
   12 |   1.0918 |     39.043 |   1.1160 |     40.357 |     0.3
   13 |   1.0771 |     38.194 |   1.0974 |     39.926 |     0.3
   14 |   1.0623 |     37.990 |   1.0913 |     39.433 |     0.4
   15 |   1.0485 |     37.333 |   1.0748 |     38.108 |     0.4
   16 |   1.0321 |     36.539 |   1.0600 |     38.016 |     0.4
   17 |   1.0159 |     36.335 |   1.0533 |     38.170 |     0.4
   18 |   1.0012 |     35.490 |   1.0357 |     36.876 |     0.5
   19 |   0.9866 |     34.917 |   1.0347 |     36.476 |     0.5
   20 |   0.9695 |     34.188 |   1.0068 |     35.675 |     0.5
   21 |   0.9580 |     33.995 |   0.9977 |     34.781 |     0.5
   22 |   0.9416 |     33.262 |   0.9980 |     34.566 |     0.6
   23 |   0.9264 |     32.831 |   0.9976 |     35.644 |     0.6
   24 |   0.9133 |     32.324 |   0.9702 |     34.227 |     0.6
   25 |   0.8969 |     31.700 |   1.0190 |     36.168 |     0.6
   26 |   0.9058 |     32.357 |   0.9685 |     33.179 |     0.6
   27 |   0.8775 |     30.834 |   0.9564 |     32.255 |     0.7
   28 |   0.8559 |     30.045 |   0.9573 |     32.810 |     0.7
   29 |   0.8434 |     29.284 |   0.9456 |     32.039 |     0.7
   30 |   0.8288 |     28.920 |   0.9373 |     31.084 |     0.8
   31 |   0.8184 |     28.892 |   0.9308 |     31.146 |     0.8
   32 |   0.8009 |     27.695 |   0.9380 |     31.485 |     0.8
   33 |   0.7867 |     27.044 |   0.9304 |     31.023 |     0.8
   34 |   0.7773 |     26.956 |   0.9167 |     30.622 |     0.9
   35 |   0.7606 |     26.454 |   0.9202 |     30.715 |     0.9
   36 |   0.7475 |     25.770 |   0.9090 |     29.729 |     0.9
   37 |   0.7300 |     24.942 |   0.9003 |     29.791 |     0.9
   38 |   0.7155 |     24.346 |   0.9243 |     30.376 |     1.0
   39 |   0.7005 |     24.015 |   0.8899 |     28.312 |     1.0
   40 |   0.6856 |     23.188 |   0.9224 |     30.160 |     1.0
   41 |   0.6746 |     22.829 |   0.9030 |     28.959 |     1.0
   42 |   0.6573 |     22.432 |   0.9032 |     28.651 |     1.0
   43 |   0.6402 |     21.781 |   0.9350 |     28.805 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,524,514

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2132 |     60.935 |   1.6187 |     45.903 |     0.1
    2 |   1.4834 |     46.077 |   1.4262 |     45.903 |     0.1
    3 |   1.4052 |     46.160 |   1.3843 |     45.903 |     0.2
    4 |   1.3735 |     45.917 |   1.3534 |     45.625 |     0.2
    5 |   1.3498 |     45.780 |   1.3304 |     45.287 |     0.3
    6 |   1.3260 |     45.415 |   1.3179 |     44.054 |     0.4
    7 |   1.3078 |     45.112 |   1.3018 |     44.855 |     0.4
    8 |   1.2925 |     44.908 |   1.2910 |     44.147 |     0.5
    9 |   1.2757 |     44.428 |   1.2786 |     44.362 |     0.5
   10 |   1.2622 |     44.135 |   1.2636 |     43.746 |     0.6
   11 |   1.2513 |     44.020 |   1.2492 |     43.746 |     0.7
   12 |   1.2364 |     43.518 |   1.2410 |     44.177 |     0.7
   13 |   1.2214 |     43.507 |   1.2314 |     43.007 |     0.8
   14 |   1.2062 |     43.021 |   1.2161 |     43.222 |     0.9
   15 |   1.1915 |     42.536 |   1.1981 |     42.452 |     0.9
   16 |   1.1739 |     41.360 |   1.1777 |     42.113 |     1.0
   17 |   1.1550 |     40.958 |   1.1718 |     41.189 |     1.0
   18 |   1.1401 |     40.456 |   1.1586 |     40.819 |     1.1
   19 |   1.1234 |     39.777 |   1.1479 |     40.450 |     1.2
   20 |   1.1122 |     39.501 |   1.1261 |     40.203 |     1.2
   21 |   1.0942 |     38.762 |   1.1243 |     40.111 |     1.3
   22 |   1.0765 |     38.227 |   1.1060 |     38.725 |     1.3
   23 |   1.0598 |     37.570 |   1.1018 |     38.940 |     1.4
   24 |   1.0362 |     36.930 |   1.0826 |     37.985 |     1.5
   25 |   1.0233 |     36.329 |   1.0570 |     37.246 |     1.5
   26 |   1.0037 |     35.419 |   1.0643 |     37.554 |     1.6
   27 |   0.9850 |     34.696 |   1.0421 |     36.969 |     1.6
   28 |   0.9609 |     33.670 |   1.0352 |     36.506 |     1.7
   29 |   0.9479 |     33.146 |   1.0360 |     35.860 |     1.8
   30 |   0.9271 |     32.429 |   1.0096 |     35.243 |     1.8
   31 |   0.9009 |     31.397 |   0.9921 |     34.473 |     1.9
   32 |   0.8851 |     31.022 |   0.9893 |     33.734 |     1.9
   33 |   0.8637 |     29.714 |   0.9828 |     32.779 |     2.0
   34 |   0.8482 |     28.931 |   0.9682 |     32.132 |     2.1
   35 |   0.8216 |     28.065 |   0.9643 |     31.639 |     2.1
   36 |   0.8023 |     27.331 |   0.9529 |     31.208 |     2.2
   37 |   0.7851 |     26.823 |   0.9514 |     31.054 |     2.2
   38 |   0.7641 |     25.897 |   0.9428 |     30.930 |     2.3
   39 |   0.7592 |     25.654 |   0.9402 |     30.468 |     2.4
   40 |   0.7299 |     24.412 |   0.9305 |     29.729 |     2.4
   41 |   0.7049 |     23.541 |   0.9422 |     30.068 |     2.5
   42 |   0.6844 |     22.950 |   0.9376 |     29.945 |     2.6
   43 |   0.6643 |     22.112 |   0.9306 |     29.082 |     2.6
   44 |   0.6432 |     21.367 |   0.9370 |     29.267 |     2.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,606,946

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4651 |     65.685 |   1.9059 |     48.891 |     0.1
    2 |   1.6866 |     48.041 |   1.5374 |     45.903 |     0.1
    3 |   1.4767 |     46.138 |   1.4382 |     45.903 |     0.2
    4 |   1.4202 |     46.050 |   1.4210 |     46.734 |     0.3
    5 |   1.3919 |     45.857 |   1.3765 |     45.656 |     0.4
    6 |   1.3691 |     45.482 |   1.3574 |     45.009 |     0.5
    7 |   1.3507 |     45.278 |   1.3411 |     44.670 |     0.5
    8 |   1.3395 |     45.322 |   1.3276 |     45.194 |     0.6
    9 |   1.3256 |     45.123 |   1.3206 |     45.132 |     0.7
   10 |   1.3136 |     44.869 |   1.3062 |     44.578 |     0.8
   11 |   1.2981 |     44.621 |   1.2965 |     43.869 |     0.8
   12 |   1.2848 |     44.262 |   1.2806 |     44.208 |     0.9
   13 |   1.2673 |     43.904 |   1.2653 |     44.609 |     1.0
   14 |   1.2540 |     44.080 |   1.2537 |     43.315 |     1.1
   15 |   1.2343 |     43.181 |   1.2382 |     42.668 |     1.1
   16 |   1.2189 |     42.657 |   1.2252 |     42.730 |     1.2
   17 |   1.2029 |     42.216 |   1.2133 |     41.990 |     1.3
   18 |   1.1898 |     41.973 |   1.2028 |     41.898 |     1.4
   19 |   1.1752 |     41.316 |   1.1932 |     41.559 |     1.4
   20 |   1.1625 |     41.013 |   1.1790 |     41.436 |     1.5
   21 |   1.1483 |     40.511 |   1.1704 |     40.974 |     1.6
   22 |   1.1354 |     39.860 |   1.1657 |     40.419 |     1.7
   23 |   1.1173 |     39.176 |   1.1565 |     40.789 |     1.7
   24 |   1.1049 |     38.850 |   1.1489 |     40.727 |     1.8
   25 |   1.0980 |     38.734 |   1.1431 |     39.926 |     1.9
   26 |   1.0796 |     38.139 |   1.1452 |     40.142 |     2.0
   27 |   1.0629 |     37.438 |   1.1348 |     39.988 |     2.0
   28 |   1.0524 |     37.184 |   1.1265 |     39.063 |     2.1
   29 |   1.0341 |     36.257 |   1.1207 |     39.464 |     2.2
   30 |   1.0172 |     35.899 |   1.1067 |     38.571 |     2.3
   31 |   1.0014 |     35.016 |   1.1100 |     38.509 |     2.3
   32 |   0.9871 |     34.464 |   1.1129 |     38.355 |     2.4
   33 |   0.9681 |     33.344 |   1.0958 |     38.232 |     2.5
   34 |   0.9433 |     32.655 |   1.0950 |     37.461 |     2.6
   35 |   0.9320 |     32.241 |   1.0894 |     37.246 |     2.6
   36 |   0.9109 |     31.303 |   1.0883 |     37.030 |     2.7
   37 |   0.8814 |     30.376 |   1.0920 |     36.784 |     2.8
   38 |   0.8490 |     28.489 |   1.0878 |     36.845 |     2.9
   39 |   0.8299 |     27.618 |   1.0865 |     36.014 |     2.9
   40 |   0.8037 |     26.603 |   1.0818 |     35.397 |     3.0
   41 |   0.7703 |     25.328 |   1.0917 |     34.966 |     3.1
   42 |   0.7454 |     24.357 |   1.0897 |     34.720 |     3.2
   43 |   0.7209 |     23.668 |   1.0808 |     34.104 |     3.2
   44 |   0.6878 |     22.338 |   1.0918 |     33.826 |     3.3
   45 |   0.6660 |     21.511 |   1.0999 |     33.857 |     3.4
   46 |   0.6365 |     20.402 |   1.0880 |     32.532 |     3.5
   47 |   0.6138 |     19.464 |   1.0912 |     32.686 |     3.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 718,626

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2205 |     60.758 |   1.6108 |     49.045 |     0.0
    2 |   1.4783 |     46.254 |   1.4163 |     45.903 |     0.1
    3 |   1.4051 |     46.127 |   1.3833 |     45.903 |     0.1
    4 |   1.3783 |     45.862 |   1.3654 |     45.194 |     0.1
    5 |   1.3527 |     45.294 |   1.3464 |     45.410 |     0.1
    6 |   1.3280 |     45.222 |   1.3195 |     45.625 |     0.2
    7 |   1.3057 |     45.228 |   1.3007 |     45.779 |     0.2
    8 |   1.2882 |     44.814 |   1.2943 |     44.701 |     0.2
    9 |   1.2707 |     44.279 |   1.2675 |     44.455 |     0.2
   10 |   1.2520 |     43.953 |   1.2501 |     43.839 |     0.3
   11 |   1.2338 |     43.358 |   1.2353 |     42.853 |     0.3
   12 |   1.2196 |     43.589 |   1.2325 |     42.391 |     0.3
   13 |   1.2052 |     42.833 |   1.2145 |     43.592 |     0.3
   14 |   1.1958 |     42.618 |   1.2040 |     43.130 |     0.4
   15 |   1.1835 |     42.342 |   1.1934 |     44.763 |     0.4
   16 |   1.1742 |     42.094 |   1.1870 |     43.130 |     0.4
   17 |   1.1676 |     42.282 |   1.1738 |     42.791 |     0.4
   18 |   1.1561 |     41.879 |   1.1679 |     42.575 |     0.5
   19 |   1.1477 |     41.686 |   1.1653 |     42.853 |     0.5
   20 |   1.1412 |     41.377 |   1.1608 |     42.545 |     0.5
   21 |   1.1363 |     41.278 |   1.1552 |     42.483 |     0.5
   22 |   1.1305 |     41.079 |   1.1465 |     42.514 |     0.6
   23 |   1.1252 |     40.864 |   1.1387 |     42.083 |     0.6
   24 |   1.1174 |     40.809 |   1.1378 |     42.267 |     0.6
   25 |   1.1130 |     40.665 |   1.1290 |     41.374 |     0.6
   26 |   1.1088 |     40.721 |   1.1310 |     41.990 |     0.7
   27 |   1.1019 |     40.235 |   1.1244 |     41.959 |     0.7
   28 |   1.0997 |     40.676 |   1.1202 |     41.651 |     0.7
   29 |   1.0944 |     40.230 |   1.1288 |     42.976 |     0.7
   30 |   1.0895 |     40.218 |   1.1185 |     41.497 |     0.8
   31 |   1.0829 |     39.976 |   1.1282 |     41.312 |     0.8
   32 |   1.0835 |     39.805 |   1.1197 |     41.929 |     0.8
   33 |   1.0834 |     39.821 |   1.1077 |     40.450 |     0.8
   34 |   1.0737 |     39.330 |   1.1188 |     40.974 |     0.9
   35 |   1.0750 |     39.396 |   1.1027 |     40.789 |     0.9
   36 |   1.0715 |     39.567 |   1.1069 |     40.357 |     0.9
   37 |   1.0651 |     39.380 |   1.0997 |     40.511 |     0.9
   38 |   1.0635 |     39.209 |   1.0950 |     40.234 |     1.0
   39 |   1.0558 |     38.762 |   1.1038 |     40.203 |     1.0
   40 |   1.0549 |     38.635 |   1.0963 |     40.388 |     1.0
   41 |   1.0492 |     38.602 |   1.0818 |     39.464 |     1.0
   42 |   1.0431 |     38.083 |   1.0871 |     40.203 |     1.1
   43 |   1.0393 |     38.293 |   1.0722 |     38.694 |     1.1
   44 |   1.0342 |     37.940 |   1.0750 |     39.372 |     1.1
   45 |   1.0297 |     38.012 |   1.0699 |     38.417 |     1.1
   46 |   1.0246 |     37.449 |   1.0682 |     39.125 |     1.2
   47 |   1.0197 |     36.980 |   1.0660 |     39.279 |     1.2
   48 |   1.0109 |     36.638 |   1.0682 |     38.509 |     1.2
   49 |   1.0079 |     36.737 |   1.0599 |     38.047 |     1.2
   50 |   1.0014 |     36.489 |   1.0479 |     37.307 |     1.3
   51 |   0.9966 |     36.274 |   1.0535 |     38.725 |     1.3
   52 |   0.9920 |     35.932 |   1.0465 |     37.338 |     1.3
   53 |   0.9885 |     35.844 |   1.0540 |     38.108 |     1.3
   54 |   0.9823 |     35.479 |   1.0403 |     37.616 |     1.4
   55 |   0.9778 |     35.303 |   1.0398 |     37.184 |     1.4
   56 |   0.9739 |     35.005 |   1.0380 |     37.246 |     1.4
   57 |   0.9718 |     35.353 |   1.0348 |     36.691 |     1.4
   58 |   0.9669 |     35.016 |   1.0319 |     37.061 |     1.5
   59 |   0.9583 |     34.966 |   1.0328 |     36.999 |     1.5
   60 |   0.9546 |     34.365 |   1.0255 |     36.322 |     1.5
   61 |   0.9522 |     34.277 |   1.0188 |     36.044 |     1.5
   62 |   0.9460 |     34.128 |   1.0286 |     36.168 |     1.6
   63 |   0.9428 |     34.056 |   1.0296 |     36.352 |     1.6
   64 |   0.9405 |     34.122 |   1.0172 |     36.476 |     1.6
   65 |   0.9323 |     33.780 |   1.0151 |     35.890 |     1.6
   66 |   0.9245 |     33.328 |   1.0149 |     35.705 |     1.7
   67 |   0.9216 |     32.964 |   1.0290 |     36.506 |     1.7
   68 |   0.9201 |     32.997 |   1.0113 |     35.983 |     1.7
   69 |   0.9115 |     32.853 |   1.0132 |     34.997 |     1.7
   70 |   0.9058 |     32.555 |   0.9994 |     34.412 |     1.8
   71 |   0.9049 |     32.616 |   0.9958 |     34.781 |     1.8
   72 |   0.9019 |     32.406 |   0.9999 |     35.952 |     1.8
   73 |   0.8903 |     32.136 |   0.9938 |     34.689 |     1.8
   74 |   0.8872 |     32.037 |   0.9909 |     34.935 |     1.9
   75 |   0.8792 |     31.413 |   0.9925 |     34.227 |     1.9
   76 |   0.8728 |     31.513 |   0.9913 |     35.059 |     1.9
   77 |   0.8685 |     31.143 |   0.9906 |     34.288 |     1.9
   78 |   0.8619 |     31.248 |   0.9947 |     34.874 |     2.0
   79 |   0.8614 |     30.950 |   0.9903 |     34.781 |     2.0
   80 |   0.8513 |     30.481 |   0.9850 |     33.826 |     2.0
   81 |   0.8466 |     30.067 |   0.9876 |     34.196 |     2.0
   82 |   0.8414 |     30.089 |   0.9798 |     33.826 |     2.1
   83 |   0.8425 |     30.073 |   0.9845 |     33.949 |     2.1
   84 |   0.8327 |     29.488 |   0.9886 |     34.134 |     2.1
   85 |   0.8273 |     29.460 |   0.9813 |     34.134 |     2.1
   86 |   0.8195 |     29.069 |   0.9733 |     33.148 |     2.2
   87 |   0.8135 |     29.047 |   0.9714 |     33.826 |     2.2
   88 |   0.8101 |     28.898 |   0.9712 |     33.487 |     2.2
   89 |   0.8044 |     28.567 |   0.9759 |     33.210 |     2.2
   90 |   0.7978 |     28.324 |   0.9731 |     33.672 |     2.3
   91 |   0.7906 |     28.026 |   0.9752 |     33.087 |     2.3
   92 |   0.7877 |     27.877 |   0.9751 |     33.272 |     2.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 639,906

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0786 |     56.819 |   1.5177 |     45.656 |     0.0
    2 |   1.4215 |     45.642 |   1.3738 |     46.211 |     0.1
    3 |   1.3401 |     44.963 |   1.3202 |     44.855 |     0.1
    4 |   1.3011 |     44.522 |   1.2991 |     44.855 |     0.1
    5 |   1.2791 |     44.108 |   1.2778 |     45.071 |     0.1
    6 |   1.2532 |     43.490 |   1.2547 |     44.085 |     0.2
    7 |   1.2337 |     43.198 |   1.2472 |     43.253 |     0.2
    8 |   1.2140 |     42.762 |   1.2238 |     43.469 |     0.2
    9 |   1.1963 |     42.315 |   1.2081 |     42.452 |     0.3
   10 |   1.1760 |     41.747 |   1.1991 |     42.206 |     0.3
   11 |   1.1557 |     40.963 |   1.1690 |     41.651 |     0.3
   12 |   1.1335 |     39.876 |   1.1531 |     40.481 |     0.3
   13 |   1.1160 |     39.468 |   1.1455 |     40.357 |     0.4
   14 |   1.0903 |     38.277 |   1.1275 |     39.341 |     0.4
   15 |   1.0709 |     37.510 |   1.1058 |     39.587 |     0.4
   16 |   1.0441 |     36.632 |   1.0953 |     38.047 |     0.4
   17 |   1.0242 |     35.441 |   1.0718 |     38.016 |     0.5
   18 |   0.9960 |     34.244 |   1.0628 |     36.815 |     0.5
   19 |   0.9710 |     33.411 |   1.0331 |     34.997 |     0.5
   20 |   0.9438 |     32.423 |   1.0243 |     34.812 |     0.6
   21 |   0.9200 |     31.231 |   1.0027 |     33.549 |     0.6
   22 |   0.8965 |     30.205 |   0.9966 |     33.303 |     0.6
   23 |   0.8727 |     29.378 |   0.9739 |     32.471 |     0.6
   24 |   0.8435 |     28.396 |   0.9640 |     32.224 |     0.7
   25 |   0.8229 |     27.541 |   0.9650 |     31.824 |     0.7
   26 |   0.7974 |     26.520 |   0.9332 |     30.684 |     0.7
   27 |   0.7702 |     25.897 |   0.9251 |     29.791 |     0.7
   28 |   0.7429 |     24.705 |   0.9324 |     30.129 |     0.8
   29 |   0.7181 |     23.546 |   0.9223 |     29.852 |     0.8
   30 |   0.7037 |     23.221 |   0.9180 |     29.082 |     0.8
   31 |   0.6751 |     21.991 |   0.9205 |     28.928 |     0.9
   32 |   0.6833 |     22.454 |   0.9209 |     29.298 |     0.9
   33 |   0.6404 |     20.904 |   0.9187 |     29.606 |     0.9
   34 |   0.6168 |     20.319 |   0.9140 |     28.497 |     0.9
   35 |   0.5972 |     19.497 |   0.9202 |     28.558 |     1.0
   36 |   0.5731 |     18.515 |   0.9139 |     27.665 |     1.0
   37 |   0.5598 |     18.200 |   0.9275 |     28.589 |     1.0
   38 |   0.5401 |     17.461 |   0.9349 |     28.589 |     1.1
   39 |   0.5138 |     16.578 |   0.9263 |     27.511 |     1.1
   40 |   0.4998 |     16.474 |   0.9469 |     28.343 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 542,754

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0277 |     55.390 |   1.4734 |     45.656 |     0.0
    2 |   1.3965 |     45.917 |   1.3416 |     44.948 |     0.0
    3 |   1.3159 |     44.682 |   1.2992 |     44.331 |     0.1
    4 |   1.2735 |     44.224 |   1.2592 |     43.685 |     0.1
    5 |   1.2392 |     43.534 |   1.2436 |     43.438 |     0.1
    6 |   1.2114 |     42.828 |   1.2086 |     42.175 |     0.1
    7 |   1.1821 |     41.725 |   1.1927 |     41.528 |     0.1
    8 |   1.1577 |     40.599 |   1.1727 |     41.097 |     0.2
    9 |   1.1311 |     39.639 |   1.1443 |     40.542 |     0.2
   10 |   1.1117 |     39.385 |   1.1323 |     40.419 |     0.2
   11 |   1.0855 |     38.265 |   1.1030 |     39.094 |     0.2
   12 |   1.0652 |     37.322 |   1.0911 |     38.447 |     0.2
   13 |   1.0418 |     36.682 |   1.0749 |     37.400 |     0.3
   14 |   1.0146 |     35.457 |   1.0548 |     36.722 |     0.3
   15 |   0.9918 |     34.315 |   1.0331 |     35.521 |     0.3
   16 |   0.9700 |     33.300 |   1.0120 |     34.720 |     0.3
   17 |   0.9458 |     32.715 |   1.0009 |     34.319 |     0.3
   18 |   0.9185 |     31.546 |   0.9933 |     33.333 |     0.4
   19 |   0.9205 |     31.728 |   0.9783 |     32.440 |     0.4
   20 |   0.8812 |     30.415 |   0.9524 |     31.547 |     0.4
   21 |   0.8552 |     29.196 |   0.9512 |     31.608 |     0.4
   22 |   0.8334 |     28.335 |   0.9406 |     31.269 |     0.4
   23 |   0.8120 |     27.541 |   0.9287 |     31.054 |     0.5
   24 |   0.7827 |     26.393 |   0.9103 |     30.129 |     0.5
   25 |   0.7668 |     25.858 |   0.9247 |     29.945 |     0.5
   26 |   0.7451 |     24.754 |   0.8931 |     29.174 |     0.5
   27 |   0.7290 |     24.297 |   0.9080 |     29.267 |     0.5
   28 |   0.7028 |     23.348 |   0.9003 |     28.127 |     0.6
   29 |   0.6883 |     23.237 |   0.8920 |     28.373 |     0.6
   30 |   0.6656 |     22.244 |   0.8870 |     27.973 |     0.6
   31 |   0.6508 |     21.643 |   0.8950 |     27.911 |     0.6
   32 |   0.6288 |     20.815 |   0.8879 |     27.480 |     0.6
   33 |   0.6099 |     20.198 |   0.8826 |     27.572 |     0.6
   34 |   0.5901 |     19.508 |   0.8952 |     27.326 |     0.7
   35 |   0.5795 |     18.940 |   0.8962 |     27.942 |     0.7
   36 |   0.5665 |     18.802 |   0.8981 |     27.295 |     0.7
   37 |   0.5540 |     18.272 |   0.8935 |     27.665 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,887,394

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1092 |     57.669 |   1.5565 |     45.933 |     0.1
    2 |   1.4501 |     46.000 |   1.3929 |     45.718 |     0.2
    3 |   1.3629 |     45.526 |   1.3420 |     44.917 |     0.2
    4 |   1.3177 |     44.842 |   1.3091 |     44.578 |     0.3
    5 |   1.2868 |     44.351 |   1.2870 |     44.486 |     0.4
    6 |   1.2640 |     43.953 |   1.2636 |     43.777 |     0.5
    7 |   1.2412 |     43.694 |   1.2455 |     43.222 |     0.6
    8 |   1.2206 |     42.944 |   1.2159 |     42.052 |     0.6
    9 |   1.1986 |     42.359 |   1.2067 |     42.237 |     0.7
   10 |   1.1809 |     41.653 |   1.1891 |     41.620 |     0.8
   11 |   1.1622 |     40.985 |   1.1802 |     41.128 |     0.9
   12 |   1.1423 |     40.401 |   1.1593 |     41.189 |     1.0
   13 |   1.1242 |     39.722 |   1.1387 |     40.357 |     1.0
   14 |   1.1013 |     38.806 |   1.1279 |     39.587 |     1.1
   15 |   1.0836 |     38.045 |   1.1146 |     39.741 |     1.2
   16 |   1.0612 |     36.914 |   1.0908 |     37.092 |     1.3
   17 |   1.0356 |     35.866 |   1.0770 |     38.324 |     1.4
   18 |   1.0165 |     35.297 |   1.0712 |     37.800 |     1.4
   19 |   1.0000 |     34.608 |   1.0546 |     36.907 |     1.5
   20 |   0.9780 |     33.913 |   1.0447 |     36.383 |     1.6
   21 |   0.9655 |     33.190 |   1.0361 |     37.092 |     1.7
   22 |   0.9483 |     32.749 |   1.0112 |     34.904 |     1.8
   23 |   0.9325 |     32.219 |   1.0068 |     34.843 |     1.8
   24 |   0.9133 |     31.441 |   1.0071 |     35.213 |     1.9
   25 |   0.8935 |     30.669 |   0.9938 |     33.611 |     2.0
   26 |   0.8729 |     29.620 |   0.9811 |     33.179 |     2.1
   27 |   0.8502 |     28.936 |   0.9729 |     33.641 |     2.2
   28 |   0.8364 |     28.556 |   0.9543 |     31.608 |     2.2
   29 |   0.8103 |     27.198 |   0.9502 |     30.900 |     2.3
   30 |   0.7938 |     26.912 |   0.9510 |     32.039 |     2.4
   31 |   0.7731 |     25.725 |   0.9363 |     31.978 |     2.5
   32 |   0.7557 |     25.378 |   0.9474 |     31.485 |     2.5
   33 |   0.7354 |     24.567 |   0.9376 |     31.793 |     2.6
   34 |   0.7084 |     23.712 |   0.9215 |     30.376 |     2.7
   35 |   0.6941 |     22.708 |   0.9233 |     30.407 |     2.8
   36 |   0.6636 |     21.715 |   0.9125 |     29.698 |     2.9
   37 |   0.6456 |     21.295 |   0.9193 |     29.698 |     2.9
   38 |   0.6262 |     20.573 |   0.9096 |     29.390 |     3.0
   39 |   0.6026 |     19.905 |   0.9168 |     29.205 |     3.1
   40 |   0.5880 |     19.282 |   0.9089 |     28.620 |     3.2
   41 |   0.5627 |     18.101 |   0.9278 |     29.328 |     3.3
   42 |   0.5425 |     17.571 |   0.9167 |     28.774 |     3.3
   43 |   0.5240 |     17.036 |   0.9442 |     29.051 |     3.4
   44 |   0.5196 |     16.832 |   0.9374 |     28.158 |     3.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 772,002

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1903 |     59.798 |   1.6018 |     49.045 |     0.0
    2 |   1.4779 |     46.133 |   1.4210 |     45.872 |     0.1
    3 |   1.3954 |     46.133 |   1.3704 |     45.132 |     0.1
    4 |   1.3603 |     45.631 |   1.3404 |     45.163 |     0.1
    5 |   1.3336 |     45.482 |   1.3262 |     45.102 |     0.1
    6 |   1.3240 |     45.360 |   1.3128 |     44.732 |     0.2
    7 |   1.3019 |     45.206 |   1.2970 |     44.640 |     0.2
    8 |   1.2876 |     44.786 |   1.2942 |     45.009 |     0.2
    9 |   1.2749 |     44.334 |   1.2810 |     44.516 |     0.3
   10 |   1.2599 |     43.860 |   1.2685 |     44.270 |     0.3
   11 |   1.2466 |     43.523 |   1.2611 |     44.023 |     0.3
   12 |   1.2325 |     43.551 |   1.2365 |     43.438 |     0.3
   13 |   1.2139 |     42.966 |   1.2174 |     43.099 |     0.4
   14 |   1.1903 |     42.138 |   1.2070 |     42.976 |     0.4
   15 |   1.1720 |     41.261 |   1.1897 |     42.206 |     0.4
   16 |   1.1486 |     40.378 |   1.1703 |     42.575 |     0.5
   17 |   1.1271 |     39.556 |   1.1576 |     41.066 |     0.5
   18 |   1.1045 |     38.916 |   1.1398 |     40.850 |     0.5
   19 |   1.0799 |     37.901 |   1.1396 |     40.265 |     0.5
   20 |   1.0613 |     37.096 |   1.1205 |     39.864 |     0.6
   21 |   1.0331 |     35.965 |   1.1039 |     38.817 |     0.6
   22 |   1.0105 |     35.364 |   1.0871 |     37.215 |     0.6
   23 |   0.9864 |     34.172 |   1.0794 |     36.599 |     0.7
   24 |   0.9604 |     33.273 |   1.0972 |     36.661 |     0.7
   25 |   0.9348 |     32.340 |   1.0630 |     36.137 |     0.7
   26 |   0.9102 |     31.176 |   1.0461 |     35.613 |     0.7
   27 |   0.8898 |     30.619 |   1.0406 |     34.750 |     0.8
   28 |   0.8654 |     29.847 |   1.0339 |     34.196 |     0.8
   29 |   0.8379 |     28.396 |   1.0336 |     34.781 |     0.8
   30 |   0.8133 |     27.375 |   1.0247 |     33.611 |     0.9
   31 |   0.7923 |     26.674 |   1.0219 |     33.087 |     0.9
   32 |   0.7702 |     25.852 |   1.0285 |     32.163 |     0.9
   33 |   0.7448 |     25.008 |   1.0239 |     31.208 |     0.9
   34 |   0.7295 |     24.164 |   1.0210 |     30.869 |     1.0
   35 |   0.7089 |     23.629 |   1.0121 |     31.423 |     1.0
   36 |   0.6891 |     22.741 |   1.0392 |     31.485 |     1.0
   37 |   0.6713 |     22.057 |   1.0163 |     31.023 |     1.1
   38 |   0.6425 |     21.080 |   1.0292 |     30.561 |     1.1
   39 |   0.6208 |     20.242 |   1.0289 |     30.253 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,194,530

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5793 |     68.537 |   2.0274 |     59.550 |     0.1
    2 |   1.7875 |     50.634 |   1.5853 |     45.933 |     0.1
    3 |   1.5044 |     46.017 |   1.4538 |     45.903 |     0.2
    4 |   1.4340 |     46.105 |   1.4180 |     45.903 |     0.2
    5 |   1.4089 |     46.166 |   1.3997 |     45.903 |     0.3
    6 |   1.3848 |     46.055 |   1.3752 |     45.964 |     0.4
    7 |   1.3543 |     45.697 |   1.3436 |     45.471 |     0.4
    8 |   1.3324 |     45.195 |   1.3259 |     45.625 |     0.5
    9 |   1.3186 |     45.068 |   1.3166 |     45.040 |     0.5
   10 |   1.3082 |     45.101 |   1.3044 |     45.009 |     0.6
   11 |   1.2958 |     44.842 |   1.2930 |     45.471 |     0.7
   12 |   1.2809 |     44.527 |   1.2810 |     44.239 |     0.7
   13 |   1.2680 |     44.086 |   1.2682 |     44.670 |     0.8
   14 |   1.2521 |     43.998 |   1.2534 |     43.777 |     0.8
   15 |   1.2423 |     43.551 |   1.2371 |     43.315 |     0.9
   16 |   1.2326 |     43.573 |   1.2364 |     43.438 |     0.9
   17 |   1.2180 |     43.153 |   1.2175 |     42.452 |     1.0
   18 |   1.2075 |     43.131 |   1.2128 |     42.699 |     1.1
   19 |   1.1990 |     42.530 |   1.2039 |     42.237 |     1.1
   20 |   1.1905 |     42.585 |   1.1987 |     42.113 |     1.2
   21 |   1.1828 |     42.177 |   1.1926 |     41.497 |     1.2
   22 |   1.1758 |     41.730 |   1.1897 |     41.651 |     1.3
   23 |   1.1708 |     41.438 |   1.1815 |     41.466 |     1.4
   24 |   1.1640 |     41.725 |   1.1718 |     41.528 |     1.4
   25 |   1.1560 |     41.305 |   1.1680 |     41.559 |     1.5
   26 |   1.1529 |     41.107 |   1.1676 |     41.251 |     1.5
   27 |   1.1456 |     40.792 |   1.1619 |     41.651 |     1.6
   28 |   1.1402 |     40.892 |   1.1590 |     41.251 |     1.7
   29 |   1.1358 |     40.538 |   1.1513 |     41.004 |     1.7
   30 |   1.1307 |     40.693 |   1.1494 |     41.004 |     1.8
   31 |   1.1238 |     40.340 |   1.1503 |     41.251 |     1.8
   32 |   1.1228 |     40.075 |   1.1428 |     41.097 |     1.9
   33 |   1.1187 |     39.761 |   1.1405 |     40.696 |     1.9
   34 |   1.1094 |     39.424 |   1.1323 |     40.511 |     2.0
   35 |   1.1041 |     39.154 |   1.1325 |     41.251 |     2.1
   36 |   1.1026 |     39.209 |   1.1381 |     40.450 |     2.1
   37 |   1.0993 |     39.170 |   1.1284 |     39.741 |     2.2
   38 |   1.0949 |     39.352 |   1.1271 |     39.895 |     2.2
   39 |   1.0880 |     38.955 |   1.1262 |     40.018 |     2.3
   40 |   1.0870 |     38.911 |   1.1222 |     39.926 |     2.4
   41 |   1.0785 |     38.624 |   1.1181 |     39.988 |     2.4
   42 |   1.0772 |     38.602 |   1.1239 |     40.080 |     2.5
   43 |   1.0718 |     38.425 |   1.1161 |     39.926 |     2.5
   44 |   1.0662 |     38.177 |   1.1089 |     39.926 |     2.6
   45 |   1.0607 |     38.089 |   1.1129 |     39.772 |     2.7
   46 |   1.0578 |     37.659 |   1.1080 |     40.018 |     2.7
   47 |   1.0527 |     37.576 |   1.1269 |     39.834 |     2.8
   48 |   1.0739 |     38.596 |   1.1024 |     39.587 |     2.8
   49 |   1.0487 |     37.399 |   1.1050 |     39.834 |     2.9
   50 |   1.0457 |     37.626 |   1.1026 |     39.618 |     2.9
   51 |   1.0380 |     37.063 |   1.0944 |     39.310 |     3.0
   52 |   1.0347 |     36.969 |   1.0957 |     38.879 |     3.1
   53 |   1.0306 |     36.627 |   1.0977 |     38.755 |     3.1
   54 |   1.0296 |     36.610 |   1.0964 |     38.509 |     3.2
   55 |   1.0234 |     36.511 |   1.0908 |     39.002 |     3.2
   56 |   1.0168 |     36.307 |   1.0979 |     38.786 |     3.3
   57 |   1.0171 |     36.197 |   1.0812 |     37.770 |     3.4
   58 |   1.0100 |     35.656 |   1.0862 |     38.016 |     3.4
   59 |   1.0026 |     35.513 |   1.0785 |     38.108 |     3.5
   60 |   1.0002 |     35.314 |   1.0760 |     37.492 |     3.5
   61 |   1.0007 |     35.573 |   1.0767 |     37.862 |     3.6
   62 |   0.9901 |     35.303 |   1.0734 |     38.139 |     3.7
   63 |   0.9873 |     35.016 |   1.0821 |     37.554 |     3.7
   64 |   0.9826 |     34.806 |   1.0673 |     36.722 |     3.8
   65 |   0.9769 |     34.630 |   1.0750 |     37.461 |     3.8
   66 |   0.9775 |     34.547 |   1.0604 |     36.722 |     3.9
   67 |   0.9684 |     34.089 |   1.0662 |     36.691 |     4.0
   68 |   0.9660 |     34.062 |   1.0694 |     36.938 |     4.0
   69 |   0.9625 |     33.808 |   1.0604 |     37.215 |     4.1
   70 |   0.9559 |     33.708 |   1.0610 |     36.198 |     4.1
   71 |   0.9498 |     33.449 |   1.0542 |     36.722 |     4.2
   72 |   0.9476 |     33.571 |   1.0600 |     36.815 |     4.2
   73 |   0.9468 |     33.245 |   1.0508 |     36.260 |     4.3
   74 |   0.9408 |     33.267 |   1.0602 |     36.075 |     4.4
   75 |   0.9321 |     32.738 |   1.0453 |     36.168 |     4.4
   76 |   0.9280 |     32.550 |   1.0468 |     35.798 |     4.5
   77 |   0.9240 |     32.638 |   1.0469 |     36.229 |     4.5
   78 |   0.9158 |     32.103 |   1.0564 |     35.952 |     4.6
   79 |   0.9121 |     32.015 |   1.0464 |     35.983 |     4.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 475,554

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5353 |     68.526 |   1.9843 |     59.458 |     0.0
    2 |   1.7356 |     49.575 |   1.5587 |     45.903 |     0.1
    3 |   1.4866 |     46.017 |   1.4396 |     45.872 |     0.1
    4 |   1.4166 |     45.934 |   1.3932 |     45.656 |     0.1
    5 |   1.3680 |     45.835 |   1.3527 |     45.225 |     0.1
    6 |   1.3357 |     44.880 |   1.3262 |     44.640 |     0.2
    7 |   1.3078 |     44.174 |   1.3020 |     44.732 |     0.2
    8 |   1.2838 |     44.246 |   1.2795 |     44.362 |     0.2
    9 |   1.2615 |     43.876 |   1.2630 |     43.900 |     0.2
   10 |   1.2458 |     43.567 |   1.2509 |     43.346 |     0.3
   11 |   1.2329 |     43.209 |   1.2400 |     42.976 |     0.3
   12 |   1.2180 |     42.867 |   1.2225 |     41.929 |     0.3
   13 |   1.2021 |     42.271 |   1.2197 |     42.329 |     0.3
   14 |   1.1883 |     41.785 |   1.2027 |     41.651 |     0.4
   15 |   1.1731 |     41.212 |   1.1907 |     42.083 |     0.4
   16 |   1.1602 |     40.881 |   1.1802 |     41.990 |     0.4
   17 |   1.1442 |     40.500 |   1.1684 |     41.898 |     0.4
   18 |   1.1313 |     40.003 |   1.1535 |     40.450 |     0.5
   19 |   1.1160 |     39.396 |   1.1450 |     40.049 |     0.5
   20 |   1.1068 |     38.911 |   1.1509 |     40.481 |     0.5
   21 |   1.0976 |     39.038 |   1.1501 |     40.819 |     0.5
   22 |   1.0848 |     38.194 |   1.1407 |     40.542 |     0.6
   23 |   1.0745 |     37.830 |   1.1301 |     39.988 |     0.6
   24 |   1.0637 |     37.565 |   1.1202 |     39.464 |     0.6
   25 |   1.0532 |     37.008 |   1.1195 |     39.649 |     0.6
   26 |   1.0449 |     36.853 |   1.1086 |     38.232 |     0.7
   27 |   1.0345 |     36.202 |   1.1037 |     38.725 |     0.7
   28 |   1.0211 |     36.026 |   1.1061 |     39.002 |     0.7
   29 |   1.0164 |     35.501 |   1.0924 |     37.708 |     0.7
   30 |   1.0002 |     34.547 |   1.0939 |     38.447 |     0.8
   31 |   0.9934 |     34.724 |   1.0903 |     38.201 |     0.8
   32 |   0.9894 |     34.183 |   1.0828 |     37.924 |     0.8
   33 |   0.9702 |     33.626 |   1.1029 |     38.293 |     0.8
   34 |   0.9614 |     33.306 |   1.0811 |     37.585 |     0.9
   35 |   0.9520 |     32.776 |   1.0791 |     38.262 |     0.9
   36 |   0.9366 |     32.131 |   1.0769 |     37.985 |     0.9
   37 |   0.9337 |     32.280 |   1.0704 |     35.952 |     0.9
   38 |   0.9226 |     31.689 |   1.0578 |     35.921 |     1.0
   39 |   0.9060 |     31.121 |   1.0701 |     36.260 |     1.0
   40 |   0.8944 |     30.674 |   1.0586 |     35.397 |     1.0
   41 |   0.8794 |     30.078 |   1.0578 |     35.952 |     1.0
   42 |   0.8677 |     29.631 |   1.0507 |     35.521 |     1.1
   43 |   0.8571 |     28.992 |   1.0670 |     35.213 |     1.1
   44 |   0.8487 |     28.754 |   1.0638 |     34.165 |     1.1
   45 |   0.8342 |     27.993 |   1.0436 |     33.949 |     1.1
   46 |   0.8159 |     27.254 |   1.0510 |     34.350 |     1.2
   47 |   0.8075 |     26.983 |   1.0334 |     33.333 |     1.2
   48 |   0.7897 |     26.211 |   1.0369 |     33.641 |     1.2
   49 |   0.7791 |     25.924 |   1.0551 |     33.734 |     1.2
   50 |   0.7672 |     25.577 |   1.0397 |     33.179 |     1.3
   51 |   0.7491 |     24.837 |   1.0405 |     32.317 |     1.3
   52 |   0.7360 |     24.390 |   1.0218 |     32.286 |     1.3
   53 |   0.7230 |     23.938 |   1.0542 |     32.964 |     1.3
   54 |   0.7107 |     23.381 |   1.0462 |     31.885 |     1.4
   55 |   0.6955 |     22.840 |   1.0506 |     32.317 |     1.4
   56 |   0.6781 |     21.897 |   1.0428 |     32.348 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 699,042

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4063 |     64.212 |   1.8314 |     49.569 |     0.0
    2 |   1.6213 |     47.777 |   1.4841 |     45.903 |     0.1
    3 |   1.4424 |     45.934 |   1.4098 |     45.718 |     0.1
    4 |   1.3924 |     45.879 |   1.3766 |     45.810 |     0.2
    5 |   1.3585 |     45.476 |   1.3450 |     45.132 |     0.2
    6 |   1.3323 |     45.388 |   1.3251 |     45.287 |     0.2
    7 |   1.3106 |     44.820 |   1.3070 |     45.071 |     0.3
    8 |   1.2946 |     44.709 |   1.2938 |     44.701 |     0.3
    9 |   1.2785 |     44.329 |   1.2759 |     43.623 |     0.4
   10 |   1.2642 |     43.810 |   1.2682 |     44.609 |     0.4
   11 |   1.2492 |     43.275 |   1.2500 |     42.945 |     0.4
   12 |   1.2308 |     42.850 |   1.2334 |     42.822 |     0.5
   13 |   1.2144 |     42.712 |   1.2165 |     42.668 |     0.5
   14 |   1.1949 |     41.956 |   1.2103 |     41.959 |     0.6
   15 |   1.1792 |     41.372 |   1.1877 |     41.713 |     0.6
   16 |   1.1611 |     40.417 |   1.1791 |     41.220 |     0.6
   17 |   1.1428 |     39.496 |   1.1573 |     40.203 |     0.7
   18 |   1.1232 |     38.801 |   1.1431 |     39.741 |     0.7
   19 |   1.0951 |     37.570 |   1.1170 |     38.232 |     0.8
   20 |   1.0761 |     36.820 |   1.1030 |     37.123 |     0.8
   21 |   1.0521 |     35.573 |   1.1101 |     37.831 |     0.8
   22 |   1.0400 |     35.468 |   1.0870 |     37.030 |     0.9
   23 |   1.0103 |     34.100 |   1.0664 |     36.198 |     0.9
   24 |   0.9872 |     33.653 |   1.0640 |     35.890 |     1.0
   25 |   0.9618 |     32.048 |   1.0615 |     36.260 |     1.0
   26 |   0.9451 |     31.557 |   1.0365 |     35.428 |     1.0
   27 |   0.9153 |     30.613 |   1.0309 |     34.781 |     1.1
   28 |   0.8898 |     29.808 |   1.0134 |     34.134 |     1.1
   29 |   0.8549 |     28.523 |   0.9995 |     33.426 |     1.2
   30 |   0.8280 |     27.458 |   0.9995 |     33.087 |     1.2
   31 |   0.8059 |     26.437 |   1.0024 |     32.656 |     1.2
   32 |   0.7769 |     25.461 |   0.9825 |     31.793 |     1.3
   33 |   0.7527 |     24.528 |   0.9758 |     31.762 |     1.3
   34 |   0.7222 |     23.425 |   0.9814 |     31.423 |     1.4
   35 |   0.6970 |     22.261 |   0.9762 |     30.776 |     1.4
   36 |   0.6742 |     21.648 |   0.9778 |     30.283 |     1.4
   37 |   0.6464 |     20.749 |   0.9793 |     29.821 |     1.5
   38 |   0.6226 |     19.624 |   0.9749 |     29.267 |     1.5
   39 |   0.6015 |     19.056 |   0.9879 |     29.452 |     1.6
   40 |   0.5753 |     18.167 |   0.9856 |     29.236 |     1.6
   41 |   0.5509 |     17.262 |   0.9963 |     29.359 |     1.6
   42 |   0.5385 |     16.965 |   0.9942 |     29.328 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 472,226

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0867 |     56.791 |   1.5117 |     45.903 |     0.0
    2 |   1.4206 |     45.769 |   1.3654 |     46.149 |     0.0
    3 |   1.3384 |     45.206 |   1.3252 |     45.995 |     0.1
    4 |   1.2949 |     44.632 |   1.2915 |     44.331 |     0.1
    5 |   1.2676 |     43.976 |   1.2623 |     44.147 |     0.1
    6 |   1.2400 |     43.407 |   1.2399 |     44.239 |     0.1
    7 |   1.2146 |     43.258 |   1.2192 |     43.038 |     0.1
    8 |   1.1911 |     42.155 |   1.1967 |     41.990 |     0.1
    9 |   1.1690 |     41.256 |   1.1820 |     41.590 |     0.2
   10 |   1.1460 |     40.489 |   1.1596 |     40.789 |     0.2
   11 |   1.1286 |     39.948 |   1.1473 |     40.296 |     0.2
   12 |   1.1103 |     39.076 |   1.1248 |     39.926 |     0.2
   13 |   1.0882 |     38.293 |   1.1121 |     39.341 |     0.2
   14 |   1.0699 |     37.968 |   1.0983 |     39.341 |     0.2
   15 |   1.0511 |     37.201 |   1.0849 |     38.447 |     0.3
   16 |   1.0294 |     36.064 |   1.0735 |     38.108 |     0.3
   17 |   1.0090 |     35.474 |   1.0528 |     37.215 |     0.3
   18 |   0.9902 |     34.884 |   1.0399 |     36.784 |     0.3
   19 |   0.9692 |     34.177 |   1.0276 |     35.274 |     0.3
   20 |   0.9461 |     33.046 |   1.0179 |     35.428 |     0.3
   21 |   0.9303 |     32.324 |   1.0182 |     35.213 |     0.4
   22 |   0.9084 |     31.535 |   0.9895 |     33.272 |     0.4
   23 |   0.8884 |     30.702 |   0.9807 |     32.779 |     0.4
   24 |   0.8677 |     29.858 |   0.9642 |     32.193 |     0.4
   25 |   0.8431 |     28.909 |   0.9576 |     31.916 |     0.4
   26 |   0.8254 |     28.252 |   0.9626 |     32.163 |     0.5
   27 |   0.8043 |     27.320 |   0.9504 |     31.208 |     0.5
   28 |   0.7813 |     26.481 |   0.9347 |     30.499 |     0.5
   29 |   0.7589 |     25.494 |   0.9351 |     30.160 |     0.5
   30 |   0.7389 |     25.119 |   0.9079 |     29.975 |     0.5
   31 |   0.7140 |     23.977 |   0.9240 |     29.975 |     0.5
   32 |   0.6928 |     23.552 |   0.9247 |     29.359 |     0.6
   33 |   0.6780 |     22.675 |   0.9320 |     29.390 |     0.6
   34 |   0.6594 |     21.996 |   0.9112 |     28.497 |     0.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,555,106

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3918 |     64.267 |   1.8385 |     49.230 |     0.1
    2 |   1.6312 |     47.733 |   1.4823 |     45.903 |     0.2
    3 |   1.4410 |     45.956 |   1.4102 |     46.488 |     0.2
    4 |   1.3876 |     45.818 |   1.3703 |     45.995 |     0.3
    5 |   1.3561 |     45.609 |   1.3467 |     44.824 |     0.4
    6 |   1.3306 |     44.980 |   1.3299 |     44.578 |     0.5
    7 |   1.3144 |     44.786 |   1.3121 |     44.393 |     0.6
    8 |   1.3004 |     44.433 |   1.3020 |     44.640 |     0.6
    9 |   1.2833 |     44.174 |   1.2895 |     44.547 |     0.7
   10 |   1.2685 |     43.871 |   1.2686 |     43.993 |     0.8
   11 |   1.2518 |     43.595 |   1.2512 |     43.284 |     0.9
   12 |   1.2330 |     43.203 |   1.2351 |     42.822 |     1.0
   13 |   1.2190 |     42.602 |   1.2196 |     42.329 |     1.1
   14 |   1.2008 |     42.089 |   1.2083 |     42.514 |     1.1
   15 |   1.1857 |     41.372 |   1.1918 |     41.867 |     1.2
   16 |   1.1637 |     40.577 |   1.1767 |     41.312 |     1.3
   17 |   1.1451 |     39.838 |   1.1621 |     40.111 |     1.4
   18 |   1.1254 |     39.242 |   1.1568 |     40.881 |     1.5
   19 |   1.1057 |     38.475 |   1.1431 |     39.895 |     1.5
   20 |   1.0836 |     37.874 |   1.1196 |     39.464 |     1.6
   21 |   1.0583 |     36.450 |   1.1034 |     38.324 |     1.7
   22 |   1.0349 |     35.330 |   1.0913 |     37.708 |     1.8
   23 |   1.0112 |     34.470 |   1.0801 |     36.506 |     1.9
   24 |   0.9875 |     33.515 |   1.0694 |     36.229 |     1.9
   25 |   0.9559 |     32.269 |   1.0490 |     35.028 |     2.0
   26 |   0.9267 |     31.033 |   1.0343 |     34.288 |     2.1
   27 |   0.9002 |     29.874 |   1.0327 |     33.919 |     2.2
   28 |   0.8712 |     29.157 |   1.0261 |     34.104 |     2.3
   29 |   0.8376 |     27.684 |   1.0160 |     33.241 |     2.3
   30 |   0.8111 |     26.906 |   0.9917 |     32.224 |     2.4
   31 |   0.7841 |     25.610 |   1.0050 |     32.656 |     2.5
   32 |   0.7520 |     24.578 |   0.9926 |     31.608 |     2.6
   33 |   0.7278 |     23.795 |   1.0026 |     31.639 |     2.7
   34 |   0.6909 |     22.239 |   0.9893 |     31.177 |     2.7
   35 |   0.6597 |     21.251 |   0.9834 |     31.177 |     2.8
   36 |   0.6416 |     20.821 |   1.0125 |     31.238 |     2.9
   37 |   0.6221 |     20.109 |   1.0038 |     30.684 |     3.0
   38 |   0.5845 |     18.680 |   1.0078 |     30.222 |     3.1
   39 |   0.5664 |     18.189 |   1.0082 |     30.376 |     3.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 699,042

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4239 |     65.254 |   1.8180 |     49.045 |     0.0
    2 |   1.5989 |     46.949 |   1.4661 |     45.687 |     0.1
    3 |   1.4264 |     45.879 |   1.4017 |     45.533 |     0.1
    4 |   1.3807 |     45.658 |   1.3702 |     46.118 |     0.2
    5 |   1.3503 |     45.620 |   1.3373 |     44.794 |     0.2
    6 |   1.3202 |     44.831 |   1.3163 |     45.009 |     0.2
    7 |   1.2963 |     44.356 |   1.2984 |     44.547 |     0.3
    8 |   1.2759 |     44.240 |   1.2751 |     44.609 |     0.3
    9 |   1.2575 |     43.667 |   1.2646 |     43.623 |     0.4
   10 |   1.2423 |     43.782 |   1.2477 |     43.376 |     0.4
   11 |   1.2272 |     43.347 |   1.2283 |     42.360 |     0.4
   12 |   1.2117 |     42.916 |   1.2214 |     42.699 |     0.5
   13 |   1.1987 |     42.232 |   1.2065 |     42.021 |     0.5
   14 |   1.1845 |     41.918 |   1.2113 |     42.206 |     0.5
   15 |   1.1768 |     41.548 |   1.1832 |     40.881 |     0.6
   16 |   1.1628 |     40.610 |   1.1733 |     41.004 |     0.6
   17 |   1.1496 |     40.158 |   1.1623 |     40.080 |     0.7
   18 |   1.1370 |     39.904 |   1.1524 |     40.173 |     0.7
   19 |   1.1255 |     39.347 |   1.1494 |     40.357 |     0.7
   20 |   1.1147 |     39.099 |   1.1411 |     39.556 |     0.8
   21 |   1.1042 |     38.674 |   1.1288 |     39.341 |     0.8
   22 |   1.0942 |     38.409 |   1.1188 |     39.187 |     0.9
   23 |   1.0873 |     38.381 |   1.1136 |     38.940 |     0.9
   24 |   1.0765 |     38.061 |   1.1036 |     38.478 |     0.9
   25 |   1.0651 |     37.460 |   1.1059 |     38.386 |     1.0
   26 |   1.0605 |     37.261 |   1.0982 |     39.002 |     1.0
   27 |   1.0491 |     36.704 |   1.0876 |     37.554 |     1.1
   28 |   1.0401 |     36.357 |   1.0820 |     37.677 |     1.1
   29 |   1.0334 |     36.257 |   1.0716 |     36.938 |     1.1
   30 |   1.0263 |     35.976 |   1.0653 |     36.414 |     1.2
   31 |   1.0139 |     35.248 |   1.0606 |     36.445 |     1.2
   32 |   1.0042 |     35.049 |   1.0544 |     36.722 |     1.3
   33 |   0.9949 |     34.575 |   1.0478 |     36.938 |     1.3
   34 |   0.9887 |     34.431 |   1.0473 |     35.767 |     1.3
   35 |   0.9788 |     34.128 |   1.0380 |     35.428 |     1.4
   36 |   0.9698 |     33.499 |   1.0417 |     36.229 |     1.4
   37 |   0.9553 |     32.947 |   1.0277 |     35.397 |     1.5
   38 |   0.9452 |     32.837 |   1.0220 |     34.781 |     1.5
   39 |   0.9339 |     32.208 |   1.0096 |     34.874 |     1.5
   40 |   0.9212 |     31.518 |   1.0145 |     33.949 |     1.6
   41 |   0.9210 |     31.849 |   0.9974 |     33.395 |     1.6
   42 |   0.9006 |     30.564 |   0.9917 |     33.734 |     1.6
   43 |   0.8882 |     30.200 |   0.9822 |     33.025 |     1.7
   44 |   0.8791 |     29.852 |   0.9811 |     33.025 |     1.7
   45 |   0.8620 |     29.058 |   0.9776 |     32.563 |     1.8
   46 |   0.8429 |     28.076 |   0.9599 |     31.885 |     1.8
   47 |   0.8389 |     28.434 |   0.9634 |     31.762 |     1.8
   48 |   0.8225 |     27.723 |   0.9574 |     31.300 |     1.9
   49 |   0.8133 |     27.491 |   0.9518 |     31.269 |     1.9
   50 |   0.7946 |     26.834 |   0.9508 |     31.115 |     2.0
   51 |   0.7921 |     26.647 |   0.9390 |     30.530 |     2.0
   52 |   0.7699 |     25.748 |   0.9432 |     31.423 |     2.0
   53 |   0.7489 |     24.986 |   0.9284 |     29.698 |     2.1
   54 |   0.7397 |     24.661 |   0.9274 |     30.129 |     2.1
   55 |   0.7265 |     24.181 |   0.9440 |     30.653 |     2.2
   56 |   0.7172 |     23.745 |   0.9327 |     30.776 |     2.2
   57 |   0.7031 |     23.248 |   0.9294 |     30.037 |     2.2
   58 |   0.6844 |     22.542 |   0.9218 |     29.698 |     2.3
   59 |   0.6666 |     22.189 |   0.9181 |     29.421 |     2.3
   60 |   0.6557 |     21.808 |   0.9258 |     29.513 |     2.4
   61 |   0.6511 |     21.505 |   0.9371 |     29.513 |     2.4
   62 |   0.6467 |     21.455 |   0.9208 |     28.928 |     2.4
   63 |   0.6227 |     20.689 |   0.9219 |     29.082 |     2.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,248,162

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4155 |     64.068 |   1.8745 |     49.291 |     0.1
    2 |   1.6520 |     47.705 |   1.4900 |     45.687 |     0.1
    3 |   1.4420 |     45.824 |   1.4032 |     45.564 |     0.2
    4 |   1.3774 |     45.713 |   1.3566 |     45.687 |     0.2
    5 |   1.3384 |     45.195 |   1.3299 |     45.194 |     0.3
    6 |   1.3097 |     44.908 |   1.3068 |     45.317 |     0.4
    7 |   1.2879 |     44.467 |   1.2854 |     43.685 |     0.4
    8 |   1.2655 |     43.898 |   1.2602 |     43.407 |     0.5
    9 |   1.2425 |     43.231 |   1.2455 |     43.253 |     0.5
   10 |   1.2230 |     42.795 |   1.2210 |     42.329 |     0.6
   11 |   1.2027 |     42.183 |   1.2036 |     41.436 |     0.7
   12 |   1.1869 |     41.559 |   1.1906 |     41.651 |     0.7
   13 |   1.1672 |     40.864 |   1.1763 |     41.466 |     0.8
   14 |   1.1522 |     40.152 |   1.1637 |     41.220 |     0.8
   15 |   1.1376 |     39.518 |   1.1520 |     40.234 |     0.9
   16 |   1.1233 |     39.330 |   1.1402 |     40.604 |     1.0
   17 |   1.1066 |     38.779 |   1.1270 |     39.464 |     1.0
   18 |   1.0949 |     38.221 |   1.1196 |     39.341 |     1.1
   19 |   1.0787 |     37.581 |   1.1114 |     38.909 |     1.1
   20 |   1.0692 |     37.063 |   1.0989 |     38.940 |     1.2
   21 |   1.0508 |     36.351 |   1.0838 |     36.815 |     1.3
   22 |   1.0364 |     35.540 |   1.0729 |     37.153 |     1.3
   23 |   1.0236 |     35.375 |   1.0673 |     36.661 |     1.4
   24 |   1.0157 |     34.707 |   1.0648 |     36.722 |     1.4
   25 |   1.0015 |     34.332 |   1.0423 |     36.229 |     1.5
   26 |   0.9832 |     33.791 |   1.0462 |     35.767 |     1.6
   27 |   0.9703 |     33.074 |   1.0417 |     35.798 |     1.6
   28 |   0.9577 |     32.550 |   1.0293 |     34.104 |     1.7
   29 |   0.9467 |     32.252 |   1.0220 |     34.473 |     1.7
   30 |   0.9323 |     31.640 |   1.0103 |     33.795 |     1.8
   31 |   0.9156 |     31.104 |   1.0089 |     34.689 |     1.9
   32 |   0.9004 |     30.597 |   0.9996 |     34.165 |     1.9
   33 |   0.8868 |     29.841 |   0.9887 |     32.902 |     2.0
   34 |   0.8695 |     29.163 |   0.9786 |     33.426 |     2.0
   35 |   0.8556 |     28.870 |   0.9795 |     32.378 |     2.1
   36 |   0.8361 |     28.037 |   0.9719 |     32.471 |     2.2
   37 |   0.8201 |     27.358 |   0.9663 |     31.762 |     2.2
   38 |   0.8032 |     26.972 |   0.9562 |     31.331 |     2.3
   39 |   0.7845 |     25.775 |   0.9624 |     32.009 |     2.3
   40 |   0.7690 |     25.571 |   0.9445 |     31.208 |     2.4
   41 |   0.7576 |     25.345 |   0.9559 |     30.684 |     2.5
   42 |   0.7366 |     24.319 |   0.9436 |     31.208 |     2.5
   43 |   0.7196 |     23.651 |   0.9493 |     31.177 |     2.6
   44 |   0.7091 |     23.386 |   0.9495 |     30.684 |     2.6
   45 |   0.6908 |     22.675 |   0.9249 |     29.760 |     2.7
   46 |   0.6718 |     22.013 |   0.9247 |     29.513 |     2.8
   47 |   0.6505 |     21.290 |   0.9328 |     29.328 |     2.8
   48 |   0.6333 |     20.567 |   0.9407 |     29.174 |     2.9
   49 |   0.6200 |     20.181 |   0.9390 |     29.082 |     2.9
   50 |   0.6126 |     19.414 |   0.9505 |     29.452 |     3.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 388,898

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2967 |     59.969 |   1.6785 |     48.829 |     0.0
    2 |   1.5274 |     47.186 |   1.4276 |     45.625 |     0.1
    3 |   1.3847 |     45.713 |   1.3531 |     45.040 |     0.1
    4 |   1.3327 |     44.825 |   1.3227 |     44.609 |     0.1
    5 |   1.2997 |     43.976 |   1.2915 |     44.886 |     0.1
    6 |   1.2708 |     43.501 |   1.2787 |     44.116 |     0.2
    7 |   1.2471 |     43.126 |   1.2478 |     43.130 |     0.2
    8 |   1.2281 |     42.502 |   1.2334 |     43.407 |     0.2
    9 |   1.2061 |     41.967 |   1.2124 |     42.298 |     0.2
   10 |   1.1881 |     41.642 |   1.1970 |     41.651 |     0.3
   11 |   1.1683 |     40.881 |   1.1779 |     41.097 |     0.3
   12 |   1.1461 |     40.235 |   1.1670 |     41.220 |     0.3
   13 |   1.1278 |     39.529 |   1.1499 |     39.926 |     0.3
   14 |   1.1060 |     38.414 |   1.1403 |     40.142 |     0.4
   15 |   1.0826 |     37.813 |   1.1134 |     38.786 |     0.4
   16 |   1.0615 |     36.765 |   1.1008 |     38.417 |     0.4
   17 |   1.0344 |     35.501 |   1.0853 |     36.691 |     0.4
   18 |   1.0133 |     34.961 |   1.0741 |     37.061 |     0.5
   19 |   0.9962 |     34.249 |   1.0599 |     36.722 |     0.5
   20 |   0.9709 |     33.002 |   1.0483 |     35.644 |     0.5
   21 |   0.9429 |     32.031 |   1.0322 |     34.689 |     0.5
   22 |   0.9211 |     31.066 |   1.0300 |     34.319 |     0.6
   23 |   0.8967 |     29.951 |   1.0190 |     33.826 |     0.6
   24 |   0.8683 |     29.014 |   0.9980 |     32.840 |     0.6
   25 |   0.8461 |     27.772 |   0.9949 |     32.286 |     0.6
   26 |   0.8247 |     27.171 |   0.9987 |     33.148 |     0.7
   27 |   0.8033 |     26.785 |   0.9866 |     32.039 |     0.7
   28 |   0.7687 |     25.279 |   0.9893 |     31.670 |     0.7
   29 |   0.7430 |     24.214 |   0.9739 |     31.208 |     0.7
   30 |   0.7218 |     23.337 |   0.9765 |     31.054 |     0.8
   31 |   0.7046 |     22.879 |   0.9699 |     30.314 |     0.8
   32 |   0.6806 |     21.737 |   0.9619 |     30.468 |     0.8
   33 |   0.6571 |     21.224 |   0.9616 |     29.698 |     0.8
   34 |   0.6285 |     20.021 |   0.9724 |     29.236 |     0.9
   35 |   0.6155 |     19.624 |   0.9816 |     29.760 |     0.9
   36 |   0.5932 |     18.769 |   0.9578 |     29.174 |     0.9
   37 |   0.5683 |     17.825 |   0.9935 |     30.160 |     0.9
   38 |   0.5540 |     17.605 |   0.9819 |     29.267 |     1.0
   39 |   0.5309 |     16.827 |   0.9785 |     28.589 |     1.0
   40 |   0.5175 |     16.523 |   0.9898 |     29.174 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,392,418

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0983 |     57.315 |   1.5295 |     45.903 |     0.1
    2 |   1.4435 |     46.055 |   1.3933 |     46.457 |     0.1
    3 |   1.3645 |     45.316 |   1.3398 |     45.256 |     0.2
    4 |   1.3155 |     44.593 |   1.3056 |     44.331 |     0.2
    5 |   1.2888 |     44.169 |   1.2814 |     43.469 |     0.3
    6 |   1.2642 |     43.909 |   1.2608 |     43.777 |     0.4
    7 |   1.2355 |     43.529 |   1.2330 |     44.023 |     0.4
    8 |   1.2115 |     42.982 |   1.2082 |     42.298 |     0.5
    9 |   1.1924 |     42.171 |   1.2014 |     42.298 |     0.5
   10 |   1.1760 |     41.570 |   1.1799 |     41.713 |     0.6
   11 |   1.1589 |     40.952 |   1.1613 |     41.466 |     0.7
   12 |   1.1402 |     40.356 |   1.1511 |     41.251 |     0.7
   13 |   1.1269 |     39.932 |   1.1367 |     41.097 |     0.8
   14 |   1.1304 |     40.577 |   1.1396 |     41.867 |     0.8
   15 |   1.1060 |     39.656 |   1.1145 |     40.265 |     0.9
   16 |   1.0888 |     38.950 |   1.1028 |     39.433 |     1.0
   17 |   1.0779 |     38.453 |   1.0995 |     40.450 |     1.0
   18 |   1.0643 |     37.874 |   1.0908 |     39.217 |     1.1
   19 |   1.0528 |     37.730 |   1.0832 |     39.187 |     1.1
   20 |   1.0437 |     37.626 |   1.0682 |     38.663 |     1.2
   21 |   1.0324 |     36.820 |   1.0642 |     38.509 |     1.2
   22 |   1.0162 |     36.169 |   1.0633 |     38.478 |     1.3
   23 |   1.0085 |     36.472 |   1.0411 |     38.078 |     1.4
   24 |   0.9908 |     35.496 |   1.0353 |     37.092 |     1.4
   25 |   0.9784 |     34.878 |   1.0295 |     35.890 |     1.5
   26 |   0.9669 |     34.271 |   1.0096 |     35.829 |     1.5
   27 |   0.9529 |     33.769 |   1.0002 |     35.274 |     1.6
   28 |   0.9338 |     33.267 |   0.9946 |     34.258 |     1.7
   29 |   0.9194 |     32.517 |   0.9655 |     33.703 |     1.7
   30 |   0.9164 |     32.566 |   0.9872 |     34.412 |     1.8
   31 |   0.8970 |     31.888 |   0.9667 |     33.056 |     1.8
   32 |   0.8779 |     30.917 |   0.9503 |     33.210 |     1.9
   33 |   0.8597 |     30.117 |   0.9407 |     32.317 |     2.0
   34 |   0.8578 |     30.233 |   0.9438 |     32.748 |     2.0
   35 |   0.8391 |     29.229 |   0.9416 |     32.717 |     2.1
   36 |   0.8190 |     28.269 |   0.9408 |     31.731 |     2.1
   37 |   0.8017 |     27.756 |   0.9398 |     31.331 |     2.2
   38 |   0.7889 |     27.336 |   0.9237 |     30.715 |     2.3
   39 |   0.7663 |     26.112 |   0.9200 |     31.731 |     2.3
   40 |   0.7523 |     25.648 |   0.9180 |     30.037 |     2.4
   41 |   0.7397 |     25.461 |   0.9150 |     30.530 |     2.4
   42 |   0.7209 |     24.495 |   0.9045 |     29.975 |     2.5
   43 |   0.7054 |     23.850 |   0.8968 |     29.328 |     2.6
   44 |   0.6918 |     23.458 |   0.9025 |     28.990 |     2.6
   45 |   0.6786 |     23.022 |   0.9036 |     29.606 |     2.7
   46 |   0.6545 |     22.200 |   0.8955 |     29.020 |     2.7
   47 |   0.6466 |     21.946 |   0.9040 |     29.236 |     2.8
   48 |   0.6241 |     21.036 |   0.9052 |     28.928 |     2.9
   49 |   0.6052 |     20.126 |   0.9059 |     28.189 |     2.9
   50 |   0.5956 |     19.806 |   0.9057 |     28.035 |     3.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,588,386

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4706 |     65.966 |   1.8941 |     49.045 |     0.1
    2 |   1.6912 |     48.356 |   1.5410 |     45.903 |     0.2
    3 |   1.4840 |     46.039 |   1.4475 |     45.903 |     0.2
    4 |   1.4328 |     46.044 |   1.4160 |     45.903 |     0.3
    5 |   1.4051 |     45.984 |   1.3985 |     45.903 |     0.4
    6 |   1.3834 |     45.840 |   1.3699 |     45.132 |     0.5
    7 |   1.3688 |     45.658 |   1.3562 |     45.256 |     0.6
    8 |   1.3529 |     45.537 |   1.3398 |     45.471 |     0.6
    9 |   1.3321 |     45.465 |   1.3330 |     45.471 |     0.7
   10 |   1.3191 |     45.355 |   1.3145 |     45.379 |     0.8
   11 |   1.3036 |     45.200 |   1.2997 |     45.040 |     0.9
   12 |   1.2882 |     44.682 |   1.2889 |     44.640 |     1.0
   13 |   1.2708 |     44.345 |   1.2739 |     44.547 |     1.0
   14 |   1.2571 |     43.992 |   1.2552 |     43.438 |     1.1
   15 |   1.2436 |     43.622 |   1.2455 |     43.161 |     1.2
   16 |   1.2316 |     43.507 |   1.2360 |     43.315 |     1.3
   17 |   1.2176 |     42.872 |   1.2277 |     43.407 |     1.4
   18 |   1.2046 |     42.585 |   1.2138 |     42.113 |     1.5
   19 |   1.1950 |     41.885 |   1.2049 |     42.083 |     1.5
   20 |   1.1827 |     41.559 |   1.1949 |     41.651 |     1.6
   21 |   1.1703 |     41.129 |   1.1841 |     41.251 |     1.7
   22 |   1.1584 |     41.018 |   1.1757 |     41.004 |     1.8
   23 |   1.1483 |     40.373 |   1.1651 |     40.296 |     1.9
   24 |   1.1341 |     39.976 |   1.1592 |     41.128 |     1.9
   25 |   1.1246 |     39.667 |   1.1630 |     41.066 |     2.0
   26 |   1.1112 |     39.187 |   1.1491 |     40.758 |     2.1
   27 |   1.0952 |     38.768 |   1.1382 |     40.173 |     2.2
   28 |   1.0847 |     38.249 |   1.1396 |     40.173 |     2.3
   29 |   1.0699 |     37.852 |   1.1295 |     40.080 |     2.3
   30 |   1.0562 |     37.344 |   1.1206 |     39.556 |     2.4
   31 |   1.0453 |     37.085 |   1.1242 |     39.864 |     2.5
   32 |   1.0324 |     36.346 |   1.1141 |     39.033 |     2.6
   33 |   1.0188 |     35.728 |   1.1116 |     38.971 |     2.7
   34 |   1.0089 |     35.341 |   1.1029 |     38.694 |     2.7
   35 |   0.9867 |     34.255 |   1.0944 |     37.523 |     2.8
   36 |   0.9738 |     33.874 |   1.0931 |     37.431 |     2.9
   37 |   0.9555 |     33.052 |   1.0824 |     36.630 |     3.0
   38 |   0.9324 |     32.241 |   1.0831 |     36.969 |     3.1
   39 |   0.9149 |     31.485 |   1.0829 |     36.815 |     3.1
   40 |   0.8951 |     30.448 |   1.0752 |     35.890 |     3.2
   41 |   0.8771 |     29.869 |   1.0689 |     35.736 |     3.3
   42 |   0.8605 |     28.980 |   1.0665 |     35.213 |     3.4
   43 |   0.8372 |     28.285 |   1.0682 |     35.028 |     3.5
   44 |   0.8188 |     27.496 |   1.0683 |     34.781 |     3.5
   45 |   0.8013 |     26.994 |   1.0570 |     34.473 |     3.6
   46 |   0.7787 |     25.825 |   1.0629 |     34.381 |     3.7
   47 |   0.7574 |     25.030 |   1.0651 |     34.134 |     3.8
   48 |   0.7479 |     24.986 |   1.0488 |     33.210 |     3.9
   49 |   0.7224 |     23.679 |   1.0513 |     32.502 |     3.9
   50 |   0.7040 |     23.110 |   1.0470 |     33.087 |     4.0
   51 |   0.6826 |     22.366 |   1.0675 |     32.779 |     4.1
   52 |   0.6675 |     21.742 |   1.0671 |     31.885 |     4.2
   53 |   0.6485 |     21.180 |   1.0656 |     32.348 |     4.3
   54 |   0.6346 |     20.722 |   1.0739 |     32.224 |     4.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,020,258

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1967 |     59.815 |   1.6100 |     45.903 |     0.1
    2 |   1.4807 |     46.166 |   1.4201 |     45.903 |     0.1
    3 |   1.4008 |     45.791 |   1.3741 |     45.194 |     0.2
    4 |   1.3686 |     45.664 |   1.3526 |     45.163 |     0.3
    5 |   1.3366 |     45.404 |   1.3209 |     44.824 |     0.4
    6 |   1.3001 |     44.897 |   1.2936 |     45.071 |     0.4
    7 |   1.2754 |     44.638 |   1.2633 |     43.685 |     0.5
    8 |   1.2510 |     43.942 |   1.2463 |     43.346 |     0.6
    9 |   1.2298 |     43.302 |   1.2373 |     44.208 |     0.7
   10 |   1.2132 |     42.944 |   1.2188 |     42.637 |     0.7
   11 |   1.1983 |     42.480 |   1.2036 |     42.791 |     0.8
   12 |   1.1824 |     42.045 |   1.1923 |     42.329 |     0.9
   13 |   1.1707 |     41.879 |   1.1739 |     41.158 |     1.0
   14 |   1.1524 |     40.925 |   1.1643 |     41.189 |     1.0
   15 |   1.1398 |     40.759 |   1.1573 |     41.282 |     1.1
   16 |   1.1304 |     40.285 |   1.1491 |     41.312 |     1.2
   17 |   1.1145 |     39.959 |   1.1495 |     41.590 |     1.3
   18 |   1.1024 |     39.435 |   1.1250 |     39.834 |     1.3
   19 |   1.0925 |     39.115 |   1.1199 |     39.772 |     1.4
   20 |   1.0832 |     39.071 |   1.1146 |     40.511 |     1.5
   21 |   1.0748 |     38.745 |   1.1068 |     40.080 |     1.6
   22 |   1.0658 |     38.056 |   1.0974 |     38.755 |     1.6
   23 |   1.0552 |     38.017 |   1.0972 |     39.556 |     1.7
   24 |   1.0506 |     37.659 |   1.0985 |     40.111 |     1.8
   25 |   1.0422 |     37.526 |   1.0877 |     39.248 |     1.8
   26 |   1.0324 |     37.195 |   1.0841 |     38.879 |     1.9
   27 |   1.0215 |     36.837 |   1.0805 |     38.601 |     2.0
   28 |   1.0132 |     36.241 |   1.0733 |     39.187 |     2.1
   29 |   1.0054 |     35.673 |   1.0704 |     37.924 |     2.1
   30 |   0.9968 |     35.695 |   1.0620 |     37.369 |     2.2
   31 |   0.9850 |     35.253 |   1.0579 |     37.461 |     2.3
   32 |   0.9902 |     35.518 |   1.0526 |     37.061 |     2.4
   33 |   0.9747 |     34.630 |   1.0467 |     37.492 |     2.4
   34 |   0.9618 |     34.238 |   1.0411 |     37.338 |     2.5
   35 |   0.9887 |     35.700 |   1.0596 |     37.431 |     2.6
   36 |   0.9636 |     34.580 |   1.0434 |     37.215 |     2.7
   37 |   0.9481 |     33.984 |   1.0275 |     36.999 |     2.7
   38 |   0.9371 |     33.306 |   1.0270 |     36.322 |     2.8
   39 |   0.9278 |     33.295 |   1.0455 |     36.445 |     2.9
   40 |   0.9151 |     32.561 |   1.0147 |     35.490 |     3.0
   41 |   0.9058 |     32.070 |   1.0131 |     35.397 |     3.0
   42 |   0.8929 |     31.612 |   1.0204 |     35.798 |     3.1
   43 |   0.8794 |     30.658 |   1.0022 |     34.288 |     3.2
   44 |   0.8648 |     30.470 |   1.0037 |     34.566 |     3.3
   45 |   0.8582 |     29.962 |   0.9986 |     34.535 |     3.3
   46 |   0.8499 |     29.814 |   0.9864 |     33.672 |     3.4
   47 |   0.8343 |     29.003 |   0.9794 |     33.765 |     3.5
   48 |   0.8175 |     28.313 |   0.9732 |     32.625 |     3.6
   49 |   0.8093 |     28.103 |   0.9802 |     33.210 |     3.6
   50 |   0.7911 |     27.452 |   0.9678 |     32.748 |     3.7
   51 |   0.7805 |     26.972 |   0.9865 |     33.118 |     3.8
   52 |   0.7696 |     26.426 |   0.9834 |     33.118 |     3.9
   53 |   0.7571 |     26.051 |   0.9685 |     31.824 |     3.9
   54 |   0.7413 |     25.334 |   0.9807 |     31.423 |     4.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 699,042

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3845 |     63.754 |   1.7806 |     48.768 |     0.0
    2 |   1.5909 |     47.440 |   1.4619 |     45.933 |     0.1
    3 |   1.4241 |     45.984 |   1.3943 |     45.471 |     0.1
    4 |   1.3758 |     45.620 |   1.3541 |     45.317 |     0.2
    5 |   1.3370 |     44.985 |   1.3269 |     44.609 |     0.2
    6 |   1.3135 |     44.461 |   1.3145 |     44.855 |     0.2
    7 |   1.2950 |     44.378 |   1.2927 |     44.424 |     0.3
    8 |   1.2744 |     44.113 |   1.2798 |     44.455 |     0.3
    9 |   1.2615 |     43.644 |   1.2566 |     43.130 |     0.4
   10 |   1.2469 |     43.440 |   1.2472 |     44.023 |     0.4
   11 |   1.2328 |     43.209 |   1.2347 |     42.945 |     0.4
   12 |   1.2211 |     42.635 |   1.2213 |     42.699 |     0.5
   13 |   1.2071 |     42.188 |   1.2146 |     42.329 |     0.5
   14 |   1.2000 |     42.387 |   1.2044 |     41.990 |     0.6
   15 |   1.1890 |     41.719 |   1.1925 |     41.620 |     0.6
   16 |   1.1776 |     41.416 |   1.1881 |     41.744 |     0.6
   17 |   1.1668 |     41.046 |   1.1766 |     40.974 |     0.7
   18 |   1.1591 |     40.936 |   1.1757 |     41.066 |     0.7
   19 |   1.1480 |     40.130 |   1.1601 |     40.388 |     0.7
   20 |   1.1378 |     39.744 |   1.1591 |     40.635 |     0.8
   21 |   1.1266 |     39.325 |   1.1437 |     39.495 |     0.8
   22 |   1.1189 |     38.889 |   1.1375 |     40.018 |     0.9
   23 |   1.1086 |     38.745 |   1.1319 |     40.573 |     0.9
   24 |   1.0967 |     38.023 |   1.1245 |     39.248 |     0.9
   25 |   1.0932 |     38.254 |   1.1160 |     39.310 |     1.0
   26 |   1.0784 |     37.510 |   1.1061 |     38.447 |     1.0
   27 |   1.0790 |     37.631 |   1.1034 |     39.279 |     1.1
   28 |   1.0645 |     37.002 |   1.0881 |     37.585 |     1.1
   29 |   1.0540 |     36.826 |   1.0834 |     37.646 |     1.1
   30 |   1.0426 |     36.208 |   1.0736 |     38.262 |     1.2
   31 |   1.0328 |     36.031 |   1.0650 |     37.461 |     1.2
   32 |   1.0253 |     35.772 |   1.0601 |     37.123 |     1.3
   33 |   1.0126 |     35.231 |   1.0609 |     36.845 |     1.3
   34 |   1.0009 |     34.773 |   1.0462 |     35.705 |     1.3
   35 |   0.9937 |     34.299 |   1.0391 |     35.952 |     1.4
   36 |   0.9872 |     34.359 |   1.0352 |     36.014 |     1.4
   37 |   0.9713 |     33.322 |   1.0302 |     35.367 |     1.5
   38 |   0.9641 |     33.278 |   1.0262 |     35.428 |     1.5
   39 |   0.9558 |     32.837 |   1.0209 |     35.120 |     1.5
   40 |   0.9482 |     32.622 |   1.0134 |     34.042 |     1.6
   41 |   0.9338 |     31.982 |   1.0104 |     34.412 |     1.6
   42 |   0.9236 |     31.739 |   1.0088 |     34.689 |     1.7
   43 |   0.9126 |     31.342 |   1.0039 |     33.611 |     1.7
   44 |   0.9028 |     30.757 |   1.0075 |     34.042 |     1.7
   45 |   0.8876 |     30.360 |   0.9965 |     32.902 |     1.8
   46 |   0.8765 |     30.249 |   0.9905 |     33.426 |     1.8
   47 |   0.8656 |     29.560 |   0.9880 |     33.056 |     1.9
   48 |   0.8615 |     29.466 |   0.9807 |     32.871 |     1.9
   49 |   0.8387 |     28.335 |   0.9735 |     32.810 |     1.9
   50 |   0.8254 |     28.065 |   0.9755 |     31.916 |     2.0
   51 |   0.8172 |     27.491 |   0.9608 |     31.885 |     2.0
   52 |   0.8005 |     26.862 |   0.9921 |     33.364 |     2.1
   53 |   0.7975 |     27.005 |   0.9486 |     31.208 |     2.1
   54 |   0.7710 |     25.924 |   0.9427 |     30.900 |     2.1
   55 |   0.7632 |     25.648 |   0.9510 |     31.608 |     2.2
   56 |   0.7448 |     24.909 |   0.9651 |     31.485 |     2.2
   57 |   0.7269 |     24.010 |   0.9402 |     30.869 |     2.3
   58 |   0.7186 |     23.750 |   0.9409 |     30.715 |     2.3
   59 |   0.7095 |     23.298 |   0.9542 |     30.561 |     2.3
   60 |   0.7060 |     23.287 |   0.9450 |     29.975 |     2.4
   61 |   0.6824 |     22.570 |   0.9423 |     30.006 |     2.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 324,258

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5236 |     67.091 |   1.9456 |     54.005 |     0.0
    2 |   1.7230 |     49.189 |   1.5527 |     45.903 |     0.0
    3 |   1.4817 |     46.011 |   1.4368 |     45.903 |     0.1
    4 |   1.4167 |     46.033 |   1.3978 |     45.903 |     0.1
    5 |   1.3800 |     45.901 |   1.3624 |     45.903 |     0.1
    6 |   1.3459 |     45.504 |   1.3345 |     44.824 |     0.1
    7 |   1.3207 |     44.693 |   1.3113 |     44.239 |     0.1
    8 |   1.2970 |     44.500 |   1.2924 |     44.917 |     0.1
    9 |   1.2766 |     44.389 |   1.2709 |     44.054 |     0.2
   10 |   1.2600 |     43.953 |   1.2560 |     43.931 |     0.2
   11 |   1.2435 |     43.556 |   1.2549 |     44.023 |     0.2
   12 |   1.2308 |     43.562 |   1.2396 |     43.561 |     0.2
   13 |   1.2174 |     43.115 |   1.2209 |     42.822 |     0.2
   14 |   1.2067 |     43.115 |   1.2152 |     42.760 |     0.2
   15 |   1.1965 |     42.447 |   1.2136 |     42.945 |     0.3
   16 |   1.1870 |     42.514 |   1.1996 |     42.976 |     0.3
   17 |   1.1743 |     42.210 |   1.1940 |     42.853 |     0.3
   18 |   1.1662 |     41.835 |   1.1870 |     42.853 |     0.3
   19 |   1.1572 |     41.394 |   1.1835 |     42.452 |     0.3
   20 |   1.1510 |     41.305 |   1.1757 |     42.483 |     0.3
   21 |   1.1440 |     41.029 |   1.1674 |     41.497 |     0.4
   22 |   1.1375 |     40.566 |   1.1665 |     41.559 |     0.4
   23 |   1.1307 |     40.456 |   1.1581 |     41.651 |     0.4
   24 |   1.1236 |     40.483 |   1.1622 |     41.559 |     0.4
   25 |   1.1174 |     39.937 |   1.1521 |     41.559 |     0.4
   26 |   1.1103 |     39.727 |   1.1460 |     41.189 |     0.4
   27 |   1.1047 |     39.424 |   1.1370 |     41.220 |     0.5
   28 |   1.0981 |     39.099 |   1.1354 |     41.282 |     0.5
   29 |   1.0893 |     38.740 |   1.1369 |     40.327 |     0.5
   30 |   1.0863 |     38.834 |   1.1278 |     39.372 |     0.5
   31 |   1.0834 |     38.773 |   1.1260 |     39.587 |     0.5
   32 |   1.0741 |     38.177 |   1.1143 |     39.649 |     0.6
   33 |   1.0671 |     37.907 |   1.1100 |     39.125 |     0.6
   34 |   1.0619 |     37.841 |   1.1147 |     39.526 |     0.6
   35 |   1.0575 |     37.355 |   1.1133 |     38.786 |     0.6
   36 |   1.0451 |     36.544 |   1.1121 |     39.772 |     0.6
   37 |   1.0418 |     36.980 |   1.0951 |     38.386 |     0.6
   38 |   1.0348 |     36.263 |   1.0897 |     38.201 |     0.7
   39 |   1.0253 |     35.943 |   1.0813 |     37.800 |     0.7
   40 |   1.0196 |     35.877 |   1.0764 |     38.417 |     0.7
   41 |   1.0046 |     35.055 |   1.0618 |     37.616 |     0.7
   42 |   0.9980 |     34.646 |   1.0573 |     36.999 |     0.7
   43 |   0.9882 |     34.497 |   1.0657 |     37.954 |     0.7
   44 |   0.9842 |     34.371 |   1.0544 |     37.246 |     0.8
   45 |   0.9761 |     33.973 |   1.0619 |     37.554 |     0.8
   46 |   0.9733 |     34.161 |   1.0485 |     36.784 |     0.8
   47 |   0.9594 |     33.289 |   1.0407 |     35.921 |     0.8
   48 |   0.9552 |     33.013 |   1.0303 |     35.952 |     0.8
   49 |   0.9433 |     32.693 |   1.0324 |     35.582 |     0.8
   50 |   0.9397 |     32.434 |   1.0390 |     35.705 |     0.9
   51 |   0.9322 |     32.351 |   1.0168 |     35.490 |     0.9
   52 |   0.9170 |     31.623 |   1.0125 |     34.566 |     0.9
   53 |   0.9078 |     31.044 |   1.0098 |     34.658 |     0.9
   54 |   0.8967 |     30.796 |   0.9970 |     34.104 |     0.9
   55 |   0.8891 |     30.459 |   0.9944 |     33.549 |     1.0
   56 |   0.8830 |     30.227 |   0.9960 |     33.857 |     1.0
   57 |   0.8707 |     29.654 |   1.0001 |     33.395 |     1.0
   58 |   0.8635 |     29.267 |   0.9914 |     33.241 |     1.0
   59 |   0.8507 |     29.052 |   0.9864 |     33.518 |     1.0
   60 |   0.8504 |     28.986 |   1.0049 |     33.241 |     1.0
   61 |   0.8434 |     28.798 |   0.9871 |     33.087 |     1.1
   62 |   0.8325 |     28.219 |   0.9736 |     32.717 |     1.1
   63 |   0.8186 |     27.667 |   0.9630 |     31.608 |     1.1
   64 |   0.8104 |     27.491 |   0.9649 |     31.824 |     1.1
   65 |   0.8081 |     27.232 |   0.9909 |     33.303 |     1.1
   66 |   0.8024 |     27.011 |   0.9774 |     32.039 |     1.1
   67 |   0.7859 |     26.470 |   0.9567 |     31.547 |     1.2
   68 |   0.7736 |     25.924 |   0.9781 |     31.547 |     1.2
   69 |   0.7674 |     25.339 |   0.9617 |     31.516 |     1.2
   70 |   0.7621 |     25.499 |   0.9691 |     30.992 |     1.2
   71 |   0.7521 |     25.168 |   0.9674 |     31.331 |     1.2
   72 |   0.7502 |     24.975 |   0.9565 |     31.485 |     1.3
   73 |   0.7390 |     24.484 |   0.9767 |     31.547 |     1.3
   74 |   0.7290 |     24.346 |   0.9540 |     30.591 |     1.3
   75 |   0.7190 |     23.546 |   0.9520 |     30.746 |     1.3
   76 |   0.7110 |     23.723 |   0.9595 |     30.561 |     1.3
   77 |   0.7029 |     23.000 |   0.9622 |     30.314 |     1.3
   78 |   0.6943 |     22.984 |   0.9637 |     31.084 |     1.4
   79 |   0.6853 |     22.586 |   0.9548 |     29.636 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 44 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 273,186

Training started
X_train.shape: torch.Size([3021, 702])
Y_train.shape: torch.Size([3021, 7])
X_dev.shape: torch.Size([541, 250])
Y_dev.shape: torch.Size([541, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4119 |     63.572 |   1.7482 |     48.799 |     0.0
    2 |   1.5573 |     46.568 |   1.4408 |     45.625 |     0.0
    3 |   1.4043 |     45.708 |   1.3691 |     45.410 |     0.1
    4 |   1.3482 |     45.388 |   1.3320 |     44.331 |     0.1
    5 |   1.3156 |     44.467 |   1.3049 |     44.054 |     0.1
    6 |   1.2892 |     44.240 |   1.2882 |     44.177 |     0.1
    7 |   1.2691 |     44.075 |   1.2670 |     44.147 |     0.1
    8 |   1.2483 |     43.529 |   1.2507 |     43.007 |     0.1
    9 |   1.2248 |     42.933 |   1.2206 |     43.222 |     0.2
   10 |   1.2041 |     42.403 |   1.2021 |     42.760 |     0.2
   11 |   1.1864 |     41.824 |   1.1963 |     41.744 |     0.2
   12 |   1.1651 |     40.836 |   1.1719 |     40.635 |     0.2
   13 |   1.1504 |     40.141 |   1.1598 |     41.220 |     0.2
   14 |   1.1312 |     39.606 |   1.1575 |     40.604 |     0.2
   15 |   1.1169 |     39.060 |   1.1361 |     39.710 |     0.3
   16 |   1.0973 |     38.359 |   1.1251 |     39.002 |     0.3
   17 |   1.0875 |     38.117 |   1.1153 |     39.526 |     0.3
   18 |   1.0694 |     37.543 |   1.1068 |     39.033 |     0.3
   19 |   1.0542 |     37.129 |   1.0951 |     38.417 |     0.3
   20 |   1.0313 |     36.048 |   1.0872 |     38.293 |     0.3
   21 |   1.0205 |     35.617 |   1.0749 |     38.047 |     0.4
   22 |   1.0040 |     34.911 |   1.0989 |     38.786 |     0.4
   23 |   0.9952 |     34.679 |   1.0567 |     36.722 |     0.4
   24 |   0.9684 |     33.273 |   1.0573 |     36.352 |     0.4
   25 |   0.9503 |     32.633 |   1.0458 |     35.921 |     0.4
   26 |   0.9331 |     31.833 |   1.0341 |     34.504 |     0.4
   27 |   0.9120 |     30.978 |   1.0294 |     34.966 |     0.5
   28 |   0.8963 |     30.476 |   1.0170 |     34.042 |     0.5
   29 |   0.8771 |     29.659 |   1.0050 |     33.580 |     0.5
   30 |   0.8623 |     29.146 |   1.0050 |     33.487 |     0.5
   31 |   0.8489 |     28.412 |   1.0000 |     31.701 |     0.5
   32 |   0.8264 |     28.015 |   0.9904 |     32.193 |     0.5
   33 |   0.8134 |     27.243 |   0.9743 |     31.793 |     0.6
   34 |   0.7919 |     26.647 |   0.9776 |     31.577 |     0.6
   35 |   0.7805 |     26.062 |   0.9673 |     31.177 |     0.6
   36 |   0.7625 |     25.615 |   0.9698 |     31.300 |     0.6
   37 |   0.7493 |     24.959 |   0.9680 |     30.622 |     0.6
   38 |   0.7369 |     24.782 |   0.9663 |     30.838 |     0.7
   39 |   0.7171 |     23.921 |   0.9675 |     30.129 |     0.7
   40 |   0.7085 |     23.563 |   0.9686 |     30.838 |     0.7
   41 |   0.7043 |     23.419 |   0.9564 |     29.636 |     0.7
   42 |   0.6892 |     23.083 |   0.9599 |     29.544 |     0.7
   43 |   0.6690 |     22.283 |   0.9606 |     29.544 |     0.7
   44 |   0.6580 |     21.704 |   0.9478 |     29.298 |     0.8
   45 |   0.6428 |     21.295 |   0.9591 |     29.544 |     0.8
   46 |   0.6325 |     20.959 |   0.9619 |     29.636 |     0.8
   47 |   0.6184 |     20.137 |   0.9697 |     29.575 |     0.8
   48 |   0.6097 |     19.944 |   0.9585 |     29.298 |     0.8
Early stopping

