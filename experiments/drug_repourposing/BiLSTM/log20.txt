Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 2,020,290

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2626 |     62.187 |   1.6399 |     46.120 |     0.1
    2 |   1.4972 |     46.165 |   1.4096 |     46.120 |     0.2
    3 |   1.3979 |     45.950 |   1.3634 |     45.841 |     0.2
    4 |   1.3583 |     45.653 |   1.3336 |     45.593 |     0.3
    5 |   1.3234 |     45.383 |   1.3103 |     45.313 |     0.4
    6 |   1.2992 |     44.865 |   1.2831 |     44.475 |     0.5
    7 |   1.2724 |     44.567 |   1.2555 |     44.041 |     0.6
    8 |   1.2490 |     43.680 |   1.2402 |     43.979 |     0.6
    9 |   1.2299 |     43.328 |   1.2212 |     43.544 |     0.7
   10 |   1.2116 |     43.135 |   1.1985 |     42.365 |     0.8
   11 |   1.1968 |     42.716 |   1.2013 |     42.582 |     0.9
   12 |   1.1869 |     42.270 |   1.1873 |     42.675 |     1.0
   13 |   1.1774 |     42.309 |   1.1773 |     41.682 |     1.0
   14 |   1.1697 |     42.171 |   1.1705 |     42.055 |     1.1
   15 |   1.1571 |     41.515 |   1.1634 |     41.434 |     1.2
   16 |   1.1509 |     41.488 |   1.1605 |     41.713 |     1.3
   17 |   1.1433 |     41.223 |   1.1513 |     40.906 |     1.4
   18 |   1.1352 |     41.267 |   1.1560 |     41.744 |     1.5
   19 |   1.1287 |     40.904 |   1.1439 |     41.155 |     1.5
   20 |   1.1231 |     40.904 |   1.1357 |     40.658 |     1.6
   21 |   1.1162 |     40.738 |   1.1352 |     40.596 |     1.7
   22 |   1.1105 |     40.474 |   1.1323 |     40.379 |     1.8
   23 |   1.1061 |     40.342 |   1.1279 |     40.906 |     1.9
   24 |   1.1021 |     40.364 |   1.1265 |     40.379 |     1.9
   25 |   1.0995 |     40.391 |   1.1268 |     40.844 |     2.0
   26 |   1.0914 |     40.099 |   1.1192 |     40.813 |     2.1
   27 |   1.0875 |     39.697 |   1.1255 |     40.906 |     2.2
   28 |   1.0850 |     40.006 |   1.1204 |     39.851 |     2.3
   29 |   1.0854 |     39.642 |   1.1156 |     39.634 |     2.3
   30 |   1.0810 |     39.581 |   1.1161 |     39.944 |     2.4
   31 |   1.0785 |     39.862 |   1.1126 |     39.261 |     2.5
   32 |   1.0736 |     39.262 |   1.1160 |     40.192 |     2.6
   33 |   1.0709 |     39.317 |   1.1097 |     39.572 |     2.7
   34 |   1.0675 |     39.603 |   1.1050 |     39.727 |     2.7
   35 |   1.0663 |     39.399 |   1.1029 |     39.261 |     2.8
   36 |   1.0604 |     39.240 |   1.1029 |     38.951 |     2.9
   37 |   1.0574 |     38.705 |   1.1031 |     38.858 |     3.0
   38 |   1.0559 |     39.003 |   1.0997 |     39.168 |     3.1
   39 |   1.0587 |     39.069 |   1.1405 |     41.341 |     3.2
   40 |   1.0523 |     38.893 |   1.0943 |     39.075 |     3.2
   41 |   1.0463 |     38.860 |   1.0947 |     38.703 |     3.3
   42 |   1.0414 |     38.342 |   1.0957 |     39.292 |     3.4
   43 |   1.0420 |     38.275 |   1.0934 |     38.485 |     3.5
   44 |   1.0349 |     38.176 |   1.0892 |     38.144 |     3.6
   45 |   1.0274 |     37.961 |   1.0806 |     37.709 |     3.6
   46 |   1.0236 |     37.653 |   1.0838 |     37.958 |     3.7
   47 |   1.0237 |     37.785 |   1.0832 |     38.858 |     3.8
   48 |   1.0247 |     37.752 |   1.0775 |     37.803 |     3.9
   49 |   1.0158 |     37.140 |   1.0735 |     38.175 |     4.0
   50 |   1.0134 |     37.322 |   1.0793 |     38.082 |     4.0
   51 |   1.0084 |     36.815 |   1.0766 |     37.989 |     4.1
   52 |   1.0034 |     36.634 |   1.0782 |     38.020 |     4.2
   53 |   1.0010 |     36.540 |   1.0729 |     37.492 |     4.3
   54 |   0.9946 |     36.545 |   1.0694 |     37.430 |     4.4
   55 |   0.9931 |     36.347 |   1.0716 |     37.803 |     4.5
   56 |   0.9898 |     36.490 |   1.0601 |     37.337 |     4.5
   57 |   0.9879 |     36.066 |   1.0597 |     37.275 |     4.6
   58 |   0.9827 |     36.209 |   1.0703 |     37.678 |     4.7
   59 |   0.9817 |     36.154 |   1.0692 |     37.585 |     4.8
   60 |   0.9735 |     35.631 |   1.0544 |     37.896 |     4.9
   61 |   0.9684 |     35.581 |   1.0584 |     36.965 |     4.9
   62 |   0.9673 |     35.526 |   1.0606 |     37.027 |     5.0
   63 |   0.9647 |     35.394 |   1.0635 |     36.809 |     5.1
   64 |   0.9755 |     35.488 |   1.0651 |     37.244 |     5.2
   65 |   0.9575 |     34.876 |   1.0462 |     36.468 |     5.3
   66 |   0.9514 |     35.036 |   1.0580 |     36.809 |     5.4
   67 |   0.9463 |     34.672 |   1.0501 |     36.034 |     5.4
   68 |   0.9421 |     34.457 |   1.0405 |     35.723 |     5.5
   69 |   0.9370 |     34.066 |   1.0450 |     36.747 |     5.6
   70 |   0.9337 |     33.840 |   1.0342 |     36.189 |     5.7
   71 |   0.9278 |     33.328 |   1.0360 |     35.847 |     5.8
   72 |   0.9259 |     33.587 |   1.0337 |     35.599 |     5.8
   73 |   0.9196 |     33.405 |   1.0281 |     35.040 |     5.9
   74 |   0.9145 |     32.981 |   1.0275 |     35.568 |     6.0
   75 |   0.9133 |     33.080 |   1.0301 |     35.537 |     6.1
   76 |   0.9075 |     32.738 |   1.0280 |     35.164 |     6.2
   77 |   0.8967 |     32.187 |   1.0257 |     35.071 |     6.2
   78 |   0.9010 |     32.700 |   1.0232 |     35.102 |     6.3
   79 |   0.8943 |     32.231 |   1.0206 |     35.413 |     6.4
   80 |   0.8873 |     31.939 |   1.0307 |     35.754 |     6.5
   81 |   0.8832 |     31.730 |   1.0233 |     35.227 |     6.6
   82 |   0.8773 |     31.796 |   1.0178 |     35.258 |     6.7
   83 |   0.8735 |     31.515 |   1.0216 |     35.258 |     6.7
   84 |   0.8707 |     31.074 |   1.0049 |     33.923 |     6.8
   85 |   0.8625 |     30.771 |   1.0128 |     34.575 |     6.9
   86 |   0.8582 |     30.601 |   1.0046 |     33.768 |     7.0
   87 |   0.8563 |     30.490 |   1.0075 |     34.078 |     7.1
   88 |   0.8471 |     30.430 |   1.0029 |     33.737 |     7.1
   89 |   0.8419 |     29.846 |   1.0073 |     34.140 |     7.2
   90 |   0.8354 |     29.636 |   1.0169 |     34.513 |     7.3
   91 |   0.8268 |     29.543 |   1.0154 |     34.482 |     7.4
   92 |   0.8260 |     29.466 |   1.0045 |     33.861 |     7.5
   93 |   0.8198 |     29.344 |   0.9930 |     33.520 |     7.6
   94 |   0.8216 |     29.295 |   0.9953 |     32.992 |     7.6
   95 |   0.8138 |     28.926 |   0.9948 |     33.489 |     7.7
   96 |   0.8073 |     28.788 |   0.9937 |     33.271 |     7.8
   97 |   0.8024 |     28.634 |   0.9873 |     33.675 |     7.9
   98 |   0.7924 |     28.094 |   0.9877 |     32.371 |     8.0
   99 |   0.7881 |     27.967 |   0.9915 |     33.209 |     8.1
  100 |   0.7829 |     27.697 |   0.9924 |     33.985 |     8.1
  101 |   0.7822 |     27.802 |   0.9921 |     33.520 |     8.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 736,482

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1021 |     57.504 |   1.5282 |     46.120 |     0.0
    2 |   1.4444 |     46.083 |   1.3839 |     46.462 |     0.1
    3 |   1.3613 |     45.653 |   1.3311 |     44.258 |     0.1
    4 |   1.3105 |     44.766 |   1.2897 |     44.848 |     0.1
    5 |   1.2801 |     44.309 |   1.2700 |     44.941 |     0.2
    6 |   1.2546 |     43.758 |   1.2471 |     44.196 |     0.2
    7 |   1.2355 |     43.421 |   1.2264 |     43.855 |     0.2
    8 |   1.2165 |     43.041 |   1.2169 |     43.017 |     0.3
    9 |   1.1989 |     42.143 |   1.1894 |     42.458 |     0.3
   10 |   1.1783 |     41.829 |   1.1794 |     41.372 |     0.3
   11 |   1.1600 |     41.146 |   1.1620 |     40.782 |     0.4
   12 |   1.1423 |     40.612 |   1.1548 |     41.061 |     0.4
   13 |   1.1259 |     40.066 |   1.1365 |     39.944 |     0.4
   14 |   1.1073 |     39.477 |   1.1272 |     39.510 |     0.5
   15 |   1.0913 |     38.860 |   1.1129 |     38.672 |     0.5
   16 |   1.0785 |     38.259 |   1.1015 |     37.927 |     0.5
   17 |   1.0604 |     37.416 |   1.0959 |     39.013 |     0.6
   18 |   1.0425 |     36.661 |   1.0880 |     37.772 |     0.6
   19 |   1.0254 |     36.028 |   1.0689 |     36.809 |     0.6
   20 |   1.0065 |     35.201 |   1.0635 |     37.461 |     0.7
   21 |   0.9900 |     34.650 |   1.0411 |     35.537 |     0.7
   22 |   0.9666 |     33.829 |   1.0448 |     35.599 |     0.7
   23 |   0.9518 |     33.311 |   1.0307 |     34.699 |     0.8
   24 |   0.9308 |     32.386 |   1.0168 |     34.420 |     0.8
   25 |   0.9150 |     31.851 |   1.0117 |     34.420 |     0.8
   26 |   0.9090 |     31.499 |   1.0078 |     34.016 |     0.9
   27 |   0.8838 |     30.815 |   0.9882 |     33.799 |     0.9
   28 |   0.8657 |     29.857 |   0.9927 |     33.302 |     1.0
   29 |   0.8476 |     29.201 |   0.9841 |     32.868 |     1.0
   30 |   0.8312 |     28.733 |   0.9786 |     33.426 |     1.0
   31 |   0.8147 |     28.132 |   0.9719 |     32.526 |     1.1
   32 |   0.7908 |     27.096 |   0.9737 |     32.557 |     1.1
   33 |   0.7736 |     26.033 |   0.9592 |     31.813 |     1.1
   34 |   0.7608 |     25.713 |   0.9680 |     31.719 |     1.2
   35 |   0.7429 |     24.909 |   0.9621 |     31.254 |     1.2
   36 |   0.7289 |     24.457 |   0.9645 |     31.006 |     1.2
   37 |   0.7033 |     23.543 |   0.9713 |     31.037 |     1.3
   38 |   0.6920 |     23.344 |   0.9588 |     31.254 |     1.3
   39 |   0.6718 |     22.579 |   0.9581 |     30.664 |     1.3
   40 |   0.6577 |     22.127 |   0.9455 |     30.478 |     1.4
   41 |   0.6527 |     21.763 |   0.9676 |     30.944 |     1.4
   42 |   0.6358 |     21.383 |   0.9468 |     29.640 |     1.4
   43 |   0.6133 |     20.485 |   0.9473 |     29.795 |     1.5
   44 |   0.6015 |     19.884 |   0.9422 |     30.043 |     1.5
   45 |   0.5847 |     19.559 |   0.9528 |     29.112 |     1.5
   46 |   0.5728 |     18.832 |   0.9448 |     29.516 |     1.6
   47 |   0.5572 |     18.617 |   0.9576 |     29.516 |     1.6
   48 |   0.5476 |     18.116 |   0.9522 |     29.205 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,178,946

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5453 |     67.455 |   2.0062 |     57.697 |     0.1
    2 |   1.8012 |     51.256 |   1.5824 |     46.120 |     0.1
    3 |   1.5131 |     45.972 |   1.4484 |     46.120 |     0.2
    4 |   1.4383 |     46.017 |   1.4069 |     46.120 |     0.2
    5 |   1.4058 |     45.939 |   1.3807 |     46.120 |     0.3
    6 |   1.3772 |     45.532 |   1.3536 |     45.127 |     0.4
    7 |   1.3508 |     45.317 |   1.3338 |     45.593 |     0.4
    8 |   1.3283 |     45.030 |   1.3116 |     45.251 |     0.5
    9 |   1.3062 |     44.518 |   1.2884 |     44.444 |     0.6
   10 |   1.2835 |     44.055 |   1.2704 |     44.258 |     0.6
   11 |   1.2661 |     43.614 |   1.2513 |     43.731 |     0.7
   12 |   1.2443 |     43.394 |   1.2325 |     42.613 |     0.7
   13 |   1.2267 |     42.683 |   1.2197 |     42.489 |     0.8
   14 |   1.2137 |     42.562 |   1.2097 |     42.706 |     0.9
   15 |   1.2020 |     42.347 |   1.2032 |     41.899 |     0.9
   16 |   1.1908 |     41.906 |   1.1908 |     41.651 |     1.0
   17 |   1.1812 |     41.642 |   1.1849 |     41.434 |     1.0
   18 |   1.1718 |     41.300 |   1.1760 |     40.627 |     1.1
   19 |   1.1586 |     40.634 |   1.1734 |     41.372 |     1.2
   20 |   1.1494 |     40.413 |   1.1623 |     40.006 |     1.2
   21 |   1.1337 |     39.675 |   1.1562 |     39.758 |     1.3
   22 |   1.1260 |     39.603 |   1.1445 |     39.634 |     1.3
   23 |   1.1109 |     39.080 |   1.1379 |     39.385 |     1.4
   24 |   1.1016 |     38.496 |   1.1350 |     39.199 |     1.5
   25 |   1.0929 |     38.468 |   1.1322 |     39.075 |     1.5
   26 |   1.0805 |     37.636 |   1.1302 |     39.137 |     1.6
   27 |   1.0717 |     37.890 |   1.1256 |     38.765 |     1.6
   28 |   1.0607 |     37.207 |   1.1157 |     38.889 |     1.7
   29 |   1.0507 |     37.118 |   1.1146 |     38.237 |     1.8
   30 |   1.0437 |     36.667 |   1.1063 |     37.709 |     1.8
   31 |   1.0284 |     36.116 |   1.1038 |     37.958 |     1.9
   32 |   1.0215 |     35.840 |   1.1061 |     38.392 |     2.0
   33 |   1.0145 |     35.736 |   1.0935 |     37.244 |     2.0
   34 |   1.0005 |     35.118 |   1.1079 |     37.461 |     2.1
   35 |   0.9944 |     34.942 |   1.0859 |     36.747 |     2.1
   36 |   0.9788 |     34.369 |   1.0994 |     37.741 |     2.2
   37 |   0.9661 |     33.741 |   1.0915 |     36.530 |     2.3
   38 |   0.9519 |     33.140 |   1.0800 |     36.468 |     2.3
   39 |   0.9403 |     32.606 |   1.0738 |     36.158 |     2.4
   40 |   0.9280 |     32.127 |   1.0798 |     35.599 |     2.4
   41 |   0.9105 |     31.300 |   1.0697 |     35.289 |     2.5
   42 |   0.8997 |     30.887 |   1.0685 |     35.413 |     2.6
   43 |   0.8870 |     30.176 |   1.0663 |     35.040 |     2.6
   44 |   0.8701 |     29.813 |   1.0687 |     35.102 |     2.7
   45 |   0.8579 |     29.185 |   1.0725 |     34.575 |     2.7
   46 |   0.8437 |     28.474 |   1.0766 |     34.202 |     2.8
   47 |   0.8251 |     28.000 |   1.0747 |     34.016 |     2.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,427,394

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0455 |     56.072 |   1.4850 |     45.748 |     0.1
    2 |   1.4087 |     45.427 |   1.3492 |     45.003 |     0.1
    3 |   1.3166 |     44.551 |   1.2857 |     44.196 |     0.2
    4 |   1.2695 |     43.758 |   1.2521 |     43.793 |     0.2
    5 |   1.2381 |     43.466 |   1.2262 |     43.358 |     0.3
    6 |   1.2060 |     42.545 |   1.2045 |     42.334 |     0.3
    7 |   1.1785 |     41.488 |   1.1797 |     41.837 |     0.4
    8 |   1.1544 |     40.650 |   1.1620 |     41.341 |     0.5
    9 |   1.1253 |     39.901 |   1.1502 |     40.223 |     0.5
   10 |   1.0994 |     38.959 |   1.1373 |     40.192 |     0.6
   11 |   1.0760 |     37.840 |   1.1202 |     39.137 |     0.6
   12 |   1.0452 |     36.920 |   1.0976 |     37.927 |     0.7
   13 |   1.0161 |     35.152 |   1.0810 |     37.709 |     0.8
   14 |   0.9845 |     34.044 |   1.0510 |     35.661 |     0.8
   15 |   0.9495 |     32.683 |   1.0535 |     35.692 |     0.9
   16 |   0.9131 |     30.876 |   1.0184 |     34.171 |     0.9
   17 |   0.8701 |     29.449 |   0.9960 |     33.395 |     1.0
   18 |   0.8235 |     27.273 |   0.9815 |     32.278 |     1.1
   19 |   0.7782 |     25.862 |   0.9712 |     31.533 |     1.1
   20 |   0.7352 |     23.956 |   1.0070 |     32.402 |     1.2
   21 |   0.7009 |     22.953 |   0.9605 |     30.975 |     1.2
   22 |   0.6488 |     20.898 |   0.9456 |     30.199 |     1.3
   23 |   0.5899 |     18.755 |   0.9360 |     29.547 |     1.4
   24 |   0.5460 |     17.388 |   0.9589 |     28.926 |     1.4
   25 |   0.5086 |     16.022 |   0.9629 |     29.423 |     1.5
   26 |   0.4670 |     14.601 |   0.9677 |     29.174 |     1.5
   27 |   0.4174 |     12.876 |   0.9816 |     28.430 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 504,642

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4269 |     64.187 |   1.7946 |     49.007 |     0.0
    2 |   1.6169 |     47.339 |   1.4795 |     46.120 |     0.1
    3 |   1.4437 |     45.890 |   1.4008 |     45.779 |     0.1
    4 |   1.3890 |     45.802 |   1.3631 |     46.089 |     0.1
    5 |   1.3521 |     45.477 |   1.3283 |     45.717 |     0.2
    6 |   1.3179 |     44.876 |   1.2966 |     44.320 |     0.2
    7 |   1.2904 |     44.386 |   1.2746 |     43.855 |     0.2
    8 |   1.2664 |     43.725 |   1.2544 |     43.575 |     0.3
    9 |   1.2438 |     43.300 |   1.2356 |     43.420 |     0.3
   10 |   1.2219 |     42.865 |   1.2225 |     42.644 |     0.3
   11 |   1.2066 |     42.419 |   1.2062 |     43.296 |     0.4
   12 |   1.1892 |     42.000 |   1.1894 |     41.434 |     0.4
   13 |   1.1674 |     41.107 |   1.1757 |     41.899 |     0.5
   14 |   1.1507 |     40.298 |   1.1664 |     41.496 |     0.5
   15 |   1.1314 |     39.785 |   1.1497 |     40.596 |     0.5
   16 |   1.1146 |     39.174 |   1.1346 |     39.913 |     0.6
   17 |   1.0963 |     38.215 |   1.1334 |     40.410 |     0.6
   18 |   1.0789 |     37.388 |   1.1118 |     38.796 |     0.6
   19 |   1.0566 |     36.298 |   1.1069 |     38.858 |     0.7
   20 |   1.0481 |     36.182 |   1.0897 |     37.399 |     0.7
   21 |   1.0324 |     35.372 |   1.0779 |     37.182 |     0.7
   22 |   1.0109 |     34.584 |   1.0635 |     36.530 |     0.8
   23 |   0.9902 |     33.895 |   1.0651 |     36.313 |     0.8
   24 |   0.9761 |     33.416 |   1.0513 |     35.723 |     0.8
   25 |   0.9582 |     32.287 |   1.0464 |     35.164 |     0.9
   26 |   0.9377 |     31.736 |   1.0504 |     35.258 |     0.9
   27 |   0.9194 |     30.937 |   1.0369 |     34.916 |     0.9
   28 |   0.9020 |     30.050 |   1.0268 |     34.389 |     1.0
   29 |   0.8816 |     29.559 |   1.0198 |     34.140 |     1.0
   30 |   0.8688 |     28.953 |   1.0204 |     33.830 |     1.0
   31 |   0.8511 |     28.342 |   1.0139 |     33.675 |     1.1
   32 |   0.8337 |     27.785 |   1.0003 |     33.364 |     1.1
   33 |   0.8206 |     27.295 |   0.9960 |     32.682 |     1.1
   34 |   0.7969 |     26.259 |   0.9933 |     32.216 |     1.2
   35 |   0.7834 |     25.758 |   0.9890 |     31.937 |     1.2
   36 |   0.7612 |     24.826 |   0.9843 |     31.999 |     1.3
   37 |   0.7495 |     24.523 |   0.9853 |     32.061 |     1.3
   38 |   0.7287 |     23.972 |   0.9817 |     31.409 |     1.3
   39 |   0.7107 |     23.444 |   0.9857 |     31.782 |     1.4
   40 |   0.6962 |     22.419 |   0.9795 |     31.440 |     1.4
   41 |   0.6848 |     22.584 |   1.0048 |     32.092 |     1.4
   42 |   0.6893 |     22.595 |   0.9826 |     31.285 |     1.5
   43 |   0.6583 |     21.372 |   0.9819 |     30.881 |     1.5
   44 |   0.6516 |     21.168 |   0.9784 |     30.509 |     1.5
   45 |   0.6359 |     20.771 |   0.9881 |     30.633 |     1.6
   46 |   0.6105 |     19.829 |   0.9836 |     30.168 |     1.6
   47 |   0.6002 |     19.758 |   0.9893 |     29.609 |     1.6
   48 |   0.5867 |     19.229 |   0.9925 |     29.454 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,000,802

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0130 |     55.449 |   1.4791 |     45.841 |     0.0
    2 |   1.3992 |     45.581 |   1.3428 |     46.586 |     0.1
    3 |   1.3234 |     44.705 |   1.3013 |     44.320 |     0.1
    4 |   1.2900 |     44.110 |   1.2701 |     43.762 |     0.2
    5 |   1.2581 |     43.185 |   1.2464 |     43.513 |     0.2
    6 |   1.2316 |     42.760 |   1.2267 |     43.141 |     0.2
    7 |   1.2069 |     42.309 |   1.2032 |     41.682 |     0.3
    8 |   1.1819 |     41.713 |   1.1740 |     41.651 |     0.3
    9 |   1.1591 |     41.174 |   1.1650 |     41.527 |     0.4
   10 |   1.1377 |     40.320 |   1.1528 |     41.310 |     0.4
   11 |   1.1186 |     40.017 |   1.1467 |     40.906 |     0.4
   12 |   1.1046 |     39.289 |   1.1218 |     39.385 |     0.5
   13 |   1.0820 |     38.127 |   1.1116 |     39.354 |     0.5
   14 |   1.0642 |     37.592 |   1.1065 |     39.106 |     0.6
   15 |   1.0447 |     36.507 |   1.0943 |     38.175 |     0.6
   16 |   1.0257 |     36.127 |   1.0869 |     37.585 |     0.7
   17 |   1.0069 |     35.339 |   1.0731 |     37.896 |     0.7
   18 |   0.9963 |     34.975 |   1.0629 |     36.747 |     0.7
   19 |   0.9811 |     34.573 |   1.0407 |     36.158 |     0.8
   20 |   0.9610 |     33.736 |   1.0394 |     36.220 |     0.8
   21 |   0.9471 |     33.245 |   1.0289 |     35.382 |     0.9
   22 |   0.9239 |     32.094 |   1.0169 |     34.854 |     0.9
   23 |   0.9082 |     31.636 |   1.0044 |     34.327 |     0.9
   24 |   0.8925 |     31.157 |   1.0006 |     34.327 |     1.0
   25 |   0.8753 |     30.364 |   0.9986 |     33.799 |     1.0
   26 |   0.8530 |     29.532 |   0.9829 |     33.364 |     1.1
   27 |   0.8344 |     28.689 |   0.9903 |     33.302 |     1.1
   28 |   0.8163 |     27.989 |   0.9879 |     33.023 |     1.1
   29 |   0.7973 |     27.179 |   0.9816 |     32.837 |     1.2
   30 |   0.7726 |     26.474 |   0.9706 |     31.906 |     1.2
   31 |   0.7590 |     25.510 |   0.9826 |     32.526 |     1.3
   32 |   0.7702 |     26.309 |   0.9617 |     32.185 |     1.3
   33 |   0.7259 |     24.645 |   0.9537 |     30.944 |     1.3
   34 |   0.7031 |     23.548 |   0.9708 |     31.285 |     1.4
   35 |   0.6851 |     22.810 |   0.9496 |     30.540 |     1.4
   36 |   0.6614 |     22.099 |   0.9441 |     30.385 |     1.5
   37 |   0.6403 |     21.201 |   0.9517 |     30.509 |     1.5
   38 |   0.6245 |     20.832 |   0.9366 |     30.074 |     1.6
   39 |   0.6108 |     20.105 |   0.9620 |     29.919 |     1.6
   40 |   0.5910 |     19.669 |   0.9539 |     29.050 |     1.6
   41 |   0.5683 |     18.810 |   0.9451 |     29.205 |     1.7
   42 |   0.5510 |     18.061 |   0.9525 |     29.392 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 640,034

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0750 |     57.427 |   1.4850 |     46.120 |     0.0
    2 |   1.4180 |     45.851 |   1.3630 |     45.841 |     0.1
    3 |   1.3369 |     45.317 |   1.3073 |     45.127 |     0.1
    4 |   1.2932 |     44.573 |   1.2807 |     44.134 |     0.1
    5 |   1.2609 |     43.890 |   1.2555 |     44.444 |     0.1
    6 |   1.2343 |     43.399 |   1.2265 |     42.768 |     0.2
    7 |   1.2080 |     42.408 |   1.2050 |     42.613 |     0.2
    8 |   1.1759 |     41.427 |   1.1879 |     42.582 |     0.2
    9 |   1.1437 |     40.127 |   1.1590 |     40.161 |     0.3
   10 |   1.1158 |     39.289 |   1.1400 |     40.006 |     0.3
   11 |   1.0852 |     38.176 |   1.1189 |     38.547 |     0.3
   12 |   1.0440 |     36.138 |   1.0888 |     36.996 |     0.3
   13 |   1.0103 |     35.212 |   1.0740 |     36.127 |     0.4
   14 |   0.9848 |     33.873 |   1.0740 |     36.623 |     0.4
   15 |   0.9520 |     32.782 |   1.0353 |     34.978 |     0.4
   16 |   0.9044 |     30.964 |   1.0290 |     34.233 |     0.5
   17 |   0.8658 |     29.284 |   1.0158 |     33.923 |     0.5
   18 |   0.8330 |     28.264 |   1.0119 |     33.799 |     0.5
   19 |   0.7933 |     26.722 |   0.9872 |     32.402 |     0.5
   20 |   0.7563 |     25.129 |   0.9832 |     32.247 |     0.6
   21 |   0.7216 |     23.780 |   0.9764 |     30.944 |     0.6
   22 |   0.6976 |     23.245 |   0.9796 |     30.912 |     0.6
   23 |   0.6700 |     22.110 |   0.9698 |     30.261 |     0.7
   24 |   0.6302 |     20.485 |   0.9749 |     30.354 |     0.7
   25 |   0.6067 |     19.818 |   0.9661 |     29.950 |     0.7
   26 |   0.5771 |     18.496 |   0.9617 |     29.392 |     0.8
   27 |   0.5549 |     18.154 |   0.9855 |     29.640 |     0.8
   28 |   0.5348 |     17.366 |   0.9949 |     29.609 |     0.8
   29 |   0.5043 |     16.534 |   0.9876 |     29.112 |     0.8
   30 |   0.4819 |     15.576 |   1.0027 |     29.361 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 359,970

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4976 |     67.063 |   1.9131 |     53.507 |     0.0
    2 |   1.7016 |     48.832 |   1.5201 |     46.120 |     0.0
    3 |   1.4708 |     45.983 |   1.4185 |     46.120 |     0.1
    4 |   1.4028 |     45.972 |   1.3705 |     46.151 |     0.1
    5 |   1.3647 |     45.702 |   1.3466 |     46.369 |     0.1
    6 |   1.3386 |     45.212 |   1.3283 |     45.438 |     0.1
    7 |   1.3173 |     44.992 |   1.3031 |     44.786 |     0.1
    8 |   1.2980 |     44.562 |   1.2886 |     44.475 |     0.2
    9 |   1.2806 |     44.039 |   1.2704 |     43.203 |     0.2
   10 |   1.2606 |     43.433 |   1.2476 |     43.886 |     0.2
   11 |   1.2423 |     43.174 |   1.2324 |     43.824 |     0.2
   12 |   1.2271 |     42.882 |   1.2207 |     43.203 |     0.2
   13 |   1.2117 |     42.623 |   1.2123 |     43.358 |     0.3
   14 |   1.1989 |     42.209 |   1.2112 |     42.489 |     0.3
   15 |   1.1874 |     41.438 |   1.1935 |     42.148 |     0.3
   16 |   1.1748 |     41.080 |   1.1847 |     41.341 |     0.3
   17 |   1.1659 |     40.601 |   1.1753 |     40.720 |     0.3
   18 |   1.1556 |     40.397 |   1.1726 |     41.682 |     0.4
   19 |   1.1430 |     40.154 |   1.1585 |     40.130 |     0.4
   20 |   1.1341 |     39.846 |   1.1629 |     41.589 |     0.4
   21 |   1.1214 |     39.240 |   1.1535 |     40.627 |     0.4
   22 |   1.1136 |     39.284 |   1.1482 |     40.441 |     0.4
   23 |   1.1043 |     38.893 |   1.1400 |     39.944 |     0.5
   24 |   1.0916 |     38.325 |   1.1435 |     39.882 |     0.5
   25 |   1.0865 |     38.105 |   1.1285 |     39.789 |     0.5
   26 |   1.0745 |     37.950 |   1.1246 |     39.199 |     0.5
   27 |   1.0674 |     37.433 |   1.1188 |     38.827 |     0.5
   28 |   1.0579 |     36.986 |   1.1143 |     38.113 |     0.6
   29 |   1.0476 |     36.788 |   1.1061 |     38.237 |     0.6
   30 |   1.0386 |     36.264 |   1.1109 |     39.417 |     0.6
   31 |   1.0273 |     35.774 |   1.1029 |     38.299 |     0.6
   32 |   1.0205 |     35.719 |   1.1000 |     37.927 |     0.6
   33 |   1.0124 |     35.350 |   1.1034 |     38.703 |     0.7
   34 |   1.0013 |     35.207 |   1.0980 |     37.834 |     0.7
   35 |   0.9938 |     35.003 |   1.0886 |     37.058 |     0.7
   36 |   0.9844 |     34.755 |   1.0906 |     37.027 |     0.7
   37 |   0.9776 |     34.270 |   1.0773 |     36.468 |     0.7
   38 |   0.9684 |     33.741 |   1.0716 |     36.561 |     0.8
   39 |   0.9576 |     33.306 |   1.0726 |     36.313 |     0.8
   40 |   0.9482 |     32.937 |   1.0714 |     36.002 |     0.8
   41 |   0.9410 |     32.380 |   1.0663 |     36.065 |     0.8
   42 |   0.9324 |     32.176 |   1.0673 |     35.909 |     0.8
   43 |   0.9204 |     31.961 |   1.0586 |     35.289 |     0.9
   44 |   0.9137 |     31.741 |   1.0490 |     35.599 |     0.9
   45 |   0.9045 |     31.361 |   1.0475 |     34.513 |     0.9
   46 |   0.8944 |     31.085 |   1.0436 |     34.668 |     0.9
   47 |   0.8854 |     30.645 |   1.0590 |     34.792 |     0.9
   48 |   0.8732 |     29.675 |   1.0461 |     34.482 |     1.0
   49 |   0.8670 |     29.807 |   1.0342 |     33.985 |     1.0
   50 |   0.8539 |     29.438 |   1.0434 |     33.892 |     1.0
   51 |   0.8546 |     29.322 |   1.0352 |     33.426 |     1.0
   52 |   0.8305 |     28.512 |   1.0399 |     34.606 |     1.0
   53 |   0.8250 |     28.209 |   1.0344 |     33.364 |     1.1
   54 |   0.8147 |     27.664 |   1.0276 |     33.178 |     1.1
   55 |   0.8035 |     27.229 |   1.0374 |     32.992 |     1.1
   56 |   0.7999 |     27.185 |   1.0288 |     32.868 |     1.1
   57 |   0.7802 |     26.253 |   1.0231 |     33.178 |     1.1
   58 |   0.7710 |     26.176 |   1.0290 |     33.364 |     1.2
   59 |   0.7588 |     25.449 |   1.0193 |     32.061 |     1.2
   60 |   0.7525 |     25.565 |   1.0307 |     33.675 |     1.2
   61 |   0.7441 |     25.185 |   1.0182 |     32.713 |     1.2
   62 |   0.7294 |     24.540 |   1.0177 |     32.247 |     1.2
   63 |   0.7175 |     24.017 |   1.0112 |     32.309 |     1.3
   64 |   0.7072 |     23.791 |   1.0205 |     32.030 |     1.3
   65 |   0.7015 |     23.460 |   1.0079 |     31.688 |     1.3
   66 |   0.6835 |     22.837 |   1.0194 |     31.533 |     1.3
   67 |   0.6745 |     22.584 |   1.0296 |     31.378 |     1.3
   68 |   0.6680 |     22.529 |   1.0173 |     31.440 |     1.4
   69 |   0.6582 |     21.829 |   1.0231 |     31.316 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,427,394

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0243 |     54.832 |   1.4629 |     45.779 |     0.1
    2 |   1.3970 |     45.802 |   1.3429 |     44.848 |     0.1
    3 |   1.3201 |     44.474 |   1.3031 |     45.220 |     0.2
    4 |   1.2859 |     44.061 |   1.2717 |     44.693 |     0.2
    5 |   1.2584 |     43.961 |   1.2497 |     43.606 |     0.3
    6 |   1.2322 |     43.201 |   1.2250 |     43.544 |     0.4
    7 |   1.2085 |     42.650 |   1.1992 |     43.110 |     0.4
    8 |   1.1862 |     42.022 |   1.1878 |     42.148 |     0.5
    9 |   1.1659 |     41.355 |   1.1706 |     41.744 |     0.5
   10 |   1.1468 |     40.771 |   1.1654 |     40.999 |     0.6
   11 |   1.1353 |     40.413 |   1.1509 |     40.627 |     0.7
   12 |   1.1166 |     39.647 |   1.1323 |     40.130 |     0.7
   13 |   1.0993 |     39.124 |   1.1242 |     39.851 |     0.8
   14 |   1.0904 |     38.981 |   1.1072 |     39.106 |     0.9
   15 |   1.0737 |     38.595 |   1.1011 |     38.579 |     0.9
   16 |   1.0603 |     37.625 |   1.0891 |     38.361 |     1.0
   17 |   1.0489 |     37.515 |   1.0838 |     39.137 |     1.0
   18 |   1.0370 |     36.854 |   1.0715 |     37.616 |     1.1
   19 |   1.0204 |     36.303 |   1.0596 |     36.623 |     1.2
   20 |   1.0066 |     35.807 |   1.0518 |     36.437 |     1.2
   21 |   0.9944 |     35.388 |   1.0454 |     36.685 |     1.3
   22 |   0.9812 |     35.129 |   1.0414 |     36.251 |     1.3
   23 |   0.9667 |     34.386 |   1.0242 |     35.258 |     1.4
   24 |   0.9505 |     33.576 |   1.0165 |     35.164 |     1.5
   25 |   0.9413 |     33.179 |   1.0155 |     35.599 |     1.5
   26 |   0.9315 |     32.926 |   1.0084 |     35.196 |     1.6
   27 |   0.9180 |     32.358 |   1.0072 |     35.196 |     1.7
   28 |   0.9069 |     32.237 |   0.9923 |     34.389 |     1.7
   29 |   0.8969 |     31.433 |   1.0227 |     35.971 |     1.8
   30 |   0.8850 |     30.948 |   1.0318 |     34.854 |     1.8
   31 |   0.8923 |     31.405 |   0.9828 |     33.737 |     1.9
   32 |   0.8762 |     30.744 |   0.9791 |     33.209 |     2.0
   33 |   0.8794 |     30.793 |   0.9703 |     33.426 |     2.0
   34 |   0.8426 |     29.190 |   0.9580 |     32.495 |     2.1
   35 |   0.8286 |     28.606 |   0.9596 |     33.209 |     2.1
   36 |   0.8201 |     28.342 |   0.9590 |     32.526 |     2.2
   37 |   0.8021 |     27.857 |   0.9503 |     31.906 |     2.3
   38 |   0.7836 |     27.124 |   0.9414 |     31.782 |     2.3
   39 |   0.7771 |     26.700 |   0.9525 |     32.092 |     2.4
   40 |   0.7609 |     25.928 |   0.9420 |     31.999 |     2.4
   41 |   0.7451 |     25.311 |   0.9484 |     32.309 |     2.5
   42 |   0.7333 |     25.245 |   0.9574 |     31.533 |     2.6
   43 |   0.7229 |     24.733 |   0.9339 |     31.192 |     2.6
   44 |   0.7151 |     24.512 |   0.9343 |     31.068 |     2.7
   45 |   0.6942 |     23.609 |   0.9307 |     30.726 |     2.8
   46 |   0.6799 |     22.986 |   0.9315 |     30.106 |     2.8
   47 |   0.6727 |     22.727 |   0.9337 |     31.161 |     2.9
   48 |   0.6532 |     22.127 |   0.9299 |     30.478 |     2.9
   49 |   0.6414 |     21.466 |   0.9318 |     29.764 |     3.0
   50 |   0.6272 |     21.185 |   0.9314 |     29.516 |     3.1
   51 |   0.6233 |     20.975 |   0.9239 |     29.392 |     3.1
   52 |   0.6046 |     20.143 |   0.9288 |     29.143 |     3.2
   53 |   0.5952 |     20.022 |   0.9309 |     28.616 |     3.2
   54 |   0.5841 |     19.433 |   1.0052 |     31.875 |     3.3
   55 |   0.6245 |     21.124 |   0.9606 |     29.609 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 537,186

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4989 |     64.997 |   1.9336 |     58.752 |     0.0
    2 |   1.7209 |     48.639 |   1.5348 |     46.151 |     0.1
    3 |   1.4856 |     45.972 |   1.4375 |     46.151 |     0.1
    4 |   1.4269 |     45.972 |   1.4018 |     46.120 |     0.1
    5 |   1.4018 |     45.994 |   1.3797 |     46.120 |     0.2
    6 |   1.3786 |     45.983 |   1.3607 |     46.120 |     0.2
    7 |   1.3569 |     45.510 |   1.3381 |     45.313 |     0.2
    8 |   1.3344 |     45.168 |   1.3243 |     45.220 |     0.3
    9 |   1.3169 |     44.953 |   1.3032 |     44.879 |     0.3
   10 |   1.3019 |     44.623 |   1.2891 |     44.972 |     0.3
   11 |   1.2882 |     44.595 |   1.2811 |     44.320 |     0.4
   12 |   1.2775 |     44.358 |   1.2697 |     44.196 |     0.4
   13 |   1.2638 |     43.890 |   1.2591 |     43.731 |     0.4
   14 |   1.2511 |     43.421 |   1.2442 |     43.079 |     0.5
   15 |   1.2382 |     43.289 |   1.2342 |     42.644 |     0.5
   16 |   1.2285 |     42.810 |   1.2230 |     42.365 |     0.5
   17 |   1.2181 |     42.595 |   1.2203 |     42.582 |     0.6
   18 |   1.2093 |     42.309 |   1.2141 |     41.993 |     0.6
   19 |   1.2001 |     42.066 |   1.2046 |     41.682 |     0.6
   20 |   1.1966 |     42.336 |   1.2018 |     42.551 |     0.7
   21 |   1.1884 |     42.061 |   1.1957 |     42.582 |     0.7
   22 |   1.1828 |     41.961 |   1.1883 |     42.148 |     0.7
   23 |   1.1769 |     41.906 |   1.1862 |     41.496 |     0.8
   24 |   1.1697 |     41.592 |   1.1715 |     41.837 |     0.8
   25 |   1.1626 |     41.675 |   1.1825 |     41.496 |     0.8
   26 |   1.1553 |     41.074 |   1.1682 |     40.968 |     0.9
   27 |   1.1492 |     40.975 |   1.1609 |     40.906 |     0.9
   28 |   1.1452 |     40.672 |   1.1586 |     40.813 |     0.9
   29 |   1.1367 |     40.623 |   1.1620 |     41.372 |     1.0
   30 |   1.1318 |     40.275 |   1.1532 |     40.627 |     1.0
   31 |   1.1271 |     40.006 |   1.1471 |     40.348 |     1.0
   32 |   1.1203 |     39.928 |   1.1402 |     40.037 |     1.1
   33 |   1.1130 |     39.565 |   1.1368 |     39.696 |     1.1
   34 |   1.1043 |     39.085 |   1.1325 |     39.479 |     1.1
   35 |   1.1053 |     39.433 |   1.1334 |     39.789 |     1.2
   36 |   1.0950 |     38.942 |   1.1325 |     39.696 |     1.2
   37 |   1.0923 |     39.036 |   1.1311 |     39.696 |     1.2
   38 |   1.0826 |     38.744 |   1.1231 |     39.789 |     1.3
   39 |   1.0772 |     38.375 |   1.1165 |     38.858 |     1.3
   40 |   1.0699 |     38.132 |   1.1174 |     39.541 |     1.4
   41 |   1.0662 |     37.873 |   1.1102 |     38.641 |     1.4
   42 |   1.0610 |     37.802 |   1.1126 |     38.392 |     1.4
   43 |   1.0534 |     37.471 |   1.1069 |     38.641 |     1.5
   44 |   1.0525 |     37.708 |   1.1084 |     38.299 |     1.5
   45 |   1.0458 |     37.377 |   1.1085 |     38.547 |     1.5
   46 |   1.0391 |     37.052 |   1.0987 |     38.485 |     1.6
   47 |   1.0368 |     37.074 |   1.0999 |     38.920 |     1.6
   48 |   1.0300 |     36.590 |   1.0931 |     38.206 |     1.6
   49 |   1.0258 |     36.777 |   1.0986 |     38.299 |     1.7
   50 |   1.0162 |     36.231 |   1.0939 |     38.641 |     1.7
   51 |   1.0150 |     36.209 |   1.0811 |     37.337 |     1.7
   52 |   1.0102 |     36.220 |   1.0871 |     37.958 |     1.8
   53 |   1.0052 |     36.105 |   1.0827 |     37.927 |     1.8
   54 |   1.0000 |     35.730 |   1.0819 |     37.213 |     1.8
   55 |   0.9939 |     35.256 |   1.0771 |     37.896 |     1.9
   56 |   0.9868 |     35.328 |   1.0760 |     37.585 |     1.9
   57 |   0.9827 |     34.876 |   1.0739 |     38.020 |     1.9
   58 |   0.9748 |     34.937 |   1.0696 |     37.306 |     2.0
   59 |   0.9702 |     34.358 |   1.0669 |     37.182 |     2.0
   60 |   0.9680 |     34.496 |   1.0630 |     37.244 |     2.0
   61 |   0.9630 |     34.358 |   1.0623 |     36.778 |     2.1
   62 |   0.9531 |     33.736 |   1.0636 |     36.934 |     2.1
   63 |   0.9504 |     33.427 |   1.0552 |     35.816 |     2.1
   64 |   0.9465 |     33.829 |   1.0500 |     36.096 |     2.2
   65 |   0.9389 |     33.207 |   1.0514 |     36.220 |     2.2
   66 |   0.9419 |     33.058 |   1.0532 |     36.344 |     2.2
   67 |   0.9297 |     32.959 |   1.0451 |     35.785 |     2.3
   68 |   0.9231 |     32.766 |   1.0478 |     36.499 |     2.3
   69 |   0.9186 |     32.634 |   1.0494 |     36.220 |     2.3
   70 |   0.9169 |     32.342 |   1.0428 |     36.034 |     2.4
   71 |   0.9149 |     32.413 |   1.0365 |     35.971 |     2.4
   72 |   0.9096 |     32.011 |   1.0402 |     35.971 |     2.4
   73 |   0.9006 |     32.083 |   1.0373 |     35.909 |     2.5
   74 |   0.8969 |     31.636 |   1.0266 |     35.382 |     2.5
   75 |   0.8897 |     31.449 |   1.0278 |     35.568 |     2.5
   76 |   0.8863 |     31.135 |   1.0286 |     35.289 |     2.6
   77 |   0.8831 |     30.992 |   1.0312 |     35.506 |     2.6
   78 |   0.8767 |     30.760 |   1.0299 |     34.792 |     2.6
   79 |   0.8741 |     30.518 |   1.0239 |     34.916 |     2.7
   80 |   0.8659 |     30.303 |   1.0325 |     35.909 |     2.7
   81 |   0.8636 |     30.138 |   1.0237 |     33.830 |     2.7
   82 |   0.8583 |     29.752 |   1.0214 |     34.358 |     2.8
   83 |   0.8520 |     29.741 |   1.0224 |     33.985 |     2.8
   84 |   0.8468 |     29.631 |   1.0225 |     33.954 |     2.8
   85 |   0.8446 |     29.614 |   1.0260 |     34.482 |     2.9
   86 |   0.8354 |     28.904 |   1.0214 |     33.799 |     2.9
   87 |   0.8274 |     28.490 |   1.0191 |     33.830 |     2.9
   88 |   0.8216 |     28.468 |   1.0274 |     34.389 |     3.0
   89 |   0.8189 |     28.518 |   1.0143 |     33.830 |     3.0
   90 |   0.8112 |     28.413 |   1.0226 |     33.861 |     3.0
   91 |   0.8045 |     28.143 |   1.0159 |     34.171 |     3.1
   92 |   0.8061 |     27.669 |   1.0135 |     33.768 |     3.1
   93 |   0.7973 |     27.576 |   1.0215 |     33.271 |     3.1
   94 |   0.7931 |     27.653 |   1.0244 |     34.078 |     3.2
   95 |   0.7880 |     27.030 |   1.0205 |     33.271 |     3.2
   96 |   0.7788 |     26.860 |   1.0147 |     33.333 |     3.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 736,482

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2399 |     61.212 |   1.6000 |     46.120 |     0.0
    2 |   1.4713 |     46.022 |   1.3997 |     46.120 |     0.1
    3 |   1.3956 |     45.967 |   1.3707 |     45.438 |     0.1
    4 |   1.3550 |     45.647 |   1.3293 |     45.500 |     0.1
    5 |   1.3227 |     45.444 |   1.3091 |     45.500 |     0.1
    6 |   1.3025 |     45.273 |   1.2897 |     43.575 |     0.2
    7 |   1.2793 |     44.413 |   1.2645 |     44.258 |     0.2
    8 |   1.2568 |     43.680 |   1.2425 |     43.886 |     0.2
    9 |   1.2396 |     43.736 |   1.2287 |     43.420 |     0.2
   10 |   1.2254 |     43.532 |   1.2233 |     43.203 |     0.3
   11 |   1.2082 |     42.948 |   1.2071 |     42.551 |     0.3
   12 |   1.1941 |     42.419 |   1.1895 |     41.806 |     0.3
   13 |   1.1808 |     41.868 |   1.1842 |     41.837 |     0.3
   14 |   1.1678 |     41.691 |   1.1942 |     42.055 |     0.4
   15 |   1.1580 |     41.444 |   1.1638 |     41.310 |     0.4
   16 |   1.1448 |     40.887 |   1.1579 |     41.092 |     0.4
   17 |   1.1349 |     40.606 |   1.1632 |     40.875 |     0.4
   18 |   1.1251 |     40.320 |   1.1459 |     40.410 |     0.5
   19 |   1.1166 |     39.862 |   1.1429 |     40.379 |     0.5
   20 |   1.1020 |     39.471 |   1.1367 |     40.286 |     0.5
   21 |   1.0937 |     39.185 |   1.1318 |     40.130 |     0.6
   22 |   1.0851 |     38.893 |   1.1305 |     39.199 |     0.6
   23 |   1.0758 |     38.623 |   1.1099 |     38.858 |     0.6
   24 |   1.0668 |     38.358 |   1.1142 |     39.013 |     0.6
   25 |   1.0597 |     37.994 |   1.1046 |     38.579 |     0.7
   26 |   1.0478 |     37.802 |   1.0980 |     38.579 |     0.7
   27 |   1.0363 |     37.030 |   1.0975 |     38.361 |     0.7
   28 |   1.0261 |     36.876 |   1.0890 |     38.454 |     0.7
   29 |   1.0165 |     36.353 |   1.0800 |     37.741 |     0.8
   30 |   1.0053 |     35.829 |   1.0804 |     37.368 |     0.8
   31 |   0.9917 |     35.284 |   1.0765 |     37.151 |     0.8
   32 |   0.9893 |     35.113 |   1.0726 |     37.275 |     0.8
   33 |   0.9785 |     34.876 |   1.0727 |     37.244 |     0.9
   34 |   0.9672 |     34.325 |   1.0732 |     37.834 |     0.9
   35 |   0.9591 |     33.636 |   1.0702 |     37.244 |     0.9
   36 |   0.9464 |     33.493 |   1.0704 |     37.058 |     0.9
   37 |   0.9395 |     33.311 |   1.0683 |     37.275 |     1.0
   38 |   0.9299 |     32.744 |   1.0543 |     35.847 |     1.0
   39 |   0.9177 |     32.507 |   1.0439 |     35.258 |     1.0
   40 |   0.9076 |     31.835 |   1.0477 |     35.847 |     1.0
   41 |   0.9000 |     31.565 |   1.0491 |     36.158 |     1.1
   42 |   0.8836 |     31.096 |   1.0354 |     35.258 |     1.1
   43 |   0.8728 |     30.457 |   1.0339 |     34.978 |     1.1
   44 |   0.8645 |     29.873 |   1.0374 |     34.637 |     1.2
   45 |   0.8569 |     29.851 |   1.0369 |     34.699 |     1.2
   46 |   0.8353 |     29.140 |   1.0242 |     33.830 |     1.2
   47 |   0.8212 |     28.446 |   1.0230 |     33.861 |     1.2
   48 |   0.8152 |     28.253 |   1.0111 |     33.426 |     1.3
   49 |   0.8092 |     27.895 |   1.0102 |     33.520 |     1.3
   50 |   0.7943 |     27.598 |   1.0065 |     33.985 |     1.3
   51 |   0.7843 |     27.063 |   1.0110 |     32.899 |     1.3
   52 |   0.7682 |     26.545 |   1.0108 |     32.682 |     1.4
   53 |   0.7551 |     25.917 |   1.0103 |     32.433 |     1.4
   54 |   0.7638 |     26.336 |   1.0051 |     32.837 |     1.4
   55 |   0.7420 |     25.725 |   1.0287 |     32.619 |     1.4
   56 |   0.7424 |     25.521 |   1.0047 |     32.651 |     1.5
   57 |   0.7147 |     24.562 |   1.0005 |     32.061 |     1.5
   58 |   0.7819 |     27.129 |   1.0072 |     32.775 |     1.5
   59 |   0.7295 |     25.019 |   1.0088 |     32.992 |     1.5
   60 |   0.7008 |     23.939 |   0.9989 |     32.030 |     1.6
   61 |   0.6808 |     23.129 |   1.0094 |     31.906 |     1.6
   62 |   0.6651 |     22.463 |   1.0079 |     31.875 |     1.6
   63 |   0.6572 |     22.358 |   1.0110 |     31.192 |     1.7
   64 |   0.6475 |     22.198 |   0.9965 |     31.409 |     1.7
   65 |   0.6330 |     21.218 |   1.0259 |     30.664 |     1.7
   66 |   0.6224 |     21.058 |   1.0278 |     30.726 |     1.7
   67 |   0.6210 |     20.766 |   1.0124 |     31.161 |     1.8
   68 |   0.6145 |     20.689 |   1.0049 |     30.850 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,127,106

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4732 |     64.628 |   1.9287 |     51.676 |     0.1
    2 |   1.6764 |     48.000 |   1.4921 |     46.120 |     0.1
    3 |   1.4469 |     45.950 |   1.4040 |     45.903 |     0.2
    4 |   1.3975 |     45.758 |   1.3761 |     46.276 |     0.2
    5 |   1.3746 |     45.658 |   1.3621 |     45.717 |     0.3
    6 |   1.3600 |     45.515 |   1.3481 |     45.313 |     0.4
    7 |   1.3456 |     45.311 |   1.3365 |     45.065 |     0.4
    8 |   1.3248 |     44.848 |   1.3134 |     44.941 |     0.5
    9 |   1.3034 |     44.061 |   1.2945 |     44.103 |     0.6
   10 |   1.2852 |     43.774 |   1.2763 |     44.041 |     0.6
   11 |   1.2675 |     43.449 |   1.2655 |     44.351 |     0.7
   12 |   1.2539 |     43.493 |   1.2515 |     43.979 |     0.7
   13 |   1.2410 |     43.218 |   1.2522 |     43.824 |     0.8
   14 |   1.2336 |     43.350 |   1.2336 |     43.296 |     0.9
   15 |   1.2128 |     42.275 |   1.2173 |     42.768 |     0.9
   16 |   1.1955 |     41.818 |   1.2101 |     42.831 |     1.0
   17 |   1.1786 |     41.152 |   1.1929 |     42.303 |     1.1
   18 |   1.1600 |     40.287 |   1.1757 |     41.310 |     1.1
   19 |   1.1376 |     39.598 |   1.1665 |     40.317 |     1.2
   20 |   1.1183 |     38.760 |   1.1500 |     39.758 |     1.2
   21 |   1.0956 |     37.851 |   1.1285 |     38.951 |     1.3
   22 |   1.0766 |     37.008 |   1.1148 |     38.206 |     1.4
   23 |   1.0430 |     35.416 |   1.0980 |     37.647 |     1.4
   24 |   1.0383 |     35.300 |   1.0798 |     37.368 |     1.5
   25 |   1.0028 |     33.791 |   1.0733 |     36.872 |     1.5
   26 |   0.9696 |     32.749 |   1.0589 |     35.878 |     1.6
   27 |   0.9433 |     31.912 |   1.0565 |     35.723 |     1.7
   28 |   0.9199 |     30.799 |   1.0449 |     35.351 |     1.7
   29 |   0.8908 |     29.791 |   1.0338 |     34.916 |     1.8
   30 |   0.8670 |     28.931 |   1.0133 |     33.861 |     1.9
   31 |   0.8382 |     27.713 |   1.0125 |     34.078 |     1.9
   32 |   0.8137 |     27.229 |   1.0019 |     33.271 |     2.0
   33 |   0.7849 |     25.829 |   1.0052 |     33.240 |     2.0
   34 |   0.7627 |     25.135 |   0.9970 |     32.961 |     2.1
   35 |   0.7264 |     23.658 |   0.9978 |     32.309 |     2.2
   36 |   0.6975 |     22.656 |   0.9918 |     31.937 |     2.2
   37 |   0.6813 |     21.928 |   0.9833 |     31.130 |     2.3
   38 |   0.6502 |     20.876 |   0.9801 |     30.385 |     2.4
   39 |   0.6192 |     19.835 |   0.9854 |     31.037 |     2.4
   40 |   0.6003 |     19.118 |   0.9865 |     30.540 |     2.5
   41 |   0.5655 |     17.642 |   0.9904 |     30.323 |     2.5
   42 |   0.5419 |     16.843 |   0.9977 |     30.137 |     2.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 388,930

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3864 |     62.964 |   1.8103 |     48.510 |     0.0
    2 |   1.6083 |     46.981 |   1.4583 |     45.903 |     0.1
    3 |   1.4158 |     45.713 |   1.3705 |     45.779 |     0.1
    4 |   1.3524 |     45.052 |   1.3283 |     45.189 |     0.1
    5 |   1.3171 |     44.567 |   1.3006 |     44.196 |     0.1
    6 |   1.2927 |     44.231 |   1.2842 |     44.351 |     0.2
    7 |   1.2677 |     43.691 |   1.2613 |     43.700 |     0.2
    8 |   1.2454 |     43.394 |   1.2309 |     43.296 |     0.2
    9 |   1.2232 |     42.882 |   1.2198 |     43.141 |     0.2
   10 |   1.2013 |     42.000 |   1.2027 |     41.837 |     0.3
   11 |   1.1850 |     41.394 |   1.1894 |     42.055 |     0.3
   12 |   1.1683 |     40.738 |   1.1717 |     40.782 |     0.3
   13 |   1.1501 |     40.121 |   1.1598 |     40.286 |     0.3
   14 |   1.1392 |     39.934 |   1.1571 |     40.968 |     0.4
   15 |   1.1267 |     39.647 |   1.1452 |     40.130 |     0.4
   16 |   1.1162 |     39.361 |   1.1340 |     39.944 |     0.4
   17 |   1.1063 |     39.179 |   1.1380 |     39.696 |     0.4
   18 |   1.0965 |     38.782 |   1.1238 |     39.230 |     0.5
   19 |   1.0830 |     38.342 |   1.1135 |     38.796 |     0.5
   20 |   1.0753 |     38.149 |   1.1112 |     39.354 |     0.5
   21 |   1.0657 |     37.851 |   1.1209 |     38.734 |     0.5
   22 |   1.0547 |     37.207 |   1.0977 |     38.361 |     0.6
   23 |   1.0443 |     36.793 |   1.0969 |     38.361 |     0.6
   24 |   1.0355 |     36.446 |   1.0910 |     38.113 |     0.6
   25 |   1.0237 |     36.050 |   1.0838 |     37.678 |     0.6
   26 |   1.0165 |     35.427 |   1.0719 |     37.461 |     0.7
   27 |   1.0020 |     35.135 |   1.0724 |     36.996 |     0.7
   28 |   0.9915 |     34.446 |   1.0620 |     36.996 |     0.7
   29 |   0.9830 |     34.270 |   1.0592 |     36.530 |     0.7
   30 |   0.9701 |     33.829 |   1.0577 |     36.561 |     0.8
   31 |   0.9688 |     33.846 |   1.0496 |     36.189 |     0.8
   32 |   0.9521 |     33.091 |   1.0401 |     36.158 |     0.8
   33 |   0.9378 |     32.358 |   1.0290 |     35.568 |     0.8
   34 |   0.9308 |     32.165 |   1.0186 |     35.040 |     0.9
   35 |   0.9147 |     31.372 |   1.0295 |     35.164 |     0.9
   36 |   0.9043 |     30.909 |   1.0164 |     34.947 |     0.9
   37 |   0.8934 |     30.722 |   1.0073 |     33.178 |     0.9
   38 |   0.8776 |     29.928 |   1.0057 |     33.395 |     1.0
   39 |   0.8645 |     29.455 |   0.9919 |     33.520 |     1.0
   40 |   0.8525 |     28.997 |   1.0073 |     33.520 |     1.0
   41 |   0.8427 |     28.634 |   0.9815 |     32.868 |     1.0
   42 |   0.8331 |     28.094 |   0.9903 |     33.302 |     1.1
   43 |   0.8163 |     27.504 |   0.9809 |     31.968 |     1.1
   44 |   0.8025 |     26.953 |   0.9754 |     31.564 |     1.1
   45 |   0.7853 |     26.380 |   0.9708 |     32.092 |     1.1
   46 |   0.7764 |     26.132 |   0.9662 |     31.502 |     1.2
   47 |   0.7721 |     25.691 |   0.9757 |     31.626 |     1.2
   48 |   0.7589 |     25.570 |   0.9843 |     32.713 |     1.2
   49 |   0.7502 |     25.333 |   0.9602 |     30.975 |     1.3
   50 |   0.7210 |     23.780 |   0.9553 |     30.137 |     1.3
   51 |   0.7162 |     23.994 |   0.9687 |     31.130 |     1.3
   52 |   0.6995 |     23.025 |   0.9622 |     30.944 |     1.3
   53 |   0.6901 |     22.959 |   0.9605 |     30.633 |     1.4
   54 |   0.6720 |     22.270 |   0.9612 |     29.950 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 522,466

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4023 |     62.826 |   1.7944 |     48.386 |     0.0
    2 |   1.6170 |     47.433 |   1.4656 |     46.151 |     0.1
    3 |   1.4252 |     45.719 |   1.3755 |     45.345 |     0.1
    4 |   1.3605 |     45.383 |   1.3289 |     44.227 |     0.1
    5 |   1.3203 |     44.523 |   1.2957 |     44.072 |     0.2
    6 |   1.2906 |     44.110 |   1.2734 |     43.265 |     0.2
    7 |   1.2665 |     43.664 |   1.2520 |     43.172 |     0.2
    8 |   1.2455 |     43.163 |   1.2318 |     42.955 |     0.3
    9 |   1.2263 |     42.413 |   1.2140 |     42.303 |     0.3
   10 |   1.2069 |     41.829 |   1.2066 |     42.955 |     0.3
   11 |   1.1934 |     41.410 |   1.1911 |     41.868 |     0.4
   12 |   1.1761 |     40.920 |   1.1766 |     41.030 |     0.4
   13 |   1.1626 |     40.700 |   1.1742 |     41.465 |     0.5
   14 |   1.1517 |     40.540 |   1.1588 |     41.310 |     0.5
   15 |   1.1394 |     40.039 |   1.1484 |     40.503 |     0.5
   16 |   1.1277 |     39.818 |   1.1465 |     40.472 |     0.6
   17 |   1.1174 |     39.510 |   1.1336 |     39.603 |     0.6
   18 |   1.1082 |     39.096 |   1.1257 |     39.292 |     0.6
   19 |   1.0952 |     38.468 |   1.1169 |     38.796 |     0.7
   20 |   1.0867 |     38.116 |   1.1067 |     38.672 |     0.7
   21 |   1.0765 |     37.576 |   1.1082 |     38.206 |     0.7
   22 |   1.0650 |     37.504 |   1.0988 |     37.958 |     0.8
   23 |   1.0589 |     36.992 |   1.0977 |     38.299 |     0.8
   24 |   1.0458 |     36.562 |   1.0868 |     37.523 |     0.8
   25 |   1.0361 |     36.220 |   1.0848 |     37.927 |     0.9
   26 |   1.0242 |     35.708 |   1.0769 |     36.654 |     0.9
   27 |   1.0152 |     35.537 |   1.0676 |     37.244 |     0.9
   28 |   1.0048 |     35.163 |   1.0557 |     36.592 |     1.0
   29 |   0.9977 |     34.650 |   1.0587 |     35.878 |     1.0
   30 |   0.9838 |     34.413 |   1.0462 |     35.258 |     1.0
   31 |   0.9740 |     33.747 |   1.0363 |     35.258 |     1.1
   32 |   0.9585 |     32.981 |   1.0358 |     34.730 |     1.1
   33 |   0.9526 |     33.003 |   1.0274 |     34.668 |     1.1
   34 |   0.9344 |     32.127 |   1.0246 |     34.792 |     1.2
   35 |   0.9255 |     31.879 |   1.0143 |     34.451 |     1.2
   36 |   0.9137 |     31.460 |   1.0147 |     34.451 |     1.2
   37 |   0.8993 |     30.826 |   1.0067 |     34.078 |     1.3
   38 |   0.8898 |     30.132 |   0.9975 |     33.085 |     1.3
   39 |   0.8840 |     30.209 |   0.9901 |     33.426 |     1.3
   40 |   0.8690 |     29.691 |   0.9871 |     33.147 |     1.4
   41 |   0.8489 |     28.777 |   0.9842 |     33.209 |     1.4
   42 |   0.8411 |     28.490 |   0.9778 |     32.464 |     1.4
   43 |   0.8281 |     28.066 |   0.9758 |     32.713 |     1.5
   44 |   0.8143 |     27.791 |   0.9778 |     32.247 |     1.5
   45 |   0.8056 |     27.163 |   0.9782 |     32.402 |     1.5
   46 |   0.7897 |     26.854 |   0.9757 |     32.495 |     1.6
   47 |   0.7789 |     26.364 |   0.9649 |     32.433 |     1.6
   48 |   0.7678 |     25.956 |   0.9530 |     31.502 |     1.6
   49 |   0.7745 |     25.879 |   0.9754 |     31.750 |     1.7
   50 |   0.7522 |     25.394 |   0.9560 |     31.254 |     1.7
   51 |   0.7268 |     24.121 |   0.9539 |     31.161 |     1.7
   52 |   0.7188 |     23.939 |   0.9542 |     30.695 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 519,362

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5239 |     67.405 |   1.9850 |     58.752 |     0.0
    2 |   1.7849 |     50.138 |   1.5926 |     46.120 |     0.1
    3 |   1.5246 |     46.039 |   1.4591 |     46.151 |     0.1
    4 |   1.4456 |     46.050 |   1.4175 |     46.151 |     0.1
    5 |   1.4115 |     45.934 |   1.3857 |     46.120 |     0.2
    6 |   1.3802 |     45.906 |   1.3572 |     46.307 |     0.2
    7 |   1.3488 |     45.857 |   1.3321 |     45.345 |     0.2
    8 |   1.3213 |     44.920 |   1.3064 |     44.910 |     0.3
    9 |   1.2981 |     44.430 |   1.2831 |     44.196 |     0.3
   10 |   1.2767 |     44.231 |   1.2657 |     44.134 |     0.3
   11 |   1.2583 |     43.917 |   1.2530 |     44.134 |     0.4
   12 |   1.2443 |     43.890 |   1.2413 |     44.103 |     0.4
   13 |   1.2331 |     43.636 |   1.2297 |     44.134 |     0.4
   14 |   1.2214 |     43.234 |   1.2182 |     43.544 |     0.5
   15 |   1.2099 |     42.793 |   1.2081 |     42.986 |     0.5
   16 |   1.2044 |     42.650 |   1.2039 |     42.862 |     0.5
   17 |   1.1927 |     42.248 |   1.1950 |     42.458 |     0.6
   18 |   1.1857 |     41.840 |   1.1900 |     41.868 |     0.6
   19 |   1.1799 |     41.945 |   1.1853 |     42.179 |     0.6
   20 |   1.1720 |     42.050 |   1.1765 |     42.365 |     0.7
   21 |   1.1630 |     41.537 |   1.1791 |     42.458 |     0.7
   22 |   1.1609 |     41.355 |   1.1710 |     41.248 |     0.7
   23 |   1.1534 |     40.975 |   1.1648 |     41.217 |     0.8
   24 |   1.1474 |     40.826 |   1.1568 |     41.465 |     0.8
   25 |   1.1416 |     40.771 |   1.1608 |     41.186 |     0.8
   26 |   1.1385 |     40.545 |   1.1548 |     40.596 |     0.9
   27 |   1.1331 |     40.507 |   1.1565 |     40.565 |     0.9
   28 |   1.1267 |     40.127 |   1.1451 |     40.192 |     0.9
   29 |   1.1241 |     40.088 |   1.1497 |     40.627 |     1.0
   30 |   1.1153 |     39.851 |   1.1466 |     40.875 |     1.0
   31 |   1.1127 |     39.658 |   1.1301 |     39.758 |     1.0
   32 |   1.1093 |     39.510 |   1.1319 |     40.348 |     1.1
   33 |   1.1031 |     39.587 |   1.1312 |     40.130 |     1.1
   34 |   1.0994 |     39.460 |   1.1335 |     40.161 |     1.1
   35 |   1.0941 |     39.256 |   1.1291 |     39.882 |     1.2
   36 |   1.0870 |     38.755 |   1.1279 |     40.037 |     1.2
   37 |   1.0870 |     39.025 |   1.1162 |     39.354 |     1.2
   38 |   1.0809 |     38.766 |   1.1204 |     38.982 |     1.3
   39 |   1.0794 |     38.590 |   1.1110 |     39.261 |     1.3
   40 |   1.0722 |     38.424 |   1.1059 |     38.485 |     1.3
   41 |   1.0689 |     38.309 |   1.1095 |     39.199 |     1.4
   42 |   1.0640 |     38.298 |   1.1068 |     39.013 |     1.4
   43 |   1.0600 |     37.917 |   1.1012 |     38.516 |     1.4
   44 |   1.0527 |     37.499 |   1.1074 |     38.579 |     1.5
   45 |   1.0514 |     37.488 |   1.1021 |     39.075 |     1.5
   46 |   1.0495 |     37.647 |   1.0954 |     38.330 |     1.5
   47 |   1.0419 |     37.587 |   1.0897 |     38.144 |     1.6
   48 |   1.0376 |     37.074 |   1.0875 |     38.268 |     1.6
   49 |   1.0355 |     37.344 |   1.0908 |     38.206 |     1.6
   50 |   1.0312 |     36.865 |   1.0971 |     38.765 |     1.7
   51 |   1.0267 |     36.744 |   1.0823 |     38.144 |     1.7
   52 |   1.0243 |     36.667 |   1.0829 |     38.144 |     1.7
   53 |   1.0204 |     36.639 |   1.0834 |     38.237 |     1.8
   54 |   1.0180 |     36.556 |   1.0768 |     38.082 |     1.8
   55 |   1.0137 |     36.331 |   1.0721 |     37.927 |     1.8
   56 |   1.0124 |     36.347 |   1.0783 |     38.268 |     1.9
   57 |   1.0060 |     36.116 |   1.0761 |     37.678 |     1.9
   58 |   1.0043 |     36.022 |   1.0696 |     38.144 |     1.9
   59 |   0.9982 |     35.989 |   1.0693 |     37.523 |     2.0
   60 |   1.0004 |     35.967 |   1.0730 |     37.896 |     2.0
   61 |   0.9897 |     35.388 |   1.0640 |     37.492 |     2.0
   62 |   0.9888 |     35.482 |   1.0566 |     36.716 |     2.1
   63 |   0.9837 |     35.190 |   1.0574 |     36.375 |     2.1
   64 |   0.9794 |     35.047 |   1.0582 |     36.468 |     2.1
   65 |   0.9743 |     34.997 |   1.0530 |     36.375 |     2.2
   66 |   0.9740 |     34.804 |   1.0555 |     36.934 |     2.2
   67 |   0.9691 |     34.518 |   1.0522 |     36.375 |     2.2
   68 |   0.9679 |     34.738 |   1.0482 |     36.158 |     2.3
   69 |   0.9584 |     34.264 |   1.0511 |     36.158 |     2.3
   70 |   0.9573 |     33.950 |   1.0429 |     35.971 |     2.3
   71 |   0.9502 |     33.917 |   1.0483 |     36.313 |     2.4
   72 |   0.9458 |     34.017 |   1.0433 |     36.220 |     2.4
   73 |   0.9438 |     33.785 |   1.0405 |     35.599 |     2.4
   74 |   0.9398 |     33.818 |   1.0432 |     35.940 |     2.5
   75 |   0.9333 |     33.003 |   1.0441 |     36.158 |     2.5
   76 |   0.9336 |     33.014 |   1.0332 |     35.258 |     2.5
   77 |   0.9431 |     33.680 |   1.0493 |     36.034 |     2.6
   78 |   0.9259 |     32.981 |   1.0344 |     35.878 |     2.6
   79 |   0.9181 |     32.711 |   1.0287 |     35.351 |     2.6
   80 |   0.9108 |     32.364 |   1.0337 |     35.692 |     2.7
   81 |   0.9106 |     32.435 |   1.0346 |     35.723 |     2.7
   82 |   0.9054 |     32.017 |   1.0252 |     34.978 |     2.7
   83 |   0.8963 |     31.719 |   1.0239 |     35.444 |     2.8
   84 |   0.8956 |     31.829 |   1.0196 |     34.947 |     2.8
   85 |   0.8887 |     31.433 |   1.0140 |     34.482 |     2.8
   86 |   0.8822 |     31.091 |   1.0228 |     34.668 |     2.9
   87 |   0.8737 |     30.446 |   1.0105 |     34.047 |     2.9
   88 |   0.8732 |     30.590 |   1.0191 |     34.327 |     2.9
   89 |   0.8674 |     30.435 |   1.0114 |     34.295 |     3.0
   90 |   0.8585 |     29.945 |   1.0153 |     33.830 |     3.0
   91 |   0.8633 |     29.862 |   1.0166 |     34.171 |     3.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 733,282

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4378 |     64.793 |   1.8920 |     54.811 |     0.0
    2 |   1.6821 |     48.837 |   1.5003 |     46.120 |     0.1
    3 |   1.4581 |     45.978 |   1.4139 |     46.493 |     0.1
    4 |   1.4007 |     45.906 |   1.3718 |     45.810 |     0.2
    5 |   1.3636 |     45.410 |   1.3397 |     45.717 |     0.2
    6 |   1.3354 |     45.399 |   1.3188 |     44.475 |     0.2
    7 |   1.3146 |     44.612 |   1.2971 |     44.258 |     0.3
    8 |   1.2970 |     44.567 |   1.2869 |     44.165 |     0.3
    9 |   1.2805 |     44.231 |   1.2677 |     44.258 |     0.4
   10 |   1.2616 |     43.702 |   1.2539 |     43.824 |     0.4
   11 |   1.2459 |     43.339 |   1.2404 |     43.234 |     0.4
   12 |   1.2282 |     42.815 |   1.2258 |     43.172 |     0.5
   13 |   1.2096 |     42.039 |   1.2139 |     42.241 |     0.5
   14 |   1.1940 |     41.278 |   1.2057 |     41.899 |     0.6
   15 |   1.1747 |     40.788 |   1.1943 |     42.117 |     0.6
   16 |   1.1578 |     40.061 |   1.1842 |     40.875 |     0.7
   17 |   1.1357 |     39.234 |   1.1623 |     40.627 |     0.7
   18 |   1.1202 |     38.573 |   1.1603 |     40.813 |     0.7
   19 |   1.0982 |     37.736 |   1.1428 |     39.758 |     0.8
   20 |   1.0780 |     36.926 |   1.1293 |     39.168 |     0.8
   21 |   1.0604 |     36.292 |   1.1217 |     38.423 |     0.9
   22 |   1.0340 |     35.118 |   1.1062 |     37.678 |     0.9
   23 |   1.0061 |     33.725 |   1.0957 |     37.058 |     0.9
   24 |   0.9835 |     33.113 |   1.0926 |     37.058 |     1.0
   25 |   0.9568 |     31.950 |   1.0693 |     35.599 |     1.0
   26 |   0.9223 |     30.722 |   1.0583 |     35.196 |     1.1
   27 |   0.8986 |     29.994 |   1.0467 |     34.295 |     1.1
   28 |   0.8691 |     29.036 |   1.0473 |     34.544 |     1.1
   29 |   0.8357 |     27.416 |   1.0439 |     33.551 |     1.2
   30 |   0.8094 |     26.661 |   1.0403 |     33.302 |     1.2
   31 |   0.7910 |     25.840 |   1.0393 |     33.116 |     1.3
   32 |   0.7645 |     24.964 |   1.0407 |     33.302 |     1.3
   33 |   0.7309 |     23.537 |   1.0263 |     32.464 |     1.4
   34 |   0.7084 |     22.893 |   1.0331 |     32.837 |     1.4
   35 |   0.6814 |     22.028 |   1.0387 |     31.999 |     1.4
   36 |   0.6509 |     20.848 |   1.0352 |     32.588 |     1.5
   37 |   0.6344 |     20.083 |   1.0530 |     32.371 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 621,506

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1992 |     60.121 |   1.5706 |     46.120 |     0.0
    2 |   1.4612 |     46.143 |   1.4086 |     46.120 |     0.0
    3 |   1.3789 |     45.956 |   1.3413 |     45.903 |     0.1
    4 |   1.3332 |     45.824 |   1.3175 |     45.034 |     0.1
    5 |   1.3040 |     44.893 |   1.2958 |     44.972 |     0.1
    6 |   1.2793 |     44.512 |   1.2737 |     44.879 |     0.1
    7 |   1.2616 |     43.835 |   1.2584 |     44.289 |     0.1
    8 |   1.2422 |     43.614 |   1.2375 |     43.141 |     0.1
    9 |   1.2216 |     43.135 |   1.2093 |     43.855 |     0.2
   10 |   1.1977 |     42.782 |   1.1955 |     43.079 |     0.2
   11 |   1.1829 |     42.386 |   1.1875 |     42.737 |     0.2
   12 |   1.1646 |     41.697 |   1.1671 |     41.527 |     0.2
   13 |   1.1475 |     40.672 |   1.1566 |     41.775 |     0.2
   14 |   1.1369 |     40.501 |   1.1614 |     42.086 |     0.2
   15 |   1.1260 |     40.094 |   1.1522 |     41.527 |     0.3
   16 |   1.1168 |     39.780 |   1.1345 |     40.844 |     0.3
   17 |   1.1042 |     39.488 |   1.1413 |     40.658 |     0.3
   18 |   1.0942 |     39.581 |   1.1314 |     41.186 |     0.3
   19 |   1.0854 |     39.003 |   1.1169 |     40.037 |     0.3
   20 |   1.0781 |     38.782 |   1.1179 |     39.820 |     0.3
   21 |   1.0719 |     38.612 |   1.1016 |     39.199 |     0.4
   22 |   1.0595 |     38.094 |   1.1021 |     38.423 |     0.4
   23 |   1.0500 |     37.945 |   1.0933 |     38.920 |     0.4
   24 |   1.0356 |     37.322 |   1.1005 |     39.882 |     0.4
   25 |   1.0274 |     36.777 |   1.1024 |     39.727 |     0.4
   26 |   1.0184 |     36.612 |   1.0886 |     37.865 |     0.4
   27 |   1.0055 |     36.375 |   1.0814 |     37.461 |     0.5
   28 |   0.9974 |     35.680 |   1.0799 |     37.834 |     0.5
   29 |   0.9888 |     35.493 |   1.0874 |     38.454 |     0.5
   30 |   0.9783 |     35.152 |   1.0809 |     38.827 |     0.5
   31 |   0.9686 |     34.959 |   1.0737 |     36.468 |     0.5
   32 |   0.9583 |     34.601 |   1.0793 |     38.082 |     0.5
   33 |   0.9474 |     34.105 |   1.0627 |     36.840 |     0.6
   34 |   0.9384 |     33.592 |   1.0596 |     36.965 |     0.6
   35 |   0.9215 |     32.573 |   1.0592 |     35.475 |     0.6
   36 |   0.9078 |     32.292 |   1.0485 |     35.754 |     0.6
   37 |   0.9000 |     31.923 |   1.0421 |     35.227 |     0.6
   38 |   0.8855 |     31.295 |   1.0315 |     34.544 |     0.6
   39 |   0.8706 |     30.490 |   1.0455 |     35.754 |     0.7
   40 |   0.8630 |     30.364 |   1.0369 |     34.792 |     0.7
   41 |   0.8462 |     29.719 |   1.0253 |     34.109 |     0.7
   42 |   0.8345 |     29.146 |   1.0187 |     34.202 |     0.7
   43 |   0.8170 |     28.215 |   1.0224 |     34.358 |     0.7
   44 |   0.8120 |     28.154 |   1.0147 |     33.799 |     0.8
   45 |   0.7947 |     27.559 |   1.0225 |     33.954 |     0.8
   46 |   0.7857 |     27.163 |   1.0251 |     33.209 |     0.8
   47 |   0.7783 |     26.986 |   1.0384 |     33.364 |     0.8
   48 |   0.7606 |     26.446 |   1.0266 |     33.333 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 699,074

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4839 |     65.339 |   1.9434 |     53.973 |     0.0
    2 |   1.7112 |     48.975 |   1.5131 |     46.120 |     0.1
    3 |   1.4609 |     45.978 |   1.4072 |     46.120 |     0.1
    4 |   1.3919 |     45.807 |   1.3674 |     45.376 |     0.2
    5 |   1.3514 |     45.300 |   1.3328 |     44.600 |     0.2
    6 |   1.3244 |     44.854 |   1.3080 |     44.848 |     0.2
    7 |   1.3047 |     44.534 |   1.2932 |     44.289 |     0.3
    8 |   1.2904 |     44.077 |   1.2816 |     44.196 |     0.3
    9 |   1.2762 |     44.011 |   1.2658 |     44.072 |     0.4
   10 |   1.2619 |     43.906 |   1.2499 |     44.227 |     0.4
   11 |   1.2466 |     43.713 |   1.2425 |     43.762 |     0.4
   12 |   1.2356 |     43.234 |   1.2306 |     44.041 |     0.5
   13 |   1.2214 |     43.008 |   1.2222 |     42.831 |     0.5
   14 |   1.2098 |     42.661 |   1.2108 |     43.855 |     0.6
   15 |   1.1967 |     42.044 |   1.1972 |     41.899 |     0.6
   16 |   1.1845 |     41.510 |   1.1938 |     42.551 |     0.6
   17 |   1.1691 |     40.948 |   1.1751 |     40.999 |     0.7
   18 |   1.1611 |     40.645 |   1.1682 |     40.875 |     0.7
   19 |   1.1480 |     40.380 |   1.1628 |     41.248 |     0.8
   20 |   1.1401 |     40.171 |   1.1559 |     40.472 |     0.8
   21 |   1.1306 |     39.895 |   1.1437 |     39.758 |     0.8
   22 |   1.1213 |     39.306 |   1.1381 |     40.037 |     0.9
   23 |   1.1133 |     39.322 |   1.1346 |     39.727 |     0.9
   24 |   1.1049 |     38.815 |   1.1289 |     39.199 |     1.0
   25 |   1.0981 |     38.744 |   1.1246 |     39.013 |     1.0
   26 |   1.0875 |     38.264 |   1.1126 |     38.982 |     1.1
   27 |   1.0817 |     38.237 |   1.1121 |     38.734 |     1.1
   28 |   1.0791 |     38.298 |   1.1091 |     39.044 |     1.1
   29 |   1.0688 |     37.664 |   1.0990 |     38.423 |     1.2
   30 |   1.0614 |     37.625 |   1.0960 |     38.641 |     1.2
   31 |   1.0519 |     37.058 |   1.0867 |     38.020 |     1.3
   32 |   1.0428 |     37.124 |   1.0888 |     38.175 |     1.3
   33 |   1.0399 |     36.871 |   1.0833 |     38.051 |     1.3
   34 |   1.0265 |     36.413 |   1.0794 |     37.151 |     1.4
   35 |   1.0226 |     36.088 |   1.0770 |     37.461 |     1.4
   36 |   1.0158 |     35.752 |   1.0669 |     37.678 |     1.5
   37 |   1.0086 |     35.499 |   1.0717 |     37.306 |     1.5
   38 |   1.0005 |     35.328 |   1.0587 |     36.530 |     1.5
   39 |   0.9913 |     34.970 |   1.0648 |     37.244 |     1.6
   40 |   0.9889 |     35.196 |   1.0613 |     36.747 |     1.6
   41 |   0.9807 |     34.474 |   1.0518 |     36.654 |     1.7
   42 |   0.9875 |     35.229 |   1.0619 |     37.399 |     1.7
   43 |   0.9781 |     34.331 |   1.0521 |     36.002 |     1.7
   44 |   0.9667 |     34.121 |   1.0450 |     36.065 |     1.8
   45 |   0.9579 |     33.565 |   1.0398 |     35.630 |     1.8
   46 |   0.9465 |     33.025 |   1.0297 |     35.692 |     1.9
   47 |   0.9409 |     33.085 |   1.0327 |     35.289 |     1.9
   48 |   0.9363 |     32.672 |   1.0340 |     35.754 |     1.9
   49 |   0.9245 |     32.275 |   1.0315 |     35.351 |     2.0
   50 |   0.9210 |     31.890 |   1.0271 |     35.040 |     2.0
   51 |   0.9105 |     31.421 |   1.0296 |     34.978 |     2.1
   52 |   0.9073 |     31.322 |   1.0189 |     34.295 |     2.1
   53 |   0.8968 |     30.959 |   1.0092 |     34.016 |     2.2
   54 |   0.8936 |     31.025 |   1.0196 |     34.792 |     2.2
   55 |   0.8825 |     30.325 |   1.0100 |     33.923 |     2.2
   56 |   0.8747 |     29.763 |   1.0048 |     33.861 |     2.3
   57 |   0.8647 |     29.796 |   0.9982 |     33.426 |     2.3
   58 |   0.8581 |     29.372 |   0.9979 |     33.737 |     2.4
   59 |   0.8467 |     28.777 |   0.9961 |     33.457 |     2.4
   60 |   0.8371 |     28.551 |   0.9967 |     32.930 |     2.4
   61 |   0.8343 |     28.198 |   0.9950 |     33.147 |     2.5
   62 |   0.8269 |     28.028 |   0.9828 |     32.402 |     2.5
   63 |   0.8133 |     27.361 |   0.9860 |     32.682 |     2.6
   64 |   0.8045 |     27.322 |   0.9845 |     32.961 |     2.6
   65 |   0.8014 |     27.113 |   0.9737 |     32.340 |     2.6
   66 |   0.7895 |     26.667 |   0.9813 |     32.278 |     2.7
   67 |   0.7844 |     26.463 |   0.9782 |     31.937 |     2.7
   68 |   0.7680 |     25.884 |   0.9907 |     32.682 |     2.8
   69 |   0.7575 |     25.322 |   0.9711 |     31.192 |     2.8
   70 |   0.7445 |     24.898 |   0.9810 |     31.161 |     2.8
   71 |   0.7398 |     24.893 |   0.9632 |     31.192 |     2.9
   72 |   0.7213 |     24.193 |   0.9775 |     31.378 |     2.9
   73 |   0.7125 |     23.725 |   0.9641 |     30.385 |     3.0
   74 |   0.7079 |     23.675 |   0.9730 |     30.602 |     3.0
   75 |   0.6945 |     22.887 |   0.9652 |     30.323 |     3.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 486,082

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3920 |     63.603 |   1.7695 |     48.510 |     0.0
    2 |   1.5956 |     47.383 |   1.4596 |     46.120 |     0.1
    3 |   1.4299 |     45.840 |   1.3824 |     45.996 |     0.1
    4 |   1.3689 |     45.840 |   1.3444 |     45.593 |     0.1
    5 |   1.3371 |     45.576 |   1.3165 |     45.345 |     0.2
    6 |   1.3144 |     45.003 |   1.3003 |     45.096 |     0.2
    7 |   1.2978 |     44.628 |   1.2900 |     44.475 |     0.2
    8 |   1.2821 |     44.039 |   1.2687 |     43.669 |     0.3
    9 |   1.2656 |     43.923 |   1.2544 |     43.482 |     0.3
   10 |   1.2498 |     43.675 |   1.2402 |     43.513 |     0.3
   11 |   1.2329 |     43.504 |   1.2227 |     42.675 |     0.4
   12 |   1.2185 |     42.915 |   1.2157 |     42.551 |     0.4
   13 |   1.2035 |     42.386 |   1.2018 |     42.706 |     0.4
   14 |   1.1917 |     41.967 |   1.1952 |     42.210 |     0.5
   15 |   1.1787 |     41.658 |   1.1823 |     41.279 |     0.5
   16 |   1.1668 |     41.185 |   1.1750 |     41.899 |     0.5
   17 |   1.1565 |     40.909 |   1.1720 |     40.906 |     0.6
   18 |   1.1470 |     40.601 |   1.1575 |     40.968 |     0.6
   19 |   1.1369 |     40.242 |   1.1512 |     40.255 |     0.6
   20 |   1.1293 |     40.171 |   1.1431 |     40.068 |     0.7
   21 |   1.1173 |     39.532 |   1.1433 |     40.627 |     0.7
   22 |   1.1094 |     39.383 |   1.1479 |     40.596 |     0.7
   23 |   1.1000 |     39.096 |   1.1320 |     40.006 |     0.8
   24 |   1.0926 |     38.512 |   1.1196 |     39.044 |     0.8
   25 |   1.0855 |     38.237 |   1.1181 |     39.106 |     0.8
   26 |   1.0747 |     38.165 |   1.1151 |     38.547 |     0.9
   27 |   1.0672 |     37.636 |   1.1128 |     38.672 |     0.9
   28 |   1.0563 |     37.179 |   1.1043 |     38.051 |     0.9
   29 |   1.0488 |     36.738 |   1.0942 |     37.647 |     1.0
   30 |   1.0376 |     36.402 |   1.0900 |     37.213 |     1.0
   31 |   1.0272 |     36.220 |   1.0799 |     37.213 |     1.0
   32 |   1.0188 |     35.466 |   1.0752 |     36.530 |     1.1
   33 |   1.0102 |     35.146 |   1.0715 |     36.996 |     1.1
   34 |   0.9969 |     34.270 |   1.0669 |     36.375 |     1.1
   35 |   0.9872 |     34.094 |   1.0592 |     35.816 |     1.2
   36 |   0.9769 |     33.868 |   1.0549 |     36.468 |     1.2
   37 |   0.9714 |     33.433 |   1.0605 |     35.754 |     1.2
   38 |   0.9609 |     32.986 |   1.0570 |     36.406 |     1.3
   39 |   0.9479 |     32.590 |   1.0309 |     35.040 |     1.3
   40 |   0.9357 |     32.116 |   1.0265 |     35.102 |     1.3
   41 |   0.9289 |     32.006 |   1.0212 |     34.885 |     1.4
   42 |   0.9161 |     31.488 |   1.0166 |     33.737 |     1.4
   43 |   0.9027 |     30.650 |   1.0147 |     33.985 |     1.5
   44 |   0.8911 |     30.248 |   1.0037 |     34.264 |     1.5
   45 |   0.8812 |     29.807 |   1.0094 |     34.078 |     1.5
   46 |   0.8717 |     29.691 |   1.0008 |     33.799 |     1.6
   47 |   0.8600 |     29.080 |   0.9956 |     33.240 |     1.6
   48 |   0.8466 |     28.893 |   0.9923 |     33.240 |     1.6
   49 |   0.8639 |     29.455 |   1.0294 |     34.854 |     1.7
   50 |   0.8983 |     30.837 |   1.0059 |     33.675 |     1.7
   51 |   0.8630 |     29.477 |   0.9906 |     33.178 |     1.7
   52 |   0.8384 |     28.452 |   0.9840 |     32.402 |     1.8
   53 |   0.8240 |     28.000 |   0.9814 |     32.713 |     1.8
   54 |   0.8227 |     27.961 |   0.9800 |     32.216 |     1.8
   55 |   0.8015 |     27.069 |   0.9703 |     32.185 |     1.9
   56 |   0.7889 |     26.766 |   0.9746 |     32.030 |     1.9
   57 |   0.7776 |     26.435 |   0.9727 |     31.968 |     1.9
   58 |   0.7711 |     25.978 |   0.9734 |     31.471 |     2.0
   59 |   0.7561 |     25.405 |   0.9679 |     31.564 |     2.0
   60 |   0.7456 |     24.931 |   0.9606 |     30.509 |     2.0
   61 |   0.7376 |     24.970 |   0.9738 |     31.068 |     2.1
   62 |   0.7352 |     24.738 |   0.9540 |     29.981 |     2.1
   63 |   0.7218 |     24.066 |   0.9557 |     30.137 |     2.1
   64 |   0.7018 |     23.163 |   0.9636 |     30.602 |     2.2
   65 |   0.6958 |     23.190 |   0.9665 |     30.819 |     2.2
   66 |   0.6864 |     22.573 |   0.9618 |     30.416 |     2.2
   67 |   0.6789 |     22.683 |   0.9499 |     29.516 |     2.3
   68 |   0.6679 |     22.215 |   0.9514 |     29.888 |     2.3
   69 |   0.6583 |     21.747 |   0.9543 |     29.671 |     2.3
   70 |   0.6453 |     21.548 |   0.9583 |     29.547 |     2.4
   71 |   0.6374 |     21.157 |   0.9502 |     29.236 |     2.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,662,114

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2199 |     60.843 |   1.5924 |     46.120 |     0.1
    2 |   1.4786 |     45.978 |   1.4102 |     46.120 |     0.1
    3 |   1.4013 |     46.017 |   1.3778 |     45.779 |     0.2
    4 |   1.3740 |     45.824 |   1.3484 |     45.562 |     0.3
    5 |   1.3406 |     45.477 |   1.3163 |     45.220 |     0.3
    6 |   1.3107 |     44.667 |   1.2940 |     45.500 |     0.4
    7 |   1.2896 |     44.711 |   1.2807 |     44.041 |     0.4
    8 |   1.2721 |     44.077 |   1.2695 |     44.475 |     0.5
    9 |   1.2513 |     43.928 |   1.2373 |     44.010 |     0.6
   10 |   1.2263 |     43.355 |   1.2190 |     43.700 |     0.6
   11 |   1.2038 |     42.909 |   1.2016 |     41.930 |     0.7
   12 |   1.1834 |     41.653 |   1.1885 |     41.589 |     0.8
   13 |   1.1601 |     40.727 |   1.1713 |     41.465 |     0.8
   14 |   1.1344 |     39.642 |   1.1490 |     40.255 |     0.9
   15 |   1.1085 |     38.711 |   1.1389 |     39.975 |     1.0
   16 |   1.0872 |     38.044 |   1.1226 |     39.013 |     1.0
   17 |   1.0632 |     37.185 |   1.1119 |     38.361 |     1.1
   18 |   1.0427 |     36.496 |   1.0923 |     37.244 |     1.1
   19 |   1.0150 |     35.052 |   1.0892 |     36.592 |     1.2
   20 |   0.9840 |     33.686 |   1.0684 |     36.096 |     1.3
   21 |   0.9604 |     32.843 |   1.0569 |     35.661 |     1.3
   22 |   0.9328 |     31.625 |   1.0475 |     34.668 |     1.4
   23 |   0.9036 |     30.452 |   1.0329 |     34.109 |     1.5
   24 |   0.8716 |     29.113 |   1.0255 |     33.706 |     1.5
   25 |   0.8458 |     28.088 |   1.0093 |     32.651 |     1.6
   26 |   0.8030 |     26.788 |   1.0040 |     32.402 |     1.7
   27 |   0.7821 |     26.077 |   0.9916 |     31.626 |     1.7
   28 |   0.7446 |     24.369 |   0.9883 |     30.788 |     1.8
   29 |   0.7072 |     23.273 |   0.9855 |     30.944 |     1.8
   30 |   0.6836 |     22.523 |   0.9908 |     29.733 |     1.9
   31 |   0.6579 |     21.405 |   0.9766 |     29.733 |     2.0
   32 |   0.6285 |     20.689 |   0.9817 |     29.702 |     2.0
   33 |   0.6022 |     19.559 |   0.9897 |     28.802 |     2.1
   34 |   0.5747 |     18.556 |   0.9884 |     29.702 |     2.2
   35 |   0.5491 |     17.526 |   0.9955 |     29.919 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 519,362

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4891 |     67.008 |   1.8963 |     49.752 |     0.2
    2 |   1.6781 |     48.033 |   1.5118 |     46.120 |     0.4
    3 |   1.4642 |     45.972 |   1.4187 |     46.120 |     0.6
    4 |   1.4115 |     45.967 |   1.3862 |     46.120 |     0.8
    5 |   1.3796 |     45.846 |   1.3668 |     45.872 |     1.0
    6 |   1.3581 |     45.317 |   1.3415 |     45.158 |     1.2
    7 |   1.3438 |     44.964 |   1.3356 |     45.313 |     1.4
    8 |   1.3297 |     44.364 |   1.3150 |     44.848 |     1.6
    9 |   1.3176 |     44.347 |   1.3066 |     44.475 |     1.8
   10 |   1.3051 |     44.193 |   1.2995 |     44.320 |     2.0
   11 |   1.2930 |     44.006 |   1.2852 |     44.538 |     2.2
   12 |   1.2787 |     44.077 |   1.2726 |     44.010 |     2.4
   13 |   1.2648 |     43.433 |   1.2674 |     43.265 |     2.6
   14 |   1.2537 |     43.223 |   1.2568 |     43.762 |     2.8
   15 |   1.2396 |     42.948 |   1.2503 |     43.917 |     3.0
   16 |   1.2286 |     42.837 |   1.2329 |     42.986 |     3.2
   17 |   1.2115 |     42.336 |   1.2261 |     42.737 |     3.4
   18 |   1.1988 |     41.813 |   1.2221 |     42.582 |     3.6
   19 |   1.1858 |     41.333 |   1.2107 |     41.620 |     3.8
   20 |   1.1676 |     40.501 |   1.2042 |     41.186 |     4.0
   21 |   1.1553 |     39.747 |   1.1907 |     41.434 |     4.3
   22 |   1.1417 |     39.449 |   1.1942 |     40.782 |     4.5
   23 |   1.1241 |     38.744 |   1.1770 |     40.751 |     4.7
   24 |   1.1105 |     38.187 |   1.1710 |     40.006 |     4.9
   25 |   1.0959 |     37.983 |   1.1721 |     39.944 |     5.1
   26 |   1.0860 |     37.328 |   1.1576 |     39.696 |     5.3
   27 |   1.0615 |     36.452 |   1.1493 |     39.230 |     5.5
   28 |   1.0462 |     35.945 |   1.1332 |     38.237 |     5.7
   29 |   1.0259 |     35.196 |   1.1384 |     37.927 |     5.9
   30 |   1.0039 |     34.303 |   1.1286 |     37.585 |     6.1
   31 |   0.9836 |     33.455 |   1.1403 |     38.237 |     6.3
   32 |   0.9697 |     32.865 |   1.1260 |     37.585 |     6.5
   33 |   0.9441 |     31.961 |   1.1197 |     36.996 |     6.7
   34 |   0.9225 |     30.953 |   1.1067 |     36.468 |     6.9
   35 |   0.9041 |     30.220 |   1.1177 |     36.903 |     7.1
   36 |   0.8805 |     29.113 |   1.1004 |     36.189 |     7.3
   37 |   0.8544 |     28.226 |   1.0979 |     35.661 |     7.5
   38 |   0.8306 |     27.449 |   1.1204 |     36.158 |     7.7
   39 |   0.8134 |     26.601 |   1.1134 |     35.382 |     7.9
   40 |   0.7891 |     25.741 |   1.1045 |     34.792 |     8.1
   41 |   0.7993 |     26.022 |   1.1297 |     35.878 |     8.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,097,954

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2486 |     61.917 |   1.6487 |     46.151 |     0.1
    2 |   1.5012 |     45.978 |   1.4315 |     46.151 |     0.1
    3 |   1.4151 |     46.050 |   1.3918 |     45.903 |     0.2
    4 |   1.3916 |     46.000 |   1.3825 |     45.748 |     0.2
    5 |   1.3679 |     45.306 |   1.3493 |     45.407 |     0.3
    6 |   1.3403 |     45.405 |   1.3129 |     45.065 |     0.3
    7 |   1.3120 |     45.080 |   1.2892 |     45.251 |     0.4
    8 |   1.2900 |     44.793 |   1.2766 |     44.413 |     0.4
    9 |   1.2715 |     44.645 |   1.2569 |     44.693 |     0.5
   10 |   1.2523 |     44.441 |   1.2443 |     43.886 |     0.6
   11 |   1.2378 |     43.851 |   1.2344 |     43.917 |     0.6
   12 |   1.2263 |     43.669 |   1.2300 |     42.862 |     0.7
   13 |   1.2197 |     43.383 |   1.2221 |     44.196 |     0.7
   14 |   1.2055 |     43.107 |   1.2137 |     43.824 |     0.8
   15 |   1.1965 |     42.634 |   1.2020 |     42.613 |     0.8
   16 |   1.1905 |     42.601 |   1.1995 |     42.179 |     0.9
   17 |   1.1775 |     42.501 |   1.1875 |     42.955 |     1.0
   18 |   1.1668 |     41.989 |   1.1800 |     42.613 |     1.0
   19 |   1.1578 |     41.559 |   1.1808 |     41.713 |     1.1
   20 |   1.1575 |     41.691 |   1.1702 |     41.775 |     1.1
   21 |   1.1408 |     41.135 |   1.1659 |     41.496 |     1.2
   22 |   1.1350 |     40.909 |   1.1547 |     40.999 |     1.2
   23 |   1.1276 |     40.628 |   1.1649 |     41.837 |     1.3
   24 |   1.1208 |     40.700 |   1.1558 |     41.217 |     1.4
   25 |   1.1123 |     40.006 |   1.1435 |     40.844 |     1.4
   26 |   1.1033 |     39.906 |   1.1460 |     40.751 |     1.5
   27 |   1.0959 |     39.598 |   1.1330 |     40.348 |     1.5
   28 |   1.0876 |     39.344 |   1.1247 |     40.348 |     1.6
   29 |   1.0825 |     38.964 |   1.1253 |     40.130 |     1.6
   30 |   1.0771 |     38.915 |   1.1176 |     39.696 |     1.7
   31 |   1.0666 |     38.193 |   1.1210 |     39.696 |     1.7
   32 |   1.0608 |     38.551 |   1.1068 |     39.572 |     1.8
   33 |   1.0515 |     37.978 |   1.1029 |     39.572 |     1.9
   34 |   1.0437 |     37.658 |   1.1037 |     39.354 |     1.9
   35 |   1.0381 |     37.879 |   1.0919 |     38.827 |     2.0
   36 |   1.0327 |     37.129 |   1.0955 |     38.516 |     2.0
   37 |   1.0307 |     37.565 |   1.0991 |     39.013 |     2.1
   38 |   1.0238 |     36.782 |   1.0950 |     38.734 |     2.1
   39 |   1.0176 |     36.777 |   1.0849 |     37.865 |     2.2
   40 |   1.0086 |     36.623 |   1.0814 |     37.461 |     2.3
   41 |   1.0024 |     35.807 |   1.0832 |     37.709 |     2.3
   42 |   0.9960 |     35.625 |   1.0745 |     37.182 |     2.4
   43 |   0.9900 |     35.736 |   1.0729 |     37.585 |     2.4
   44 |   0.9879 |     35.526 |   1.0865 |     37.865 |     2.5
   45 |   0.9816 |     35.223 |   1.0785 |     37.896 |     2.5
   46 |   0.9817 |     35.152 |   1.0605 |     37.244 |     2.6
   47 |   0.9711 |     34.694 |   1.0600 |     36.934 |     2.7
   48 |   0.9678 |     34.898 |   1.0625 |     37.306 |     2.7
   49 |   0.9587 |     34.121 |   1.0609 |     37.151 |     2.8
   50 |   0.9596 |     34.402 |   1.0578 |     36.840 |     2.8
   51 |   0.9543 |     34.287 |   1.0598 |     36.747 |     2.9
   52 |   0.9399 |     33.521 |   1.0495 |     37.182 |     2.9
   53 |   0.9334 |     33.207 |   1.0428 |     35.909 |     3.0
   54 |   0.9275 |     32.843 |   1.0382 |     36.189 |     3.1
   55 |   0.9207 |     32.843 |   1.0450 |     35.661 |     3.1
   56 |   0.9172 |     32.678 |   1.0301 |     35.506 |     3.2
   57 |   0.9081 |     32.143 |   1.0335 |     35.878 |     3.2
   58 |   0.9005 |     31.851 |   1.0308 |     35.971 |     3.3
   59 |   0.8875 |     31.421 |   1.0463 |     35.444 |     3.3
   60 |   0.8840 |     31.317 |   1.0187 |     35.537 |     3.4
   61 |   0.8691 |     30.705 |   1.0086 |     34.544 |     3.5
   62 |   0.8671 |     30.479 |   1.0089 |     34.761 |     3.5
   63 |   0.8660 |     30.259 |   1.0123 |     34.885 |     3.6
   64 |   0.8471 |     29.537 |   1.0042 |     34.854 |     3.6
   65 |   0.8462 |     29.383 |   1.0121 |     34.637 |     3.7
   66 |   0.8351 |     29.168 |   1.0030 |     34.544 |     3.7
   67 |   0.8323 |     28.860 |   0.9938 |     33.551 |     3.8
   68 |   0.8168 |     28.375 |   1.0049 |     34.761 |     3.8
   69 |   0.8101 |     28.165 |   0.9898 |     33.954 |     3.9
   70 |   0.8029 |     27.565 |   0.9848 |     33.799 |     4.0
   71 |   0.7885 |     27.229 |   0.9839 |     33.457 |     4.0
   72 |   0.8181 |     28.573 |   0.9937 |     33.861 |     4.1
   73 |   0.7796 |     26.683 |   0.9730 |     33.426 |     4.1
   74 |   0.7646 |     26.171 |   0.9794 |     33.302 |     4.2
   75 |   0.7571 |     25.989 |   0.9961 |     34.109 |     4.2
   76 |   0.7462 |     25.570 |   0.9673 |     32.402 |     4.3
   77 |   0.7390 |     25.322 |   0.9679 |     32.619 |     4.4
   78 |   0.7291 |     24.826 |   0.9588 |     32.619 |     4.4
   79 |   0.7189 |     24.749 |   0.9629 |     32.123 |     4.5
   80 |   0.7147 |     24.496 |   0.9714 |     33.209 |     4.5
   81 |   0.7052 |     23.890 |   0.9578 |     32.930 |     4.6
   82 |   0.6914 |     23.311 |   0.9594 |     32.154 |     4.6
   83 |   0.6907 |     23.262 |   0.9645 |     32.526 |     4.7
   84 |   0.6946 |     23.658 |   0.9782 |     32.806 |     4.8
   85 |   0.6835 |     23.047 |   0.9590 |     31.968 |     4.8
   86 |   0.6630 |     22.309 |   0.9540 |     31.875 |     4.9
   87 |   0.6521 |     21.912 |   0.9607 |     31.440 |     4.9
   88 |   0.6452 |     21.614 |   0.9649 |     31.471 |     5.0
   89 |   0.6340 |     21.003 |   0.9706 |     31.782 |     5.1
   90 |   0.6300 |     20.871 |   0.9739 |     31.657 |     5.1
   91 |   0.6358 |     21.019 |   0.9533 |     31.813 |     5.2
   92 |   0.6138 |     20.413 |   0.9624 |     30.944 |     5.2
   93 |   0.6065 |     20.248 |   0.9679 |     31.037 |     5.3
   94 |   0.5986 |     19.785 |   0.9594 |     30.788 |     5.3
   95 |   0.5882 |     19.410 |   0.9756 |     30.944 |     5.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,887,458

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1141 |     57.719 |   1.5303 |     46.120 |     0.1
    2 |   1.4379 |     45.917 |   1.3740 |     45.779 |     0.2
    3 |   1.3440 |     44.711 |   1.3135 |     45.065 |     0.2
    4 |   1.3063 |     44.501 |   1.2908 |     44.662 |     0.3
    5 |   1.2748 |     44.066 |   1.2536 |     44.196 |     0.4
    6 |   1.2450 |     43.499 |   1.2383 |     43.855 |     0.5
    7 |   1.2181 |     42.887 |   1.2168 |     42.768 |     0.6
    8 |   1.1981 |     42.463 |   1.1988 |     42.055 |     0.7
    9 |   1.1693 |     41.410 |   1.1793 |     40.999 |     0.7
   10 |   1.1470 |     40.485 |   1.1606 |     40.223 |     0.8
   11 |   1.1286 |     40.006 |   1.1544 |     40.223 |     0.9
   12 |   1.1064 |     39.096 |   1.1348 |     39.944 |     1.0
   13 |   1.0820 |     38.182 |   1.1203 |     39.199 |     1.1
   14 |   1.0626 |     37.207 |   1.1055 |     38.703 |     1.2
   15 |   1.0363 |     36.479 |   1.1004 |     37.647 |     1.2
   16 |   1.0153 |     35.311 |   1.0711 |     36.840 |     1.3
   17 |   0.9949 |     33.884 |   1.0676 |     36.344 |     1.4
   18 |   0.9732 |     33.603 |   1.0469 |     35.568 |     1.5
   19 |   0.9476 |     32.507 |   1.0357 |     35.320 |     1.6
   20 |   0.9268 |     31.664 |   1.0282 |     34.606 |     1.7
   21 |   0.9051 |     31.157 |   1.0125 |     34.171 |     1.7
   22 |   0.8864 |     30.364 |   1.0136 |     33.489 |     1.8
   23 |   0.8619 |     29.570 |   1.0125 |     33.582 |     1.9
   24 |   0.8412 |     28.424 |   0.9964 |     32.619 |     2.0
   25 |   0.8189 |     27.532 |   0.9956 |     32.092 |     2.1
   26 |   0.7929 |     26.705 |   0.9768 |     31.937 |     2.2
   27 |   0.7763 |     26.072 |   1.0055 |     32.340 |     2.2
   28 |   0.7536 |     25.267 |   0.9873 |     31.844 |     2.3
   29 |   0.7300 |     24.358 |   0.9768 |     31.068 |     2.4
   30 |   0.7051 |     23.366 |   0.9600 |     31.006 |     2.5
   31 |   0.6798 |     22.408 |   0.9785 |     31.719 |     2.6
   32 |   0.6585 |     21.631 |   0.9802 |     31.626 |     2.7
   33 |   0.6392 |     21.008 |   0.9764 |     30.664 |     2.7
   34 |   0.6215 |     20.452 |   0.9671 |     29.981 |     2.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 422,210

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5507 |     68.149 |   2.0090 |     58.814 |     0.1
    2 |   1.7602 |     49.598 |   1.5556 |     46.120 |     0.2
    3 |   1.4925 |     45.972 |   1.4395 |     46.120 |     0.3
    4 |   1.4275 |     45.989 |   1.3993 |     46.493 |     0.4
    5 |   1.3924 |     45.983 |   1.3687 |     46.120 |     0.5
    6 |   1.3658 |     45.664 |   1.3520 |     45.655 |     0.6
    7 |   1.3426 |     45.598 |   1.3287 |     45.624 |     0.8
    8 |   1.3237 |     45.212 |   1.3143 |     45.003 |     0.9
    9 |   1.3062 |     44.601 |   1.2976 |     44.941 |     1.0
   10 |   1.2887 |     44.198 |   1.2785 |     44.507 |     1.1
   11 |   1.2709 |     43.967 |   1.2618 |     43.606 |     1.2
   12 |   1.2528 |     43.273 |   1.2526 |     43.669 |     1.3
   13 |   1.2419 |     43.460 |   1.2379 |     43.948 |     1.4
   14 |   1.2271 |     42.992 |   1.2265 |     43.296 |     1.5
   15 |   1.2121 |     42.496 |   1.2098 |     43.141 |     1.6
   16 |   1.1979 |     42.408 |   1.2043 |     42.644 |     1.7
   17 |   1.1823 |     41.262 |   1.1857 |     41.682 |     1.8
   18 |   1.1683 |     40.937 |   1.1769 |     41.403 |     1.9
   19 |   1.1522 |     40.154 |   1.1691 |     40.751 |     2.0
   20 |   1.1357 |     39.438 |   1.1652 |     40.441 |     2.2
   21 |   1.1236 |     39.025 |   1.1470 |     39.944 |     2.3
   22 |   1.1046 |     38.281 |   1.1352 |     38.951 |     2.4
   23 |   1.0915 |     38.116 |   1.1246 |     38.672 |     2.5
   24 |   1.0710 |     37.300 |   1.1194 |     38.144 |     2.6
   25 |   1.0582 |     36.705 |   1.1086 |     37.647 |     2.7
   26 |   1.0404 |     35.972 |   1.0980 |     37.616 |     2.8
   27 |   1.0199 |     35.262 |   1.0840 |     37.244 |     2.9
   28 |   1.0029 |     34.804 |   1.0868 |     37.647 |     3.0
   29 |   0.9806 |     33.565 |   1.0685 |     36.778 |     3.1
   30 |   0.9595 |     32.970 |   1.0573 |     36.096 |     3.2
   31 |   0.9350 |     31.752 |   1.0402 |     34.854 |     3.3
   32 |   0.9133 |     30.722 |   1.0429 |     34.885 |     3.4
   33 |   0.8927 |     29.758 |   1.0326 |     33.830 |     3.6
   34 |   0.8650 |     28.705 |   1.0269 |     33.271 |     3.7
   35 |   0.8406 |     27.840 |   1.0245 |     32.682 |     3.8
   36 |   0.8112 |     26.689 |   1.0290 |     32.775 |     3.9
   37 |   0.8719 |     29.245 |   1.0116 |     32.557 |     4.0
   38 |   0.7769 |     25.366 |   0.9950 |     31.657 |     4.1
   39 |   0.7512 |     24.242 |   1.0002 |     31.875 |     4.2
   40 |   0.7402 |     24.083 |   1.0056 |     31.502 |     4.3
   41 |   0.7155 |     23.124 |   1.0080 |     31.719 |     4.4
   42 |   0.6951 |     22.485 |   1.0046 |     31.006 |     4.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 403,650

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5037 |     66.259 |   1.9230 |     57.480 |     0.1
    2 |   1.7249 |     49.647 |   1.5389 |     46.120 |     0.2
    3 |   1.4867 |     45.983 |   1.4311 |     46.120 |     0.3
    4 |   1.4244 |     45.961 |   1.3980 |     46.120 |     0.4
    5 |   1.3951 |     45.873 |   1.3738 |     46.120 |     0.6
    6 |   1.3687 |     45.818 |   1.3552 |     45.562 |     0.7
    7 |   1.3408 |     45.416 |   1.3212 |     44.910 |     0.8
    8 |   1.3187 |     44.898 |   1.2990 |     44.475 |     0.9
    9 |   1.2986 |     44.441 |   1.2875 |     44.755 |     1.0
   10 |   1.2829 |     44.320 |   1.2710 |     44.351 |     1.1
   11 |   1.2692 |     44.127 |   1.2572 |     44.196 |     1.2
   12 |   1.2568 |     44.110 |   1.2514 |     44.475 |     1.3
   13 |   1.2476 |     43.873 |   1.2395 |     43.793 |     1.5
   14 |   1.2354 |     43.658 |   1.2318 |     43.731 |     1.6
   15 |   1.2267 |     43.444 |   1.2235 |     44.041 |     1.7
   16 |   1.2178 |     43.251 |   1.2163 |     44.072 |     1.8
   17 |   1.2074 |     42.837 |   1.2085 |     43.451 |     1.9
   18 |   1.2004 |     42.496 |   1.2005 |     43.017 |     2.0
   19 |   1.1921 |     42.567 |   1.1933 |     43.296 |     2.1
   20 |   1.1815 |     42.033 |   1.1821 |     41.868 |     2.2
   21 |   1.1761 |     41.978 |   1.1808 |     42.272 |     2.4
   22 |   1.1663 |     41.262 |   1.1727 |     41.962 |     2.5
   23 |   1.1578 |     40.920 |   1.1659 |     41.775 |     2.6
   24 |   1.1521 |     40.942 |   1.1664 |     41.775 |     2.7
   25 |   1.1489 |     40.926 |   1.1638 |     41.527 |     2.8
   26 |   1.1400 |     40.413 |   1.1634 |     41.434 |     2.9
   27 |   1.1344 |     40.193 |   1.1542 |     40.813 |     3.0
   28 |   1.1304 |     40.309 |   1.1535 |     40.751 |     3.1
   29 |   1.1257 |     40.231 |   1.1489 |     41.279 |     3.3
   30 |   1.1204 |     40.083 |   1.1452 |     40.286 |     3.4
   31 |   1.1141 |     39.636 |   1.1391 |     40.503 |     3.5
   32 |   1.1115 |     39.813 |   1.1335 |     40.006 |     3.6
   33 |   1.1064 |     39.394 |   1.1370 |     40.161 |     3.7
   34 |   1.1008 |     39.295 |   1.1234 |     39.572 |     3.8
   35 |   1.0998 |     39.350 |   1.1245 |     39.851 |     3.9
   36 |   1.0933 |     39.052 |   1.1225 |     39.789 |     4.0
   37 |   1.0901 |     39.146 |   1.1195 |     39.323 |     4.2
   38 |   1.0848 |     38.656 |   1.1193 |     39.510 |     4.3
   39 |   1.0821 |     38.821 |   1.1169 |     39.665 |     4.4
   40 |   1.0762 |     38.793 |   1.1131 |     39.013 |     4.5
   41 |   1.0770 |     38.457 |   1.1102 |     39.044 |     4.6
   42 |   1.0699 |     38.523 |   1.1038 |     38.237 |     4.7
   43 |   1.0673 |     38.231 |   1.0994 |     38.765 |     4.8
   44 |   1.0616 |     37.923 |   1.1019 |     38.796 |     4.9
   45 |   1.0604 |     38.094 |   1.0979 |     38.579 |     5.0
   46 |   1.0551 |     38.006 |   1.0990 |     38.392 |     5.2
   47 |   1.0512 |     37.702 |   1.0940 |     38.299 |     5.3
   48 |   1.0480 |     37.592 |   1.0858 |     37.803 |     5.4
   49 |   1.0412 |     37.350 |   1.0900 |     38.237 |     5.5
   50 |   1.0427 |     37.433 |   1.0859 |     37.927 |     5.6
   51 |   1.0351 |     37.052 |   1.0895 |     37.834 |     5.7
   52 |   1.0343 |     37.124 |   1.0813 |     37.616 |     5.8
   53 |   1.0303 |     36.981 |   1.0839 |     38.454 |     5.9
   54 |   1.0302 |     36.700 |   1.0781 |     38.113 |     6.1
   55 |   1.0221 |     36.694 |   1.0838 |     37.834 |     6.2
   56 |   1.0199 |     36.678 |   1.0747 |     37.368 |     6.3
   57 |   1.0133 |     36.408 |   1.0768 |     37.089 |     6.4
   58 |   1.0114 |     36.375 |   1.0752 |     37.865 |     6.5
   59 |   1.0083 |     36.220 |   1.0711 |     37.306 |     6.6
   60 |   1.0087 |     36.253 |   1.0655 |     37.523 |     6.7
   61 |   1.0033 |     36.160 |   1.0673 |     37.772 |     6.8
   62 |   1.0001 |     35.934 |   1.0616 |     37.275 |     7.0
   63 |   0.9990 |     35.548 |   1.0598 |     37.151 |     7.1
   64 |   0.9906 |     35.499 |   1.0631 |     37.027 |     7.2
   65 |   0.9857 |     35.394 |   1.0511 |     36.437 |     7.3
   66 |   0.9789 |     35.129 |   1.0539 |     36.747 |     7.4
   67 |   0.9790 |     34.997 |   1.0455 |     35.506 |     7.5
   68 |   0.9716 |     34.402 |   1.0462 |     36.344 |     7.6
   69 |   0.9687 |     34.435 |   1.0464 |     36.468 |     7.7
   70 |   0.9647 |     34.215 |   1.0430 |     35.909 |     7.8
   71 |   0.9603 |     34.083 |   1.0412 |     35.878 |     8.0
   72 |   0.9586 |     34.072 |   1.0406 |     35.537 |     8.1
   73 |   0.9526 |     33.708 |   1.0316 |     35.599 |     8.2
   74 |   0.9520 |     33.691 |   1.0371 |     35.351 |     8.3
   75 |   0.9427 |     33.201 |   1.0389 |     35.537 |     8.4
   76 |   0.9436 |     33.190 |   1.0297 |     35.351 |     8.5
   77 |   0.9363 |     32.826 |   1.0332 |     35.258 |     8.6
   78 |   0.9305 |     32.749 |   1.0252 |     34.792 |     8.7
   79 |   0.9244 |     32.242 |   1.0306 |     35.009 |     8.9
   80 |   0.9322 |     32.645 |   1.0597 |     36.406 |     9.0
   81 |   0.9432 |     33.047 |   1.0180 |     34.327 |     9.1
   82 |   0.9149 |     32.088 |   1.0117 |     34.544 |     9.2
   83 |   0.9102 |     31.769 |   1.0126 |     34.202 |     9.3
   84 |   0.9028 |     31.680 |   1.0103 |     34.730 |     9.4
   85 |   0.8971 |     30.959 |   1.0023 |     33.985 |     9.5
   86 |   0.8883 |     30.793 |   1.0089 |     34.016 |     9.6
   87 |   0.8844 |     30.408 |   1.0010 |     34.295 |     9.7
   88 |   0.8858 |     30.771 |   1.0043 |     34.389 |     9.9
   89 |   0.8774 |     30.430 |   1.0029 |     34.358 |    10.0
   90 |   0.8796 |     30.612 |   1.0199 |     34.451 |    10.1
   91 |   0.8710 |     30.072 |   0.9957 |     33.240 |    10.2
   92 |   0.8653 |     29.895 |   0.9967 |     33.675 |    10.3
   93 |   0.8575 |     29.642 |   1.0011 |     33.861 |    10.4
   94 |   0.8560 |     29.548 |   0.9896 |     33.613 |    10.5
   95 |   0.8486 |     29.251 |   0.9974 |     33.364 |    10.6
   96 |   0.8468 |     29.140 |   0.9985 |     33.985 |    10.8
   97 |   0.8464 |     29.405 |   0.9818 |     33.209 |    10.9
   98 |   0.8363 |     28.788 |   0.9964 |     33.675 |    11.0
   99 |   0.8352 |     28.766 |   0.9871 |     33.551 |    11.1
  100 |   0.8284 |     28.793 |   0.9901 |     33.457 |    11.2
  101 |   0.8229 |     28.342 |   0.9832 |     33.644 |    11.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 457,122

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5547 |     68.810 |   1.9685 |     53.973 |     0.9
    2 |   1.7464 |     49.262 |   1.5673 |     46.151 |     1.8
    3 |   1.5052 |     46.044 |   1.4501 |     46.120 |     2.7
    4 |   1.4399 |     46.044 |   1.4157 |     46.120 |     3.6
    5 |   1.4153 |     46.083 |   1.3981 |     46.120 |     4.6
    6 |   1.4039 |     46.017 |   1.3935 |     46.151 |     5.5
    7 |   1.3952 |     46.055 |   1.3788 |     46.151 |     6.4
    8 |   1.3814 |     46.039 |   1.3604 |     46.151 |     7.3
    9 |   1.3516 |     45.521 |   1.3370 |     45.158 |     8.2
   10 |   1.3256 |     44.815 |   1.3161 |     44.569 |     9.1
   11 |   1.3084 |     44.545 |   1.2969 |     44.941 |    10.0
   12 |   1.2919 |     44.281 |   1.2864 |     44.258 |    11.0
   13 |   1.2791 |     44.353 |   1.2709 |     44.786 |    11.9
   14 |   1.2673 |     44.198 |   1.2616 |     44.258 |    12.8
   15 |   1.2548 |     43.813 |   1.2535 |     43.855 |    13.7
   16 |   1.2452 |     43.576 |   1.2387 |     43.141 |    14.6
   17 |   1.2347 |     43.146 |   1.2345 |     43.855 |    15.5
   18 |   1.2240 |     42.915 |   1.2236 |     42.986 |    16.4
   19 |   1.2119 |     42.623 |   1.2127 |     42.706 |    17.3
   20 |   1.2012 |     41.846 |   1.2021 |     41.558 |    18.2
   21 |   1.1906 |     41.741 |   1.1945 |     41.434 |    19.2
   22 |   1.1813 |     41.460 |   1.1950 |     41.589 |    20.1
   23 |   1.1706 |     41.146 |   1.1828 |     41.930 |    21.0
   24 |   1.1632 |     41.140 |   1.1742 |     41.589 |    21.9
   25 |   1.1512 |     40.749 |   1.1752 |     41.403 |    22.8
   26 |   1.1424 |     40.281 |   1.1614 |     41.092 |    23.7
   27 |   1.1368 |     40.402 |   1.1558 |     40.844 |    24.6
   28 |   1.1294 |     40.099 |   1.1536 |     40.192 |    25.5
   29 |   1.1193 |     39.758 |   1.1460 |     39.820 |    26.5
   30 |   1.1128 |     39.758 |   1.1457 |     40.286 |    27.4
   31 |   1.1097 |     39.471 |   1.1396 |     40.130 |    28.3
   32 |   1.0997 |     39.163 |   1.1327 |     39.820 |    29.2
   33 |   1.0921 |     38.997 |   1.1312 |     39.913 |    30.1
   34 |   1.0861 |     38.667 |   1.1258 |     39.261 |    31.0
   35 |   1.0780 |     38.468 |   1.1238 |     39.013 |    31.9
   36 |   1.0724 |     38.132 |   1.1294 |     39.603 |    32.8
   37 |   1.0670 |     38.121 |   1.1219 |     39.137 |    33.7
   38 |   1.0598 |     37.791 |   1.1119 |     39.572 |    34.6
   39 |   1.0524 |     37.686 |   1.1055 |     38.113 |    35.6
   40 |   1.0420 |     37.146 |   1.1092 |     38.858 |    36.5
   41 |   1.0345 |     36.413 |   1.1029 |     38.796 |    37.4
   42 |   1.0326 |     36.876 |   1.0922 |     38.237 |    38.3
   43 |   1.0242 |     36.182 |   1.0831 |     37.741 |    39.2
   44 |   1.0157 |     35.983 |   1.0886 |     37.337 |    40.1
   45 |   1.0072 |     35.455 |   1.0801 |     37.399 |    41.0
   46 |   0.9992 |     35.355 |   1.0869 |     37.213 |    41.9
   47 |   0.9927 |     35.179 |   1.0790 |     37.058 |    42.8
   48 |   0.9824 |     34.628 |   1.0724 |     37.120 |    43.8
   49 |   0.9742 |     34.309 |   1.0693 |     36.158 |    44.7
   50 |   0.9683 |     34.138 |   1.0785 |     37.554 |    45.6
   51 |   0.9581 |     33.455 |   1.0641 |     36.158 |    46.5
   52 |   0.9482 |     32.986 |   1.0621 |     35.568 |    47.4
   53 |   0.9414 |     32.567 |   1.0551 |     35.816 |    48.3
   54 |   0.9326 |     32.419 |   1.0567 |     35.878 |    49.2
   55 |   0.9288 |     32.000 |   1.0522 |     35.444 |    50.1
   56 |   0.9193 |     31.851 |   1.0522 |     35.661 |    51.0
   57 |   0.9077 |     31.229 |   1.0460 |     35.692 |    52.0
   58 |   0.8979 |     30.782 |   1.0382 |     35.320 |    52.9
   59 |   0.8869 |     30.380 |   1.0335 |     34.668 |    53.8
   60 |   0.8926 |     30.612 |   1.0358 |     35.164 |    54.7
   61 |   0.8774 |     30.176 |   1.0212 |     34.482 |    55.6
   62 |   0.8635 |     29.185 |   1.0254 |     34.451 |    56.5
   63 |   0.8548 |     28.975 |   1.0228 |     34.233 |    57.4
   64 |   0.8436 |     28.336 |   1.0256 |     34.264 |    58.3
   65 |   0.8295 |     27.917 |   1.0249 |     33.923 |    59.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 718,658

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1860 |     60.138 |   1.5881 |     48.665 |     0.1
    2 |   1.4744 |     46.044 |   1.4037 |     46.493 |     0.2
    3 |   1.3911 |     46.110 |   1.3615 |     45.469 |     0.3
    4 |   1.3529 |     45.807 |   1.3240 |     45.779 |     0.5
    5 |   1.3266 |     45.207 |   1.3112 |     44.910 |     0.6
    6 |   1.3061 |     44.821 |   1.2924 |     45.065 |     0.7
    7 |   1.2921 |     44.501 |   1.2820 |     44.382 |     0.8
    8 |   1.2772 |     44.534 |   1.2662 |     44.041 |     0.9
    9 |   1.2601 |     43.961 |   1.2575 |     43.979 |     1.0
   10 |   1.2446 |     43.669 |   1.2396 |     43.917 |     1.1
   11 |   1.2264 |     43.372 |   1.2205 |     42.986 |     1.2
   12 |   1.2071 |     42.496 |   1.2027 |     42.800 |     1.4
   13 |   1.1857 |     42.171 |   1.1927 |     42.489 |     1.5
   14 |   1.1681 |     41.631 |   1.1784 |     42.427 |     1.6
   15 |   1.1499 |     41.014 |   1.1670 |     41.434 |     1.7
   16 |   1.1294 |     40.176 |   1.1592 |     40.596 |     1.8
   17 |   1.1172 |     39.350 |   1.1487 |     40.844 |     1.9
   18 |   1.1021 |     39.152 |   1.1326 |     39.572 |     2.0
   19 |   1.0776 |     38.176 |   1.1209 |     39.510 |     2.1
   20 |   1.0604 |     37.554 |   1.1060 |     38.175 |     2.3
   21 |   1.0395 |     36.656 |   1.1095 |     38.889 |     2.4
   22 |   1.0248 |     35.994 |   1.0920 |     37.523 |     2.5
   23 |   0.9980 |     34.799 |   1.0860 |     37.430 |     2.6
   24 |   0.9797 |     34.369 |   1.0719 |     36.406 |     2.7
   25 |   0.9558 |     33.030 |   1.0793 |     36.313 |     2.8
   26 |   0.9257 |     31.636 |   1.0433 |     35.289 |     2.9
   27 |   0.8986 |     30.584 |   1.0346 |     34.389 |     3.1
   28 |   0.8722 |     29.333 |   1.0270 |     33.923 |     3.2
   29 |   0.8500 |     28.678 |   1.0189 |     33.737 |     3.3
   30 |   0.8306 |     28.298 |   1.0246 |     34.420 |     3.4
   31 |   0.8083 |     27.427 |   1.0058 |     32.868 |     3.5
   32 |   0.8038 |     27.096 |   1.0034 |     32.713 |     3.6
   33 |   0.7695 |     25.614 |   1.0004 |     32.651 |     3.7
   34 |   0.7387 |     24.562 |   0.9928 |     32.092 |     3.8
   35 |   0.7128 |     23.587 |   0.9960 |     32.092 |     4.0
   36 |   0.6937 |     22.997 |   0.9876 |     31.347 |     4.1
   37 |   0.6708 |     22.204 |   0.9869 |     31.099 |     4.2
   38 |   0.6448 |     21.063 |   0.9863 |     31.099 |     4.3
   39 |   0.6268 |     20.733 |   1.0039 |     30.819 |     4.4
   40 |   0.6037 |     19.807 |   1.0013 |     30.819 |     4.5
   41 |   0.5810 |     18.694 |   1.0030 |     30.416 |     4.6
   42 |   0.5634 |     18.534 |   1.0203 |     30.447 |     4.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 403,650

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5076 |     66.650 |   1.9324 |     56.673 |     0.1
    2 |   1.7111 |     48.523 |   1.5307 |     46.120 |     0.2
    3 |   1.4789 |     45.972 |   1.4299 |     46.120 |     0.3
    4 |   1.4190 |     45.956 |   1.3923 |     46.493 |     0.4
    5 |   1.3901 |     46.061 |   1.3690 |     45.965 |     0.5
    6 |   1.3652 |     45.218 |   1.3549 |     45.438 |     0.6
    7 |   1.3488 |     44.860 |   1.3309 |     44.941 |     0.8
    8 |   1.3327 |     44.567 |   1.3201 |     45.003 |     0.9
    9 |   1.3217 |     44.540 |   1.3136 |     44.569 |     1.0
   10 |   1.3122 |     44.231 |   1.3028 |     44.444 |     1.1
   11 |   1.2998 |     44.110 |   1.2949 |     44.382 |     1.2
   12 |   1.2899 |     43.912 |   1.2871 |     44.351 |     1.3
   13 |   1.2777 |     43.691 |   1.2729 |     44.103 |     1.4
   14 |   1.2608 |     43.587 |   1.2646 |     44.600 |     1.5
   15 |   1.2393 |     43.223 |   1.2460 |     43.606 |     1.6
   16 |   1.2248 |     43.014 |   1.2345 |     43.606 |     1.7
   17 |   1.2082 |     42.738 |   1.2229 |     42.613 |     1.8
   18 |   1.1924 |     41.901 |   1.2131 |     42.179 |     1.9
   19 |   1.1765 |     41.355 |   1.1971 |     41.496 |     2.0
   20 |   1.1607 |     40.248 |   1.1907 |     41.124 |     2.2
   21 |   1.1449 |     39.725 |   1.1798 |     40.999 |     2.3
   22 |   1.1270 |     38.639 |   1.1706 |     40.161 |     2.4
   23 |   1.1116 |     38.160 |   1.1632 |     40.099 |     2.5
   24 |   1.0885 |     37.085 |   1.1465 |     38.951 |     2.6
   25 |   1.0716 |     36.044 |   1.1432 |     38.610 |     2.7
   26 |   1.0527 |     35.791 |   1.1371 |     38.858 |     2.8
   27 |   1.0384 |     35.333 |   1.1194 |     37.896 |     2.9
   28 |   1.0092 |     33.917 |   1.1119 |     37.523 |     3.0
   29 |   0.9917 |     33.394 |   1.1084 |     37.337 |     3.1
   30 |   0.9682 |     32.402 |   1.1105 |     37.244 |     3.2
   31 |   0.9479 |     31.945 |   1.1115 |     37.772 |     3.3
   32 |   0.9229 |     30.821 |   1.0939 |     36.127 |     3.4
   33 |   0.8971 |     30.039 |   1.0928 |     35.475 |     3.6
   34 |   0.8745 |     29.113 |   1.0963 |     35.506 |     3.7
   35 |   0.8607 |     28.579 |   1.0939 |     35.289 |     3.8
   36 |   0.8309 |     27.455 |   1.0880 |     34.295 |     3.9
   37 |   0.8026 |     26.419 |   1.0884 |     34.358 |     4.0
   38 |   0.7882 |     25.851 |   1.0927 |     34.451 |     4.1
   39 |   0.7560 |     24.595 |   1.0918 |     34.109 |     4.2
   40 |   0.7262 |     23.251 |   1.0927 |     33.644 |     4.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 388,194

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4347 |     64.099 |   1.8928 |     49.100 |     0.7
    2 |   1.6818 |     48.347 |   1.5060 |     46.151 |     1.3
    3 |   1.4574 |     45.857 |   1.4012 |     46.369 |     2.0
    4 |   1.3813 |     45.686 |   1.3493 |     45.438 |     2.6
    5 |   1.3424 |     45.124 |   1.3225 |     44.693 |     3.3
    6 |   1.3187 |     44.782 |   1.3039 |     44.258 |     3.9
    7 |   1.2994 |     44.061 |   1.2871 |     44.444 |     4.6
    8 |   1.2803 |     43.669 |   1.2683 |     43.824 |     5.3
    9 |   1.2631 |     43.763 |   1.2626 |     43.669 |     5.9
   10 |   1.2474 |     43.234 |   1.2413 |     43.451 |     6.6
   11 |   1.2308 |     42.771 |   1.2235 |     42.582 |     7.2
   12 |   1.2182 |     42.441 |   1.2152 |     42.303 |     7.9
   13 |   1.2049 |     41.906 |   1.2034 |     42.396 |     8.5
   14 |   1.1941 |     41.939 |   1.1974 |     41.962 |     9.2
   15 |   1.1830 |     41.460 |   1.1935 |     41.620 |     9.9
   16 |   1.1734 |     41.168 |   1.1841 |     41.186 |    10.5
   17 |   1.1642 |     40.959 |   1.1740 |     41.186 |    11.2
   18 |   1.1524 |     40.595 |   1.1678 |     40.906 |    11.8
   19 |   1.1419 |     40.154 |   1.1616 |     40.192 |    12.5
   20 |   1.1318 |     39.829 |   1.1562 |     40.161 |    13.2
   21 |   1.1200 |     39.466 |   1.1494 |     40.286 |    13.8
   22 |   1.1106 |     38.953 |   1.1416 |     40.037 |    14.5
   23 |   1.0994 |     38.672 |   1.1287 |     39.385 |    15.1
   24 |   1.0864 |     38.022 |   1.1254 |     39.168 |    15.8
   25 |   1.0730 |     37.581 |   1.1157 |     38.734 |    16.5
   26 |   1.0614 |     37.152 |   1.1082 |     38.672 |    17.1
   27 |   1.0511 |     36.755 |   1.1040 |     38.392 |    17.8
   28 |   1.0391 |     36.353 |   1.0896 |     38.144 |    18.5
   29 |   1.0260 |     35.802 |   1.0815 |     37.306 |    19.1
   30 |   1.0198 |     35.532 |   1.0805 |     37.523 |    19.8
   31 |   1.0045 |     35.003 |   1.0711 |     37.089 |    20.4
   32 |   1.0047 |     35.207 |   1.0718 |     36.965 |    21.1
   33 |   0.9865 |     34.397 |   1.0645 |     36.872 |    21.8
   34 |   0.9736 |     34.011 |   1.0596 |     36.592 |    22.4
   35 |   0.9614 |     33.636 |   1.0444 |     35.816 |    23.1
   36 |   0.9528 |     32.942 |   1.0490 |     35.475 |    23.7
   37 |   0.9421 |     32.639 |   1.0456 |     36.065 |    24.4
   38 |   0.9269 |     31.901 |   1.0350 |     34.978 |    25.1
   39 |   0.9152 |     31.284 |   1.0333 |     35.320 |    25.7
   40 |   0.9020 |     30.700 |   1.0410 |     36.220 |    26.4
   41 |   0.8935 |     30.562 |   1.0151 |     34.358 |    27.0
   42 |   0.8799 |     30.264 |   1.0198 |     34.885 |    27.7
   43 |   0.8803 |     30.061 |   1.0046 |     34.295 |    28.3
   44 |   0.8575 |     29.339 |   1.0100 |     34.109 |    29.0
   45 |   0.8466 |     28.584 |   1.0023 |     33.271 |    29.7
   46 |   0.8337 |     28.105 |   0.9968 |     33.737 |    30.3
   47 |   0.8269 |     28.110 |   0.9954 |     32.402 |    31.0
   48 |   0.8142 |     27.421 |   0.9958 |     33.426 |    31.6
   49 |   0.7991 |     26.810 |   0.9711 |     31.782 |    32.3
   50 |   0.7889 |     26.650 |   0.9827 |     32.651 |    33.0
   51 |   0.7771 |     25.934 |   0.9877 |     32.402 |    33.6
   52 |   0.7777 |     26.397 |   0.9698 |     31.937 |    34.3
   53 |   0.7557 |     25.548 |   0.9590 |     31.564 |    35.0
   54 |   0.7386 |     24.810 |   0.9692 |     31.502 |    35.6
   55 |   0.7261 |     24.215 |   0.9705 |     30.881 |    36.3
   56 |   0.7162 |     23.785 |   0.9555 |     31.254 |    36.9
   57 |   0.7059 |     23.642 |   0.9593 |     30.602 |    37.6
   58 |   0.6939 |     22.975 |   0.9522 |     30.168 |    38.2
   59 |   0.6767 |     21.928 |   0.9611 |     30.726 |    38.9
   60 |   0.6725 |     22.116 |   0.9575 |     29.919 |    39.6
   61 |   0.6619 |     21.846 |   0.9545 |     30.757 |    40.2
   62 |   0.6459 |     21.047 |   0.9516 |     30.230 |    40.9
   63 |   0.6357 |     20.579 |   0.9613 |     30.447 |    41.5
   64 |   0.6263 |     20.457 |   0.9539 |     30.012 |    42.2
   65 |   0.6113 |     19.807 |   0.9689 |     30.168 |    42.9
   66 |   0.6019 |     19.680 |   0.9578 |     30.230 |    43.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 718,658

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2332 |     61.311 |   1.6253 |     46.120 |     0.1
    2 |   1.4937 |     46.055 |   1.4149 |     46.120 |     0.2
    3 |   1.4040 |     45.972 |   1.3686 |     46.120 |     0.3
    4 |   1.3649 |     45.747 |   1.3409 |     46.027 |     0.5
    5 |   1.3304 |     45.383 |   1.3420 |     46.555 |     0.6
    6 |   1.3101 |     45.080 |   1.2954 |     45.531 |     0.7
    7 |   1.2938 |     44.678 |   1.2728 |     44.227 |     0.8
    8 |   1.2689 |     44.055 |   1.2590 |     44.289 |     0.9
    9 |   1.2506 |     43.967 |   1.2501 |     44.444 |     1.0
   10 |   1.2383 |     43.763 |   1.2321 |     43.265 |     1.1
   11 |   1.2213 |     43.107 |   1.2194 |     43.203 |     1.2
   12 |   1.2061 |     42.733 |   1.2166 |     42.893 |     1.4
   13 |   1.1947 |     42.358 |   1.1937 |     42.986 |     1.5
   14 |   1.1819 |     42.028 |   1.1863 |     42.148 |     1.6
   15 |   1.1720 |     42.072 |   1.1804 |     42.241 |     1.7
   16 |   1.1623 |     41.361 |   1.1689 |     41.310 |     1.8
   17 |   1.1531 |     41.438 |   1.1629 |     41.713 |     1.9
   18 |   1.1465 |     41.212 |   1.1536 |     40.689 |     2.0
   19 |   1.1353 |     40.744 |   1.1526 |     40.906 |     2.1
   20 |   1.1265 |     40.193 |   1.1483 |     40.751 |     2.3
   21 |   1.1185 |     40.121 |   1.1378 |     40.658 |     2.4
   22 |   1.1117 |     39.961 |   1.1350 |     40.130 |     2.5
   23 |   1.1031 |     39.813 |   1.1266 |     40.317 |     2.6
   24 |   1.1000 |     39.708 |   1.1229 |     40.813 |     2.7
   25 |   1.0899 |     39.664 |   1.1191 |     39.665 |     2.8
   26 |   1.0834 |     39.047 |   1.1115 |     39.292 |     2.9
   27 |   1.0747 |     38.937 |   1.1107 |     40.068 |     3.0
   28 |   1.0693 |     38.860 |   1.1040 |     39.261 |     3.2
   29 |   1.0666 |     38.871 |   1.0981 |     38.765 |     3.3
   30 |   1.0582 |     38.661 |   1.1009 |     39.479 |     3.4
   31 |   1.0573 |     38.639 |   1.1001 |     39.323 |     3.5
   32 |   1.0486 |     38.226 |   1.0939 |     38.547 |     3.6
   33 |   1.0437 |     38.496 |   1.0871 |     39.168 |     3.7
   34 |   1.0392 |     38.336 |   1.0857 |     38.299 |     3.8
   35 |   1.0375 |     38.138 |   1.0947 |     38.734 |     3.9
   36 |   1.0323 |     37.840 |   1.0766 |     38.268 |     4.1
   37 |   1.0261 |     37.603 |   1.0821 |     38.516 |     4.2
   38 |   1.0248 |     37.686 |   1.0775 |     38.579 |     4.3
   39 |   1.0210 |     37.570 |   1.0782 |     38.827 |     4.4
   40 |   1.0164 |     37.245 |   1.0725 |     38.268 |     4.5
   41 |   1.0126 |     37.168 |   1.0728 |     38.454 |     4.6
   42 |   1.0099 |     37.218 |   1.0658 |     37.896 |     4.7
   43 |   1.0061 |     37.124 |   1.0687 |     37.616 |     4.8
   44 |   1.0011 |     36.716 |   1.0619 |     37.772 |     5.0
   45 |   0.9987 |     36.909 |   1.0682 |     37.834 |     5.1
   46 |   0.9956 |     36.782 |   1.0706 |     37.399 |     5.2
   47 |   0.9922 |     36.606 |   1.0518 |     37.678 |     5.3
   48 |   0.9849 |     36.187 |   1.0491 |     37.616 |     5.4
   49 |   0.9826 |     36.259 |   1.0481 |     37.306 |     5.5
   50 |   0.9789 |     36.061 |   1.0462 |     37.275 |     5.6
   51 |   0.9748 |     36.039 |   1.0429 |     36.903 |     5.8
   52 |   0.9732 |     35.802 |   1.0425 |     37.089 |     5.9
   53 |   0.9697 |     35.741 |   1.0468 |     37.368 |     6.0
   54 |   0.9642 |     35.526 |   1.0390 |     36.903 |     6.1
   55 |   0.9587 |     35.063 |   1.0388 |     36.437 |     6.2
   56 |   0.9546 |     34.865 |   1.0408 |     36.437 |     6.3
   57 |   0.9551 |     34.727 |   1.0347 |     36.002 |     6.4
   58 |   0.9493 |     34.628 |   1.0378 |     36.592 |     6.5
   59 |   0.9449 |     34.430 |   1.0310 |     36.623 |     6.7
   60 |   0.9429 |     34.336 |   1.0339 |     36.189 |     6.8
   61 |   0.9378 |     34.077 |   1.0281 |     35.258 |     6.9
   62 |   0.9336 |     33.846 |   1.0262 |     35.878 |     7.0
   63 |   0.9317 |     33.945 |   1.0177 |     35.537 |     7.1
   64 |   0.9270 |     33.658 |   1.0164 |     35.071 |     7.2
   65 |   0.9231 |     33.344 |   1.0161 |     35.320 |     7.3
   66 |   0.9219 |     33.322 |   1.0222 |     35.940 |     7.4
   67 |   0.9145 |     32.970 |   1.0189 |     35.785 |     7.6
   68 |   0.9114 |     32.826 |   1.0242 |     35.196 |     7.7
   69 |   0.9075 |     32.926 |   1.0116 |     34.978 |     7.8
   70 |   0.9052 |     32.821 |   1.0084 |     34.916 |     7.9
   71 |   0.8983 |     32.138 |   1.0153 |     35.133 |     8.0
   72 |   0.8977 |     32.457 |   1.0132 |     34.978 |     8.1
   73 |   0.8912 |     32.193 |   1.0166 |     35.102 |     8.2
   74 |   0.8861 |     31.730 |   1.0055 |     34.637 |     8.3
   75 |   0.8785 |     31.405 |   1.0025 |     34.637 |     8.5
   76 |   0.8753 |     31.466 |   1.0067 |     34.295 |     8.6
   77 |   0.8686 |     31.201 |   1.0039 |     34.451 |     8.7
   78 |   0.8659 |     30.920 |   0.9987 |     34.947 |     8.8
   79 |   0.8618 |     30.804 |   0.9900 |     34.140 |     8.9
   80 |   0.8563 |     30.711 |   0.9953 |     34.389 |     9.0
   81 |   0.8582 |     30.760 |   1.0051 |     34.792 |     9.1
   82 |   0.8514 |     30.656 |   0.9992 |     33.799 |     9.2
   83 |   0.8421 |     30.375 |   0.9972 |     34.420 |     9.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 2,019,554

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2344 |     61.928 |   1.6076 |     46.120 |     0.1
    2 |   1.4842 |     46.138 |   1.4148 |     46.120 |     0.2
    3 |   1.4095 |     46.160 |   1.3900 |     46.120 |     0.2
    4 |   1.3912 |     46.088 |   1.3690 |     46.120 |     0.3
    5 |   1.3623 |     45.846 |   1.3404 |     45.593 |     0.4
    6 |   1.3360 |     45.609 |   1.3136 |     45.717 |     0.5
    7 |   1.3157 |     44.909 |   1.3012 |     45.220 |     0.6
    8 |   1.2979 |     44.909 |   1.2830 |     44.662 |     0.7
    9 |   1.2805 |     44.534 |   1.2651 |     44.227 |     0.7
   10 |   1.2612 |     44.369 |   1.2564 |     44.507 |     0.8
   11 |   1.2461 |     43.884 |   1.2416 |     43.513 |     0.9
   12 |   1.2333 |     43.631 |   1.2309 |     43.513 |     1.0
   13 |   1.2156 |     43.466 |   1.2179 |     43.327 |     1.1
   14 |   1.2029 |     43.355 |   1.2018 |     43.017 |     1.1
   15 |   1.1839 |     42.452 |   1.1935 |     41.620 |     1.2
   16 |   1.1679 |     41.680 |   1.1766 |     41.496 |     1.3
   17 |   1.1531 |     41.361 |   1.1593 |     41.248 |     1.4
   18 |   1.1344 |     40.231 |   1.1489 |     39.913 |     1.5
   19 |   1.1166 |     39.702 |   1.1376 |     39.199 |     1.5
   20 |   1.1004 |     39.146 |   1.1244 |     38.672 |     1.6
   21 |   1.0831 |     38.386 |   1.1159 |     39.168 |     1.7
   22 |   1.0681 |     37.642 |   1.1125 |     39.727 |     1.8
   23 |   1.0521 |     37.168 |   1.0914 |     37.865 |     1.9
   24 |   1.0318 |     36.391 |   1.0819 |     37.461 |     2.0
   25 |   1.0120 |     35.658 |   1.0754 |     37.741 |     2.0
   26 |   0.9940 |     35.174 |   1.0578 |     36.468 |     2.1
   27 |   0.9749 |     34.165 |   1.0630 |     36.344 |     2.2
   28 |   0.9571 |     33.515 |   1.0550 |     36.096 |     2.3
   29 |   0.9434 |     32.997 |   1.0384 |     35.940 |     2.4
   30 |   0.9145 |     31.978 |   1.0232 |     34.451 |     2.5
   31 |   0.8967 |     31.212 |   1.0217 |     34.947 |     2.5
   32 |   0.8732 |     30.496 |   1.0138 |     34.016 |     2.6
   33 |   0.8530 |     29.846 |   1.0116 |     34.420 |     2.7
   34 |   0.8266 |     28.551 |   0.9982 |     33.333 |     2.8
   35 |   0.8134 |     28.088 |   0.9917 |     32.651 |     2.9
   36 |   0.7855 |     26.799 |   0.9915 |     32.123 |     3.0
   37 |   0.7625 |     26.017 |   0.9907 |     32.371 |     3.0
   38 |   0.7439 |     25.229 |   0.9767 |     31.440 |     3.1
   39 |   0.7155 |     24.408 |   0.9689 |     30.881 |     3.2
   40 |   0.6969 |     23.515 |   0.9741 |     30.850 |     3.3
   41 |   0.6780 |     23.185 |   0.9756 |     30.726 |     3.4
   42 |   0.6477 |     21.791 |   0.9790 |     30.664 |     3.4
   43 |   0.6213 |     20.612 |   0.9750 |     30.788 |     3.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,160,386

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5386 |     66.529 |   1.9702 |     57.200 |     0.3
    2 |   1.7517 |     49.047 |   1.5558 |     46.151 |     0.6
    3 |   1.4985 |     45.978 |   1.4430 |     46.120 |     0.8
    4 |   1.4328 |     46.039 |   1.4039 |     46.120 |     1.1
    5 |   1.4025 |     45.972 |   1.3784 |     46.120 |     1.4
    6 |   1.3790 |     45.928 |   1.3600 |     45.655 |     1.7
    7 |   1.3595 |     45.680 |   1.3414 |     45.469 |     2.0
    8 |   1.3367 |     45.339 |   1.3182 |     45.158 |     2.3
    9 |   1.3184 |     45.140 |   1.3027 |     44.755 |     2.5
   10 |   1.3031 |     44.562 |   1.2920 |     44.662 |     2.8
   11 |   1.2905 |     44.507 |   1.2769 |     44.258 |     3.1
   12 |   1.2768 |     44.182 |   1.2646 |     44.475 |     3.4
   13 |   1.2633 |     44.121 |   1.2567 |     43.606 |     3.7
   14 |   1.2505 |     43.631 |   1.2460 |     43.669 |     3.9
   15 |   1.2418 |     43.438 |   1.2417 |     43.731 |     4.2
   16 |   1.2314 |     43.163 |   1.2341 |     43.389 |     4.5
   17 |   1.2238 |     43.152 |   1.2224 |     43.048 |     4.8
   18 |   1.2155 |     43.019 |   1.2157 |     43.700 |     5.1
   19 |   1.2039 |     42.760 |   1.2065 |     42.706 |     5.4
   20 |   1.1996 |     42.799 |   1.2061 |     43.358 |     5.6
   21 |   1.1925 |     42.320 |   1.1993 |     42.986 |     5.9
   22 |   1.1844 |     42.287 |   1.1938 |     43.048 |     6.2
   23 |   1.1747 |     41.983 |   1.1823 |     41.775 |     6.5
   24 |   1.1670 |     41.631 |   1.1760 |     41.527 |     6.8
   25 |   1.1611 |     41.289 |   1.1727 |     42.427 |     7.0
   26 |   1.1521 |     41.262 |   1.1654 |     41.155 |     7.3
   27 |   1.1454 |     40.722 |   1.1581 |     40.286 |     7.6
   28 |   1.1405 |     40.628 |   1.1562 |     41.155 |     7.9
   29 |   1.1402 |     40.788 |   1.1496 |     40.348 |     8.2
   30 |   1.1275 |     40.386 |   1.1450 |     40.596 |     8.5
   31 |   1.1238 |     40.132 |   1.1443 |     41.279 |     8.7
   32 |   1.1140 |     39.780 |   1.1375 |     40.068 |     9.0
   33 |   1.1087 |     39.427 |   1.1378 |     40.223 |     9.3
   34 |   1.1033 |     39.636 |   1.1274 |     39.789 |     9.6
   35 |   1.0986 |     39.218 |   1.1253 |     40.037 |     9.9
   36 |   1.1035 |     39.510 |   1.1254 |     40.192 |    10.1
   37 |   1.0902 |     39.135 |   1.1226 |     39.758 |    10.4
   38 |   1.0885 |     39.047 |   1.1143 |     39.510 |    10.7
   39 |   1.0779 |     38.545 |   1.1054 |     38.858 |    11.0
   40 |   1.0770 |     38.446 |   1.1080 |     39.385 |    11.3
   41 |   1.0712 |     38.579 |   1.1053 |     39.665 |    11.6
   42 |   1.0630 |     38.215 |   1.0979 |     38.579 |    11.8
   43 |   1.0574 |     37.994 |   1.0948 |     38.641 |    12.1
   44 |   1.0509 |     37.730 |   1.0888 |     38.516 |    12.4
   45 |   1.0468 |     37.702 |   1.0887 |     38.454 |    12.7
   46 |   1.0421 |     37.636 |   1.0860 |     37.492 |    13.0
   47 |   1.0350 |     37.207 |   1.0826 |     37.647 |    13.2
   48 |   1.0314 |     37.339 |   1.0779 |     37.275 |    13.5
   49 |   1.0258 |     36.942 |   1.0769 |     38.051 |    13.8
   50 |   1.0200 |     36.799 |   1.0800 |     37.927 |    14.1
   51 |   1.0178 |     36.369 |   1.0680 |     37.368 |    14.4
   52 |   1.0114 |     36.452 |   1.0699 |     36.965 |    14.6
   53 |   1.0065 |     36.000 |   1.0662 |     37.213 |    14.9
   54 |   1.0014 |     35.868 |   1.0531 |     36.840 |    15.2
   55 |   0.9956 |     35.636 |   1.0600 |     35.878 |    15.5
   56 |   0.9913 |     35.565 |   1.0558 |     37.120 |    15.8
   57 |   0.9898 |     35.543 |   1.0514 |     36.096 |    16.1
   58 |   0.9785 |     35.102 |   1.0496 |     36.716 |    16.3
   59 |   0.9760 |     35.135 |   1.0492 |     36.282 |    16.6
   60 |   0.9701 |     34.700 |   1.0343 |     35.661 |    16.9
   61 |   0.9622 |     34.700 |   1.0342 |     35.289 |    17.2
   62 |   0.9582 |     34.077 |   1.0307 |     36.251 |    17.5
   63 |   0.9505 |     33.934 |   1.0307 |     35.537 |    17.8
   64 |   0.9473 |     34.160 |   1.0331 |     35.537 |    18.0
   65 |   0.9469 |     33.719 |   1.0251 |     35.258 |    18.3
   66 |   0.9332 |     33.080 |   1.0259 |     35.009 |    18.6
   67 |   0.9336 |     33.179 |   1.0154 |     34.420 |    18.9
   68 |   0.9260 |     33.063 |   1.0152 |     34.513 |    19.2
   69 |   0.9195 |     32.298 |   1.0055 |     33.830 |    19.4
   70 |   0.9080 |     32.457 |   1.0080 |     34.575 |    19.7
   71 |   0.9083 |     32.275 |   1.0139 |     34.792 |    20.0
   72 |   0.9020 |     31.813 |   0.9961 |     33.861 |    20.3
   73 |   0.8925 |     31.499 |   1.0018 |     33.923 |    20.6
   74 |   0.8855 |     31.185 |   1.0021 |     33.985 |    20.9
   75 |   0.8815 |     31.207 |   0.9970 |     33.675 |    21.1
   76 |   0.8753 |     31.008 |   0.9984 |     33.271 |    21.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 305,762

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4903 |     67.377 |   1.9182 |     53.973 |     0.0
    2 |   1.7041 |     49.278 |   1.5286 |     46.120 |     0.0
    3 |   1.4828 |     45.972 |   1.4325 |     46.120 |     0.1
    4 |   1.4258 |     46.083 |   1.4004 |     46.120 |     0.1
    5 |   1.4019 |     45.994 |   1.3831 |     46.120 |     0.1
    6 |   1.3826 |     45.851 |   1.3623 |     45.345 |     0.1
    7 |   1.3672 |     45.311 |   1.3497 |     45.469 |     0.1
    8 |   1.3517 |     45.157 |   1.3338 |     45.251 |     0.1
    9 |   1.3349 |     44.981 |   1.3200 |     45.251 |     0.2
   10 |   1.3206 |     44.937 |   1.3061 |     45.189 |     0.2
   11 |   1.3047 |     44.639 |   1.2953 |     45.003 |     0.2
   12 |   1.2897 |     44.545 |   1.2831 |     43.855 |     0.2
   13 |   1.2767 |     44.138 |   1.2735 |     43.637 |     0.2
   14 |   1.2584 |     43.890 |   1.2514 |     43.669 |     0.2
   15 |   1.2453 |     43.174 |   1.2399 |     43.824 |     0.3
   16 |   1.2260 |     42.744 |   1.2250 |     42.893 |     0.3
   17 |   1.2108 |     42.072 |   1.2124 |     41.713 |     0.3
   18 |   1.1960 |     41.719 |   1.1942 |     41.403 |     0.3
   19 |   1.1818 |     41.229 |   1.1900 |     41.092 |     0.3
   20 |   1.1666 |     40.871 |   1.1854 |     41.310 |     0.3
   21 |   1.1535 |     40.176 |   1.1853 |     41.030 |     0.4
   22 |   1.1395 |     39.912 |   1.1627 |     39.851 |     0.4
   23 |   1.1266 |     39.482 |   1.1604 |     40.410 |     0.4
   24 |   1.1117 |     38.882 |   1.1515 |     40.161 |     0.4
   25 |   1.0978 |     38.380 |   1.1452 |     39.261 |     0.4
   26 |   1.0829 |     37.835 |   1.1384 |     39.417 |     0.4
   27 |   1.0678 |     37.306 |   1.1334 |     39.292 |     0.5
   28 |   1.0541 |     36.887 |   1.1207 |     38.144 |     0.5
   29 |   1.0386 |     36.171 |   1.1169 |     38.082 |     0.5
   30 |   1.0258 |     35.752 |   1.1104 |     38.020 |     0.5
   31 |   1.0113 |     34.656 |   1.1146 |     37.741 |     0.5
   32 |   0.9985 |     34.463 |   1.1013 |     37.244 |     0.5
   33 |   0.9802 |     33.736 |   1.0949 |     36.840 |     0.6
   34 |   0.9689 |     33.350 |   1.1009 |     36.903 |     0.6
   35 |   0.9538 |     32.634 |   1.0902 |     36.996 |     0.6
   36 |   0.9412 |     32.220 |   1.1008 |     36.499 |     0.6
   37 |   0.9273 |     32.011 |   1.0868 |     36.375 |     0.6
   38 |   0.9102 |     31.278 |   1.0970 |     35.940 |     0.7
   39 |   0.8978 |     30.843 |   1.0767 |     35.506 |     0.7
   40 |   0.8841 |     30.342 |   1.1013 |     36.406 |     0.7
   41 |   0.8720 |     29.565 |   1.0907 |     34.978 |     0.7
   42 |   0.8555 |     29.333 |   1.0883 |     35.754 |     0.7
   43 |   0.8423 |     28.397 |   1.0880 |     35.196 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 305,762

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4784 |     66.975 |   1.9178 |     53.973 |     0.0
    2 |   1.6974 |     48.242 |   1.5206 |     46.120 |     0.0
    3 |   1.4708 |     45.978 |   1.4219 |     46.493 |     0.0
    4 |   1.4131 |     46.149 |   1.3849 |     46.120 |     0.1
    5 |   1.3810 |     45.846 |   1.3592 |     45.003 |     0.1
    6 |   1.3602 |     45.229 |   1.3544 |     45.655 |     0.1
    7 |   1.3424 |     45.179 |   1.3293 |     45.096 |     0.1
    8 |   1.3304 |     44.700 |   1.3176 |     44.475 |     0.1
    9 |   1.3162 |     44.562 |   1.3129 |     44.879 |     0.1
   10 |   1.3056 |     44.617 |   1.3004 |     44.693 |     0.2
   11 |   1.2938 |     44.303 |   1.2928 |     44.196 |     0.2
   12 |   1.2825 |     44.127 |   1.2907 |     44.320 |     0.2
   13 |   1.2743 |     43.680 |   1.2764 |     43.793 |     0.2
   14 |   1.2584 |     43.240 |   1.2625 |     43.793 |     0.2
   15 |   1.2444 |     42.986 |   1.2503 |     43.606 |     0.2
   16 |   1.2272 |     42.501 |   1.2366 |     42.955 |     0.3
   17 |   1.2112 |     41.972 |   1.2261 |     42.458 |     0.3
   18 |   1.1966 |     41.344 |   1.2121 |     42.489 |     0.3
   19 |   1.1780 |     40.760 |   1.2039 |     42.055 |     0.3
   20 |   1.1617 |     39.961 |   1.1977 |     41.496 |     0.3
   21 |   1.1424 |     39.284 |   1.1882 |     42.210 |     0.3
   22 |   1.1201 |     38.496 |   1.1743 |     40.161 |     0.4
   23 |   1.0991 |     37.581 |   1.1539 |     40.161 |     0.4
   24 |   1.0758 |     36.848 |   1.1518 |     39.106 |     0.4
   25 |   1.0543 |     36.121 |   1.1348 |     38.703 |     0.4
   26 |   1.0280 |     35.157 |   1.1372 |     38.454 |     0.4
   27 |   1.0028 |     33.829 |   1.1286 |     37.523 |     0.4
   28 |   0.9788 |     32.865 |   1.1184 |     36.592 |     0.5
   29 |   0.9546 |     31.736 |   1.1113 |     37.027 |     0.5
   30 |   0.9284 |     30.595 |   1.1132 |     36.220 |     0.5
   31 |   0.9042 |     29.620 |   1.1095 |     36.406 |     0.5
   32 |   0.9040 |     29.884 |   1.1514 |     37.306 |     0.5
   33 |   0.8832 |     29.113 |   1.1037 |     35.413 |     0.5
   34 |   0.8360 |     27.300 |   1.0955 |     35.351 |     0.6
   35 |   0.8091 |     25.906 |   1.1148 |     34.513 |     0.6
   36 |   0.7853 |     25.223 |   1.0975 |     34.451 |     0.6
   37 |   0.7644 |     24.496 |   1.1037 |     34.171 |     0.6
   38 |   0.7399 |     23.598 |   1.1101 |     33.582 |     0.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,589,346

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4072 |     64.149 |   1.8828 |     50.435 |     0.1
    2 |   1.6661 |     47.961 |   1.4890 |     46.089 |     0.2
    3 |   1.4489 |     45.917 |   1.4048 |     45.996 |     0.2
    4 |   1.3969 |     45.752 |   1.3757 |     46.058 |     0.3
    5 |   1.3660 |     45.554 |   1.3498 |     45.065 |     0.4
    6 |   1.3335 |     45.196 |   1.3146 |     45.282 |     0.5
    7 |   1.3028 |     44.479 |   1.2878 |     44.817 |     0.6
    8 |   1.2793 |     43.807 |   1.2688 |     44.103 |     0.7
    9 |   1.2615 |     44.000 |   1.2590 |     43.793 |     0.7
   10 |   1.2441 |     43.444 |   1.2417 |     44.538 |     0.8
   11 |   1.2256 |     42.975 |   1.2208 |     43.451 |     0.9
   12 |   1.2100 |     42.259 |   1.2104 |     43.358 |     1.0
   13 |   1.1952 |     41.961 |   1.2051 |     42.675 |     1.1
   14 |   1.1818 |     41.515 |   1.2047 |     42.768 |     1.1
   15 |   1.1673 |     41.014 |   1.1825 |     41.899 |     1.2
   16 |   1.1533 |     40.650 |   1.1706 |     40.534 |     1.3
   17 |   1.1385 |     39.796 |   1.1620 |     40.255 |     1.4
   18 |   1.1246 |     39.614 |   1.1566 |     40.441 |     1.5
   19 |   1.1152 |     39.174 |   1.1456 |     39.789 |     1.6
   20 |   1.1041 |     38.738 |   1.1422 |     40.379 |     1.6
   21 |   1.0899 |     38.292 |   1.1266 |     39.199 |     1.7
   22 |   1.0816 |     38.099 |   1.1235 |     39.075 |     1.8
   23 |   1.0689 |     37.708 |   1.1287 |     39.572 |     1.9
   24 |   1.0570 |     36.848 |   1.1051 |     38.454 |     2.0
   25 |   1.0459 |     36.661 |   1.0989 |     38.082 |     2.1
   26 |   1.0347 |     36.336 |   1.0941 |     37.803 |     2.1
   27 |   1.0217 |     35.477 |   1.0783 |     36.840 |     2.2
   28 |   1.0030 |     34.821 |   1.0755 |     36.623 |     2.3
   29 |   0.9951 |     34.628 |   1.0593 |     36.034 |     2.4
   30 |   0.9823 |     34.154 |   1.0523 |     35.537 |     2.5
   31 |   0.9711 |     33.730 |   1.0676 |     36.654 |     2.5
   32 |   0.9643 |     33.526 |   1.0460 |     35.785 |     2.6
   33 |   0.9477 |     32.694 |   1.0374 |     35.071 |     2.7
   34 |   0.9313 |     32.088 |   1.0286 |     35.009 |     2.8
   35 |   0.9183 |     31.526 |   1.0203 |     34.668 |     2.9
   36 |   0.9064 |     30.821 |   1.0269 |     34.482 |     3.0
   37 |   0.8923 |     30.209 |   1.0237 |     34.668 |     3.0
   38 |   0.8777 |     29.752 |   1.0067 |     33.985 |     3.1
   39 |   0.8630 |     29.030 |   1.0074 |     33.209 |     3.2
   40 |   0.8475 |     28.926 |   0.9977 |     33.085 |     3.3
   41 |   0.8306 |     27.901 |   0.9936 |     32.495 |     3.4
   42 |   0.8193 |     27.647 |   0.9927 |     32.744 |     3.5
   43 |   0.8056 |     26.612 |   1.0002 |     33.737 |     3.5
   44 |   0.7910 |     26.353 |   0.9849 |     32.930 |     3.6
   45 |   0.7766 |     26.072 |   0.9853 |     33.023 |     3.7
   46 |   0.7578 |     25.168 |   0.9821 |     32.526 |     3.8
   47 |   0.7427 |     24.435 |   0.9713 |     31.968 |     3.9
   48 |   0.7326 |     24.353 |   0.9767 |     32.154 |     3.9
   49 |   0.7117 |     23.477 |   0.9783 |     31.875 |     4.0
   50 |   0.6946 |     22.937 |   0.9801 |     31.471 |     4.1
   51 |   0.6771 |     22.424 |   0.9822 |     31.564 |     4.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,593,698

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2106 |     59.526 |   1.6131 |     48.665 |     0.3
    2 |   1.4858 |     46.116 |   1.4070 |     46.120 |     0.6
    3 |   1.3950 |     46.039 |   1.3653 |     46.120 |     0.9
    4 |   1.3591 |     45.658 |   1.3312 |     45.345 |     1.1
    5 |   1.3337 |     45.653 |   1.3143 |     46.369 |     1.4
    6 |   1.3154 |     45.300 |   1.2949 |     45.003 |     1.7
    7 |   1.2969 |     44.860 |   1.2900 |     44.569 |     2.0
    8 |   1.2809 |     44.595 |   1.2743 |     44.848 |     2.3
    9 |   1.2649 |     44.336 |   1.2608 |     44.134 |     2.6
   10 |   1.2435 |     43.747 |   1.2384 |     43.824 |     2.8
   11 |   1.2216 |     43.339 |   1.2160 |     43.451 |     3.1
   12 |   1.2021 |     42.595 |   1.2029 |     42.334 |     3.4
   13 |   1.1798 |     41.961 |   1.1861 |     41.465 |     3.7
   14 |   1.1587 |     40.865 |   1.1709 |     41.465 |     4.0
   15 |   1.1334 |     39.807 |   1.1543 |     40.317 |     4.3
   16 |   1.1116 |     39.295 |   1.1405 |     39.665 |     4.5
   17 |   1.0908 |     38.711 |   1.1289 |     38.672 |     4.8
   18 |   1.0663 |     37.295 |   1.1106 |     38.796 |     5.1
   19 |   1.0412 |     36.424 |   1.0910 |     37.244 |     5.4
   20 |   1.0184 |     35.394 |   1.0784 |     37.368 |     5.7
   21 |   0.9926 |     34.678 |   1.0804 |     37.834 |     6.0
   22 |   0.9686 |     33.592 |   1.0651 |     35.692 |     6.2
   23 |   0.9463 |     32.485 |   1.0377 |     35.258 |     6.5
   24 |   0.9140 |     31.581 |   1.0335 |     35.071 |     6.8
   25 |   0.8971 |     30.788 |   1.0233 |     34.047 |     7.1
   26 |   0.8624 |     29.185 |   1.0098 |     33.333 |     7.4
   27 |   0.8321 |     27.961 |   1.0112 |     33.023 |     7.7
   28 |   0.8077 |     26.804 |   1.0159 |     32.651 |     7.9
   29 |   0.7800 |     25.835 |   0.9910 |     32.557 |     8.2
   30 |   0.7495 |     24.705 |   0.9809 |     31.564 |     8.5
   31 |   0.7247 |     23.857 |   0.9645 |     31.378 |     8.8
   32 |   0.6912 |     22.893 |   0.9690 |     30.416 |     9.1
   33 |   0.6642 |     21.719 |   0.9678 |     29.609 |     9.4
   34 |   0.6319 |     20.507 |   0.9792 |     30.230 |     9.7
   35 |   0.6076 |     19.730 |   0.9691 |     29.236 |     9.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 732,354

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5079 |     66.590 |   1.9103 |     49.193 |     0.1
    2 |   1.6967 |     47.747 |   1.5294 |     46.120 |     0.1
    3 |   1.4830 |     45.972 |   1.4361 |     46.120 |     0.2
    4 |   1.4282 |     45.972 |   1.4036 |     46.120 |     0.2
    5 |   1.3991 |     46.006 |   1.3806 |     46.120 |     0.3
    6 |   1.3787 |     45.934 |   1.3564 |     46.120 |     0.3
    7 |   1.3574 |     45.736 |   1.3405 |     45.562 |     0.4
    8 |   1.3417 |     45.306 |   1.3272 |     45.127 |     0.4
    9 |   1.3318 |     45.278 |   1.3151 |     45.686 |     0.5
   10 |   1.3187 |     45.025 |   1.3109 |     45.065 |     0.6
   11 |   1.3106 |     44.959 |   1.2979 |     44.693 |     0.6
   12 |   1.3010 |     44.683 |   1.2944 |     44.755 |     0.7
   13 |   1.2918 |     44.380 |   1.2815 |     44.538 |     0.7
   14 |   1.2777 |     44.226 |   1.2743 |     43.793 |     0.8
   15 |   1.2656 |     43.912 |   1.2622 |     44.103 |     0.8
   16 |   1.2537 |     43.884 |   1.2568 |     43.855 |     0.9
   17 |   1.2439 |     43.736 |   1.2488 |     44.196 |     0.9
   18 |   1.2358 |     43.725 |   1.2422 |     44.382 |     1.0
   19 |   1.2259 |     43.361 |   1.2300 |     43.172 |     1.0
   20 |   1.2168 |     42.815 |   1.2236 |     43.017 |     1.1
   21 |   1.2081 |     42.529 |   1.2162 |     43.389 |     1.2
   22 |   1.1987 |     42.193 |   1.2071 |     42.272 |     1.2
   23 |   1.1848 |     41.482 |   1.2003 |     41.962 |     1.3
   24 |   1.1752 |     41.366 |   1.1954 |     42.117 |     1.3
   25 |   1.1610 |     40.920 |   1.1846 |     41.434 |     1.4
   26 |   1.1486 |     40.419 |   1.1793 |     40.751 |     1.4
   27 |   1.1379 |     39.708 |   1.1633 |     40.317 |     1.5
   28 |   1.1164 |     38.992 |   1.1585 |     39.820 |     1.5
   29 |   1.1027 |     38.364 |   1.1491 |     40.223 |     1.6
   30 |   1.0872 |     37.840 |   1.1410 |     39.417 |     1.7
   31 |   1.0700 |     37.317 |   1.1257 |     38.672 |     1.7
   32 |   1.0549 |     36.342 |   1.1392 |     39.292 |     1.8
   33 |   1.0434 |     35.928 |   1.1233 |     38.672 |     1.8
   34 |   1.0217 |     35.190 |   1.1037 |     38.113 |     1.9
   35 |   1.0003 |     34.204 |   1.1231 |     37.616 |     1.9
   36 |   0.9865 |     33.625 |   1.0923 |     36.530 |     2.0
   37 |   0.9652 |     32.760 |   1.0909 |     36.685 |     2.0
   38 |   0.9460 |     32.171 |   1.0793 |     36.375 |     2.1
   39 |   0.9235 |     31.163 |   1.0894 |     36.530 |     2.2
   40 |   0.9031 |     30.292 |   1.0836 |     36.189 |     2.2
   41 |   0.8794 |     29.350 |   1.0741 |     35.382 |     2.3
   42 |   0.8572 |     28.358 |   1.0601 |     34.792 |     2.3
   43 |   0.8381 |     27.824 |   1.0608 |     34.264 |     2.4
   44 |   0.8098 |     26.782 |   1.0658 |     34.575 |     2.4
   45 |   0.7817 |     25.625 |   1.0651 |     34.140 |     2.5
   46 |   0.7605 |     25.025 |   1.0563 |     33.302 |     2.5
   47 |   0.7408 |     24.011 |   1.0800 |     34.078 |     2.6
   48 |   0.7084 |     23.135 |   1.0514 |     32.868 |     2.7
   49 |   0.6801 |     21.961 |   1.0900 |     33.364 |     2.7
   50 |   0.6554 |     20.904 |   1.0752 |     32.495 |     2.8
   51 |   0.6299 |     20.044 |   1.0891 |     32.868 |     2.8
   52 |   0.6035 |     19.135 |   1.0916 |     32.185 |     2.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,069,218

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0027 |     55.377 |   1.4721 |     45.779 |     0.1
    2 |   1.3962 |     45.361 |   1.3402 |     45.158 |     0.1
    3 |   1.3098 |     44.419 |   1.2816 |     44.413 |     0.2
    4 |   1.2657 |     43.802 |   1.2517 |     44.662 |     0.2
    5 |   1.2257 |     42.490 |   1.2263 |     43.637 |     0.3
    6 |   1.1891 |     41.664 |   1.1989 |     42.055 |     0.4
    7 |   1.1528 |     40.275 |   1.1695 |     40.658 |     0.4
    8 |   1.1140 |     38.860 |   1.1436 |     39.913 |     0.5
    9 |   1.0698 |     36.788 |   1.1090 |     37.554 |     0.6
   10 |   1.0255 |     35.157 |   1.0950 |     37.120 |     0.6
   11 |   0.9776 |     33.383 |   1.0537 |     35.164 |     0.7
   12 |   0.9165 |     30.683 |   1.0375 |     34.699 |     0.7
   13 |   0.8625 |     28.634 |   1.0137 |     32.992 |     0.8
   14 |   0.8024 |     26.314 |   0.9858 |     32.092 |     0.9
   15 |   0.7478 |     24.149 |   0.9871 |     31.719 |     0.9
   16 |   0.6970 |     22.672 |   0.9791 |     31.378 |     1.0
   17 |   0.6509 |     21.030 |   0.9664 |     30.695 |     1.1
   18 |   0.6014 |     19.262 |   0.9717 |     29.888 |     1.1
   19 |   0.5546 |     17.427 |   0.9843 |     30.199 |     1.2
   20 |   0.5169 |     16.430 |   0.9920 |     29.050 |     1.2
   21 |   0.4753 |     15.041 |   0.9971 |     29.764 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,657,762

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4092 |     63.730 |   1.8532 |     48.355 |     0.1
    2 |   1.6427 |     47.565 |   1.4837 |     46.151 |     0.2
    3 |   1.4426 |     45.978 |   1.3981 |     45.810 |     0.2
    4 |   1.3772 |     45.791 |   1.3574 |     45.562 |     0.3
    5 |   1.3398 |     44.623 |   1.3254 |     44.755 |     0.4
    6 |   1.3135 |     44.413 |   1.2994 |     44.662 |     0.5
    7 |   1.2883 |     43.956 |   1.2866 |     44.755 |     0.6
    8 |   1.2717 |     43.934 |   1.2697 |     44.134 |     0.6
    9 |   1.2538 |     43.388 |   1.2573 |     44.289 |     0.7
   10 |   1.2427 |     43.355 |   1.2469 |     43.203 |     0.8
   11 |   1.2280 |     42.832 |   1.2415 |     43.420 |     0.9
   12 |   1.2125 |     41.972 |   1.2261 |     42.551 |     0.9
   13 |   1.2003 |     41.774 |   1.2179 |     42.862 |     1.0
   14 |   1.1850 |     41.438 |   1.2163 |     42.489 |     1.1
   15 |   1.1711 |     40.871 |   1.2069 |     42.055 |     1.2
   16 |   1.1555 |     40.309 |   1.1847 |     41.527 |     1.3
   17 |   1.1405 |     39.791 |   1.1793 |     41.682 |     1.3
   18 |   1.1245 |     39.047 |   1.1672 |     40.441 |     1.4
   19 |   1.1086 |     38.656 |   1.1632 |     40.596 |     1.5
   20 |   1.0916 |     38.050 |   1.1560 |     39.913 |     1.6
   21 |   1.0745 |     36.920 |   1.1353 |     38.579 |     1.6
   22 |   1.0513 |     35.923 |   1.1242 |     38.361 |     1.7
   23 |   1.0262 |     34.942 |   1.1069 |     38.020 |     1.8
   24 |   1.0097 |     34.457 |   1.0985 |     37.616 |     1.9
   25 |   0.9834 |     33.466 |   1.0928 |     37.772 |     2.0
   26 |   0.9641 |     32.738 |   1.0811 |     37.058 |     2.0
   27 |   0.9457 |     32.099 |   1.0917 |     37.647 |     2.1
   28 |   0.9283 |     31.394 |   1.0743 |     36.872 |     2.2
   29 |   0.9020 |     30.309 |   1.0521 |     35.723 |     2.3
   30 |   0.8796 |     29.383 |   1.0352 |     35.009 |     2.4
   31 |   0.8576 |     28.534 |   1.0369 |     34.544 |     2.4
   32 |   0.8338 |     27.769 |   1.0226 |     34.544 |     2.5
   33 |   0.8089 |     26.628 |   1.0332 |     35.164 |     2.6
   34 |   0.7866 |     25.906 |   1.0194 |     33.116 |     2.7
   35 |   0.7615 |     24.860 |   1.0124 |     34.047 |     2.7
   36 |   0.7376 |     23.829 |   1.0214 |     32.651 |     2.8
   37 |   0.7116 |     23.003 |   1.0187 |     32.837 |     2.9
   38 |   0.6895 |     22.275 |   1.0328 |     32.464 |     3.0
   39 |   0.6627 |     21.140 |   1.0181 |     31.906 |     3.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 903,522

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2215 |     60.810 |   1.6006 |     46.120 |     0.3
    2 |   1.4843 |     46.017 |   1.4110 |     46.120 |     0.7
    3 |   1.4018 |     46.110 |   1.3699 |     46.120 |     1.0
    4 |   1.3644 |     45.658 |   1.3318 |     45.438 |     1.3
    5 |   1.3317 |     45.438 |   1.3127 |     45.469 |     1.6
    6 |   1.3131 |     44.882 |   1.2993 |     45.065 |     2.0
    7 |   1.2944 |     44.766 |   1.2876 |     44.662 |     2.3
    8 |   1.2757 |     44.534 |   1.2655 |     44.631 |     2.6
    9 |   1.2575 |     44.121 |   1.2516 |     43.948 |     2.9
   10 |   1.2376 |     43.339 |   1.2306 |     42.862 |     3.3
   11 |   1.2171 |     42.661 |   1.2142 |     43.575 |     3.6
   12 |   1.1975 |     42.320 |   1.1982 |     41.651 |     3.9
   13 |   1.1702 |     41.058 |   1.1835 |     41.930 |     4.3
   14 |   1.1487 |     40.612 |   1.1616 |     40.658 |     4.6
   15 |   1.1201 |     39.587 |   1.1358 |     39.913 |     4.9
   16 |   1.0952 |     38.595 |   1.1304 |     39.665 |     5.2
   17 |   1.0739 |     37.818 |   1.1129 |     38.858 |     5.6
   18 |   1.0530 |     36.876 |   1.1102 |     38.485 |     5.9
   19 |   1.0261 |     36.000 |   1.0761 |     37.616 |     6.2
   20 |   1.0026 |     34.722 |   1.0685 |     36.561 |     6.6
   21 |   0.9777 |     34.099 |   1.0526 |     35.289 |     6.9
   22 |   0.9522 |     32.975 |   1.0446 |     35.040 |     7.2
   23 |   0.9253 |     32.105 |   1.0383 |     35.009 |     7.6
   24 |   0.9059 |     31.212 |   1.0249 |     34.699 |     7.9
   25 |   0.8755 |     29.675 |   1.0201 |     34.202 |     8.2
   26 |   0.8541 |     29.069 |   1.0251 |     33.333 |     8.5
   27 |   0.8321 |     28.309 |   1.0106 |     33.085 |     8.9
   28 |   0.8054 |     27.152 |   1.0158 |     33.271 |     9.2
   29 |   0.7815 |     26.248 |   1.0054 |     32.961 |     9.5
   30 |   0.7481 |     24.887 |   1.0003 |     31.564 |     9.8
   31 |   0.7280 |     24.369 |   1.0041 |     31.813 |    10.2
   32 |   0.7020 |     23.317 |   1.0020 |     32.092 |    10.5
   33 |   0.6788 |     22.551 |   0.9958 |     31.409 |    10.8
   34 |   0.6557 |     21.857 |   0.9989 |     31.316 |    11.2
   35 |   0.6312 |     20.804 |   1.0035 |     30.416 |    11.5
   36 |   0.6115 |     19.857 |   1.0218 |     30.788 |    11.8
   37 |   0.6004 |     19.846 |   1.0118 |     30.633 |    12.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 306,498

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5068 |     65.813 |   1.9254 |     55.245 |     0.0
    2 |   1.7083 |     49.091 |   1.5333 |     46.120 |     0.0
    3 |   1.4770 |     45.972 |   1.4258 |     46.120 |     0.0
    4 |   1.4122 |     45.972 |   1.3866 |     45.841 |     0.1
    5 |   1.3745 |     45.559 |   1.3533 |     45.438 |     0.1
    6 |   1.3524 |     45.240 |   1.3454 |     45.810 |     0.1
    7 |   1.3313 |     44.997 |   1.3181 |     44.693 |     0.1
    8 |   1.3119 |     44.821 |   1.3011 |     44.538 |     0.1
    9 |   1.2954 |     44.303 |   1.2921 |     44.569 |     0.1
   10 |   1.2809 |     44.094 |   1.2729 |     43.917 |     0.2
   11 |   1.2619 |     43.725 |   1.2637 |     43.606 |     0.2
   12 |   1.2452 |     43.262 |   1.2527 |     43.513 |     0.2
   13 |   1.2319 |     43.047 |   1.2441 |     44.134 |     0.2
   14 |   1.2143 |     42.694 |   1.2213 |     43.700 |     0.2
   15 |   1.1959 |     42.160 |   1.2058 |     42.644 |     0.2
   16 |   1.1779 |     40.915 |   1.1909 |     41.310 |     0.3
   17 |   1.1575 |     40.298 |   1.1865 |     41.651 |     0.3
   18 |   1.1401 |     39.846 |   1.1741 |     41.403 |     0.3
   19 |   1.1225 |     39.091 |   1.1753 |     41.279 |     0.3
   20 |   1.1082 |     38.413 |   1.1598 |     40.906 |     0.3
   21 |   1.0872 |     37.587 |   1.1494 |     40.099 |     0.3
   22 |   1.0705 |     36.865 |   1.1310 |     39.323 |     0.4
   23 |   1.0508 |     36.303 |   1.1259 |     38.703 |     0.4
   24 |   1.0358 |     35.521 |   1.1231 |     37.865 |     0.4
   25 |   1.0219 |     35.052 |   1.1084 |     37.554 |     0.4
   26 |   1.0009 |     34.182 |   1.1125 |     37.741 |     0.4
   27 |   0.9780 |     33.146 |   1.1044 |     37.865 |     0.4
   28 |   0.9623 |     32.826 |   1.1147 |     37.523 |     0.4
   29 |   0.9598 |     32.937 |   1.0887 |     36.934 |     0.5
   30 |   0.9245 |     31.559 |   1.1007 |     37.585 |     0.5
   31 |   0.9083 |     30.733 |   1.0953 |     36.561 |     0.5
   32 |   0.8915 |     30.000 |   1.1022 |     37.151 |     0.5
   33 |   0.8712 |     29.554 |   1.1036 |     36.561 |     0.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 718,658

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2371 |     61.366 |   1.6153 |     46.120 |     0.2
    2 |   1.4876 |     46.127 |   1.4094 |     46.120 |     0.4
    3 |   1.4005 |     45.994 |   1.3649 |     46.120 |     0.6
    4 |   1.3653 |     45.482 |   1.3391 |     46.027 |     0.8
    5 |   1.3377 |     45.565 |   1.3226 |     45.251 |     1.0
    6 |   1.3111 |     45.306 |   1.2969 |     45.065 |     1.1
    7 |   1.2923 |     44.887 |   1.2816 |     44.693 |     1.3
    8 |   1.2742 |     44.375 |   1.2647 |     43.979 |     1.5
    9 |   1.2532 |     43.890 |   1.2441 |     43.327 |     1.7
   10 |   1.2355 |     43.289 |   1.2303 |     43.358 |     1.9
   11 |   1.2208 |     42.882 |   1.2154 |     42.458 |     2.1
   12 |   1.2079 |     42.865 |   1.2083 |     43.358 |     2.3
   13 |   1.1989 |     42.579 |   1.1989 |     42.272 |     2.5
   14 |   1.1853 |     42.215 |   1.1879 |     41.589 |     2.7
   15 |   1.1792 |     41.994 |   1.1778 |     42.241 |     2.9
   16 |   1.1687 |     41.796 |   1.1795 |     42.737 |     3.1
   17 |   1.1618 |     41.598 |   1.1700 |     41.620 |     3.2
   18 |   1.1562 |     41.587 |   1.1642 |     42.086 |     3.4
   19 |   1.1470 |     41.102 |   1.1577 |     42.365 |     3.6
   20 |   1.1417 |     41.063 |   1.1481 |     41.527 |     3.8
   21 |   1.1356 |     40.810 |   1.1487 |     41.930 |     4.0
   22 |   1.1306 |     40.793 |   1.1414 |     41.030 |     4.2
   23 |   1.1254 |     40.595 |   1.1384 |     40.906 |     4.4
   24 |   1.1182 |     40.562 |   1.1324 |     40.534 |     4.6
   25 |   1.1131 |     39.967 |   1.1271 |     40.720 |     4.8
   26 |   1.1085 |     40.154 |   1.1259 |     40.720 |     5.0
   27 |   1.1041 |     39.796 |   1.1174 |     39.727 |     5.2
   28 |   1.0983 |     39.713 |   1.1291 |     39.882 |     5.4
   29 |   1.0913 |     39.741 |   1.1209 |     40.223 |     5.6
   30 |   1.0859 |     39.306 |   1.1078 |     39.479 |     5.7
   31 |   1.0780 |     39.262 |   1.1041 |     39.168 |     5.9
   32 |   1.0727 |     39.074 |   1.0996 |     39.323 |     6.1
   33 |   1.0652 |     38.667 |   1.1029 |     39.820 |     6.3
   34 |   1.0609 |     38.634 |   1.0914 |     38.672 |     6.5
   35 |   1.0542 |     38.490 |   1.0852 |     38.051 |     6.7
   36 |   1.0483 |     38.138 |   1.0815 |     37.927 |     6.9
   37 |   1.0434 |     37.592 |   1.0771 |     38.392 |     7.1
   38 |   1.0361 |     37.427 |   1.0742 |     37.337 |     7.3
   39 |   1.0328 |     37.521 |   1.0702 |     37.058 |     7.5
   40 |   1.0251 |     37.124 |   1.0617 |     36.809 |     7.7
   41 |   1.0197 |     36.749 |   1.0590 |     36.872 |     7.8
   42 |   1.0128 |     36.474 |   1.0647 |     37.989 |     8.0
   43 |   1.0118 |     36.512 |   1.0571 |     36.934 |     8.2
   44 |   1.0030 |     36.419 |   1.0575 |     37.461 |     8.4
   45 |   0.9999 |     35.923 |   1.0536 |     37.275 |     8.6
   46 |   0.9920 |     36.033 |   1.0583 |     37.523 |     8.8
   47 |   0.9849 |     35.747 |   1.0437 |     36.499 |     9.0
   48 |   0.9830 |     35.526 |   1.0469 |     36.965 |     9.2
   49 |   0.9716 |     35.107 |   1.0306 |     36.127 |     9.4
   50 |   0.9643 |     34.606 |   1.0337 |     35.971 |     9.6
   51 |   0.9651 |     34.612 |   1.0215 |     35.071 |     9.8
   52 |   0.9564 |     34.237 |   1.0272 |     36.623 |     9.9
   53 |   0.9475 |     33.697 |   1.0213 |     35.785 |    10.1
   54 |   0.9412 |     33.686 |   1.0194 |     35.506 |    10.3
   55 |   0.9405 |     33.769 |   1.0165 |     34.854 |    10.5
   56 |   0.9242 |     33.008 |   1.0038 |     34.637 |    10.7
   57 |   0.9254 |     32.948 |   1.0203 |     35.692 |    10.9
   58 |   0.9180 |     32.711 |   1.0115 |     35.071 |    11.1
   59 |   0.9152 |     32.496 |   1.0008 |     34.451 |    11.3
   60 |   0.9036 |     32.105 |   1.0067 |     34.482 |    11.5
   61 |   0.8991 |     31.736 |   0.9936 |     34.823 |    11.7
   62 |   0.8894 |     31.394 |   0.9909 |     33.520 |    11.9
   63 |   0.8904 |     31.256 |   0.9930 |     34.078 |    12.0
   64 |   0.8854 |     31.201 |   0.9948 |     33.861 |    12.2
   65 |   0.8862 |     31.554 |   1.0151 |     35.506 |    12.4
   66 |   0.8807 |     31.207 |   0.9952 |     34.295 |    12.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 504,642

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4223 |     64.314 |   1.8508 |     49.379 |     0.3
    2 |   1.6281 |     47.504 |   1.4680 |     45.903 |     0.6
    3 |   1.4244 |     45.758 |   1.3772 |     45.686 |     1.0
    4 |   1.3537 |     45.135 |   1.3292 |     44.817 |     1.3
    5 |   1.3157 |     44.606 |   1.2983 |     44.507 |     1.6
    6 |   1.2874 |     43.934 |   1.2746 |     43.824 |     1.9
    7 |   1.2662 |     43.989 |   1.2534 |     43.172 |     2.3
    8 |   1.2465 |     43.267 |   1.2419 |     43.482 |     2.6
    9 |   1.2295 |     42.837 |   1.2205 |     42.272 |     2.9
   10 |   1.2110 |     42.248 |   1.2055 |     42.024 |     3.2
   11 |   1.1948 |     41.455 |   1.2002 |     41.434 |     3.6
   12 |   1.1794 |     41.317 |   1.1854 |     41.186 |     3.9
   13 |   1.1645 |     40.722 |   1.1788 |     40.782 |     4.2
   14 |   1.1525 |     40.209 |   1.1687 |     39.882 |     4.5
   15 |   1.1363 |     39.873 |   1.1614 |     40.037 |     4.9
   16 |   1.1239 |     39.372 |   1.1515 |     40.099 |     5.2
   17 |   1.1112 |     38.937 |   1.1432 |     39.820 |     5.5
   18 |   1.0976 |     38.556 |   1.1277 |     39.261 |     5.8
   19 |   1.0828 |     37.961 |   1.1278 |     39.230 |     6.1
   20 |   1.0720 |     37.499 |   1.1151 |     38.051 |     6.5
   21 |   1.0567 |     36.981 |   1.1018 |     38.206 |     6.8
   22 |   1.0459 |     36.463 |   1.0951 |     37.865 |     7.1
   23 |   1.0367 |     36.204 |   1.0931 |     38.237 |     7.4
   24 |   1.0246 |     35.967 |   1.0830 |     37.430 |     7.8
   25 |   1.0136 |     35.427 |   1.0830 |     38.144 |     8.1
   26 |   1.0048 |     35.113 |   1.0670 |     36.809 |     8.4
   27 |   0.9907 |     34.419 |   1.0594 |     36.406 |     8.7
   28 |   0.9807 |     34.386 |   1.0599 |     35.692 |     9.1
   29 |   0.9734 |     34.022 |   1.0576 |     36.002 |     9.4
   30 |   0.9656 |     33.653 |   1.0494 |     35.599 |     9.7
   31 |   0.9533 |     33.499 |   1.0456 |     35.506 |    10.0
   32 |   0.9467 |     33.124 |   1.0383 |     35.413 |    10.4
   33 |   0.9310 |     32.590 |   1.0387 |     35.506 |    10.7
   34 |   0.9285 |     32.259 |   1.0362 |     35.009 |    11.0
   35 |   0.9189 |     32.121 |   1.0256 |     34.171 |    11.3
   36 |   0.9051 |     31.576 |   1.0250 |     34.078 |    11.6
   37 |   0.9006 |     31.135 |   1.0130 |     34.016 |    12.0
   38 |   0.8885 |     30.656 |   1.0178 |     34.513 |    12.3
   39 |   0.8851 |     30.595 |   1.0195 |     34.327 |    12.6
   40 |   0.8712 |     30.154 |   1.0083 |     33.985 |    12.9
   41 |   0.8620 |     29.708 |   1.0089 |     33.861 |    13.3
   42 |   0.8554 |     29.466 |   0.9959 |     32.744 |    13.6
   43 |   0.8393 |     28.534 |   0.9855 |     32.464 |    13.9
   44 |   0.8254 |     28.143 |   0.9915 |     33.240 |    14.2
   45 |   0.8119 |     27.603 |   0.9865 |     32.682 |    14.6
   46 |   0.8001 |     27.460 |   0.9794 |     32.154 |    14.9
   47 |   0.7916 |     26.942 |   0.9826 |     32.744 |    15.2
   48 |   0.7802 |     26.380 |   0.9814 |     31.968 |    15.5
   49 |   0.7696 |     25.625 |   0.9707 |     32.061 |    15.8
   50 |   0.7548 |     25.466 |   0.9779 |     31.130 |    16.2
   51 |   0.7437 |     25.003 |   0.9740 |     31.626 |    16.5
   52 |   0.7327 |     24.716 |   0.9614 |     31.875 |    16.8
   53 |   0.7193 |     23.840 |   0.9786 |     31.037 |    17.1
   54 |   0.7045 |     23.499 |   0.9597 |     29.981 |    17.5
   55 |   0.6976 |     23.025 |   0.9553 |     29.702 |    17.8
   56 |   0.6833 |     22.766 |   0.9576 |     30.292 |    18.1
   57 |   0.6645 |     22.061 |   0.9787 |     30.230 |    18.4
   58 |   0.6557 |     21.725 |   0.9605 |     30.292 |    18.8
   59 |   0.6464 |     21.185 |   0.9760 |     30.385 |    19.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 868,578

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1849 |     59.581 |   1.5724 |     46.120 |     0.3
    2 |   1.4685 |     46.292 |   1.4156 |     46.120 |     0.6
    3 |   1.4054 |     45.989 |   1.3770 |     46.493 |     0.9
    4 |   1.3760 |     45.939 |   1.3511 |     45.810 |     1.3
    5 |   1.3505 |     45.212 |   1.3312 |     45.748 |     1.6
    6 |   1.3300 |     45.036 |   1.3093 |     45.313 |     1.9
    7 |   1.3079 |     44.964 |   1.2901 |     44.662 |     2.2
    8 |   1.2860 |     44.386 |   1.2743 |     45.003 |     2.5
    9 |   1.2664 |     44.055 |   1.2568 |     44.072 |     2.8
   10 |   1.2446 |     43.399 |   1.2447 |     44.165 |     3.2
   11 |   1.2263 |     43.052 |   1.2277 |     42.737 |     3.5
   12 |   1.2083 |     42.540 |   1.2091 |     42.520 |     3.8
   13 |   1.1869 |     41.890 |   1.1928 |     42.179 |     4.1
   14 |   1.1635 |     41.339 |   1.1831 |     41.837 |     4.4
   15 |   1.1414 |     40.595 |   1.1668 |     41.155 |     4.8
   16 |   1.1235 |     39.862 |   1.1595 |     40.503 |     5.1
   17 |   1.0986 |     39.003 |   1.1373 |     39.665 |     5.4
   18 |   1.0735 |     37.758 |   1.1376 |     39.975 |     5.7
   19 |   1.0495 |     36.485 |   1.1233 |     39.510 |     6.0
   20 |   1.0163 |     35.344 |   1.1067 |     38.237 |     6.3
   21 |   0.9870 |     33.956 |   1.1010 |     37.120 |     6.7
   22 |   0.9589 |     32.970 |   1.0769 |     37.182 |     7.0
   23 |   0.9240 |     31.410 |   1.0616 |     35.475 |     7.3
   24 |   0.8884 |     30.022 |   1.0541 |     35.258 |     7.6
   25 |   0.8558 |     28.628 |   1.0520 |     34.885 |     7.9
   26 |   0.8204 |     27.399 |   1.0408 |     33.551 |     8.2
   27 |   0.7884 |     26.490 |   1.0413 |     33.644 |     8.6
   28 |   0.7565 |     24.848 |   1.0406 |     33.240 |     8.9
   29 |   0.7283 |     23.961 |   1.0410 |     32.713 |     9.2
   30 |   0.6912 |     22.782 |   1.0262 |     32.216 |     9.5
   31 |   0.6629 |     21.344 |   1.0444 |     32.123 |     9.8
   32 |   0.6296 |     20.402 |   1.0405 |     31.192 |    10.2
   33 |   0.6045 |     19.460 |   1.0526 |     31.409 |    10.5
   34 |   0.5744 |     18.523 |   1.0350 |     30.633 |    10.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 519,362

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5473 |     67.113 |   1.9873 |     58.287 |     0.3
    2 |   1.7137 |     48.871 |   1.5232 |     46.151 |     0.6
    3 |   1.4713 |     45.972 |   1.4263 |     46.151 |     0.9
    4 |   1.4128 |     45.934 |   1.3909 |     46.151 |     1.2
    5 |   1.3847 |     45.873 |   1.3674 |     45.934 |     1.6
    6 |   1.3684 |     45.725 |   1.3514 |     45.593 |     1.9
    7 |   1.3511 |     45.554 |   1.3371 |     45.251 |     2.2
    8 |   1.3235 |     44.986 |   1.3130 |     44.786 |     2.5
    9 |   1.3034 |     44.898 |   1.2932 |     44.879 |     2.8
   10 |   1.2873 |     44.369 |   1.2783 |     43.762 |     3.1
   11 |   1.2691 |     44.055 |   1.2623 |     43.700 |     3.4
   12 |   1.2530 |     43.592 |   1.2484 |     43.358 |     3.7
   13 |   1.2351 |     43.174 |   1.2395 |     43.420 |     4.0
   14 |   1.2227 |     43.030 |   1.2236 |     43.327 |     4.4
   15 |   1.2093 |     42.755 |   1.2145 |     43.513 |     4.7
   16 |   1.1995 |     42.270 |   1.2046 |     42.924 |     5.0
   17 |   1.1884 |     41.725 |   1.1951 |     42.303 |     5.3
   18 |   1.1789 |     41.515 |   1.1926 |     42.520 |     5.6
   19 |   1.1712 |     41.449 |   1.1921 |     43.420 |     5.9
   20 |   1.1643 |     41.212 |   1.1811 |     41.930 |     6.2
   21 |   1.1580 |     41.113 |   1.1788 |     42.241 |     6.5
   22 |   1.1468 |     40.810 |   1.1644 |     41.124 |     6.9
   23 |   1.1401 |     40.639 |   1.1639 |     41.434 |     7.2
   24 |   1.1362 |     40.512 |   1.1586 |     41.310 |     7.5
   25 |   1.1287 |     40.264 |   1.1608 |     41.217 |     7.8
   26 |   1.1213 |     40.171 |   1.1468 |     40.937 |     8.1
   27 |   1.1174 |     39.780 |   1.1482 |     40.906 |     8.4
   28 |   1.1075 |     39.581 |   1.1480 |     40.286 |     8.7
   29 |   1.1060 |     39.543 |   1.1428 |     40.999 |     9.0
   30 |   1.0970 |     39.306 |   1.1336 |     39.975 |     9.3
   31 |   1.0895 |     39.019 |   1.1351 |     40.255 |     9.6
   32 |   1.0872 |     38.937 |   1.1285 |     40.161 |    10.0
   33 |   1.0788 |     38.496 |   1.1260 |     39.572 |    10.3
   34 |   1.0756 |     38.353 |   1.1166 |     39.354 |    10.6
   35 |   1.0670 |     38.309 |   1.1158 |     39.168 |    10.9
   36 |   1.0619 |     38.088 |   1.1194 |     39.479 |    11.2
   37 |   1.0558 |     37.857 |   1.1249 |     39.665 |    11.5
   38 |   1.0507 |     37.636 |   1.1094 |     39.541 |    11.8
   39 |   1.0445 |     37.466 |   1.1013 |     38.827 |    12.1
   40 |   1.0368 |     36.942 |   1.0988 |     38.827 |    12.4
   41 |   1.0314 |     36.612 |   1.1026 |     39.137 |    12.8
   42 |   1.0263 |     36.529 |   1.0976 |     38.796 |    13.1
   43 |   1.0164 |     36.259 |   1.0887 |     38.144 |    13.4
   44 |   1.0135 |     36.149 |   1.0877 |     38.361 |    13.7
   45 |   1.0114 |     36.402 |   1.0854 |     37.585 |    14.0
   46 |   1.0009 |     35.818 |   1.0718 |     37.461 |    14.3
   47 |   0.9941 |     35.482 |   1.0815 |     38.051 |    14.6
   48 |   0.9891 |     35.532 |   1.0708 |     37.461 |    14.9
   49 |   0.9833 |     34.865 |   1.0630 |     36.685 |    15.2
   50 |   0.9744 |     34.347 |   1.0721 |     36.996 |    15.6
   51 |   0.9676 |     34.099 |   1.0749 |     37.337 |    15.9
   52 |   0.9651 |     34.033 |   1.0578 |     37.089 |    16.2
   53 |   0.9548 |     33.664 |   1.0612 |     36.996 |    16.5
   54 |   0.9520 |     33.444 |   1.0476 |     36.158 |    16.8
   55 |   0.9430 |     33.256 |   1.0469 |     35.785 |    17.1
   56 |   0.9351 |     32.826 |   1.0350 |     35.506 |    17.4
   57 |   0.9312 |     32.441 |   1.0468 |     35.506 |    17.7
   58 |   0.9236 |     32.193 |   1.0311 |     35.413 |    18.0
   59 |   0.9229 |     32.545 |   1.0340 |     35.351 |    18.3
   60 |   0.9110 |     31.725 |   1.0428 |     35.475 |    18.7
   61 |   0.9060 |     31.840 |   1.0229 |     34.513 |    19.0
   62 |   0.8973 |     31.201 |   1.0265 |     35.040 |    19.3
   63 |   0.8893 |     30.964 |   1.0212 |     34.264 |    19.6
   64 |   0.8862 |     30.727 |   1.0157 |     34.109 |    19.9
   65 |   0.8764 |     30.540 |   1.0319 |     35.568 |    20.2
   66 |   0.8718 |     30.303 |   1.0160 |     34.202 |    20.5
   67 |   0.8706 |     30.314 |   1.0140 |     34.389 |    20.8
   68 |   0.8546 |     29.548 |   1.0104 |     34.202 |    21.1
   69 |   0.8455 |     29.074 |   1.0134 |     33.582 |    21.4
   70 |   0.8386 |     28.837 |   1.0184 |     34.358 |    21.7
   71 |   0.8309 |     28.623 |   0.9960 |     32.837 |    22.1
   72 |   0.8227 |     28.281 |   1.0097 |     33.395 |    22.4
   73 |   0.8199 |     28.110 |   0.9997 |     33.551 |    22.7
   74 |   0.8096 |     27.587 |   1.0135 |     33.954 |    23.0
   75 |   0.7986 |     27.124 |   1.0025 |     33.271 |    23.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 966,594

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0184 |     55.642 |   1.4776 |     45.903 |     0.0
    2 |   1.3993 |     45.405 |   1.3379 |     44.196 |     0.1
    3 |   1.3145 |     44.187 |   1.2871 |     43.824 |     0.1
    4 |   1.2722 |     43.807 |   1.2562 |     43.762 |     0.1
    5 |   1.2412 |     42.964 |   1.2246 |     42.582 |     0.2
    6 |   1.2055 |     41.978 |   1.2043 |     41.837 |     0.2
    7 |   1.1734 |     41.113 |   1.1761 |     41.465 |     0.3
    8 |   1.1404 |     39.901 |   1.1552 |     40.658 |     0.3
    9 |   1.1061 |     38.920 |   1.1327 |     40.255 |     0.3
   10 |   1.0738 |     37.372 |   1.1111 |     38.920 |     0.4
   11 |   1.0365 |     35.884 |   1.1012 |     38.237 |     0.4
   12 |   0.9975 |     34.198 |   1.0751 |     36.934 |     0.5
   13 |   0.9564 |     32.507 |   1.0463 |     35.630 |     0.5
   14 |   0.9109 |     30.760 |   1.0124 |     33.892 |     0.5
   15 |   0.8732 |     29.471 |   1.0051 |     33.364 |     0.6
   16 |   0.8305 |     28.022 |   0.9809 |     32.495 |     0.6
   17 |   0.7845 |     26.193 |   0.9814 |     31.254 |     0.6
   18 |   0.7416 |     24.507 |   0.9599 |     31.099 |     0.7
   19 |   0.6952 |     22.623 |   0.9692 |     31.223 |     0.7
   20 |   0.6542 |     21.554 |   0.9683 |     30.292 |     0.8
   21 |   0.6139 |     19.824 |   0.9570 |     30.354 |     0.8
   22 |   0.5847 |     18.992 |   0.9537 |     30.199 |     0.8
   23 |   0.5379 |     17.278 |   0.9638 |     28.523 |     0.9
   24 |   0.5073 |     16.231 |   0.9633 |     28.678 |     0.9
   25 |   0.4687 |     14.832 |   0.9748 |     28.895 |     1.0
   26 |   0.4424 |     13.956 |   0.9952 |     28.150 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,691,042

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5103 |     66.876 |   1.9553 |     58.814 |     0.2
    2 |   1.7526 |     50.000 |   1.5674 |     46.120 |     0.4
    3 |   1.5060 |     45.972 |   1.4441 |     46.120 |     0.7
    4 |   1.4347 |     46.017 |   1.4069 |     46.120 |     0.9
    5 |   1.4069 |     46.011 |   1.3917 |     46.120 |     1.1
    6 |   1.3877 |     45.884 |   1.3629 |     45.003 |     1.3
    7 |   1.3719 |     45.284 |   1.3545 |     45.469 |     1.5
    8 |   1.3606 |     45.163 |   1.3422 |     45.345 |     1.7
    9 |   1.3482 |     45.174 |   1.3305 |     45.593 |     2.0
   10 |   1.3341 |     45.201 |   1.3230 |     45.469 |     2.2
   11 |   1.3219 |     45.262 |   1.3082 |     45.158 |     2.4
   12 |   1.3103 |     45.223 |   1.2959 |     45.096 |     2.6
   13 |   1.2943 |     44.545 |   1.2821 |     44.444 |     2.8
   14 |   1.2813 |     44.391 |   1.2709 |     44.631 |     3.0
   15 |   1.2671 |     44.044 |   1.2614 |     44.227 |     3.3
   16 |   1.2527 |     43.614 |   1.2485 |     44.072 |     3.5
   17 |   1.2359 |     43.107 |   1.2390 |     43.637 |     3.7
   18 |   1.2232 |     42.893 |   1.2280 |     43.575 |     3.9
   19 |   1.2132 |     42.275 |   1.2200 |     42.613 |     4.1
   20 |   1.1971 |     41.559 |   1.2166 |     42.272 |     4.3
   21 |   1.1886 |     41.262 |   1.2047 |     42.365 |     4.6
   22 |   1.1721 |     40.915 |   1.1960 |     41.030 |     4.8
   23 |   1.1589 |     39.989 |   1.1857 |     41.310 |     5.0
   24 |   1.1434 |     39.377 |   1.1791 |     40.813 |     5.2
   25 |   1.1265 |     38.964 |   1.1678 |     40.410 |     5.4
   26 |   1.1126 |     38.457 |   1.1577 |     39.758 |     5.6
   27 |   1.0994 |     37.818 |   1.1567 |     39.199 |     5.9
   28 |   1.0798 |     37.052 |   1.1481 |     39.075 |     6.1
   29 |   1.0597 |     36.253 |   1.1423 |     38.703 |     6.3
   30 |   1.0443 |     35.669 |   1.1342 |     38.361 |     6.5
   31 |   1.0307 |     35.140 |   1.1237 |     37.927 |     6.7
   32 |   1.0124 |     34.364 |   1.1185 |     38.113 |     6.9
   33 |   0.9914 |     33.620 |   1.1198 |     37.958 |     7.2
   34 |   0.9725 |     33.058 |   1.1029 |     36.623 |     7.4
   35 |   0.9481 |     32.088 |   1.1047 |     36.406 |     7.6
   36 |   0.9266 |     31.146 |   1.1004 |     36.189 |     7.8
   37 |   0.9047 |     30.474 |   1.1057 |     36.002 |     8.0
   38 |   0.8831 |     29.647 |   1.0831 |     35.444 |     8.2
   39 |   0.8704 |     29.085 |   1.0901 |     34.761 |     8.5
   40 |   0.8427 |     27.994 |   1.0739 |     34.295 |     8.7
   41 |   0.8172 |     26.926 |   1.1073 |     34.947 |     8.9
   42 |   0.7973 |     26.331 |   1.0867 |     34.078 |     9.1
   43 |   0.7757 |     25.532 |   1.0927 |     33.737 |     9.3
   44 |   0.7505 |     24.694 |   1.0751 |     33.395 |     9.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 537,922

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4285 |     61.691 |   1.8337 |     49.379 |     0.3
    2 |   1.6502 |     48.105 |   1.4993 |     46.120 |     0.6
    3 |   1.4563 |     45.945 |   1.4145 |     46.120 |     0.9
    4 |   1.3997 |     45.818 |   1.3765 |     45.500 |     1.3
    5 |   1.3652 |     45.488 |   1.3454 |     45.469 |     1.6
    6 |   1.3405 |     44.826 |   1.3338 |     45.376 |     1.9
    7 |   1.3215 |     44.556 |   1.3146 |     44.631 |     2.2
    8 |   1.3050 |     44.408 |   1.2976 |     44.475 |     2.5
    9 |   1.2891 |     44.099 |   1.2862 |     44.351 |     2.8
   10 |   1.2735 |     43.857 |   1.2675 |     43.296 |     3.1
   11 |   1.2556 |     43.488 |   1.2532 |     43.079 |     3.4
   12 |   1.2363 |     42.837 |   1.2406 |     43.203 |     3.8
   13 |   1.2179 |     42.567 |   1.2251 |     42.986 |     4.1
   14 |   1.1974 |     41.950 |   1.2025 |     41.837 |     4.4
   15 |   1.1762 |     40.959 |   1.1874 |     40.441 |     4.7
   16 |   1.1551 |     40.033 |   1.1795 |     40.720 |     5.0
   17 |   1.1361 |     39.366 |   1.1569 |     40.317 |     5.3
   18 |   1.1134 |     38.523 |   1.1509 |     39.634 |     5.6
   19 |   1.0925 |     37.471 |   1.1480 |     39.448 |     6.0
   20 |   1.0743 |     36.876 |   1.1298 |     37.927 |     6.3
   21 |   1.0473 |     35.669 |   1.1271 |     38.485 |     6.6
   22 |   1.0261 |     34.860 |   1.1076 |     37.461 |     6.9
   23 |   1.0011 |     33.785 |   1.0946 |     37.741 |     7.2
   24 |   0.9788 |     33.201 |   1.0972 |     36.623 |     7.5
   25 |   0.9628 |     32.391 |   1.0899 |     36.437 |     7.8
   26 |   0.9379 |     31.554 |   1.0838 |     35.940 |     8.1
   27 |   0.9078 |     30.336 |   1.0773 |     36.065 |     8.5
   28 |   0.8822 |     29.499 |   1.0867 |     35.258 |     8.8
   29 |   0.8638 |     28.556 |   1.0860 |     35.289 |     9.1
   30 |   0.8382 |     27.780 |   1.0815 |     35.040 |     9.4
   31 |   0.8175 |     26.898 |   1.0828 |     35.599 |     9.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 586,562

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2270 |     60.964 |   1.6126 |     46.120 |     0.0
    2 |   1.4828 |     46.088 |   1.4010 |     46.120 |     0.0
    3 |   1.3937 |     45.934 |   1.3585 |     46.120 |     0.1
    4 |   1.3608 |     45.669 |   1.3382 |     45.748 |     0.1
    5 |   1.3377 |     45.725 |   1.3180 |     46.058 |     0.1
    6 |   1.3191 |     45.515 |   1.3002 |     45.189 |     0.1
    7 |   1.3000 |     45.146 |   1.2838 |     44.289 |     0.1
    8 |   1.2828 |     44.540 |   1.2790 |     44.755 |     0.1
    9 |   1.2679 |     44.215 |   1.2608 |     44.165 |     0.2
   10 |   1.2561 |     43.917 |   1.2569 |     44.413 |     0.2
   11 |   1.2398 |     43.532 |   1.2394 |     43.886 |     0.2
   12 |   1.2284 |     43.587 |   1.2313 |     43.606 |     0.2
   13 |   1.2136 |     43.003 |   1.2157 |     43.017 |     0.2
   14 |   1.2015 |     42.457 |   1.2035 |     43.451 |     0.2
   15 |   1.1894 |     42.364 |   1.1955 |     42.737 |     0.3
   16 |   1.1715 |     41.691 |   1.1881 |     42.086 |     0.3
   17 |   1.1551 |     41.168 |   1.1777 |     41.496 |     0.3
   18 |   1.1388 |     40.501 |   1.1614 |     41.217 |     0.3
   19 |   1.1259 |     40.061 |   1.1515 |     40.192 |     0.3
   20 |   1.1065 |     39.273 |   1.1397 |     39.634 |     0.3
   21 |   1.0865 |     38.860 |   1.1324 |     39.913 |     0.4
   22 |   1.0753 |     38.000 |   1.1303 |     39.572 |     0.4
   23 |   1.0625 |     37.570 |   1.1133 |     38.703 |     0.4
   24 |   1.0468 |     37.201 |   1.1024 |     38.703 |     0.4
   25 |   1.0260 |     36.452 |   1.0951 |     38.051 |     0.4
   26 |   1.0160 |     35.725 |   1.0940 |     38.020 |     0.4
   27 |   0.9966 |     35.069 |   1.0837 |     37.244 |     0.5
   28 |   0.9821 |     34.479 |   1.0814 |     37.430 |     0.5
   29 |   0.9700 |     33.879 |   1.0790 |     37.306 |     0.5
   30 |   0.9493 |     33.069 |   1.0634 |     36.996 |     0.5
   31 |   0.9344 |     32.490 |   1.0630 |     36.313 |     0.5
   32 |   0.9198 |     31.675 |   1.0534 |     35.971 |     0.6
   33 |   0.9056 |     31.267 |   1.0412 |     35.196 |     0.6
   34 |   0.8835 |     30.209 |   1.0306 |     34.513 |     0.6
   35 |   0.8903 |     30.303 |   1.0295 |     34.854 |     0.6
   36 |   0.8587 |     29.096 |   1.0419 |     34.947 |     0.6
   37 |   0.8442 |     28.755 |   1.0201 |     33.116 |     0.6
   38 |   0.8168 |     27.466 |   1.0140 |     33.271 |     0.7
   39 |   0.8027 |     27.080 |   1.0082 |     32.526 |     0.7
   40 |   0.7817 |     26.375 |   1.0202 |     32.868 |     0.7
   41 |   0.7680 |     26.000 |   1.0141 |     32.713 |     0.7
   42 |   0.7531 |     25.278 |   1.0080 |     32.495 |     0.7
   43 |   0.7372 |     24.584 |   1.0104 |     33.054 |     0.7
   44 |   0.7240 |     24.264 |   1.0024 |     31.999 |     0.8
   45 |   0.7127 |     23.906 |   1.0103 |     31.595 |     0.8
   46 |   0.7040 |     23.736 |   1.0099 |     31.750 |     0.8
   47 |   0.6897 |     23.229 |   1.0213 |     31.813 |     0.8
   48 |   0.6734 |     22.595 |   1.0186 |     31.564 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 966,594

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 467])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0015 |     55.686 |   1.4494 |     45.779 |     0.1
    2 |   1.3926 |     45.504 |   1.3429 |     45.562 |     0.1
    3 |   1.3166 |     44.810 |   1.2899 |     44.507 |     0.2
    4 |   1.2714 |     44.055 |   1.2514 |     44.041 |     0.3
    5 |   1.2388 |     43.372 |   1.2275 |     43.544 |     0.4
    6 |   1.2122 |     42.353 |   1.2042 |     43.110 |     0.4
    7 |   1.1864 |     41.835 |   1.1861 |     42.986 |     0.5
    8 |   1.1582 |     40.650 |   1.1651 |     41.899 |     0.6
    9 |   1.1317 |     40.116 |   1.1487 |     40.565 |     0.6
   10 |   1.1069 |     38.871 |   1.1222 |     39.261 |     0.7
   11 |   1.0704 |     37.179 |   1.1016 |     38.082 |     0.8
   12 |   1.0404 |     36.039 |   1.0831 |     37.585 |     0.8
   13 |   1.0270 |     35.421 |   1.0673 |     36.499 |     0.9
   14 |   0.9875 |     33.840 |   1.0461 |     35.847 |     1.0
   15 |   0.9539 |     32.760 |   1.0360 |     35.351 |     1.1
   16 |   0.9230 |     31.763 |   1.0262 |     34.451 |     1.1
   17 |   0.8871 |     29.857 |   0.9955 |     33.426 |     1.2
   18 |   0.8516 |     28.534 |   0.9956 |     33.520 |     1.3
   19 |   0.8203 |     27.537 |   0.9707 |     32.278 |     1.4
   20 |   0.7891 |     26.154 |   0.9656 |     31.533 |     1.4
   21 |   0.7564 |     24.865 |   0.9561 |     31.440 |     1.5
   22 |   0.7229 |     23.730 |   0.9505 |     29.795 |     1.6
   23 |   0.6929 |     22.733 |   0.9491 |     30.633 |     1.6
   24 |   0.6611 |     21.658 |   0.9408 |     29.950 |     1.7
   25 |   0.6431 |     21.025 |   0.9540 |     30.199 |     1.8
   26 |   0.6068 |     19.884 |   0.9456 |     29.050 |     1.9
   27 |   0.5755 |     18.727 |   0.9353 |     28.492 |     1.9
   28 |   0.5430 |     17.344 |   0.9436 |     28.212 |     2.0
   29 |   0.5230 |     16.887 |   0.9608 |     28.119 |     2.1
   30 |   0.5040 |     16.446 |   0.9556 |     28.430 |     2.1
   31 |   0.4770 |     15.223 |   0.9595 |     27.778 |     2.2
Early stopping

