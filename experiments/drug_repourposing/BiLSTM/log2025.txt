Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 507,682

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0817 |     57.216 |   1.5099 |     46.186 |     0.0
    2 |   1.4173 |     45.847 |   1.3724 |     45.833 |     0.0
    3 |   1.3404 |     45.113 |   1.3132 |     44.455 |     0.1
    4 |   1.3003 |     44.247 |   1.2882 |     44.231 |     0.1
    5 |   1.2747 |     44.231 |   1.2645 |     44.423 |     0.1
    6 |   1.2538 |     43.721 |   1.2368 |     42.949 |     0.1
    7 |   1.2258 |     42.971 |   1.2106 |     42.628 |     0.1
    8 |   1.2067 |     42.664 |   1.1957 |     42.019 |     0.2
    9 |   1.1883 |     42.220 |   1.1722 |     41.859 |     0.2
   10 |   1.1680 |     41.595 |   1.1511 |     41.250 |     0.2
   11 |   1.1436 |     40.905 |   1.1308 |     40.192 |     0.2
   12 |   1.1256 |     40.138 |   1.1162 |     40.064 |     0.2
   13 |   1.1059 |     39.075 |   1.0924 |     38.878 |     0.3
   14 |   1.0805 |     38.094 |   1.0837 |     38.686 |     0.3
   15 |   1.0539 |     37.053 |   1.0455 |     36.859 |     0.3
   16 |   1.0240 |     35.782 |   1.0280 |     35.288 |     0.3
   17 |   1.0004 |     34.489 |   1.0103 |     35.481 |     0.3
   18 |   0.9824 |     33.777 |   0.9799 |     34.103 |     0.4
   19 |   0.9560 |     32.528 |   0.9725 |     34.199 |     0.4
   20 |   0.9337 |     32.090 |   0.9446 |     32.500 |     0.4
   21 |   0.9122 |     31.175 |   0.9545 |     33.814 |     0.4
   22 |   0.8905 |     30.501 |   0.9323 |     32.660 |     0.4
   23 |   0.8718 |     30.112 |   0.9125 |     31.699 |     0.5
   24 |   0.8455 |     28.917 |   0.8978 |     30.833 |     0.5
   25 |   0.8259 |     27.915 |   0.8913 |     30.609 |     0.5
   26 |   0.8050 |     27.230 |   0.8853 |     30.417 |     0.5
   27 |   0.7928 |     26.846 |   0.8765 |     29.551 |     0.5
   28 |   0.7667 |     25.707 |   0.8644 |     29.135 |     0.6
   29 |   0.7409 |     24.912 |   0.8556 |     28.333 |     0.6
   30 |   0.7217 |     23.548 |   0.8569 |     28.429 |     0.6
   31 |   0.7109 |     23.970 |   0.8532 |     28.205 |     0.6
   32 |   0.6823 |     22.628 |   0.8383 |     27.821 |     0.6
   33 |   0.6587 |     21.828 |   0.8400 |     28.429 |     0.7
   34 |   0.6396 |     21.176 |   0.8312 |     27.340 |     0.7
   35 |   0.6169 |     20.453 |   0.8374 |     26.859 |     0.7
   36 |   0.6009 |     19.784 |   0.8273 |     27.244 |     0.7
   37 |   0.5852 |     19.181 |   0.8579 |     26.538 |     0.7
   38 |   0.5890 |     19.658 |   0.8250 |     26.506 |     0.8
   39 |   0.5516 |     17.921 |   0.8264 |     26.378 |     0.8
   40 |   0.5445 |     18.025 |   0.8150 |     26.090 |     0.8
   41 |   0.5208 |     17.160 |   0.8347 |     26.987 |     0.8
   42 |   0.4979 |     15.998 |   0.8275 |     26.186 |     0.8
   43 |   0.4912 |     15.993 |   0.8371 |     25.769 |     0.9
   44 |   0.4778 |     15.483 |   0.8310 |     26.250 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,019,426

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1966 |     60.202 |   1.6431 |     46.346 |     0.1
    2 |   1.4884 |     46.170 |   1.4427 |     46.346 |     0.2
    3 |   1.4097 |     46.055 |   1.4193 |     46.346 |     0.2
    4 |   1.3924 |     46.225 |   1.4041 |     46.346 |     0.3
    5 |   1.3843 |     45.968 |   1.3934 |     46.346 |     0.4
    6 |   1.3636 |     45.688 |   1.3577 |     45.481 |     0.5
    7 |   1.3368 |     45.474 |   1.3378 |     46.058 |     0.5
    8 |   1.3118 |     45.222 |   1.3001 |     45.449 |     0.6
    9 |   1.2927 |     45.135 |   1.2773 |     44.583 |     0.7
   10 |   1.2662 |     44.477 |   1.2560 |     43.750 |     0.8
   11 |   1.2496 |     44.318 |   1.2441 |     43.974 |     0.9
   12 |   1.2335 |     44.138 |   1.2146 |     43.462 |     0.9
   13 |   1.2170 |     43.540 |   1.2003 |     43.045 |     1.0
   14 |   1.2049 |     42.943 |   1.1824 |     41.058 |     1.1
   15 |   1.1846 |     42.220 |   1.1706 |     41.442 |     1.2
   16 |   1.1715 |     41.524 |   1.1571 |     42.019 |     1.2
   17 |   1.1551 |     40.796 |   1.1413 |     40.545 |     1.3
   18 |   1.1414 |     40.374 |   1.1255 |     40.032 |     1.4
   19 |   1.1224 |     39.629 |   1.1103 |     39.487 |     1.5
   20 |   1.1142 |     38.933 |   1.1088 |     39.199 |     1.5
   21 |   1.0929 |     38.746 |   1.0877 |     38.397 |     1.6
   22 |   1.0787 |     37.788 |   1.0774 |     38.173 |     1.7
   23 |   1.0627 |     37.678 |   1.0600 |     37.564 |     1.8
   24 |   1.0396 |     36.560 |   1.0545 |     36.635 |     1.9
   25 |   1.0220 |     35.739 |   1.0459 |     36.859 |     1.9
   26 |   1.0012 |     35.010 |   1.0324 |     35.769 |     2.0
   27 |   0.9747 |     33.958 |   1.0196 |     35.128 |     2.1
   28 |   0.9583 |     33.125 |   1.0152 |     34.968 |     2.2
   29 |   0.9330 |     32.385 |   1.0130 |     34.263 |     2.2
   30 |   0.9099 |     31.399 |   0.9941 |     33.654 |     2.3
   31 |   0.8861 |     30.517 |   1.0121 |     34.647 |     2.4
   32 |   0.8636 |     29.460 |   0.9721 |     32.276 |     2.5
   33 |   0.8387 |     28.468 |   0.9804 |     33.045 |     2.6
   34 |   0.8174 |     27.433 |   0.9662 |     31.635 |     2.6
   35 |   0.7955 |     26.786 |   0.9629 |     31.667 |     2.7
   36 |   0.7574 |     25.088 |   0.9568 |     31.026 |     2.8
   37 |   0.7312 |     24.019 |   0.9570 |     31.026 |     2.9
   38 |   0.6994 |     22.787 |   0.9853 |     31.731 |     2.9
   39 |   0.6781 |     22.118 |   0.9675 |     30.192 |     3.0
   40 |   0.6467 |     21.159 |   0.9873 |     30.385 |     3.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 557,858

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3783 |     61.259 |   1.8320 |     50.321 |     0.0
    2 |   1.6181 |     47.606 |   1.4897 |     46.218 |     0.1
    3 |   1.4314 |     45.798 |   1.3955 |     46.058 |     0.1
    4 |   1.3638 |     45.518 |   1.3383 |     45.705 |     0.1
    5 |   1.3224 |     44.981 |   1.3075 |     44.391 |     0.2
    6 |   1.2955 |     44.187 |   1.2755 |     44.391 |     0.2
    7 |   1.2688 |     43.803 |   1.2549 |     43.237 |     0.3
    8 |   1.2452 |     42.965 |   1.2261 |     42.628 |     0.3
    9 |   1.2208 |     42.346 |   1.2118 |     42.500 |     0.3
   10 |   1.1975 |     41.727 |   1.1776 |     41.122 |     0.4
   11 |   1.1735 |     40.949 |   1.1534 |     39.583 |     0.4
   12 |   1.1491 |     39.924 |   1.1381 |     39.359 |     0.4
   13 |   1.1264 |     38.916 |   1.1129 |     38.045 |     0.5
   14 |   1.1006 |     37.793 |   1.0926 |     37.949 |     0.5
   15 |   1.0673 |     36.566 |   1.0665 |     36.667 |     0.6
   16 |   1.0435 |     35.536 |   1.0528 |     35.769 |     0.6
   17 |   1.0103 |     33.826 |   1.0291 |     34.551 |     0.6
   18 |   0.9765 |     32.676 |   1.0156 |     34.487 |     0.7
   19 |   0.9578 |     31.525 |   0.9933 |     32.660 |     0.7
   20 |   0.9229 |     30.528 |   0.9760 |     32.724 |     0.7
   21 |   0.8946 |     29.591 |   0.9590 |     32.821 |     0.8
   22 |   0.8642 |     28.293 |   0.9500 |     31.667 |     0.8
   23 |   0.8273 |     27.137 |   0.9253 |     30.769 |     0.9
   24 |   0.7973 |     25.904 |   0.9358 |     30.641 |     0.9
   25 |   0.7767 |     25.148 |   0.9262 |     30.417 |     0.9
   26 |   0.7594 |     24.507 |   0.9253 |     30.288 |     1.0
   27 |   0.7437 |     23.932 |   0.9108 |     28.878 |     1.0
   28 |   0.7124 |     23.039 |   0.9107 |     28.622 |     1.0
   29 |   0.6829 |     21.981 |   0.9054 |     28.590 |     1.1
   30 |   0.6541 |     20.946 |   0.8950 |     28.109 |     1.1
   31 |   0.6329 |     20.140 |   0.9168 |     27.981 |     1.2
   32 |   0.6066 |     19.187 |   0.8932 |     26.923 |     1.2
   33 |   0.5908 |     18.655 |   0.8912 |     27.212 |     1.2
   34 |   0.5725 |     17.987 |   0.9003 |     26.891 |     1.3
   35 |   0.5827 |     18.814 |   0.9206 |     27.340 |     1.3
   36 |   0.5403 |     16.984 |   0.8907 |     26.699 |     1.3
   37 |   0.5187 |     16.294 |   0.9034 |     27.340 |     1.4
   38 |   0.5038 |     15.839 |   0.9150 |     26.667 |     1.4
   39 |   0.4806 |     15.302 |   0.9105 |     26.058 |     1.5
   40 |   0.4641 |     14.513 |   0.9211 |     26.250 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 542,626

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0151 |     54.547 |   1.4753 |     45.769 |     0.0
    2 |   1.3827 |     45.436 |   1.3206 |     44.776 |     0.0
    3 |   1.3052 |     44.444 |   1.2792 |     43.910 |     0.1
    4 |   1.2638 |     43.968 |   1.2356 |     43.045 |     0.1
    5 |   1.2332 |     43.086 |   1.2091 |     41.859 |     0.1
    6 |   1.2068 |     42.582 |   1.1722 |     41.314 |     0.1
    7 |   1.1742 |     41.782 |   1.1579 |     40.962 |     0.1
    8 |   1.1514 |     40.998 |   1.1271 |     39.391 |     0.2
    9 |   1.1313 |     40.412 |   1.1158 |     40.385 |     0.2
   10 |   1.1094 |     39.426 |   1.0866 |     38.590 |     0.2
   11 |   1.0864 |     38.429 |   1.0675 |     36.955 |     0.2
   12 |   1.0625 |     37.722 |   1.0517 |     36.731 |     0.2
   13 |   1.0376 |     36.473 |   1.0397 |     36.314 |     0.2
   14 |   1.0160 |     35.415 |   1.0289 |     35.609 |     0.3
   15 |   0.9917 |     34.604 |   0.9999 |     34.936 |     0.3
   16 |   0.9740 |     33.963 |   0.9899 |     34.840 |     0.3
   17 |   0.9553 |     33.207 |   0.9708 |     33.494 |     0.3
   18 |   0.9329 |     32.117 |   0.9802 |     33.942 |     0.3
   19 |   0.9188 |     31.766 |   0.9644 |     33.462 |     0.4
   20 |   0.8940 |     30.797 |   0.9412 |     32.596 |     0.4
   21 |   0.8662 |     29.613 |   0.9341 |     32.821 |     0.4
   22 |   0.8531 |     29.065 |   0.9190 |     31.667 |     0.4
   23 |   0.8360 |     28.249 |   0.9374 |     32.372 |     0.4
   24 |   0.8200 |     27.553 |   0.9074 |     30.513 |     0.5
   25 |   0.7911 |     26.074 |   0.8924 |     29.840 |     0.5
   26 |   0.7609 |     25.526 |   0.8883 |     29.936 |     0.5
   27 |   0.7439 |     24.616 |   0.8859 |     29.071 |     0.5
   28 |   0.7141 |     23.647 |   0.8809 |     29.455 |     0.5
   29 |   0.6909 |     22.902 |   0.8841 |     29.744 |     0.6
   30 |   0.6660 |     21.926 |   0.8683 |     28.173 |     0.6
   31 |   0.6716 |     22.014 |   0.8712 |     28.494 |     0.6
   32 |   0.6510 |     21.548 |   0.8741 |     28.974 |     0.6
   33 |   0.6153 |     20.337 |   0.8587 |     27.660 |     0.6
   34 |   0.5886 |     19.149 |   0.8478 |     26.827 |     0.7
   35 |   0.5755 |     18.869 |   0.8676 |     26.635 |     0.7
   36 |   0.5489 |     17.658 |   0.8510 |     26.859 |     0.7
   37 |   0.5347 |     17.165 |   0.8542 |     26.282 |     0.7
   38 |   0.5174 |     16.678 |   0.8817 |     26.378 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 290,914

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3663 |     62.316 |   1.7433 |     48.782 |     0.0
    2 |   1.5466 |     46.598 |   1.4397 |     46.186 |     0.0
    3 |   1.3967 |     45.655 |   1.3621 |     45.897 |     0.0
    4 |   1.3437 |     45.014 |   1.3135 |     44.455 |     0.1
    5 |   1.3057 |     44.428 |   1.2904 |     43.622 |     0.1
    6 |   1.2805 |     43.875 |   1.2719 |     42.981 |     0.1
    7 |   1.2659 |     43.486 |   1.2624 |     43.718 |     0.1
    8 |   1.2469 |     43.299 |   1.2291 |     42.468 |     0.1
    9 |   1.2261 |     42.806 |   1.2159 |     42.981 |     0.1
   10 |   1.2056 |     42.384 |   1.1876 |     41.154 |     0.2
   11 |   1.1927 |     42.132 |   1.1744 |     40.481 |     0.2
   12 |   1.1764 |     41.541 |   1.1652 |     41.635 |     0.2
   13 |   1.1557 |     40.538 |   1.1381 |     40.224 |     0.2
   14 |   1.1392 |     40.028 |   1.1276 |     39.103 |     0.2
   15 |   1.1205 |     39.316 |   1.1249 |     40.192 |     0.2
   16 |   1.1119 |     38.944 |   1.1002 |     38.526 |     0.3
   17 |   1.0960 |     38.494 |   1.0951 |     38.942 |     0.3
   18 |   1.0810 |     38.149 |   1.0853 |     38.333 |     0.3
   19 |   1.0698 |     37.377 |   1.0696 |     37.628 |     0.3
   20 |   1.0588 |     37.103 |   1.0576 |     37.019 |     0.3
   21 |   1.0423 |     36.193 |   1.0574 |     36.795 |     0.3
   22 |   1.0248 |     35.766 |   1.0479 |     36.378 |     0.4
   23 |   1.0154 |     35.059 |   1.0314 |     35.545 |     0.4
   24 |   0.9971 |     34.265 |   1.0248 |     35.609 |     0.4
   25 |   0.9857 |     33.837 |   1.0130 |     34.776 |     0.4
   26 |   0.9716 |     33.383 |   1.0088 |     36.378 |     0.4
   27 |   0.9534 |     32.994 |   0.9919 |     34.423 |     0.4
   28 |   0.9386 |     32.188 |   0.9876 |     34.135 |     0.5
   29 |   0.9235 |     31.471 |   0.9778 |     33.590 |     0.5
   30 |   0.9035 |     30.895 |   0.9702 |     33.526 |     0.5
   31 |   0.8860 |     30.024 |   0.9561 |     32.532 |     0.5
   32 |   0.8705 |     29.191 |   0.9555 |     32.404 |     0.5
   33 |   0.8553 |     28.704 |   0.9619 |     33.045 |     0.5
   34 |   0.8421 |     28.397 |   0.9406 |     32.340 |     0.6
   35 |   0.8163 |     27.339 |   0.9252 |     30.705 |     0.6
   36 |   0.8039 |     26.594 |   0.9316 |     30.929 |     0.6
   37 |   0.7795 |     25.794 |   0.9186 |     30.929 |     0.6
   38 |   0.7590 |     25.093 |   0.9312 |     30.737 |     0.6
   39 |   0.7513 |     24.671 |   0.9073 |     30.224 |     0.6
   40 |   0.7412 |     24.173 |   0.9295 |     30.513 |     0.7
   41 |   0.7685 |     25.636 |   0.9336 |     31.154 |     0.7
   42 |   0.7183 |     23.543 |   0.9164 |     29.872 |     0.7
   43 |   0.6896 |     22.617 |   0.9081 |     29.904 |     0.7
   44 |   0.6640 |     21.592 |   0.9035 |     29.551 |     0.7
   45 |   0.6581 |     20.973 |   0.9082 |     29.808 |     0.7
   46 |   0.6364 |     20.217 |   0.8987 |     28.718 |     0.8
   47 |   0.6181 |     20.042 |   0.9065 |     28.878 |     0.8
   48 |   0.6196 |     19.532 |   0.9344 |     29.872 |     0.8
   49 |   0.6239 |     20.255 |   0.9113 |     28.494 |     0.8
   50 |   0.5847 |     18.732 |   0.9193 |     28.654 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 422,146

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4673 |     66.124 |   1.8673 |     52.340 |     0.0
    2 |   1.6606 |     48.532 |   1.5257 |     46.346 |     0.1
    3 |   1.4612 |     46.017 |   1.4357 |     46.346 |     0.1
    4 |   1.4059 |     45.863 |   1.3981 |     45.865 |     0.1
    5 |   1.3775 |     45.809 |   1.3586 |     45.994 |     0.1
    6 |   1.3465 |     45.579 |   1.3275 |     45.385 |     0.2
    7 |   1.3245 |     45.009 |   1.3009 |     44.231 |     0.2
    8 |   1.3021 |     44.565 |   1.2824 |     43.686 |     0.2
    9 |   1.2836 |     44.220 |   1.2617 |     43.429 |     0.2
   10 |   1.2662 |     43.990 |   1.2496 |     43.750 |     0.3
   11 |   1.2490 |     43.743 |   1.2334 |     43.301 |     0.3
   12 |   1.2366 |     43.469 |   1.2193 |     43.301 |     0.3
   13 |   1.2254 |     43.272 |   1.2020 |     43.141 |     0.3
   14 |   1.2096 |     42.839 |   1.1899 |     42.628 |     0.4
   15 |   1.1949 |     42.499 |   1.1785 |     41.891 |     0.4
   16 |   1.1893 |     42.182 |   1.1704 |     40.705 |     0.4
   17 |   1.1752 |     41.508 |   1.1564 |     40.609 |     0.4
   18 |   1.1654 |     41.217 |   1.1481 |     40.096 |     0.5
   19 |   1.1515 |     40.598 |   1.1297 |     39.744 |     0.5
   20 |   1.1501 |     40.215 |   1.1190 |     38.878 |     0.5
   21 |   1.1362 |     39.908 |   1.1131 |     38.558 |     0.6
   22 |   1.1232 |     39.568 |   1.1030 |     38.686 |     0.6
   23 |   1.1084 |     39.004 |   1.0937 |     38.077 |     0.6
   24 |   1.0953 |     38.593 |   1.0889 |     37.724 |     0.6
   25 |   1.0894 |     37.952 |   1.0770 |     37.372 |     0.7
   26 |   1.0808 |     37.903 |   1.0713 |     37.276 |     0.7
   27 |   1.0664 |     37.289 |   1.0607 |     36.571 |     0.7
   28 |   1.0578 |     36.774 |   1.0547 |     36.699 |     0.7
   29 |   1.0394 |     36.210 |   1.0531 |     36.763 |     0.8
   30 |   1.0314 |     36.117 |   1.0415 |     35.865 |     0.8
   31 |   1.0203 |     35.426 |   1.0410 |     35.641 |     0.8
   32 |   1.0151 |     35.284 |   1.0291 |     35.449 |     0.8
   33 |   0.9960 |     34.402 |   1.0267 |     35.160 |     0.9
   34 |   0.9921 |     34.593 |   1.0220 |     35.385 |     0.9
   35 |   0.9767 |     33.662 |   1.0216 |     35.096 |     0.9
   36 |   0.9671 |     33.317 |   1.0104 |     34.423 |     0.9
   37 |   0.9529 |     32.659 |   1.0098 |     34.391 |     1.0
   38 |   0.9362 |     32.418 |   1.0030 |     34.872 |     1.0
   39 |   0.9343 |     32.040 |   0.9986 |     33.750 |     1.0
   40 |   0.9285 |     32.101 |   0.9920 |     34.071 |     1.1
   41 |   0.9087 |     31.366 |   0.9932 |     34.231 |     1.1
   42 |   0.8977 |     30.638 |   0.9893 |     34.423 |     1.1
   43 |   0.8878 |     30.254 |   0.9848 |     33.429 |     1.1
   44 |   0.8809 |     29.947 |   0.9930 |     33.910 |     1.2
   45 |   0.8732 |     29.865 |   0.9902 |     33.782 |     1.2
   46 |   0.8517 |     29.169 |   0.9911 |     33.718 |     1.2
   47 |   0.8421 |     28.408 |   0.9784 |     32.788 |     1.2
   48 |   0.8341 |     28.156 |   0.9941 |     32.756 |     1.3
   49 |   0.8210 |     27.997 |   0.9868 |     32.596 |     1.3
   50 |   0.8142 |     27.827 |   0.9878 |     32.340 |     1.3
   51 |   0.7969 |     27.093 |   0.9866 |     32.308 |     1.3
   52 |   0.7870 |     26.501 |   0.9746 |     32.147 |     1.4
   53 |   0.7817 |     26.370 |   0.9980 |     32.468 |     1.4
   54 |   0.7716 |     25.893 |   0.9769 |     31.763 |     1.4
   55 |   0.7613 |     25.247 |   0.9843 |     31.506 |     1.4
   56 |   0.7624 |     25.444 |   0.9833 |     31.506 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 621,442

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1848 |     59.604 |   1.5904 |     46.346 |     0.0
    2 |   1.4630 |     46.061 |   1.4160 |     46.346 |     0.0
    3 |   1.3799 |     46.083 |   1.3713 |     45.994 |     0.1
    4 |   1.3471 |     45.491 |   1.3378 |     45.288 |     0.1
    5 |   1.3205 |     45.464 |   1.3029 |     44.808 |     0.1
    6 |   1.2962 |     44.954 |   1.2879 |     44.135 |     0.1
    7 |   1.2794 |     44.691 |   1.2684 |     44.071 |     0.1
    8 |   1.2622 |     44.176 |   1.2481 |     43.237 |     0.1
    9 |   1.2436 |     43.623 |   1.2282 |     43.333 |     0.2
   10 |   1.2240 |     43.376 |   1.2150 |     44.103 |     0.2
   11 |   1.2150 |     43.261 |   1.2038 |     43.077 |     0.2
   12 |   1.2067 |     43.069 |   1.1905 |     43.045 |     0.2
   13 |   1.1919 |     43.020 |   1.1852 |     42.724 |     0.2
   14 |   1.1833 |     42.390 |   1.1759 |     42.628 |     0.2
   15 |   1.1757 |     42.302 |   1.1620 |     41.923 |     0.3
   16 |   1.1672 |     41.973 |   1.1610 |     41.250 |     0.3
   17 |   1.1624 |     41.804 |   1.1486 |     41.731 |     0.3
   18 |   1.1505 |     41.793 |   1.1467 |     41.218 |     0.3
   19 |   1.1436 |     41.749 |   1.1357 |     41.026 |     0.3
   20 |   1.1357 |     41.404 |   1.1299 |     40.994 |     0.4
   21 |   1.1294 |     41.267 |   1.1161 |     40.545 |     0.4
   22 |   1.1258 |     41.108 |   1.1098 |     40.673 |     0.4
   23 |   1.1173 |     40.609 |   1.1103 |     40.385 |     0.4
   24 |   1.1112 |     40.757 |   1.1043 |     39.968 |     0.4
   25 |   1.1066 |     40.445 |   1.0996 |     39.455 |     0.4
   26 |   1.0986 |     40.313 |   1.0872 |     39.327 |     0.5
   27 |   1.0973 |     40.324 |   1.0891 |     39.455 |     0.5
   28 |   1.0902 |     40.056 |   1.0839 |     38.494 |     0.5
   29 |   1.0883 |     39.722 |   1.0723 |     39.006 |     0.5
   30 |   1.0815 |     39.333 |   1.0698 |     38.718 |     0.5
   31 |   1.0732 |     39.639 |   1.0651 |     39.263 |     0.6
   32 |   1.0678 |     39.114 |   1.0548 |     38.237 |     0.6
   33 |   1.0590 |     38.620 |   1.0538 |     37.692 |     0.6
   34 |   1.0532 |     38.275 |   1.0478 |     38.237 |     0.6
   35 |   1.0497 |     38.418 |   1.0542 |     37.981 |     0.6
   36 |   1.0467 |     37.990 |   1.0327 |     37.628 |     0.6
   37 |   1.0413 |     38.089 |   1.0317 |     37.788 |     0.7
   38 |   1.0360 |     37.568 |   1.0310 |     37.115 |     0.7
   39 |   1.0359 |     37.525 |   1.0307 |     37.372 |     0.7
   40 |   1.0261 |     37.771 |   1.0219 |     37.564 |     0.7
   41 |   1.0288 |     37.651 |   1.0221 |     37.179 |     0.7
   42 |   1.0219 |     37.684 |   1.0190 |     37.083 |     0.7
   43 |   1.0181 |     37.333 |   1.0210 |     36.731 |     0.8
   44 |   1.0140 |     37.399 |   1.0135 |     36.891 |     0.8
   45 |   1.0165 |     37.037 |   1.0156 |     37.628 |     0.8
   46 |   1.0096 |     37.064 |   1.0116 |     36.987 |     0.8
   47 |   1.0151 |     37.059 |   1.0154 |     36.603 |     0.8
   48 |   1.0070 |     36.840 |   1.0121 |     36.891 |     0.9
   49 |   1.0025 |     36.840 |   1.0125 |     37.179 |     0.9
   50 |   1.0019 |     36.895 |   1.0159 |     36.987 |     0.9
   51 |   1.0014 |     36.812 |   1.0051 |     36.442 |     0.9
   52 |   1.0001 |     36.632 |   1.0077 |     36.571 |     0.9
   53 |   0.9968 |     36.456 |   1.0056 |     36.538 |     0.9
   54 |   0.9952 |     36.489 |   1.0049 |     36.282 |     1.0
   55 |   0.9939 |     36.555 |   0.9959 |     36.154 |     1.0
   56 |   0.9883 |     36.434 |   1.0110 |     37.147 |     1.0
   57 |   0.9904 |     36.166 |   1.0031 |     36.218 |     1.0
   58 |   0.9885 |     36.484 |   0.9985 |     35.929 |     1.0
   59 |   0.9864 |     36.204 |   0.9935 |     35.449 |     1.1
   60 |   0.9935 |     36.297 |   1.0022 |     36.699 |     1.1
   61 |   0.9848 |     35.843 |   1.0016 |     36.314 |     1.1
   62 |   0.9804 |     35.821 |   0.9976 |     35.705 |     1.1
   63 |   0.9810 |     36.248 |   0.9922 |     35.385 |     1.1
   64 |   0.9740 |     35.941 |   0.9957 |     35.929 |     1.1
   65 |   0.9724 |     35.958 |   0.9966 |     36.186 |     1.2
   66 |   0.9747 |     35.739 |   0.9898 |     35.833 |     1.2
   67 |   0.9722 |     35.766 |   1.0023 |     36.442 |     1.2
   68 |   0.9728 |     35.865 |   0.9870 |     34.936 |     1.2
   69 |   0.9676 |     35.673 |   0.9939 |     36.282 |     1.2
   70 |   0.9633 |     35.602 |   0.9823 |     35.256 |     1.3
   71 |   0.9644 |     35.651 |   0.9894 |     35.545 |     1.3
   72 |   0.9645 |     35.667 |   0.9806 |     35.737 |     1.3
   73 |   0.9598 |     35.454 |   0.9779 |     35.449 |     1.3
   74 |   0.9544 |     35.245 |   0.9756 |     35.962 |     1.3
   75 |   0.9579 |     35.656 |   0.9885 |     35.737 |     1.3
   76 |   0.9561 |     35.465 |   0.9731 |     35.641 |     1.4
   77 |   0.9485 |     35.004 |   0.9817 |     36.186 |     1.4
   78 |   0.9477 |     35.087 |   0.9794 |     35.801 |     1.4
   79 |   0.9486 |     34.730 |   0.9758 |     35.353 |     1.4
   80 |   0.9443 |     34.774 |   0.9792 |     36.506 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 586,498

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2281 |     61.160 |   1.6077 |     46.346 |     0.0
    2 |   1.4714 |     46.269 |   1.4372 |     46.346 |     0.0
    3 |   1.4041 |     46.126 |   1.3971 |     46.346 |     0.1
    4 |   1.3657 |     45.748 |   1.3526 |     45.224 |     0.1
    5 |   1.3332 |     45.781 |   1.3293 |     45.641 |     0.1
    6 |   1.3132 |     45.688 |   1.3113 |     45.000 |     0.1
    7 |   1.2948 |     45.107 |   1.2862 |     45.545 |     0.1
    8 |   1.2818 |     44.532 |   1.2703 |     44.840 |     0.1
    9 |   1.2642 |     44.494 |   1.2547 |     44.359 |     0.2
   10 |   1.2547 |     44.466 |   1.2429 |     44.263 |     0.2
   11 |   1.2454 |     44.423 |   1.2401 |     44.231 |     0.2
   12 |   1.2377 |     44.368 |   1.2251 |     44.263 |     0.2
   13 |   1.2307 |     43.918 |   1.2152 |     43.269 |     0.2
   14 |   1.2205 |     43.486 |   1.2039 |     44.167 |     0.3
   15 |   1.2122 |     43.371 |   1.1965 |     42.212 |     0.3
   16 |   1.2051 |     42.987 |   1.1852 |     42.724 |     0.3
   17 |   1.1981 |     42.746 |   1.1784 |     42.276 |     0.3
   18 |   1.1892 |     42.658 |   1.1830 |     43.718 |     0.3
   19 |   1.1804 |     42.916 |   1.1707 |     42.532 |     0.3
   20 |   1.1739 |     42.609 |   1.1640 |     41.442 |     0.4
   21 |   1.1685 |     42.275 |   1.1562 |     41.827 |     0.4
   22 |   1.1610 |     42.352 |   1.1484 |     41.923 |     0.4
   23 |   1.1570 |     42.072 |   1.1441 |     42.115 |     0.4
   24 |   1.1527 |     42.023 |   1.1398 |     41.635 |     0.4
   25 |   1.1457 |     41.601 |   1.1289 |     41.218 |     0.4
   26 |   1.1421 |     41.727 |   1.1300 |     41.859 |     0.5
   27 |   1.1383 |     41.442 |   1.1252 |     41.410 |     0.5
   28 |   1.1328 |     41.486 |   1.1129 |     40.929 |     0.5
   29 |   1.1283 |     41.316 |   1.1186 |     40.929 |     0.5
   30 |   1.1244 |     41.201 |   1.1143 |     41.154 |     0.5
   31 |   1.1220 |     41.130 |   1.1112 |     40.801 |     0.6
   32 |   1.1196 |     40.987 |   1.1096 |     40.385 |     0.6
   33 |   1.1149 |     40.949 |   1.0974 |     39.968 |     0.6
   34 |   1.1136 |     41.113 |   1.0979 |     40.385 |     0.6
   35 |   1.1093 |     40.927 |   1.1061 |     41.026 |     0.6
   36 |   1.1089 |     40.861 |   1.0988 |     40.385 |     0.6
   37 |   1.1049 |     40.741 |   1.0945 |     40.224 |     0.7
   38 |   1.1006 |     40.850 |   1.0898 |     40.641 |     0.7
   39 |   1.0983 |     40.489 |   1.0862 |     40.160 |     0.7
   40 |   1.0953 |     40.335 |   1.0838 |     40.256 |     0.7
   41 |   1.0929 |     40.686 |   1.0767 |     40.192 |     0.7
   42 |   1.0920 |     40.467 |   1.0899 |     40.737 |     0.8
   43 |   1.0933 |     40.565 |   1.0780 |     40.481 |     0.8
   44 |   1.0904 |     40.407 |   1.0834 |     40.064 |     0.8
   45 |   1.0894 |     40.374 |   1.0806 |     40.096 |     0.8
   46 |   1.0857 |     40.719 |   1.0750 |     39.712 |     0.8
   47 |   1.0820 |     40.275 |   1.0737 |     39.808 |     0.8
   48 |   1.0818 |     40.182 |   1.0746 |     39.583 |     0.9
   49 |   1.0820 |     40.259 |   1.0679 |     39.647 |     0.9
   50 |   1.0813 |     40.401 |   1.0763 |     39.744 |     0.9
   51 |   1.0811 |     40.472 |   1.0675 |     39.327 |     0.9
   52 |   1.0790 |     40.007 |   1.0725 |     39.840 |     0.9
   53 |   1.0765 |     40.313 |   1.0686 |     40.064 |     0.9
   54 |   1.0723 |     40.067 |   1.0689 |     40.417 |     1.0
   55 |   1.0761 |     40.302 |   1.0677 |     40.128 |     1.0
   56 |   1.0717 |     40.078 |   1.0659 |     39.904 |     1.0
   57 |   1.0735 |     40.007 |   1.0679 |     39.712 |     1.0
   58 |   1.0791 |     40.133 |   1.0647 |     40.577 |     1.0
   59 |   1.0727 |     40.111 |   1.0619 |     40.256 |     1.1
   60 |   1.0723 |     40.160 |   1.0645 |     39.808 |     1.1
   61 |   1.0704 |     40.083 |   1.0625 |     39.904 |     1.1
   62 |   1.0704 |     40.171 |   1.0606 |     39.647 |     1.1
   63 |   1.0701 |     40.050 |   1.0609 |     39.679 |     1.1
   64 |   1.0694 |     40.259 |   1.0597 |     39.583 |     1.1
   65 |   1.0668 |     39.853 |   1.0680 |     40.096 |     1.2
   66 |   1.0680 |     40.028 |   1.0649 |     39.519 |     1.2
   67 |   1.0665 |     39.952 |   1.0626 |     39.423 |     1.2
   68 |   1.0669 |     39.897 |   1.0566 |     39.615 |     1.2
   69 |   1.0673 |     39.804 |   1.0605 |     39.551 |     1.2
   70 |   1.0660 |     39.979 |   1.0633 |     39.776 |     1.2
   71 |   1.0604 |     40.138 |   1.0609 |     39.263 |     1.3
   72 |   1.0621 |     40.116 |   1.0632 |     39.904 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 717,570

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3579 |     58.229 |   1.7998 |     50.096 |     0.0
    2 |   1.5962 |     47.540 |   1.4837 |     46.346 |     0.1
    3 |   1.4210 |     45.595 |   1.3926 |     45.545 |     0.1
    4 |   1.3590 |     45.694 |   1.3346 |     45.096 |     0.2
    5 |   1.3269 |     44.938 |   1.3072 |     43.910 |     0.2
    6 |   1.3037 |     44.675 |   1.2930 |     44.071 |     0.2
    7 |   1.2895 |     44.318 |   1.2685 |     43.558 |     0.3
    8 |   1.2703 |     43.918 |   1.2564 |     43.462 |     0.3
    9 |   1.2531 |     43.749 |   1.2418 |     42.724 |     0.4
   10 |   1.2396 |     43.562 |   1.2275 |     43.397 |     0.4
   11 |   1.2209 |     43.179 |   1.2114 |     42.596 |     0.4
   12 |   1.2103 |     42.795 |   1.2004 |     41.891 |     0.5
   13 |   1.1962 |     42.368 |   1.1845 |     41.506 |     0.5
   14 |   1.1827 |     41.935 |   1.1725 |     41.538 |     0.6
   15 |   1.1660 |     41.404 |   1.1430 |     40.673 |     0.6
   16 |   1.1410 |     40.450 |   1.1307 |     40.064 |     0.7
   17 |   1.1253 |     39.733 |   1.1071 |     38.814 |     0.7
   18 |   1.1005 |     38.725 |   1.0906 |     37.500 |     0.7
   19 |   1.0869 |     37.908 |   1.0816 |     38.173 |     0.8
   20 |   1.0628 |     36.884 |   1.0635 |     36.859 |     0.8
   21 |   1.0416 |     36.078 |   1.0449 |     36.250 |     0.9
   22 |   1.0158 |     34.719 |   1.0283 |     35.769 |     0.9
   23 |   1.0017 |     34.435 |   1.0182 |     34.647 |     0.9
   24 |   0.9774 |     33.498 |   1.0023 |     34.327 |     1.0
   25 |   0.9472 |     32.007 |   0.9891 |     33.878 |     1.0
   26 |   0.9278 |     31.597 |   0.9777 |     33.237 |     1.1
   27 |   0.9051 |     30.588 |   0.9627 |     32.436 |     1.1
   28 |   0.8744 |     29.372 |   0.9609 |     32.212 |     1.1
   29 |   0.8831 |     29.920 |   0.9547 |     32.083 |     1.2
   30 |   0.8294 |     27.570 |   0.9532 |     32.019 |     1.2
   31 |   0.8056 |     26.803 |   0.9346 |     31.538 |     1.3
   32 |   0.7806 |     25.657 |   0.9238 |     30.641 |     1.3
   33 |   0.7527 |     25.044 |   0.9125 |     29.840 |     1.3
   34 |   0.7255 |     23.625 |   0.9220 |     29.744 |     1.4
   35 |   0.7100 |     22.989 |   0.9098 |     29.327 |     1.4
   36 |   0.6821 |     22.069 |   0.9157 |     29.423 |     1.5
   37 |   0.6640 |     21.548 |   0.8947 |     28.718 |     1.5
   38 |   0.6345 |     20.617 |   0.9058 |     28.782 |     1.6
   39 |   0.6128 |     19.598 |   0.9028 |     28.365 |     1.6
   40 |   0.5903 |     19.001 |   0.8864 |     28.365 |     1.6
   41 |   0.5784 |     18.442 |   0.9041 |     27.885 |     1.7
   42 |   0.5507 |     17.636 |   0.9142 |     27.692 |     1.7
   43 |   0.5291 |     16.584 |   0.9075 |     28.013 |     1.8
   44 |   0.5047 |     16.042 |   0.9210 |     28.365 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 522,338

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3648 |     60.760 |   1.7801 |     49.936 |     0.0
    2 |   1.5763 |     47.173 |   1.4576 |     46.186 |     0.1
    3 |   1.4083 |     45.781 |   1.3801 |     45.994 |     0.1
    4 |   1.3555 |     45.453 |   1.3329 |     45.321 |     0.1
    5 |   1.3172 |     45.316 |   1.3152 |     45.673 |     0.2
    6 |   1.2947 |     44.987 |   1.2800 |     45.481 |     0.2
    7 |   1.2742 |     44.384 |   1.2559 |     43.333 |     0.2
    8 |   1.2550 |     44.039 |   1.2316 |     43.141 |     0.3
    9 |   1.2340 |     43.573 |   1.2237 |     43.397 |     0.3
   10 |   1.2150 |     42.949 |   1.1932 |     41.571 |     0.3
   11 |   1.1930 |     42.061 |   1.1755 |     40.609 |     0.4
   12 |   1.1737 |     40.993 |   1.1577 |     40.288 |     0.4
   13 |   1.1568 |     40.439 |   1.1394 |     39.263 |     0.5
   14 |   1.1371 |     39.585 |   1.1274 |     39.391 |     0.5
   15 |   1.1171 |     39.103 |   1.1138 |     38.750 |     0.5
   16 |   1.0866 |     37.716 |   1.0793 |     36.827 |     0.6
   17 |   1.0596 |     36.100 |   1.0625 |     35.577 |     0.6
   18 |   1.0369 |     35.399 |   1.0464 |     35.321 |     0.6
   19 |   1.0045 |     33.925 |   1.0316 |     35.449 |     0.7
   20 |   0.9853 |     33.388 |   1.0110 |     34.647 |     0.7
   21 |   0.9664 |     32.210 |   0.9949 |     33.718 |     0.7
   22 |   0.9409 |     31.717 |   0.9773 |     32.564 |     0.8
   23 |   0.9121 |     30.682 |   0.9760 |     33.333 |     0.8
   24 |   0.8853 |     29.405 |   0.9496 |     31.731 |     0.8
   25 |   0.8542 |     28.178 |   0.9421 |     31.891 |     0.9
   26 |   0.8317 |     27.290 |   0.9399 |     31.154 |     0.9
   27 |   0.8061 |     26.682 |   0.9072 |     30.449 |     0.9
   28 |   0.7815 |     25.416 |   0.9078 |     30.192 |     1.0
   29 |   0.7611 |     24.781 |   0.9252 |     30.673 |     1.0
   30 |   0.7317 |     23.778 |   0.8929 |     29.679 |     1.0
   31 |   0.7103 |     23.055 |   0.8890 |     28.942 |     1.1
   32 |   0.6835 |     21.762 |   0.8920 |     28.526 |     1.1
   33 |   0.6625 |     21.620 |   0.8981 |     29.359 |     1.2
   34 |   0.6357 |     20.502 |   0.8838 |     27.821 |     1.2
   35 |   0.6127 |     19.757 |   0.8842 |     27.756 |     1.2
   36 |   0.6002 |     19.351 |   0.8822 |     27.660 |     1.3
   37 |   0.5834 |     18.661 |   0.8881 |     27.019 |     1.3
   38 |   0.5643 |     18.217 |   0.8919 |     26.474 |     1.3
   39 |   0.5309 |     16.716 |   0.8909 |     26.314 |     1.4
   40 |   0.5268 |     16.721 |   0.8924 |     26.667 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,262,754

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4511 |     66.963 |   1.8985 |     49.135 |     0.1
    2 |   1.6814 |     47.524 |   1.5383 |     46.378 |     0.1
    3 |   1.4660 |     46.000 |   1.4494 |     46.378 |     0.2
    4 |   1.4215 |     46.083 |   1.4129 |     46.378 |     0.2
    5 |   1.3933 |     46.011 |   1.4049 |     46.410 |     0.3
    6 |   1.3839 |     45.962 |   1.3898 |     45.769 |     0.3
    7 |   1.3729 |     45.924 |   1.3824 |     46.026 |     0.4
    8 |   1.3529 |     45.705 |   1.3511 |     44.936 |     0.5
    9 |   1.3303 |     45.195 |   1.3177 |     44.551 |     0.5
   10 |   1.3063 |     44.921 |   1.2963 |     44.455 |     0.6
   11 |   1.2873 |     44.483 |   1.2794 |     43.429 |     0.6
   12 |   1.2789 |     43.705 |   1.2646 |     43.365 |     0.7
   13 |   1.2611 |     43.425 |   1.2503 |     43.237 |     0.8
   14 |   1.2467 |     43.135 |   1.2355 |     42.724 |     0.8
   15 |   1.2393 |     42.845 |   1.2311 |     42.788 |     0.9
   16 |   1.2296 |     42.943 |   1.2154 |     42.885 |     0.9
   17 |   1.2186 |     42.790 |   1.2048 |     41.987 |     1.0
   18 |   1.2114 |     42.176 |   1.2022 |     41.923 |     1.0
   19 |   1.2027 |     42.154 |   1.1888 |     41.635 |     1.1
   20 |   1.1969 |     42.099 |   1.1804 |     41.186 |     1.2
   21 |   1.1876 |     41.924 |   1.1701 |     41.058 |     1.2
   22 |   1.1784 |     41.639 |   1.1709 |     41.474 |     1.3
   23 |   1.1672 |     41.530 |   1.1594 |     41.442 |     1.3
   24 |   1.1625 |     41.185 |   1.1622 |     41.218 |     1.4
   25 |   1.1522 |     40.922 |   1.1424 |     40.256 |     1.5
   26 |   1.1472 |     40.554 |   1.1363 |     40.288 |     1.5
   27 |   1.1352 |     40.544 |   1.1556 |     40.897 |     1.6
   28 |   1.1310 |     40.341 |   1.1255 |     40.962 |     1.6
   29 |   1.1271 |     40.500 |   1.1262 |     40.673 |     1.7
   30 |   1.1168 |     40.067 |   1.1167 |     39.647 |     1.7
   31 |   1.1143 |     39.793 |   1.1190 |     39.712 |     1.8
   32 |   1.1157 |     40.094 |   1.1104 |     39.231 |     1.9
   33 |   1.0974 |     39.196 |   1.1069 |     39.519 |     1.9
   34 |   1.0939 |     38.998 |   1.1060 |     39.551 |     2.0
   35 |   1.0895 |     38.982 |   1.0922 |     38.846 |     2.0
   36 |   1.0814 |     38.779 |   1.0861 |     38.301 |     2.1
   37 |   1.0693 |     38.018 |   1.0798 |     37.885 |     2.2
   38 |   1.0615 |     37.547 |   1.0754 |     37.917 |     2.2
   39 |   1.0535 |     37.437 |   1.0666 |     36.699 |     2.3
   40 |   1.0461 |     37.196 |   1.0634 |     37.949 |     2.3
   41 |   1.0422 |     36.741 |   1.0622 |     37.788 |     2.4
   42 |   1.0369 |     36.484 |   1.0565 |     37.212 |     2.4
   43 |   1.0275 |     36.210 |   1.0486 |     37.115 |     2.5
   44 |   1.0184 |     35.651 |   1.0466 |     36.442 |     2.6
   45 |   1.0068 |     35.371 |   1.0452 |     36.603 |     2.6
   46 |   1.0080 |     35.426 |   1.0445 |     37.212 |     2.7
   47 |   0.9936 |     35.087 |   1.0352 |     35.673 |     2.7
   48 |   0.9855 |     34.473 |   1.0301 |     35.737 |     2.8
   49 |   0.9779 |     34.522 |   1.0250 |     35.577 |     2.9
   50 |   0.9671 |     33.832 |   1.0212 |     36.218 |     2.9
   51 |   0.9604 |     33.717 |   1.0124 |     35.513 |     3.0
   52 |   0.9493 |     33.048 |   1.0066 |     34.776 |     3.0
   53 |   0.9431 |     32.911 |   1.0118 |     34.551 |     3.1
   54 |   0.9289 |     32.457 |   1.0028 |     34.744 |     3.1
   55 |   0.9163 |     32.046 |   1.0103 |     34.455 |     3.2
   56 |   0.9209 |     32.112 |   0.9921 |     34.391 |     3.3
   57 |   0.9091 |     31.471 |   0.9869 |     33.814 |     3.3
   58 |   0.8941 |     30.873 |   0.9794 |     33.462 |     3.4
   59 |   0.8808 |     30.528 |   0.9793 |     33.878 |     3.4
   60 |   0.8849 |     30.375 |   0.9832 |     33.686 |     3.5
   61 |   0.8730 |     29.920 |   0.9650 |     33.397 |     3.6
   62 |   0.8595 |     29.339 |   0.9657 |     32.692 |     3.6
   63 |   0.8499 |     28.928 |   0.9755 |     33.397 |     3.7
   64 |   0.8444 |     28.742 |   0.9776 |     33.077 |     3.7
   65 |   0.8304 |     28.189 |   0.9592 |     32.404 |     3.8
   66 |   0.8168 |     27.794 |   0.9538 |     31.955 |     3.8
   67 |   0.8011 |     27.274 |   0.9628 |     32.115 |     3.9
   68 |   0.7937 |     26.803 |   0.9691 |     32.981 |     4.0
   69 |   0.7902 |     26.813 |   0.9536 |     32.853 |     4.0
   70 |   0.7679 |     25.783 |   0.9534 |     31.699 |     4.1
   71 |   0.7630 |     25.685 |   0.9530 |     30.962 |     4.1
   72 |   0.7562 |     25.351 |   0.9425 |     31.410 |     4.2
   73 |   0.7423 |     25.022 |   0.9477 |     31.571 |     4.3
   74 |   0.7371 |     24.452 |   0.9451 |     30.609 |     4.3
   75 |   0.7235 |     24.206 |   0.9554 |     31.154 |     4.4
   76 |   0.7101 |     23.619 |   0.9647 |     31.635 |     4.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 784,994

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5145 |     67.505 |   1.9715 |     58.878 |     0.0
    2 |   1.7374 |     49.447 |   1.5675 |     46.346 |     0.1
    3 |   1.4908 |     46.028 |   1.4634 |     46.346 |     0.1
    4 |   1.4293 |     46.006 |   1.4237 |     46.346 |     0.2
    5 |   1.4005 |     46.033 |   1.3969 |     46.346 |     0.2
    6 |   1.3822 |     45.957 |   1.3733 |     46.346 |     0.2
    7 |   1.3571 |     45.979 |   1.3501 |     45.994 |     0.3
    8 |   1.3347 |     45.628 |   1.3305 |     45.897 |     0.3
    9 |   1.3201 |     45.650 |   1.3155 |     45.192 |     0.4
   10 |   1.3077 |     45.425 |   1.3017 |     45.449 |     0.4
   11 |   1.2972 |     45.058 |   1.2851 |     44.615 |     0.4
   12 |   1.2817 |     44.877 |   1.2692 |     44.295 |     0.5
   13 |   1.2655 |     44.165 |   1.2533 |     44.071 |     0.5
   14 |   1.2507 |     44.149 |   1.2375 |     43.269 |     0.6
   15 |   1.2378 |     43.688 |   1.2236 |     43.205 |     0.6
   16 |   1.2235 |     43.184 |   1.2098 |     42.660 |     0.7
   17 |   1.2115 |     42.845 |   1.1996 |     42.147 |     0.7
   18 |   1.2046 |     42.582 |   1.1871 |     41.538 |     0.7
   19 |   1.1932 |     42.226 |   1.1741 |     40.769 |     0.8
   20 |   1.1849 |     41.853 |   1.1662 |     40.962 |     0.8
   21 |   1.1709 |     41.376 |   1.1549 |     41.186 |     0.9
   22 |   1.1609 |     41.157 |   1.1512 |     40.865 |     0.9
   23 |   1.1530 |     40.642 |   1.1369 |     40.192 |     0.9
   24 |   1.1430 |     40.281 |   1.1265 |     40.256 |     1.0
   25 |   1.1357 |     40.324 |   1.1203 |     39.263 |     1.0
   26 |   1.1283 |     39.815 |   1.1133 |     38.814 |     1.1
   27 |   1.1134 |     39.492 |   1.1091 |     38.718 |     1.1
   28 |   1.1038 |     38.779 |   1.1001 |     38.141 |     1.1
   29 |   1.0983 |     38.626 |   1.0878 |     37.500 |     1.2
   30 |   1.0841 |     38.177 |   1.0844 |     37.372 |     1.2
   31 |   1.0753 |     38.029 |   1.0815 |     37.724 |     1.3
   32 |   1.0759 |     37.952 |   1.0698 |     36.699 |     1.3
   33 |   1.0567 |     37.136 |   1.0611 |     36.154 |     1.3
   34 |   1.0436 |     36.610 |   1.0560 |     36.314 |     1.4
   35 |   1.0356 |     36.484 |   1.0516 |     35.833 |     1.4
   36 |   1.0363 |     36.528 |   1.0396 |     35.962 |     1.5
   37 |   1.0111 |     35.585 |   1.0313 |     36.186 |     1.5
   38 |   1.0035 |     35.224 |   1.0285 |     35.641 |     1.6
   39 |   0.9998 |     34.807 |   1.0257 |     35.192 |     1.6
   40 |   0.9846 |     34.341 |   1.0224 |     35.032 |     1.6
   41 |   0.9689 |     33.722 |   1.0153 |     34.359 |     1.7
   42 |   0.9669 |     33.328 |   1.0177 |     34.808 |     1.7
   43 |   0.9517 |     32.818 |   0.9985 |     33.301 |     1.8
   44 |   0.9390 |     32.325 |   1.0004 |     33.301 |     1.8
   45 |   0.9292 |     31.942 |   0.9964 |     32.756 |     1.8
   46 |   0.9162 |     31.262 |   0.9998 |     33.077 |     1.9
   47 |   0.9064 |     30.775 |   0.9866 |     33.462 |     1.9
   48 |   0.8915 |     30.402 |   0.9968 |     33.141 |     2.0
   49 |   0.8753 |     29.849 |   0.9908 |     32.468 |     2.0
   50 |   0.8688 |     29.520 |   0.9816 |     31.955 |     2.0
   51 |   0.8478 |     28.742 |   0.9820 |     33.237 |     2.1
   52 |   0.8463 |     28.594 |   0.9898 |     32.949 |     2.1
   53 |   0.8378 |     28.243 |   0.9871 |     33.590 |     2.2
   54 |   0.8241 |     27.460 |   0.9787 |     31.122 |     2.2
   55 |   0.8059 |     26.589 |   0.9841 |     31.571 |     2.2
   56 |   0.7929 |     26.244 |   0.9835 |     31.058 |     2.3
   57 |   0.7920 |     26.238 |   0.9712 |     31.122 |     2.3
   58 |   0.7621 |     25.192 |   0.9684 |     30.833 |     2.4
   59 |   0.7451 |     24.249 |   0.9720 |     30.545 |     2.4
   60 |   0.7351 |     23.439 |   0.9666 |     30.160 |     2.5
   61 |   0.7354 |     23.849 |   0.9625 |     29.776 |     2.5
   62 |   0.7121 |     23.028 |   0.9659 |     30.224 |     2.5
   63 |   0.7067 |     22.896 |   0.9678 |     30.032 |     2.6
   64 |   0.7015 |     22.671 |   0.9521 |     29.263 |     2.6
   65 |   0.6819 |     22.162 |   0.9663 |     29.327 |     2.7
   66 |   0.6635 |     21.351 |   0.9571 |     29.071 |     2.7
   67 |   0.6467 |     20.726 |   0.9600 |     28.686 |     2.7
   68 |   0.6346 |     20.327 |   0.9715 |     28.910 |     2.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 421,346

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5656 |     68.573 |   1.9577 |     54.263 |     0.0
    2 |   1.7101 |     47.852 |   1.5463 |     46.378 |     0.0
    3 |   1.4737 |     46.105 |   1.4466 |     46.346 |     0.1
    4 |   1.4130 |     46.000 |   1.4023 |     46.346 |     0.1
    5 |   1.3800 |     46.083 |   1.3737 |     46.346 |     0.1
    6 |   1.3524 |     45.458 |   1.3464 |     45.865 |     0.1
    7 |   1.3297 |     45.628 |   1.3258 |     45.737 |     0.2
    8 |   1.3091 |     45.266 |   1.3042 |     44.327 |     0.2
    9 |   1.2992 |     44.587 |   1.2886 |     43.942 |     0.2
   10 |   1.2749 |     44.324 |   1.2692 |     44.231 |     0.2
   11 |   1.2623 |     43.886 |   1.2650 |     44.006 |     0.3
   12 |   1.2499 |     43.606 |   1.2508 |     43.526 |     0.3
   13 |   1.2375 |     43.321 |   1.2418 |     43.205 |     0.3
   14 |   1.2283 |     42.861 |   1.2316 |     42.532 |     0.3
   15 |   1.2179 |     42.883 |   1.2213 |     41.987 |     0.4
   16 |   1.2120 |     42.757 |   1.2094 |     42.179 |     0.4
   17 |   1.2043 |     42.521 |   1.2043 |     42.115 |     0.4
   18 |   1.2058 |     42.828 |   1.2125 |     42.532 |     0.4
   19 |   1.2033 |     42.456 |   1.1911 |     42.244 |     0.5
   20 |   1.2021 |     42.850 |   1.1844 |     42.115 |     0.5
   21 |   1.1872 |     42.324 |   1.1756 |     42.019 |     0.5
   22 |   1.1764 |     42.083 |   1.1720 |     41.699 |     0.5
   23 |   1.1726 |     42.078 |   1.1724 |     41.859 |     0.5
   24 |   1.1643 |     41.754 |   1.1643 |     41.026 |     0.6
   25 |   1.1591 |     41.710 |   1.1586 |     41.346 |     0.6
   26 |   1.1561 |     41.327 |   1.1532 |     40.801 |     0.6
   27 |   1.1502 |     41.152 |   1.1574 |     41.827 |     0.6
   28 |   1.1407 |     41.130 |   1.1373 |     40.737 |     0.7
   29 |   1.1306 |     40.823 |   1.1344 |     40.032 |     0.7
   30 |   1.1303 |     40.505 |   1.1313 |     40.321 |     0.7
   31 |   1.1234 |     39.957 |   1.1296 |     39.615 |     0.7
   32 |   1.1179 |     40.155 |   1.1244 |     39.712 |     0.8
   33 |   1.1104 |     39.787 |   1.1069 |     39.263 |     0.8
   34 |   1.1071 |     39.672 |   1.1084 |     38.782 |     0.8
   35 |   1.0989 |     39.442 |   1.0962 |     38.910 |     0.8
   36 |   1.0976 |     39.108 |   1.0962 |     39.135 |     0.8
   37 |   1.0879 |     39.322 |   1.0897 |     38.365 |     0.9
   38 |   1.0788 |     38.522 |   1.0957 |     39.135 |     0.9
   39 |   1.0698 |     38.144 |   1.0815 |     39.006 |     0.9
   40 |   1.0621 |     37.558 |   1.0683 |     37.885 |     0.9
   41 |   1.0550 |     37.448 |   1.0652 |     37.147 |     1.0
   42 |   1.0488 |     37.158 |   1.0639 |     37.917 |     1.0
   43 |   1.0363 |     36.615 |   1.0684 |     37.244 |     1.0
   44 |   1.0257 |     36.341 |   1.0585 |     37.115 |     1.0
   45 |   1.0272 |     36.434 |   1.0416 |     36.635 |     1.1
   46 |   1.0107 |     35.448 |   1.0409 |     36.410 |     1.1
   47 |   1.0060 |     35.481 |   1.0391 |     36.186 |     1.1
   48 |   1.0090 |     35.782 |   1.0354 |     35.865 |     1.1
   49 |   0.9944 |     34.982 |   1.0298 |     36.154 |     1.2
   50 |   0.9845 |     34.511 |   1.0349 |     36.635 |     1.2
   51 |   0.9755 |     34.111 |   1.0121 |     35.449 |     1.2
   52 |   0.9633 |     33.706 |   1.0163 |     35.481 |     1.2
   53 |   0.9567 |     33.328 |   1.0105 |     35.096 |     1.2
   54 |   0.9480 |     33.065 |   0.9980 |     34.583 |     1.3
   55 |   0.9369 |     32.594 |   1.0022 |     34.968 |     1.3
   56 |   0.9371 |     32.522 |   0.9983 |     34.840 |     1.3
   57 |   0.9202 |     31.860 |   1.0008 |     34.808 |     1.3
   58 |   0.9167 |     31.860 |   0.9772 |     33.846 |     1.4
   59 |   0.8993 |     30.966 |   0.9756 |     33.654 |     1.4
   60 |   0.8994 |     31.071 |   0.9778 |     34.487 |     1.4
   61 |   0.8811 |     30.298 |   0.9611 |     32.788 |     1.4
   62 |   0.8707 |     29.586 |   0.9564 |     33.718 |     1.5
   63 |   0.8690 |     30.167 |   0.9628 |     33.045 |     1.5
   64 |   0.8570 |     29.421 |   0.9659 |     33.878 |     1.5
   65 |   0.8433 |     28.917 |   0.9536 |     33.045 |     1.5
   66 |   0.8367 |     28.589 |   0.9524 |     32.853 |     1.6
   67 |   0.8194 |     27.811 |   0.9374 |     32.115 |     1.6
   68 |   0.8085 |     27.652 |   0.9359 |     31.955 |     1.6
   69 |   0.7963 |     26.824 |   0.9498 |     32.340 |     1.6
   70 |   0.7939 |     26.693 |   0.9366 |     31.635 |     1.6
   71 |   0.7806 |     25.931 |   0.9433 |     31.474 |     1.7
   72 |   0.7807 |     26.079 |   0.9346 |     31.250 |     1.7
   73 |   0.7629 |     25.433 |   0.9289 |     30.673 |     1.7
   74 |   0.7557 |     25.033 |   0.9195 |     30.833 |     1.7
   75 |   0.7418 |     24.715 |   0.9327 |     30.994 |     1.8
   76 |   0.7334 |     24.222 |   0.9441 |     30.577 |     1.8
   77 |   0.7257 |     24.080 |   0.9317 |     30.064 |     1.8
   78 |   0.7147 |     23.603 |   0.9214 |     29.551 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 586,498

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2170 |     60.810 |   1.6160 |     46.346 |     0.0
    2 |   1.4727 |     46.214 |   1.4226 |     46.346 |     0.0
    3 |   1.3951 |     46.099 |   1.3921 |     46.026 |     0.1
    4 |   1.3556 |     45.880 |   1.3426 |     45.577 |     0.1
    5 |   1.3234 |     45.792 |   1.3199 |     45.994 |     0.1
    6 |   1.3081 |     45.442 |   1.3010 |     45.064 |     0.1
    7 |   1.2927 |     45.047 |   1.2865 |     44.551 |     0.1
    8 |   1.2782 |     44.757 |   1.2721 |     44.199 |     0.1
    9 |   1.2653 |     44.384 |   1.2592 |     44.647 |     0.2
   10 |   1.2551 |     44.395 |   1.2464 |     44.423 |     0.2
   11 |   1.2449 |     44.061 |   1.2332 |     43.558 |     0.2
   12 |   1.2317 |     43.699 |   1.2280 |     43.237 |     0.2
   13 |   1.2212 |     43.310 |   1.2116 |     42.692 |     0.2
   14 |   1.2041 |     43.179 |   1.1867 |     41.859 |     0.3
   15 |   1.1847 |     42.099 |   1.1782 |     41.154 |     0.3
   16 |   1.1713 |     41.311 |   1.1548 |     39.968 |     0.3
   17 |   1.1483 |     40.768 |   1.1478 |     40.096 |     0.3
   18 |   1.1308 |     39.749 |   1.1207 |     39.135 |     0.3
   19 |   1.1075 |     38.905 |   1.0989 |     37.596 |     0.3
   20 |   1.0824 |     37.957 |   1.0776 |     38.077 |     0.4
   21 |   1.0690 |     37.453 |   1.0667 |     38.109 |     0.4
   22 |   1.0466 |     36.812 |   1.0605 |     38.077 |     0.4
   23 |   1.0305 |     35.991 |   1.0461 |     36.955 |     0.4
   24 |   1.0033 |     35.289 |   1.0252 |     35.192 |     0.4
   25 |   0.9835 |     34.374 |   1.0186 |     35.224 |     0.5
   26 |   0.9668 |     33.629 |   1.0047 |     35.481 |     0.5
   27 |   0.9455 |     32.665 |   1.0027 |     34.936 |     0.5
   28 |   0.9365 |     32.555 |   0.9897 |     34.167 |     0.5
   29 |   0.9219 |     32.221 |   0.9706 |     32.788 |     0.5
   30 |   0.8926 |     31.021 |   0.9701 |     33.558 |     0.5
   31 |   0.8700 |     30.079 |   0.9467 |     32.179 |     0.6
   32 |   0.8487 |     29.071 |   0.9430 |     32.404 |     0.6
   33 |   0.8354 |     28.485 |   0.9440 |     31.891 |     0.6
   34 |   0.8108 |     27.476 |   0.9439 |     31.763 |     0.6
   35 |   0.7961 |     26.666 |   0.9363 |     31.410 |     0.6
   36 |   0.7786 |     26.392 |   0.9201 |     31.186 |     0.6
   37 |   0.7672 |     25.422 |   0.9281 |     30.545 |     0.7
   38 |   0.7406 |     24.737 |   0.9318 |     30.577 |     0.7
   39 |   0.7396 |     24.480 |   0.9226 |     29.968 |     0.7
   40 |   0.7156 |     23.751 |   0.9341 |     29.679 |     0.7
   41 |   0.6972 |     22.847 |   0.9102 |     30.096 |     0.7
   42 |   0.6804 |     22.387 |   0.9057 |     29.006 |     0.8
   43 |   0.6631 |     22.069 |   0.9024 |     28.750 |     0.8
   44 |   0.6601 |     21.926 |   0.9146 |     29.103 |     0.8
   45 |   0.6332 |     21.252 |   0.9158 |     28.654 |     0.8
   46 |   0.6198 |     20.332 |   0.9075 |     28.622 |     0.8
   47 |   0.6102 |     19.707 |   0.9146 |     28.494 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,622,498

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4911 |     66.716 |   1.9594 |     53.750 |     0.1
    2 |   1.7431 |     49.819 |   1.5861 |     46.378 |     0.2
    3 |   1.4855 |     46.000 |   1.4543 |     46.378 |     0.2
    4 |   1.4191 |     45.968 |   1.4182 |     46.378 |     0.3
    5 |   1.3945 |     45.924 |   1.3856 |     46.346 |     0.4
    6 |   1.3654 |     45.962 |   1.3666 |     46.026 |     0.5
    7 |   1.3507 |     45.480 |   1.3606 |     45.609 |     0.5
    8 |   1.3371 |     45.206 |   1.3221 |     45.449 |     0.6
    9 |   1.3181 |     45.283 |   1.3095 |     46.282 |     0.7
   10 |   1.3026 |     44.965 |   1.2996 |     46.058 |     0.8
   11 |   1.2909 |     45.058 |   1.2796 |     44.904 |     0.8
   12 |   1.2789 |     44.543 |   1.2673 |     44.295 |     0.9
   13 |   1.2645 |     44.412 |   1.2610 |     44.038 |     1.0
   14 |   1.2511 |     44.127 |   1.2525 |     44.263 |     1.1
   15 |   1.2460 |     44.066 |   1.2451 |     44.455 |     1.1
   16 |   1.2377 |     44.132 |   1.2359 |     43.910 |     1.2
   17 |   1.2310 |     43.716 |   1.2261 |     43.846 |     1.3
   18 |   1.2232 |     43.343 |   1.2183 |     43.173 |     1.4
   19 |   1.2141 |     43.617 |   1.2158 |     42.821 |     1.4
   20 |   1.2068 |     42.954 |   1.2005 |     42.692 |     1.5
   21 |   1.2029 |     42.883 |   1.1920 |     42.821 |     1.6
   22 |   1.1921 |     42.669 |   1.1872 |     42.179 |     1.7
   23 |   1.1832 |     42.247 |   1.1730 |     41.891 |     1.8
   24 |   1.1757 |     41.941 |   1.1650 |     41.378 |     1.8
   25 |   1.1702 |     41.919 |   1.1620 |     40.833 |     1.9
   26 |   1.1639 |     41.546 |   1.1580 |     41.571 |     2.0
   27 |   1.1590 |     41.491 |   1.1477 |     40.673 |     2.1
   28 |   1.1529 |     41.091 |   1.1436 |     40.641 |     2.1
   29 |   1.1472 |     40.724 |   1.1378 |     40.224 |     2.2
   30 |   1.1379 |     40.790 |   1.1369 |     40.256 |     2.3
   31 |   1.1333 |     40.642 |   1.1298 |     40.545 |     2.4
   32 |   1.1296 |     40.757 |   1.1209 |     39.968 |     2.4
   33 |   1.1246 |     40.357 |   1.1196 |     39.872 |     2.5
   34 |   1.1169 |     39.952 |   1.1114 |     39.647 |     2.6
   35 |   1.1069 |     39.837 |   1.1156 |     39.872 |     2.7
   36 |   1.1085 |     39.689 |   1.1045 |     39.231 |     2.7
   37 |   1.1003 |     39.218 |   1.0960 |     38.942 |     2.8
   38 |   1.0942 |     39.420 |   1.0938 |     38.878 |     2.9
   39 |   1.0929 |     39.009 |   1.0966 |     39.231 |     3.0
   40 |   1.0878 |     38.988 |   1.0870 |     38.910 |     3.0
   41 |   1.0827 |     38.763 |   1.1011 |     39.712 |     3.1
   42 |   1.0759 |     38.516 |   1.0730 |     39.006 |     3.2
   43 |   1.0719 |     38.483 |   1.0708 |     38.782 |     3.3
   44 |   1.0658 |     38.116 |   1.0630 |     37.981 |     3.3
   45 |   1.0614 |     38.270 |   1.0642 |     38.526 |     3.4
   46 |   1.0588 |     37.799 |   1.0672 |     38.237 |     3.5
   47 |   1.0485 |     37.777 |   1.0516 |     38.045 |     3.6
   48 |   1.0439 |     37.514 |   1.0488 |     38.109 |     3.6
   49 |   1.0409 |     37.190 |   1.0512 |     38.269 |     3.7
   50 |   1.0363 |     36.971 |   1.0457 |     37.468 |     3.8
   51 |   1.0410 |     36.878 |   1.0523 |     38.013 |     3.9
   52 |   1.0299 |     36.588 |   1.0358 |     37.596 |     4.0
   53 |   1.0213 |     36.555 |   1.0355 |     37.949 |     4.0
   54 |   1.0152 |     36.106 |   1.0284 |     37.212 |     4.1
   55 |   1.0120 |     36.259 |   1.0252 |     36.250 |     4.2
   56 |   1.0069 |     35.760 |   1.0268 |     36.699 |     4.3
   57 |   1.0019 |     35.678 |   1.0217 |     36.603 |     4.3
   58 |   0.9976 |     35.443 |   1.0300 |     36.795 |     4.4
   59 |   0.9961 |     35.536 |   1.0140 |     36.987 |     4.5
   60 |   0.9875 |     35.284 |   1.0188 |     36.122 |     4.6
   61 |   0.9837 |     35.081 |   1.0095 |     36.346 |     4.6
   62 |   0.9778 |     34.665 |   1.0113 |     35.577 |     4.7
   63 |   0.9847 |     35.169 |   1.0146 |     36.122 |     4.8
   64 |   0.9785 |     34.977 |   1.0125 |     35.801 |     4.9
   65 |   0.9678 |     34.330 |   1.0039 |     35.801 |     5.0
   66 |   0.9608 |     34.100 |   0.9917 |     35.128 |     5.0
   67 |   0.9651 |     34.391 |   0.9926 |     35.224 |     5.1
   68 |   0.9484 |     33.542 |   0.9959 |     35.064 |     5.2
   69 |   0.9450 |     33.383 |   0.9904 |     35.256 |     5.3
   70 |   0.9380 |     33.350 |   0.9863 |     35.192 |     5.3
   71 |   0.9311 |     32.911 |   0.9873 |     34.583 |     5.4
   72 |   0.9232 |     32.479 |   0.9816 |     34.487 |     5.5
   73 |   0.9138 |     32.040 |   0.9834 |     34.551 |     5.6
   74 |   0.9095 |     31.870 |   0.9867 |     34.327 |     5.6
   75 |   0.9068 |     31.586 |   0.9828 |     34.519 |     5.7
   76 |   0.9064 |     31.772 |   0.9826 |     34.231 |     5.8
   77 |   0.8884 |     31.005 |   0.9663 |     34.455 |     5.9
   78 |   0.8791 |     30.282 |   0.9740 |     33.846 |     5.9
   79 |   0.8764 |     30.199 |   0.9586 |     33.750 |     6.0
   80 |   0.8628 |     29.717 |   0.9692 |     33.269 |     6.1
   81 |   0.8562 |     29.268 |   0.9529 |     32.917 |     6.2
   82 |   0.8498 |     29.016 |   0.9427 |     32.917 |     6.2
   83 |   0.8341 |     28.775 |   0.9513 |     33.109 |     6.3
   84 |   0.8299 |     28.578 |   0.9390 |     32.756 |     6.4
   85 |   0.8328 |     28.304 |   0.9437 |     32.372 |     6.5
   86 |   0.8121 |     27.553 |   0.9404 |     32.404 |     6.6
   87 |   0.8021 |     27.071 |   0.9419 |     31.891 |     6.6
   88 |   0.7928 |     26.375 |   0.9327 |     31.795 |     6.7
   89 |   0.7766 |     25.866 |   0.9397 |     31.346 |     6.8
   90 |   0.7636 |     25.586 |   0.9374 |     31.731 |     6.9
   91 |   0.7605 |     24.984 |   0.9286 |     30.705 |     6.9
   92 |   0.7580 |     25.153 |   0.9203 |     30.994 |     7.0
   93 |   0.7432 |     24.353 |   0.9275 |     30.962 |     7.1
   94 |   0.7302 |     24.167 |   0.9217 |     30.449 |     7.2
   95 |   0.7210 |     23.652 |   0.9240 |     30.801 |     7.2
   96 |   0.7115 |     23.521 |   0.9331 |     30.769 |     7.3
   97 |   0.7045 |     22.880 |   0.9086 |     29.968 |     7.4
   98 |   0.6837 |     22.156 |   0.9185 |     30.833 |     7.5
   99 |   0.6880 |     22.480 |   0.8986 |     30.128 |     7.5
  100 |   0.6626 |     21.570 |   0.9050 |     29.840 |     7.6
  101 |   0.6595 |     21.170 |   0.9063 |     30.000 |     7.7
  102 |   0.6568 |     21.521 |   0.9211 |     29.808 |     7.8
  103 |   0.6327 |     20.316 |   0.9068 |     29.904 |     7.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 421,346

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5622 |     68.749 |   2.0065 |     58.558 |     0.0
    2 |   1.7829 |     50.559 |   1.5886 |     46.346 |     0.1
    3 |   1.4909 |     46.055 |   1.4491 |     46.346 |     0.1
    4 |   1.4176 |     46.044 |   1.4101 |     46.346 |     0.1
    5 |   1.3935 |     45.907 |   1.3888 |     46.410 |     0.1
    6 |   1.3714 |     45.803 |   1.3531 |     46.186 |     0.2
    7 |   1.3435 |     45.562 |   1.3284 |     45.256 |     0.2
    8 |   1.3223 |     45.036 |   1.3061 |     44.647 |     0.2
    9 |   1.3012 |     44.379 |   1.2853 |     44.199 |     0.2
   10 |   1.2853 |     44.433 |   1.2740 |     44.135 |     0.3
   11 |   1.2683 |     44.455 |   1.2563 |     44.295 |     0.3
   12 |   1.2555 |     44.143 |   1.2430 |     44.167 |     0.3
   13 |   1.2427 |     44.220 |   1.2336 |     44.103 |     0.3
   14 |   1.2359 |     44.034 |   1.2174 |     44.647 |     0.4
   15 |   1.2254 |     43.891 |   1.2091 |     43.045 |     0.4
   16 |   1.2181 |     43.705 |   1.2049 |     42.596 |     0.4
   17 |   1.2118 |     43.288 |   1.1942 |     42.981 |     0.4
   18 |   1.2029 |     42.724 |   1.1859 |     41.474 |     0.5
   19 |   1.1945 |     42.691 |   1.1787 |     42.083 |     0.5
   20 |   1.1888 |     42.127 |   1.1882 |     42.051 |     0.5
   21 |   1.1856 |     42.702 |   1.1645 |     41.186 |     0.5
   22 |   1.1809 |     42.094 |   1.1639 |     41.923 |     0.6
   23 |   1.1751 |     41.864 |   1.1595 |     41.827 |     0.6
   24 |   1.1651 |     41.705 |   1.1544 |     41.250 |     0.6
   25 |   1.1603 |     41.574 |   1.1481 |     41.250 |     0.6
   26 |   1.1553 |     41.190 |   1.1481 |     40.865 |     0.7
   27 |   1.1563 |     41.513 |   1.1373 |     40.577 |     0.7
   28 |   1.1492 |     41.152 |   1.1309 |     40.128 |     0.7
   29 |   1.1426 |     40.976 |   1.1268 |     40.032 |     0.7
   30 |   1.1375 |     41.102 |   1.1188 |     39.808 |     0.8
   31 |   1.1349 |     41.059 |   1.1220 |     40.000 |     0.8
   32 |   1.1315 |     40.697 |   1.1192 |     40.288 |     0.8
   33 |   1.1259 |     40.576 |   1.1109 |     39.776 |     0.8
   34 |   1.1226 |     40.593 |   1.1071 |     39.199 |     0.9
   35 |   1.1264 |     40.861 |   1.1049 |     40.064 |     0.9
   36 |   1.1135 |     40.281 |   1.1037 |     39.583 |     0.9
   37 |   1.1122 |     40.357 |   1.0974 |     38.878 |     0.9
   38 |   1.1089 |     39.963 |   1.0997 |     38.910 |     1.0
   39 |   1.1045 |     40.204 |   1.0888 |     39.487 |     1.0
   40 |   1.1105 |     40.176 |   1.0998 |     39.423 |     1.0
   41 |   1.0976 |     39.952 |   1.0913 |     39.391 |     1.0
   42 |   1.0963 |     39.645 |   1.0811 |     38.654 |     1.1
   43 |   1.0945 |     39.683 |   1.1114 |     40.673 |     1.1
   44 |   1.1067 |     40.291 |   1.0874 |     38.942 |     1.1
   45 |   1.0942 |     39.650 |   1.0810 |     38.526 |     1.1
   46 |   1.0854 |     39.289 |   1.0770 |     38.910 |     1.2
   47 |   1.0848 |     39.503 |   1.0788 |     38.526 |     1.2
   48 |   1.0819 |     39.327 |   1.0737 |     39.071 |     1.2
   49 |   1.0751 |     39.026 |   1.0676 |     38.622 |     1.2
   50 |   1.0728 |     39.146 |   1.0660 |     38.590 |     1.3
   51 |   1.0696 |     39.114 |   1.0608 |     37.628 |     1.3
   52 |   1.0641 |     38.604 |   1.0582 |     37.885 |     1.3
   53 |   1.0627 |     38.642 |   1.0549 |     37.949 |     1.3
   54 |   1.0640 |     38.653 |   1.0573 |     37.628 |     1.4
   55 |   1.0588 |     38.620 |   1.0558 |     38.718 |     1.4
   56 |   1.0569 |     38.144 |   1.0563 |     37.853 |     1.4
   57 |   1.0572 |     38.429 |   1.0469 |     38.077 |     1.5
   58 |   1.0534 |     38.577 |   1.0495 |     37.949 |     1.5
   59 |   1.0484 |     38.270 |   1.0446 |     38.045 |     1.5
   60 |   1.0489 |     38.264 |   1.0477 |     37.885 |     1.5
   61 |   1.0475 |     38.412 |   1.0380 |     37.660 |     1.6
   62 |   1.0461 |     38.330 |   1.0404 |     37.436 |     1.6
   63 |   1.0388 |     37.853 |   1.0356 |     37.115 |     1.6
   64 |   1.0393 |     37.842 |   1.0343 |     37.756 |     1.6
   65 |   1.0360 |     37.432 |   1.0394 |     37.788 |     1.7
   66 |   1.0362 |     37.552 |   1.0365 |     37.917 |     1.7
   67 |   1.0310 |     37.448 |   1.0314 |     37.308 |     1.7
   68 |   1.0303 |     37.601 |   1.0269 |     36.923 |     1.7
   69 |   1.0242 |     37.306 |   1.0213 |     37.083 |     1.8
   70 |   1.0250 |     37.196 |   1.0228 |     36.923 |     1.8
   71 |   1.0217 |     36.993 |   1.0247 |     36.987 |     1.8
   72 |   1.0184 |     36.911 |   1.0189 |     36.571 |     1.8
   73 |   1.0166 |     36.697 |   1.0256 |     36.987 |     1.9
   74 |   1.0184 |     36.944 |   1.0143 |     36.282 |     1.9
   75 |   1.0103 |     36.780 |   1.0174 |     36.923 |     1.9
   76 |   1.0105 |     36.725 |   1.0127 |     37.404 |     1.9
   77 |   1.0083 |     36.670 |   1.0134 |     36.410 |     2.0
   78 |   1.0026 |     36.407 |   1.0179 |     37.372 |     2.0
   79 |   1.0055 |     36.719 |   1.0193 |     36.891 |     2.0
   80 |   1.0162 |     37.196 |   1.0101 |     36.474 |     2.0
   81 |   0.9979 |     36.374 |   0.9966 |     36.186 |     2.1
   82 |   0.9939 |     36.182 |   0.9952 |     36.346 |     2.1
   83 |   0.9900 |     35.689 |   1.0009 |     36.218 |     2.1
   84 |   0.9941 |     35.925 |   0.9909 |     35.865 |     2.1
   85 |   0.9912 |     35.832 |   0.9920 |     36.314 |     2.2
   86 |   0.9865 |     35.848 |   0.9904 |     36.250 |     2.2
   87 |   0.9766 |     35.470 |   0.9851 |     35.994 |     2.2
   88 |   0.9765 |     35.130 |   0.9877 |     35.962 |     2.2
   89 |   0.9795 |     34.692 |   0.9813 |     35.481 |     2.3
   90 |   0.9723 |     34.774 |   0.9842 |     35.385 |     2.3
   91 |   0.9709 |     34.785 |   0.9904 |     35.705 |     2.3
   92 |   0.9606 |     34.369 |   0.9831 |     35.609 |     2.3
   93 |   0.9611 |     34.265 |   0.9704 |     34.615 |     2.4
   94 |   0.9620 |     34.467 |   0.9706 |     35.064 |     2.4
   95 |   0.9558 |     34.204 |   0.9683 |     34.551 |     2.4
   96 |   0.9552 |     34.089 |   0.9661 |     34.295 |     2.4
   97 |   0.9482 |     33.821 |   0.9663 |     34.263 |     2.5
   98 |   0.9398 |     33.350 |   0.9655 |     34.103 |     2.5
   99 |   0.9365 |     33.202 |   0.9603 |     33.686 |     2.5
  100 |   0.9353 |     33.196 |   0.9530 |     33.814 |     2.5
  101 |   0.9320 |     33.043 |   0.9564 |     33.494 |     2.6
  102 |   0.9264 |     32.922 |   0.9519 |     33.654 |     2.6
  103 |   0.9231 |     32.457 |   0.9515 |     33.558 |     2.6
  104 |   0.9225 |     32.402 |   0.9497 |     33.526 |     2.6
  105 |   0.9151 |     32.473 |   0.9542 |     34.263 |     2.7
  106 |   0.9125 |     32.325 |   0.9530 |     32.981 |     2.7
  107 |   0.9106 |     32.396 |   0.9442 |     33.333 |     2.7
  108 |   0.9108 |     32.270 |   0.9429 |     33.878 |     2.7
  109 |   0.9018 |     31.679 |   0.9495 |     33.333 |     2.8
  110 |   0.9016 |     31.761 |   0.9367 |     32.724 |     2.8
  111 |   0.8985 |     31.481 |   0.9919 |     34.519 |     2.8
  112 |   0.9302 |     32.758 |   0.9356 |     33.109 |     2.8
  113 |   0.8933 |     31.695 |   0.9330 |     32.404 |     2.9
  114 |   0.8925 |     31.509 |   0.9338 |     32.404 |     2.9
  115 |   0.8833 |     31.388 |   0.9410 |     32.917 |     2.9
  116 |   0.8837 |     31.295 |   0.9332 |     32.885 |     2.9
  117 |   0.8807 |     31.202 |   0.9328 |     32.660 |     3.0
  118 |   0.8804 |     31.197 |   0.9344 |     33.237 |     3.0
  119 |   0.8727 |     30.895 |   0.9164 |     32.147 |     3.0
  120 |   0.8649 |     30.676 |   0.9249 |     32.212 |     3.1
  121 |   0.8625 |     30.473 |   0.9285 |     32.692 |     3.1
  122 |   0.8695 |     30.539 |   0.9198 |     32.147 |     3.1
  123 |   0.8629 |     30.375 |   0.9186 |     32.340 |     3.1
  124 |   0.8601 |     30.199 |   0.9140 |     32.179 |     3.2
  125 |   0.8655 |     30.567 |   0.9136 |     31.987 |     3.2
  126 |   0.8572 |     29.947 |   0.9134 |     31.987 |     3.2
  127 |   0.8472 |     29.975 |   0.9121 |     31.891 |     3.2
  128 |   0.8455 |     29.887 |   0.9131 |     31.603 |     3.3
  129 |   0.8418 |     29.816 |   0.9130 |     31.827 |     3.3
  130 |   0.8474 |     29.679 |   0.9055 |     31.635 |     3.3
  131 |   0.8364 |     29.301 |   0.9131 |     31.603 |     3.3
  132 |   0.8356 |     29.345 |   0.9061 |     31.795 |     3.4
  133 |   0.8299 |     28.786 |   0.9064 |     31.795 |     3.4
  134 |   0.8420 |     29.619 |   0.9251 |     32.628 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 542,626

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9952 |     55.336 |   1.4754 |     45.769 |     0.0
    2 |   1.3931 |     45.496 |   1.3517 |     45.833 |     0.0
    3 |   1.3210 |     45.031 |   1.2902 |     44.391 |     0.1
    4 |   1.2872 |     44.406 |   1.2697 |     44.103 |     0.1
    5 |   1.2614 |     44.028 |   1.2371 |     42.821 |     0.1
    6 |   1.2360 |     43.343 |   1.2109 |     41.859 |     0.1
    7 |   1.2116 |     42.554 |   1.1865 |     41.955 |     0.1
    8 |   1.1870 |     41.995 |   1.1568 |     40.897 |     0.2
    9 |   1.1693 |     41.469 |   1.1362 |     40.256 |     0.2
   10 |   1.1455 |     40.861 |   1.1264 |     39.135 |     0.2
   11 |   1.1242 |     40.209 |   1.0962 |     38.686 |     0.2
   12 |   1.1011 |     39.415 |   1.0778 |     37.340 |     0.2
   13 |   1.0809 |     37.919 |   1.0541 |     36.731 |     0.3
   14 |   1.0568 |     37.169 |   1.0419 |     37.019 |     0.3
   15 |   1.0395 |     36.500 |   1.0423 |     36.218 |     0.3
   16 |   1.0140 |     35.563 |   1.0194 |     36.314 |     0.3
   17 |   0.9953 |     34.900 |   0.9963 |     34.904 |     0.3
   18 |   0.9760 |     34.095 |   0.9830 |     34.519 |     0.4
   19 |   0.9559 |     33.185 |   0.9597 |     33.205 |     0.4
   20 |   0.9311 |     32.402 |   0.9491 |     32.981 |     0.4
   21 |   0.9191 |     31.657 |   0.9444 |     32.532 |     0.4
   22 |   0.8977 |     31.032 |   0.9233 |     32.051 |     0.4
   23 |   0.8757 |     30.309 |   0.9148 |     31.538 |     0.5
   24 |   0.8530 |     29.235 |   0.9015 |     31.250 |     0.5
   25 |   0.8404 |     28.304 |   0.8987 |     30.577 |     0.5
   26 |   0.8218 |     27.794 |   0.8949 |     30.385 |     0.5
   27 |   0.8082 |     27.186 |   0.8679 |     29.359 |     0.5
   28 |   0.7747 |     26.309 |   0.8709 |     29.455 |     0.6
   29 |   0.7615 |     25.756 |   0.8821 |     29.840 |     0.6
   30 |   0.7630 |     25.964 |   0.8569 |     28.942 |     0.6
   31 |   0.7262 |     24.507 |   0.8494 |     28.942 |     0.6
   32 |   0.7072 |     23.718 |   0.8517 |     28.558 |     0.6
   33 |   0.6845 |     22.929 |   0.8410 |     27.949 |     0.7
   34 |   0.6737 |     22.699 |   0.8393 |     27.372 |     0.7
   35 |   0.6652 |     21.965 |   0.8381 |     27.436 |     0.7
   36 |   0.6480 |     21.724 |   0.8324 |     27.404 |     0.7
   37 |   0.6239 |     20.699 |   0.8334 |     26.859 |     0.7
   38 |   0.6182 |     20.348 |   0.8336 |     26.667 |     0.8
   39 |   0.6120 |     20.392 |   0.8294 |     26.378 |     0.8
   40 |   0.5894 |     19.176 |   0.8386 |     26.827 |     0.8
   41 |   0.5849 |     19.318 |   0.8373 |     26.186 |     0.8
   42 |   0.5677 |     18.973 |   0.8246 |     26.314 |     0.8
   43 |   0.5384 |     17.850 |   0.8316 |     26.282 |     0.9
   44 |   0.5255 |     17.401 |   0.8151 |     25.481 |     0.9
   45 |   0.5218 |     17.127 |   0.8245 |     25.897 |     0.9
   46 |   0.5004 |     16.108 |   0.8288 |     25.673 |     0.9
   47 |   0.5015 |     16.283 |   0.8272 |     25.385 |     0.9
   48 |   0.4862 |     16.130 |   0.8435 |     25.609 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 820,002

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3126 |     61.626 |   1.7486 |     49.167 |     0.0
    2 |   1.5737 |     47.107 |   1.4725 |     46.026 |     0.1
    3 |   1.4190 |     45.650 |   1.3911 |     45.481 |     0.1
    4 |   1.3714 |     45.496 |   1.3452 |     45.769 |     0.2
    5 |   1.3324 |     45.403 |   1.3102 |     45.641 |     0.2
    6 |   1.2976 |     44.801 |   1.2775 |     43.782 |     0.2
    7 |   1.2732 |     44.044 |   1.2556 |     43.878 |     0.3
    8 |   1.2553 |     43.601 |   1.2408 |     42.372 |     0.3
    9 |   1.2343 |     43.124 |   1.2138 |     42.724 |     0.4
   10 |   1.2141 |     42.565 |   1.1924 |     41.923 |     0.4
   11 |   1.1948 |     42.061 |   1.1761 |     41.699 |     0.5
   12 |   1.1784 |     41.261 |   1.1622 |     41.186 |     0.5
   13 |   1.1627 |     40.680 |   1.1549 |     40.577 |     0.5
   14 |   1.1532 |     40.302 |   1.1296 |     39.647 |     0.6
   15 |   1.1337 |     39.623 |   1.1274 |     39.647 |     0.6
   16 |   1.1261 |     39.064 |   1.1118 |     39.167 |     0.7
   17 |   1.1058 |     38.620 |   1.0971 |     38.590 |     0.7
   18 |   1.0943 |     38.303 |   1.0838 |     37.692 |     0.7
   19 |   1.0830 |     37.689 |   1.0782 |     36.635 |     0.8
   20 |   1.0769 |     37.508 |   1.0708 |     37.083 |     0.8
   21 |   1.0643 |     36.938 |   1.0663 |     36.923 |     0.9
   22 |   1.0519 |     36.171 |   1.0486 |     36.154 |     0.9
   23 |   1.0374 |     35.810 |   1.0388 |     35.801 |     1.0
   24 |   1.0239 |     35.245 |   1.0352 |     35.769 |     1.0
   25 |   1.0112 |     35.130 |   1.0270 |     35.833 |     1.0
   26 |   1.0021 |     34.259 |   1.0245 |     35.449 |     1.1
   27 |   0.9769 |     33.300 |   1.0073 |     35.385 |     1.1
   28 |   0.9671 |     33.207 |   1.0061 |     33.782 |     1.2
   29 |   0.9601 |     32.780 |   0.9953 |     34.423 |     1.2
   30 |   0.9400 |     32.418 |   0.9867 |     34.455 |     1.2
   31 |   0.9268 |     31.744 |   0.9746 |     33.494 |     1.3
   32 |   0.9096 |     30.950 |   0.9704 |     33.141 |     1.3
   33 |   0.8990 |     30.150 |   0.9540 |     32.853 |     1.4
   34 |   0.8814 |     29.701 |   0.9537 |     32.468 |     1.4
   35 |   0.8713 |     29.471 |   0.9494 |     31.923 |     1.5
   36 |   0.8497 |     28.506 |   0.9435 |     31.667 |     1.5
   37 |   0.8379 |     27.904 |   0.9336 |     30.865 |     1.5
   38 |   0.8189 |     27.307 |   0.9285 |     30.609 |     1.6
   39 |   0.8066 |     26.561 |   0.9196 |     30.096 |     1.6
   40 |   0.7850 |     25.915 |   0.9240 |     30.353 |     1.7
   41 |   0.7715 |     25.400 |   0.9127 |     29.840 |     1.7
   42 |   0.7552 |     24.819 |   0.9034 |     29.295 |     1.7
   43 |   0.7362 |     24.392 |   0.9065 |     29.583 |     1.8
   44 |   0.7146 |     23.340 |   0.9058 |     28.558 |     1.8
   45 |   0.7078 |     23.247 |   0.9044 |     28.846 |     1.9
   46 |   0.6801 |     22.211 |   0.9026 |     28.686 |     1.9
   47 |   0.6755 |     21.839 |   0.9013 |     28.974 |     2.0
   48 |   0.6525 |     21.450 |   0.8933 |     28.301 |     2.0
   49 |   0.6697 |     22.041 |   0.8929 |     27.596 |     2.0
   50 |   0.6462 |     21.192 |   0.8914 |     27.821 |     2.1
   51 |   0.6085 |     19.872 |   0.8921 |     27.179 |     2.1
   52 |   0.5906 |     18.858 |   0.8996 |     28.205 |     2.2
   53 |   0.5968 |     19.187 |   0.8930 |     27.276 |     2.2
   54 |   0.5809 |     18.891 |   0.8943 |     27.308 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,122,658

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1549 |     59.248 |   1.6053 |     49.103 |     0.1
    2 |   1.4721 |     46.066 |   1.4283 |     46.346 |     0.2
    3 |   1.4023 |     46.066 |   1.4058 |     46.346 |     0.3
    4 |   1.3725 |     45.836 |   1.3743 |     45.513 |     0.3
    5 |   1.3498 |     45.447 |   1.3445 |     45.481 |     0.4
    6 |   1.3273 |     45.420 |   1.3257 |     44.808 |     0.5
    7 |   1.3061 |     45.064 |   1.2988 |     44.936 |     0.6
    8 |   1.2867 |     44.636 |   1.2835 |     44.295 |     0.7
    9 |   1.2726 |     44.121 |   1.2569 |     44.327 |     0.8
   10 |   1.2544 |     44.028 |   1.2433 |     43.750 |     0.8
   11 |   1.2335 |     43.617 |   1.2239 |     42.853 |     0.9
   12 |   1.2138 |     42.784 |   1.2063 |     41.987 |     1.0
   13 |   1.1962 |     42.154 |   1.1791 |     41.090 |     1.1
   14 |   1.1804 |     42.001 |   1.1611 |     40.449 |     1.2
   15 |   1.1532 |     40.872 |   1.1366 |     40.673 |     1.3
   16 |   1.1315 |     40.061 |   1.1209 |     39.647 |     1.4
   17 |   1.1106 |     39.459 |   1.1002 |     38.494 |     1.4
   18 |   1.0891 |     38.314 |   1.0866 |     38.301 |     1.5
   19 |   1.0821 |     38.297 |   1.0812 |     37.564 |     1.6
   20 |   1.0558 |     36.895 |   1.0731 |     37.340 |     1.7
   21 |   1.0261 |     35.925 |   1.0460 |     36.282 |     1.8
   22 |   1.0005 |     34.867 |   1.0393 |     36.442 |     1.9
   23 |   0.9757 |     33.728 |   1.0236 |     34.840 |     1.9
   24 |   0.9478 |     32.353 |   1.0044 |     34.231 |     2.0
   25 |   0.9282 |     31.197 |   1.0040 |     33.942 |     2.1
   26 |   0.8993 |     30.304 |   0.9763 |     32.340 |     2.2
   27 |   0.8634 |     28.967 |   0.9693 |     31.795 |     2.3
   28 |   0.8355 |     28.128 |   0.9508 |     30.449 |     2.4
   29 |   0.8055 |     26.857 |   0.9449 |     31.378 |     2.5
   30 |   0.7961 |     26.627 |   0.9524 |     31.346 |     2.5
   31 |   0.7644 |     25.608 |   0.9363 |     30.160 |     2.6
   32 |   0.7447 |     24.721 |   0.9409 |     31.218 |     2.7
   33 |   0.7089 |     23.554 |   0.9213 |     29.744 |     2.8
   34 |   0.6785 |     22.513 |   0.9252 |     29.968 |     2.9
   35 |   0.6526 |     21.757 |   0.9201 |     29.423 |     3.0
   36 |   0.6213 |     20.507 |   0.9329 |     29.038 |     3.0
   37 |   0.6032 |     19.757 |   0.9237 |     28.718 |     3.1
   38 |   0.5867 |     19.154 |   0.9356 |     28.429 |     3.2
   39 |   0.5504 |     18.173 |   0.9207 |     27.532 |     3.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,709,346

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4491 |     65.160 |   1.8916 |     51.923 |     0.1
    2 |   1.6929 |     48.986 |   1.5461 |     46.346 |     0.2
    3 |   1.4735 |     46.011 |   1.4429 |     46.346 |     0.2
    4 |   1.4108 |     45.979 |   1.4157 |     46.442 |     0.3
    5 |   1.3809 |     45.726 |   1.3658 |     45.513 |     0.4
    6 |   1.3538 |     45.239 |   1.3464 |     45.609 |     0.5
    7 |   1.3356 |     45.392 |   1.3218 |     45.288 |     0.6
    8 |   1.3138 |     45.096 |   1.3055 |     45.192 |     0.7
    9 |   1.3005 |     44.883 |   1.2803 |     44.519 |     0.8
   10 |   1.2844 |     44.549 |   1.2683 |     44.231 |     0.8
   11 |   1.2671 |     44.428 |   1.2521 |     44.231 |     0.9
   12 |   1.2546 |     43.782 |   1.2418 |     43.622 |     1.0
   13 |   1.2383 |     43.601 |   1.2277 |     43.045 |     1.1
   14 |   1.2259 |     43.195 |   1.2135 |     42.244 |     1.2
   15 |   1.2049 |     42.434 |   1.1986 |     41.731 |     1.3
   16 |   1.1900 |     41.869 |   1.1892 |     41.667 |     1.3
   17 |   1.1766 |     41.075 |   1.1640 |     40.833 |     1.4
   18 |   1.1610 |     40.790 |   1.1525 |     39.744 |     1.5
   19 |   1.1448 |     40.001 |   1.1389 |     39.840 |     1.6
   20 |   1.1316 |     39.601 |   1.1265 |     38.846 |     1.7
   21 |   1.1159 |     38.944 |   1.1180 |     38.910 |     1.7
   22 |   1.1063 |     38.440 |   1.1014 |     38.109 |     1.8
   23 |   1.0831 |     37.673 |   1.0937 |     38.173 |     1.9
   24 |   1.0687 |     37.075 |   1.0902 |     37.532 |     2.0
   25 |   1.0590 |     36.418 |   1.0741 |     36.859 |     2.1
   26 |   1.0423 |     35.859 |   1.0684 |     37.404 |     2.2
   27 |   1.0260 |     35.432 |   1.0655 |     36.282 |     2.2
   28 |   1.0125 |     34.917 |   1.0547 |     36.186 |     2.3
   29 |   0.9913 |     34.035 |   1.0585 |     35.385 |     2.4
   30 |   0.9869 |     33.596 |   1.0455 |     34.968 |     2.5
   31 |   0.9703 |     33.070 |   1.0482 |     35.737 |     2.6
   32 |   0.9534 |     32.364 |   1.0403 |     35.513 |     2.7
   33 |   0.9333 |     31.794 |   1.0471 |     34.808 |     2.7
   34 |   0.9274 |     31.323 |   1.0343 |     34.455 |     2.8
   35 |   0.9042 |     30.610 |   1.0345 |     34.167 |     2.9
   36 |   0.8955 |     30.287 |   1.0276 |     33.846 |     3.0
   37 |   0.8773 |     29.695 |   1.0340 |     33.558 |     3.1
   38 |   0.8616 |     28.906 |   1.0268 |     34.038 |     3.2
   39 |   0.8558 |     28.769 |   1.0198 |     32.885 |     3.2
   40 |   0.8397 |     27.745 |   1.0192 |     32.724 |     3.3
   41 |   0.8129 |     26.529 |   1.0192 |     33.173 |     3.4
   42 |   0.8022 |     25.992 |   1.0167 |     32.147 |     3.5
   43 |   0.7805 |     25.016 |   1.0143 |     32.147 |     3.6
   44 |   0.7570 |     24.753 |   1.0010 |     31.923 |     3.7
   45 |   0.7435 |     24.074 |   0.9994 |     31.442 |     3.7
   46 |   0.7386 |     23.619 |   0.9977 |     31.058 |     3.8
   47 |   0.7221 |     23.351 |   0.9916 |     31.186 |     3.9
   48 |   0.7043 |     22.918 |   0.9972 |     30.449 |     4.0
   49 |   0.6815 |     21.767 |   0.9959 |     31.090 |     4.1
   50 |   0.6618 |     21.225 |   1.0021 |     30.769 |     4.2
   51 |   0.6571 |     20.776 |   0.9851 |     30.641 |     4.2
   52 |   0.6444 |     20.502 |   1.0185 |     30.417 |     4.3
   53 |   0.6326 |     20.420 |   0.9941 |     29.808 |     4.4
   54 |   0.6227 |     20.124 |   0.9764 |     29.038 |     4.5
   55 |   0.6082 |     19.855 |   0.9956 |     29.263 |     4.6
   56 |   0.5749 |     18.420 |   0.9923 |     28.622 |     4.6
   57 |   0.5656 |     18.212 |   0.9949 |     28.878 |     4.7
   58 |   0.5444 |     17.549 |   1.0193 |     28.878 |     4.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 771,298

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9836 |     55.572 |   1.4702 |     46.026 |     0.0
    2 |   1.3884 |     45.529 |   1.3335 |     45.160 |     0.1
    3 |   1.3089 |     44.428 |   1.2807 |     43.814 |     0.1
    4 |   1.2687 |     43.606 |   1.2424 |     42.596 |     0.1
    5 |   1.2367 |     42.998 |   1.2081 |     42.949 |     0.2
    6 |   1.2059 |     42.154 |   1.1772 |     41.058 |     0.2
    7 |   1.1776 |     41.513 |   1.1496 |     40.224 |     0.2
    8 |   1.1421 |     40.072 |   1.1206 |     39.071 |     0.3
    9 |   1.1111 |     39.311 |   1.0898 |     38.526 |     0.3
   10 |   1.0767 |     37.744 |   1.0666 |     37.340 |     0.3
   11 |   1.0425 |     36.095 |   1.0433 |     36.699 |     0.4
   12 |   1.0125 |     35.322 |   1.0160 |     35.481 |     0.4
   13 |   0.9863 |     33.925 |   1.0134 |     35.128 |     0.5
   14 |   0.9534 |     32.890 |   0.9640 |     33.462 |     0.5
   15 |   0.9230 |     31.317 |   0.9615 |     33.109 |     0.5
   16 |   0.8937 |     30.293 |   0.9421 |     32.468 |     0.6
   17 |   0.8761 |     29.789 |   0.9287 |     32.244 |     0.6
   18 |   0.8316 |     28.024 |   0.9093 |     31.218 |     0.6
   19 |   0.7902 |     26.364 |   0.8943 |     30.064 |     0.7
   20 |   0.7641 |     25.405 |   0.8946 |     29.872 |     0.7
   21 |   0.7328 |     24.162 |   0.8669 |     28.173 |     0.7
   22 |   0.7091 |     23.208 |   0.8700 |     28.301 |     0.8
   23 |   0.6655 |     21.620 |   0.8550 |     28.301 |     0.8
   24 |   0.6442 |     21.203 |   0.8486 |     27.179 |     0.8
   25 |   0.6063 |     19.746 |   0.8463 |     27.404 |     0.9
   26 |   0.5869 |     18.831 |   0.8495 |     27.756 |     0.9
   27 |   0.5685 |     18.557 |   0.8431 |     26.603 |     0.9
   28 |   0.5322 |     17.215 |   0.8393 |     26.058 |     1.0
   29 |   0.5142 |     16.617 |   0.8330 |     25.962 |     1.0
   30 |   0.4968 |     16.152 |   0.8575 |     26.442 |     1.0
   31 |   0.4895 |     15.702 |   0.8468 |     26.058 |     1.1
   32 |   0.4634 |     14.787 |   0.8526 |     26.122 |     1.1
   33 |   0.4358 |     14.026 |   0.8589 |     25.160 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,097,826

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1892 |     58.974 |   1.6075 |     46.378 |     0.0
    2 |   1.4705 |     46.154 |   1.4251 |     46.378 |     0.1
    3 |   1.3988 |     46.099 |   1.3942 |     46.827 |     0.1
    4 |   1.3703 |     45.874 |   1.3594 |     45.897 |     0.2
    5 |   1.3370 |     45.272 |   1.3317 |     45.449 |     0.2
    6 |   1.3169 |     45.337 |   1.3061 |     44.615 |     0.2
    7 |   1.2972 |     45.288 |   1.3050 |     45.737 |     0.3
    8 |   1.2857 |     44.521 |   1.2798 |     44.167 |     0.3
    9 |   1.2668 |     44.253 |   1.2513 |     44.487 |     0.4
   10 |   1.2494 |     44.483 |   1.2474 |     43.814 |     0.4
   11 |   1.2380 |     43.754 |   1.2376 |     44.359 |     0.4
   12 |   1.2230 |     43.677 |   1.2228 |     43.974 |     0.5
   13 |   1.2140 |     43.376 |   1.2021 |     42.885 |     0.5
   14 |   1.2061 |     43.256 |   1.1987 |     42.853 |     0.6
   15 |   1.1950 |     42.773 |   1.1844 |     42.436 |     0.6
   16 |   1.1848 |     42.532 |   1.1757 |     42.436 |     0.7
   17 |   1.1789 |     42.625 |   1.1662 |     42.051 |     0.7
   18 |   1.1740 |     42.275 |   1.1597 |     42.212 |     0.7
   19 |   1.1669 |     42.171 |   1.1542 |     42.179 |     0.8
   20 |   1.1623 |     42.127 |   1.1495 |     41.667 |     0.8
   21 |   1.1560 |     41.617 |   1.1496 |     41.122 |     0.9
   22 |   1.1455 |     41.650 |   1.1392 |     41.987 |     0.9
   23 |   1.1401 |     41.360 |   1.1327 |     41.154 |     0.9
   24 |   1.1370 |     41.004 |   1.1233 |     40.353 |     1.0
   25 |   1.1306 |     40.965 |   1.1187 |     39.872 |     1.0
   26 |   1.1286 |     40.428 |   1.1235 |     40.353 |     1.1
   27 |   1.1233 |     40.511 |   1.1132 |     40.385 |     1.1
   28 |   1.1135 |     40.231 |   1.1022 |     39.968 |     1.1
   29 |   1.1109 |     40.078 |   1.0965 |     39.808 |     1.2
   30 |   1.0959 |     39.508 |   1.0890 |     39.679 |     1.2
   31 |   1.0909 |     39.185 |   1.0790 |     38.397 |     1.3
   32 |   1.0840 |     39.393 |   1.0810 |     38.846 |     1.3
   33 |   1.0797 |     39.009 |   1.0742 |     38.782 |     1.4
   34 |   1.0721 |     38.982 |   1.0616 |     37.788 |     1.4
   35 |   1.0686 |     38.615 |   1.0613 |     38.141 |     1.4
   36 |   1.0643 |     38.730 |   1.0571 |     38.109 |     1.5
   37 |   1.0626 |     38.631 |   1.0582 |     37.660 |     1.5
   38 |   1.0565 |     38.264 |   1.0556 |     38.365 |     1.6
   39 |   1.0527 |     38.445 |   1.0538 |     38.718 |     1.6
   40 |   1.0509 |     38.401 |   1.0490 |     37.724 |     1.6
   41 |   1.0452 |     38.149 |   1.0438 |     38.077 |     1.7
   42 |   1.0406 |     38.133 |   1.0459 |     38.205 |     1.7
   43 |   1.0420 |     38.040 |   1.0471 |     38.237 |     1.8
   44 |   1.0399 |     37.864 |   1.0358 |     37.949 |     1.8
   45 |   1.0350 |     38.029 |   1.0382 |     37.051 |     1.8
   46 |   1.0319 |     37.985 |   1.0416 |     38.141 |     1.9
   47 |   1.0281 |     37.782 |   1.0323 |     37.917 |     1.9
   48 |   1.0291 |     37.771 |   1.0290 |     37.788 |     2.0
   49 |   1.0241 |     37.530 |   1.0361 |     38.365 |     2.0
   50 |   1.0257 |     37.475 |   1.0223 |     37.756 |     2.1
   51 |   1.0220 |     37.415 |   1.0332 |     37.468 |     2.1
   52 |   1.0190 |     37.284 |   1.0250 |     37.019 |     2.1
   53 |   1.0135 |     37.032 |   1.0200 |     37.564 |     2.2
   54 |   1.0127 |     37.037 |   1.0252 |     37.724 |     2.2
   55 |   1.0103 |     37.218 |   1.0225 |     36.603 |     2.3
   56 |   1.0130 |     37.021 |   1.0184 |     37.276 |     2.3
   57 |   1.0134 |     37.158 |   1.0144 |     37.212 |     2.3
   58 |   1.0121 |     37.278 |   1.0134 |     36.763 |     2.4
   59 |   1.0041 |     36.889 |   1.0142 |     36.635 |     2.4
   60 |   1.0018 |     37.043 |   1.0204 |     37.436 |     2.5
   61 |   1.0003 |     36.988 |   1.0125 |     37.147 |     2.5
   62 |   1.0005 |     36.906 |   1.0109 |     36.795 |     2.6
   63 |   0.9995 |     36.599 |   1.0206 |     37.083 |     2.6
   64 |   0.9967 |     36.654 |   1.0112 |     36.923 |     2.6
   65 |   1.0019 |     37.004 |   1.0013 |     36.699 |     2.7
   66 |   0.9929 |     36.571 |   1.0073 |     36.859 |     2.7
   67 |   0.9905 |     36.517 |   1.0007 |     36.026 |     2.8
   68 |   0.9909 |     36.675 |   1.0125 |     37.404 |     2.8
   69 |   1.0016 |     36.922 |   1.0018 |     37.115 |     2.8
   70 |   0.9894 |     36.697 |   1.0041 |     36.923 |     2.9
   71 |   0.9908 |     36.571 |   1.0081 |     36.763 |     2.9
   72 |   0.9831 |     36.248 |   0.9996 |     36.667 |     3.0
   73 |   0.9877 |     36.226 |   0.9997 |     36.923 |     3.0
   74 |   0.9818 |     36.139 |   1.0004 |     37.276 |     3.0
   75 |   0.9769 |     36.062 |   0.9928 |     36.538 |     3.1
   76 |   0.9790 |     35.969 |   0.9983 |     37.244 |     3.1
   77 |   0.9789 |     36.078 |   0.9968 |     36.378 |     3.2
   78 |   0.9760 |     36.117 |   0.9917 |     35.833 |     3.2
   79 |   0.9720 |     35.799 |   1.0093 |     36.923 |     3.3
   80 |   0.9704 |     35.958 |   0.9911 |     36.506 |     3.3
   81 |   0.9748 |     35.892 |   0.9941 |     37.115 |     3.3
   82 |   0.9652 |     35.706 |   0.9925 |     36.122 |     3.4
   83 |   0.9679 |     35.744 |   0.9967 |     36.635 |     3.4
   84 |   0.9645 |     35.744 |   0.9861 |     36.795 |     3.5
   85 |   0.9663 |     35.958 |   0.9986 |     36.635 |     3.5
   86 |   0.9632 |     35.393 |   0.9979 |     36.699 |     3.5
   87 |   0.9618 |     35.607 |   0.9953 |     36.731 |     3.6
   88 |   0.9550 |     35.180 |   0.9872 |     36.186 |     3.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 938,914

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1535 |     58.350 |   1.5840 |     46.346 |     0.0
    2 |   1.4541 |     46.061 |   1.4055 |     46.346 |     0.1
    3 |   1.3711 |     45.869 |   1.3502 |     45.705 |     0.1
    4 |   1.3365 |     45.310 |   1.3208 |     44.776 |     0.2
    5 |   1.3054 |     45.080 |   1.2924 |     44.840 |     0.2
    6 |   1.2882 |     44.669 |   1.2794 |     44.519 |     0.2
    7 |   1.2764 |     44.455 |   1.2568 |     44.455 |     0.3
    8 |   1.2545 |     44.083 |   1.2341 |     44.103 |     0.3
    9 |   1.2337 |     43.349 |   1.2180 |     43.045 |     0.3
   10 |   1.2133 |     42.565 |   1.2015 |     42.372 |     0.4
   11 |   1.1915 |     42.078 |   1.1723 |     40.994 |     0.4
   12 |   1.1668 |     41.272 |   1.1467 |     40.673 |     0.4
   13 |   1.1412 |     40.259 |   1.1173 |     38.942 |     0.5
   14 |   1.1224 |     39.563 |   1.1167 |     39.519 |     0.5
   15 |   1.1066 |     39.020 |   1.0884 |     38.269 |     0.6
   16 |   1.0736 |     37.963 |   1.0729 |     37.532 |     0.6
   17 |   1.0568 |     36.977 |   1.0626 |     37.692 |     0.6
   18 |   1.0328 |     36.621 |   1.0326 |     36.186 |     0.7
   19 |   1.0040 |     35.317 |   1.0164 |     35.929 |     0.7
   20 |   0.9866 |     34.435 |   1.0113 |     35.929 |     0.7
   21 |   0.9621 |     33.772 |   0.9904 |     34.679 |     0.8
   22 |   0.9406 |     32.764 |   0.9956 |     35.096 |     0.8
   23 |   0.9214 |     31.805 |   0.9648 |     33.173 |     0.9
   24 |   0.8973 |     30.862 |   0.9580 |     32.308 |     0.9
   25 |   0.8735 |     30.079 |   0.9571 |     32.244 |     0.9
   26 |   0.8469 |     28.824 |   0.9561 |     32.404 |     1.0
   27 |   0.8365 |     28.578 |   0.9434 |     30.865 |     1.0
   28 |   0.8072 |     27.356 |   0.9246 |     30.417 |     1.1
   29 |   0.7928 |     26.633 |   0.9312 |     30.609 |     1.1
   30 |   0.7844 |     26.266 |   0.9294 |     29.487 |     1.1
   31 |   0.7584 |     25.422 |   0.9214 |     30.064 |     1.2
   32 |   0.7314 |     24.463 |   0.9098 |     29.647 |     1.2
   33 |   0.7191 |     24.096 |   0.9160 |     28.429 |     1.2
   34 |   0.7005 |     23.444 |   0.9243 |     29.295 |     1.3
   35 |   0.6900 |     22.573 |   0.9084 |     28.526 |     1.3
   36 |   0.6792 |     22.704 |   0.9000 |     28.205 |     1.4
   37 |   0.6578 |     21.778 |   0.9050 |     27.885 |     1.4
   38 |   0.6396 |     20.639 |   0.9061 |     27.917 |     1.4
   39 |   0.6452 |     21.039 |   0.8954 |     28.013 |     1.5
   40 |   0.6146 |     19.938 |   0.8879 |     27.308 |     1.5
   41 |   0.5951 |     19.587 |   0.8999 |     27.308 |     1.5
   42 |   0.5733 |     18.628 |   0.8935 |     26.795 |     1.6
   43 |   0.5586 |     18.239 |   0.9078 |     26.859 |     1.6
   44 |   0.5535 |     17.993 |   0.8967 |     25.962 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 305,634

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5335 |     67.193 |   1.9559 |     54.551 |     0.0
    2 |   1.7361 |     49.140 |   1.5829 |     46.346 |     0.0
    3 |   1.5020 |     46.077 |   1.4656 |     46.827 |     0.1
    4 |   1.4291 |     46.055 |   1.4204 |     46.346 |     0.1
    5 |   1.4000 |     46.077 |   1.3946 |     46.346 |     0.1
    6 |   1.3762 |     45.880 |   1.3794 |     45.641 |     0.1
    7 |   1.3556 |     45.710 |   1.3577 |     45.994 |     0.1
    8 |   1.3450 |     45.327 |   1.3424 |     45.673 |     0.1
    9 |   1.3270 |     45.485 |   1.3250 |     45.641 |     0.2
   10 |   1.3099 |     45.053 |   1.3072 |     44.551 |     0.2
   11 |   1.2996 |     44.653 |   1.2921 |     44.135 |     0.2
   12 |   1.2822 |     44.379 |   1.2771 |     43.301 |     0.2
   13 |   1.2707 |     44.083 |   1.2628 |     43.013 |     0.2
   14 |   1.2606 |     43.803 |   1.2506 |     43.013 |     0.3
   15 |   1.2482 |     43.612 |   1.2434 |     43.429 |     0.3
   16 |   1.2382 |     43.256 |   1.2345 |     42.885 |     0.3
   17 |   1.2268 |     42.779 |   1.2265 |     42.276 |     0.3
   18 |   1.2195 |     42.872 |   1.2148 |     42.692 |     0.3
   19 |   1.2046 |     42.521 |   1.2063 |     42.115 |     0.3
   20 |   1.1984 |     42.001 |   1.1936 |     42.244 |     0.4
   21 |   1.1863 |     41.617 |   1.1859 |     41.827 |     0.4
   22 |   1.1706 |     41.360 |   1.1743 |     41.571 |     0.4
   23 |   1.1656 |     41.102 |   1.1615 |     40.801 |     0.4
   24 |   1.1561 |     40.735 |   1.1496 |     40.385 |     0.4
   25 |   1.1479 |     40.768 |   1.1417 |     40.256 |     0.4
   26 |   1.1330 |     40.308 |   1.1377 |     40.513 |     0.5
   27 |   1.1242 |     39.678 |   1.1248 |     39.615 |     0.5
   28 |   1.1149 |     39.344 |   1.1149 |     39.103 |     0.5
   29 |   1.1023 |     39.026 |   1.1154 |     39.776 |     0.5
   30 |   1.0908 |     38.609 |   1.1021 |     39.135 |     0.5
   31 |   1.0804 |     38.237 |   1.0965 |     38.846 |     0.6
   32 |   1.0677 |     37.727 |   1.0931 |     38.590 |     0.6
   33 |   1.0545 |     37.481 |   1.0843 |     37.500 |     0.6
   34 |   1.0471 |     36.719 |   1.0751 |     37.692 |     0.6
   35 |   1.0318 |     36.517 |   1.0703 |     37.244 |     0.6
   36 |   1.0223 |     35.886 |   1.0597 |     36.795 |     0.6
   37 |   1.0130 |     35.344 |   1.0548 |     36.763 |     0.7
   38 |   0.9979 |     34.725 |   1.0519 |     36.603 |     0.7
   39 |   0.9914 |     34.495 |   1.0416 |     35.929 |     0.7
   40 |   0.9723 |     33.783 |   1.0328 |     35.545 |     0.7
   41 |   0.9626 |     33.300 |   1.0340 |     35.513 |     0.7
   42 |   0.9524 |     32.873 |   1.0282 |     36.186 |     0.8
   43 |   0.9371 |     32.396 |   1.0173 |     34.615 |     0.8
   44 |   0.9359 |     31.766 |   1.0110 |     33.942 |     0.8
   45 |   0.9268 |     31.679 |   1.0061 |     34.103 |     0.8
   46 |   0.9048 |     30.868 |   1.0077 |     33.750 |     0.8
   47 |   0.8936 |     30.512 |   1.0068 |     34.263 |     0.8
   48 |   0.8794 |     30.035 |   0.9973 |     33.974 |     0.9
   49 |   0.8736 |     29.520 |   0.9929 |     33.333 |     0.9
   50 |   0.8670 |     29.334 |   0.9859 |     32.532 |     0.9
   51 |   0.8447 |     28.912 |   0.9825 |     32.244 |     0.9
   52 |   0.8394 |     28.397 |   0.9811 |     32.308 |     0.9
   53 |   0.8260 |     27.838 |   0.9795 |     31.859 |     1.0
   54 |   0.8166 |     27.772 |   0.9694 |     31.891 |     1.0
   55 |   0.8035 |     27.307 |   0.9639 |     31.538 |     1.0
   56 |   0.7952 |     26.835 |   0.9731 |     31.827 |     1.0
   57 |   0.8014 |     27.197 |   0.9684 |     31.635 |     1.0
   58 |   0.7811 |     26.304 |   0.9651 |     31.987 |     1.0
   59 |   0.7707 |     26.189 |   0.9785 |     31.603 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 586,498

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0540 |     57.188 |   1.5177 |     45.769 |     0.0
    2 |   1.4231 |     45.743 |   1.3661 |     45.000 |     0.1
    3 |   1.3377 |     45.102 |   1.3091 |     44.647 |     0.1
    4 |   1.2966 |     44.603 |   1.2710 |     44.263 |     0.1
    5 |   1.2672 |     44.006 |   1.2473 |     44.327 |     0.1
    6 |   1.2492 |     43.908 |   1.2252 |     42.788 |     0.2
    7 |   1.2260 |     42.938 |   1.2038 |     42.276 |     0.2
    8 |   1.2029 |     42.127 |   1.1797 |     41.859 |     0.2
    9 |   1.1857 |     41.853 |   1.1509 |     41.122 |     0.2
   10 |   1.1598 |     41.217 |   1.1283 |     40.256 |     0.3
   11 |   1.1339 |     40.089 |   1.1201 |     40.032 |     0.3
   12 |   1.1137 |     39.426 |   1.0880 |     38.173 |     0.3
   13 |   1.0867 |     38.352 |   1.0721 |     37.756 |     0.3
   14 |   1.0631 |     37.426 |   1.0522 |     37.436 |     0.4
   15 |   1.0353 |     36.369 |   1.0347 |     36.090 |     0.4
   16 |   1.0007 |     34.522 |   1.0097 |     35.192 |     0.4
   17 |   0.9790 |     33.772 |   0.9949 |     34.647 |     0.4
   18 |   0.9470 |     32.796 |   0.9740 |     34.615 |     0.5
   19 |   0.9241 |     31.449 |   0.9537 |     32.756 |     0.5
   20 |   0.8950 |     30.567 |   0.9417 |     32.019 |     0.5
   21 |   0.8568 |     29.032 |   0.9227 |     31.603 |     0.6
   22 |   0.8346 |     28.200 |   0.9122 |     31.571 |     0.6
   23 |   0.8138 |     27.482 |   0.8979 |     30.865 |     0.6
   24 |   0.7838 |     26.419 |   0.8884 |     29.679 |     0.6
   25 |   0.7550 |     25.362 |   0.8830 |     29.776 |     0.7
   26 |   0.7219 |     23.828 |   0.8730 |     29.231 |     0.7
   27 |   0.7216 |     24.091 |   0.8741 |     28.974 |     0.7
   28 |   0.6811 |     23.121 |   0.8722 |     28.654 |     0.7
   29 |   0.6463 |     21.368 |   0.8523 |     27.244 |     0.8
   30 |   0.6243 |     20.502 |   0.8636 |     27.853 |     0.8
   31 |   0.6161 |     20.244 |   0.8597 |     26.667 |     0.8
   32 |   0.5840 |     19.357 |   0.8656 |     27.019 |     0.8
   33 |   0.5702 |     18.628 |   0.8667 |     27.564 |     0.9
   34 |   0.5470 |     17.713 |   0.8446 |     26.378 |     0.9
   35 |   0.5264 |     17.204 |   0.8662 |     26.603 |     0.9
   36 |   0.5110 |     16.645 |   0.8549 |     26.314 |     1.0
   37 |   0.4965 |     16.223 |   0.8772 |     26.699 |     1.0
   38 |   0.4777 |     15.544 |   0.8689 |     25.865 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 868,450

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2261 |     60.459 |   1.6069 |     46.346 |     0.0
    2 |   1.4657 |     46.148 |   1.4323 |     46.346 |     0.1
    3 |   1.3984 |     46.291 |   1.3894 |     46.346 |     0.1
    4 |   1.3707 |     45.716 |   1.3718 |     45.385 |     0.1
    5 |   1.3349 |     45.496 |   1.3417 |     46.506 |     0.2
    6 |   1.3132 |     45.464 |   1.3005 |     44.391 |     0.2
    7 |   1.2967 |     44.784 |   1.2885 |     44.359 |     0.2
    8 |   1.2747 |     44.499 |   1.2675 |     45.064 |     0.3
    9 |   1.2567 |     44.554 |   1.2470 |     44.231 |     0.3
   10 |   1.2411 |     44.171 |   1.2235 |     43.365 |     0.3
   11 |   1.2256 |     43.447 |   1.2127 |     43.173 |     0.4
   12 |   1.2114 |     42.976 |   1.1919 |     41.891 |     0.4
   13 |   1.2023 |     42.549 |   1.1763 |     41.090 |     0.5
   14 |   1.1910 |     42.467 |   1.1784 |     41.090 |     0.5
   15 |   1.1813 |     41.941 |   1.1656 |     41.218 |     0.5
   16 |   1.1698 |     41.891 |   1.1547 |     40.737 |     0.6
   17 |   1.1604 |     41.453 |   1.1333 |     40.032 |     0.6
   18 |   1.1502 |     41.152 |   1.1327 |     40.481 |     0.6
   19 |   1.1456 |     41.294 |   1.1484 |     40.353 |     0.7
   20 |   1.1415 |     40.834 |   1.1205 |     40.224 |     0.7
   21 |   1.1322 |     40.659 |   1.1118 |     40.833 |     0.7
   22 |   1.1184 |     40.281 |   1.1033 |     40.417 |     0.8
   23 |   1.1214 |     40.193 |   1.0941 |     38.365 |     0.8
   24 |   1.1118 |     39.815 |   1.0960 |     38.974 |     0.8
   25 |   1.1023 |     39.870 |   1.0893 |     39.167 |     0.9
   26 |   1.0929 |     39.546 |   1.0768 |     38.237 |     0.9
   27 |   1.0877 |     39.327 |   1.0763 |     38.654 |     0.9
   28 |   1.0839 |     39.278 |   1.0740 |     38.526 |     1.0
   29 |   1.0773 |     39.218 |   1.0689 |     38.237 |     1.0
   30 |   1.0692 |     39.004 |   1.0606 |     38.558 |     1.0
   31 |   1.0644 |     38.697 |   1.0671 |     37.917 |     1.1
   32 |   1.0647 |     39.075 |   1.0505 |     38.013 |     1.1
   33 |   1.0549 |     38.577 |   1.0407 |     37.500 |     1.1
   34 |   1.0509 |     38.423 |   1.0460 |     37.821 |     1.2
   35 |   1.0482 |     38.297 |   1.0349 |     37.596 |     1.2
   36 |   1.0492 |     38.281 |   1.0367 |     37.372 |     1.2
   37 |   1.0365 |     38.040 |   1.0349 |     37.917 |     1.3
   38 |   1.0312 |     37.727 |   1.0427 |     38.365 |     1.3
   39 |   1.0319 |     37.755 |   1.0336 |     37.340 |     1.3
   40 |   1.0257 |     37.163 |   1.0281 |     37.051 |     1.4
   41 |   1.0221 |     37.086 |   1.0129 |     36.218 |     1.4
   42 |   1.0134 |     36.758 |   1.0117 |     36.891 |     1.4
   43 |   1.0119 |     36.960 |   1.0345 |     37.404 |     1.5
   44 |   1.0133 |     36.790 |   1.0051 |     35.545 |     1.5
   45 |   0.9981 |     36.040 |   1.0156 |     36.186 |     1.6
   46 |   0.9924 |     35.974 |   0.9986 |     35.288 |     1.6
   47 |   0.9897 |     35.810 |   1.0055 |     36.506 |     1.6
   48 |   0.9814 |     35.799 |   0.9977 |     36.026 |     1.7
   49 |   0.9700 |     35.070 |   0.9895 |     35.705 |     1.7
   50 |   0.9650 |     34.747 |   0.9785 |     35.641 |     1.7
   51 |   0.9634 |     34.292 |   0.9802 |     34.583 |     1.8
   52 |   0.9514 |     34.057 |   0.9770 |     34.295 |     1.8
   53 |   0.9435 |     33.673 |   0.9643 |     33.718 |     1.8
   54 |   0.9433 |     33.481 |   0.9714 |     34.776 |     1.9
   55 |   0.9372 |     33.465 |   0.9779 |     35.128 |     1.9
   56 |   0.9222 |     32.774 |   0.9775 |     34.455 |     1.9
   57 |   0.9216 |     32.824 |   0.9571 |     33.974 |     2.0
   58 |   0.9086 |     32.216 |   0.9395 |     33.365 |     2.0
   59 |   0.9039 |     31.986 |   0.9471 |     33.429 |     2.0
   60 |   0.8965 |     31.564 |   0.9418 |     32.756 |     2.1
   61 |   0.8944 |     31.520 |   0.9279 |     32.660 |     2.1
   62 |   0.8800 |     31.021 |   0.9335 |     33.333 |     2.1
   63 |   0.8825 |     30.966 |   0.9290 |     32.340 |     2.2
   64 |   0.8728 |     31.016 |   0.9208 |     32.212 |     2.2
   65 |   0.8606 |     29.964 |   0.9148 |     32.436 |     2.2
   66 |   0.8548 |     29.767 |   0.9034 |     31.250 |     2.3
   67 |   0.8455 |     29.410 |   0.9124 |     31.699 |     2.3
   68 |   0.8279 |     28.967 |   0.9054 |     31.154 |     2.3
   69 |   0.8199 |     28.408 |   0.8968 |     31.154 |     2.4
   70 |   0.8247 |     28.950 |   0.8936 |     31.026 |     2.4
   71 |   0.8129 |     28.457 |   0.8871 |     30.609 |     2.5
   72 |   0.8074 |     28.200 |   0.8916 |     31.186 |     2.5
   73 |   0.7923 |     27.948 |   0.8813 |     30.321 |     2.5
   74 |   0.7853 |     27.263 |   0.8761 |     30.160 |     2.6
   75 |   0.7744 |     26.835 |   0.8659 |     29.808 |     2.6
   76 |   0.8022 |     27.986 |   0.9002 |     30.833 |     2.6
   77 |   0.7938 |     27.794 |   0.8794 |     30.256 |     2.7
   78 |   0.7647 |     26.545 |   0.8729 |     30.000 |     2.7
   79 |   0.7470 |     25.712 |   0.8705 |     29.936 |     2.7
   80 |   0.7407 |     25.564 |   0.8542 |     29.583 |     2.8
   81 |   0.7312 |     25.230 |   0.8572 |     29.167 |     2.8
   82 |   0.7153 |     24.397 |   0.8675 |     29.295 |     2.8
   83 |   0.7218 |     24.633 |   0.8455 |     29.071 |     2.9
   84 |   0.7156 |     24.595 |   0.8600 |     29.199 |     2.9
   85 |   0.7081 |     24.255 |   0.8364 |     27.853 |     2.9
   86 |   0.6985 |     24.030 |   0.8448 |     28.237 |     3.0
   87 |   0.6944 |     23.778 |   0.8503 |     27.981 |     3.0
   88 |   0.6859 |     23.323 |   0.8513 |     28.365 |     3.0
   89 |   0.6719 |     22.956 |   0.8542 |     28.718 |     3.1
   90 |   0.6754 |     23.247 |   0.8350 |     28.333 |     3.1
   91 |   0.6623 |     22.617 |   0.8271 |     27.532 |     3.1
   92 |   0.6453 |     21.658 |   0.8184 |     27.340 |     3.2
   93 |   0.6426 |     21.598 |   0.8382 |     27.788 |     3.2
   94 |   0.6378 |     21.773 |   0.8164 |     27.115 |     3.2
   95 |   0.6309 |     21.170 |   0.8285 |     27.372 |     3.3
   96 |   0.6246 |     21.252 |   0.8256 |     27.468 |     3.3
   97 |   0.6083 |     20.595 |   0.8355 |     28.045 |     3.3
   98 |   0.6061 |     20.590 |   0.8247 |     26.699 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,132,770

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1989 |     60.465 |   1.6281 |     49.103 |     0.0
    2 |   1.4746 |     46.324 |   1.4143 |     46.346 |     0.1
    3 |   1.3784 |     45.968 |   1.3762 |     45.417 |     0.1
    4 |   1.3395 |     45.798 |   1.3242 |     44.968 |     0.2
    5 |   1.3167 |     45.064 |   1.3064 |     44.551 |     0.2
    6 |   1.2966 |     44.784 |   1.2798 |     44.295 |     0.2
    7 |   1.2780 |     44.132 |   1.2569 |     43.526 |     0.3
    8 |   1.2555 |     43.842 |   1.2436 |     43.750 |     0.3
    9 |   1.2369 |     43.497 |   1.2237 |     43.013 |     0.4
   10 |   1.2169 |     43.020 |   1.1996 |     42.564 |     0.4
   11 |   1.2003 |     42.510 |   1.1755 |     41.538 |     0.5
   12 |   1.1771 |     41.727 |   1.1679 |     41.827 |     0.5
   13 |   1.1624 |     41.349 |   1.1478 |     40.545 |     0.5
   14 |   1.1442 |     40.500 |   1.1224 |     39.744 |     0.6
   15 |   1.1196 |     39.815 |   1.1071 |     39.455 |     0.6
   16 |   1.0956 |     38.730 |   1.0864 |     38.718 |     0.7
   17 |   1.0704 |     38.067 |   1.0605 |     37.340 |     0.7
   18 |   1.0447 |     36.588 |   1.0486 |     36.314 |     0.8
   19 |   1.0247 |     35.706 |   1.0303 |     36.058 |     0.8
   20 |   0.9996 |     34.752 |   1.0161 |     35.064 |     0.8
   21 |   0.9749 |     33.744 |   1.0093 |     34.647 |     0.9
   22 |   0.9581 |     32.588 |   1.0024 |     34.455 |     0.9
   23 |   0.9288 |     31.635 |   0.9826 |     33.750 |     1.0
   24 |   0.9009 |     30.534 |   0.9746 |     32.628 |     1.0
   25 |   0.8883 |     29.509 |   0.9666 |     32.756 |     1.0
   26 |   0.8559 |     29.011 |   0.9565 |     31.667 |     1.1
   27 |   0.8360 |     28.117 |   0.9451 |     31.474 |     1.1
   28 |   0.8178 |     27.055 |   0.9327 |     30.962 |     1.2
   29 |   0.7971 |     26.863 |   0.9399 |     31.122 |     1.2
   30 |   0.7596 |     25.208 |   0.9407 |     30.769 |     1.3
   31 |   0.7420 |     24.699 |   0.9312 |     29.808 |     1.3
   32 |   0.7255 |     24.069 |   0.9256 |     29.647 |     1.3
   33 |   0.7013 |     23.280 |   0.9100 |     28.462 |     1.4
   34 |   0.6711 |     22.217 |   0.9176 |     28.526 |     1.4
   35 |   0.6540 |     21.368 |   0.9197 |     28.301 |     1.5
   36 |   0.6509 |     21.368 |   0.9066 |     27.853 |     1.5
   37 |   0.6232 |     20.524 |   0.9124 |     28.558 |     1.5
   38 |   0.6004 |     19.428 |   0.9130 |     28.013 |     1.6
   39 |   0.5886 |     19.149 |   0.9097 |     27.532 |     1.6
   40 |   0.5565 |     18.080 |   0.9278 |     27.596 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 388,866

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3590 |     62.678 |   1.7779 |     48.654 |     0.0
    2 |   1.5839 |     46.888 |   1.4664 |     45.769 |     0.1
    3 |   1.4231 |     45.803 |   1.3953 |     45.769 |     0.1
    4 |   1.3736 |     45.573 |   1.3511 |     45.096 |     0.1
    5 |   1.3420 |     45.387 |   1.3189 |     45.417 |     0.1
    6 |   1.3185 |     44.916 |   1.2954 |     44.263 |     0.2
    7 |   1.2973 |     44.275 |   1.2753 |     43.750 |     0.2
    8 |   1.2821 |     43.798 |   1.2593 |     43.077 |     0.2
    9 |   1.2636 |     43.623 |   1.2420 |     43.205 |     0.2
   10 |   1.2470 |     43.382 |   1.2251 |     42.628 |     0.3
   11 |   1.2263 |     43.091 |   1.2042 |     42.276 |     0.3
   12 |   1.2065 |     42.269 |   1.1849 |     41.474 |     0.3
   13 |   1.1902 |     41.815 |   1.1761 |     40.577 |     0.3
   14 |   1.1786 |     41.415 |   1.1632 |     40.385 |     0.4
   15 |   1.1636 |     41.152 |   1.1448 |     39.904 |     0.4
   16 |   1.1498 |     40.648 |   1.1314 |     39.679 |     0.4
   17 |   1.1345 |     40.226 |   1.1216 |     40.000 |     0.4
   18 |   1.1247 |     39.475 |   1.1006 |     38.333 |     0.5
   19 |   1.1084 |     39.130 |   1.0938 |     38.686 |     0.5
   20 |   1.0931 |     38.478 |   1.0799 |     37.372 |     0.5
   21 |   1.0818 |     37.716 |   1.0654 |     37.147 |     0.5
   22 |   1.0687 |     37.174 |   1.0617 |     36.635 |     0.6
   23 |   1.0550 |     36.697 |   1.0639 |     36.923 |     0.6
   24 |   1.0493 |     36.522 |   1.0382 |     35.962 |     0.6
   25 |   1.0258 |     35.717 |   1.0258 |     35.769 |     0.6
   26 |   1.0117 |     34.911 |   1.0181 |     35.160 |     0.7
   27 |   0.9958 |     34.522 |   1.0043 |     35.449 |     0.7
   28 |   0.9807 |     33.448 |   0.9955 |     34.744 |     0.7
   29 |   0.9606 |     32.983 |   0.9864 |     34.647 |     0.8
   30 |   0.9446 |     32.385 |   0.9772 |     33.269 |     0.8
   31 |   0.9283 |     31.361 |   0.9629 |     33.590 |     0.8
   32 |   0.9076 |     30.665 |   0.9601 |     33.462 |     0.8
   33 |   0.8925 |     29.728 |   0.9578 |     33.109 |     0.9
   34 |   0.8753 |     29.553 |   0.9415 |     32.628 |     0.9
   35 |   0.8617 |     28.726 |   0.9456 |     32.083 |     0.9
   36 |   0.8424 |     28.238 |   0.9391 |     31.923 |     0.9
   37 |   0.8139 |     27.055 |   0.9245 |     31.154 |     1.0
   38 |   0.7971 |     26.589 |   0.9172 |     31.090 |     1.0
   39 |   0.7976 |     26.129 |   0.9090 |     30.833 |     1.0
   40 |   0.7715 |     25.466 |   0.8976 |     30.288 |     1.0
   41 |   0.7454 |     24.584 |   0.9033 |     29.968 |     1.1
   42 |   0.7328 |     24.118 |   0.9046 |     30.513 |     1.1
   43 |   0.7135 |     23.219 |   0.8948 |     29.455 |     1.1
   44 |   0.6952 |     22.502 |   0.8965 |     29.263 |     1.1
   45 |   0.6909 |     22.387 |   0.8960 |     29.071 |     1.2
   46 |   0.6704 |     22.003 |   0.8893 |     29.071 |     1.2
   47 |   0.6461 |     21.110 |   0.8909 |     29.327 |     1.2
   48 |   0.6345 |     20.622 |   0.8846 |     28.750 |     1.2
   49 |   0.6295 |     20.420 |   0.8841 |     28.462 |     1.3
   50 |   0.6155 |     20.014 |   0.8845 |     28.397 |     1.3
   51 |   0.6013 |     19.494 |   0.9003 |     28.109 |     1.3
   52 |   0.6053 |     19.609 |   0.9011 |     28.462 |     1.4
   53 |   0.5704 |     18.442 |   0.8901 |     27.628 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 717,570

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3350 |     60.974 |   1.7715 |     48.622 |     0.0
    2 |   1.5837 |     47.184 |   1.4810 |     46.186 |     0.1
    3 |   1.4257 |     45.759 |   1.3886 |     45.833 |     0.1
    4 |   1.3601 |     45.250 |   1.3319 |     45.032 |     0.2
    5 |   1.3195 |     44.581 |   1.2991 |     44.199 |     0.2
    6 |   1.2883 |     44.225 |   1.2766 |     44.295 |     0.2
    7 |   1.2669 |     43.880 |   1.2598 |     44.487 |     0.3
    8 |   1.2574 |     43.924 |   1.2429 |     43.462 |     0.3
    9 |   1.2395 |     43.387 |   1.2283 |     42.532 |     0.3
   10 |   1.2239 |     43.113 |   1.2149 |     43.141 |     0.4
   11 |   1.2118 |     42.653 |   1.2019 |     42.372 |     0.4
   12 |   1.1964 |     42.078 |   1.1917 |     41.891 |     0.5
   13 |   1.1882 |     41.891 |   1.1856 |     40.769 |     0.5
   14 |   1.1742 |     41.606 |   1.1590 |     40.192 |     0.5
   15 |   1.1629 |     40.883 |   1.1488 |     40.417 |     0.6
   16 |   1.1524 |     40.653 |   1.1365 |     39.391 |     0.6
   17 |   1.1412 |     40.209 |   1.1239 |     39.135 |     0.7
   18 |   1.1280 |     39.837 |   1.1150 |     39.071 |     0.7
   19 |   1.1167 |     39.294 |   1.1073 |     38.878 |     0.7
   20 |   1.1087 |     38.834 |   1.0985 |     38.526 |     0.8
   21 |   1.0915 |     38.012 |   1.0772 |     37.340 |     0.8
   22 |   1.0768 |     37.848 |   1.0694 |     37.179 |     0.9
   23 |   1.0636 |     37.070 |   1.0664 |     37.051 |     0.9
   24 |   1.0515 |     36.467 |   1.0502 |     35.801 |     0.9
   25 |   1.0430 |     36.221 |   1.0362 |     35.865 |     1.0
   26 |   1.0290 |     35.580 |   1.0343 |     35.609 |     1.0
   27 |   1.0254 |     35.361 |   1.0343 |     35.673 |     1.1
   28 |   1.0115 |     34.982 |   1.0092 |     34.199 |     1.1
   29 |   0.9961 |     34.073 |   1.0013 |     34.263 |     1.1
   30 |   0.9825 |     33.569 |   1.0081 |     34.327 |     1.2
   31 |   0.9772 |     33.695 |   0.9917 |     33.718 |     1.2
   32 |   0.9763 |     33.279 |   0.9967 |     34.872 |     1.2
   33 |   0.9628 |     32.922 |   0.9911 |     33.782 |     1.3
   34 |   0.9532 |     32.599 |   0.9919 |     33.814 |     1.3
   35 |   0.9385 |     32.144 |   0.9972 |     34.295 |     1.4
   36 |   0.9354 |     31.881 |   0.9928 |     34.359 |     1.4
   37 |   0.9230 |     31.454 |   0.9719 |     33.654 |     1.4
   38 |   0.9039 |     30.758 |   0.9648 |     32.756 |     1.5
   39 |   0.9003 |     30.545 |   0.9488 |     32.628 |     1.5
   40 |   0.8825 |     30.019 |   0.9500 |     32.917 |     1.6
   41 |   0.8783 |     29.849 |   0.9581 |     32.372 |     1.6
   42 |   0.8649 |     29.186 |   0.9574 |     32.885 |     1.6
   43 |   0.8610 |     29.011 |   0.9374 |     32.660 |     1.7
   44 |   0.8391 |     28.452 |   0.9350 |     31.731 |     1.7
   45 |   0.8232 |     27.734 |   0.9332 |     32.019 |     1.8
   46 |   0.8121 |     27.175 |   0.9229 |     32.083 |     1.8
   47 |   0.8099 |     27.520 |   0.9371 |     31.923 |     1.8
   48 |   0.8249 |     27.701 |   0.9380 |     31.571 |     1.9
   49 |   0.7910 |     26.561 |   0.9125 |     31.314 |     1.9
   50 |   0.7619 |     25.241 |   0.9102 |     31.058 |     2.0
   51 |   0.7521 |     24.753 |   0.9002 |     29.712 |     2.0
   52 |   0.7240 |     23.707 |   0.8956 |     29.583 |     2.0
   53 |   0.7036 |     22.962 |   0.9007 |     29.968 |     2.1
   54 |   0.6919 |     22.584 |   0.8988 |     28.910 |     2.1
   55 |   0.6686 |     21.926 |   0.8988 |     29.295 |     2.1
   56 |   0.6513 |     21.400 |   0.8742 |     27.532 |     2.2
   57 |   0.6408 |     20.529 |   0.8831 |     28.718 |     2.2
   58 |   0.6311 |     20.173 |   0.8808 |     28.750 |     2.3
   59 |   0.6143 |     19.598 |   0.8830 |     27.660 |     2.3
   60 |   0.6065 |     19.521 |   0.8756 |     26.795 |     2.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,461,474

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0256 |     55.386 |   1.4954 |     46.378 |     0.1
    2 |   1.4122 |     45.951 |   1.3629 |     46.186 |     0.1
    3 |   1.3304 |     45.409 |   1.3037 |     44.455 |     0.2
    4 |   1.2856 |     44.077 |   1.2584 |     43.429 |     0.2
    5 |   1.2539 |     43.935 |   1.2292 |     42.821 |     0.3
    6 |   1.2268 |     43.343 |   1.2149 |     43.301 |     0.3
    7 |   1.2101 |     42.691 |   1.2012 |     42.468 |     0.4
    8 |   1.1875 |     42.423 |   1.1767 |     41.186 |     0.5
    9 |   1.1682 |     41.760 |   1.1491 |     40.417 |     0.5
   10 |   1.1475 |     41.179 |   1.1321 |     40.000 |     0.6
   11 |   1.1269 |     40.270 |   1.1111 |     38.974 |     0.6
   12 |   1.1091 |     39.442 |   1.1037 |     39.038 |     0.7
   13 |   1.0929 |     39.020 |   1.0802 |     37.788 |     0.7
   14 |   1.0690 |     38.051 |   1.0714 |     37.051 |     0.8
   15 |   1.0496 |     37.421 |   1.0422 |     36.603 |     0.9
   16 |   1.0351 |     36.648 |   1.0359 |     36.571 |     0.9
   17 |   1.0136 |     35.865 |   1.0176 |     35.737 |     1.0
   18 |   0.9952 |     35.185 |   1.0046 |     35.865 |     1.0
   19 |   0.9789 |     34.725 |   0.9957 |     34.327 |     1.1
   20 |   0.9619 |     33.991 |   1.0015 |     35.321 |     1.2
   21 |   0.9441 |     33.279 |   0.9672 |     33.878 |     1.2
   22 |   0.9179 |     32.424 |   0.9614 |     33.333 |     1.3
   23 |   0.9059 |     31.580 |   0.9645 |     33.429 |     1.3
   24 |   0.8894 |     30.775 |   0.9374 |     32.949 |     1.4
   25 |   0.8688 |     30.057 |   0.9446 |     33.205 |     1.4
   26 |   0.8526 |     29.684 |   0.9346 |     31.827 |     1.5
   27 |   0.8294 |     28.380 |   0.9309 |     31.506 |     1.6
   28 |   0.8128 |     27.690 |   0.9179 |     30.545 |     1.6
   29 |   0.7992 |     27.224 |   0.9255 |     31.442 |     1.7
   30 |   0.7799 |     26.293 |   0.9082 |     30.865 |     1.7
   31 |   0.7608 |     25.882 |   0.8986 |     29.904 |     1.8
   32 |   0.7351 |     24.584 |   0.8913 |     30.000 |     1.8
   33 |   0.7155 |     23.597 |   0.8947 |     29.519 |     1.9
   34 |   0.6924 |     22.880 |   0.8855 |     29.391 |     2.0
   35 |   0.6956 |     23.236 |   0.8952 |     29.263 |     2.0
   36 |   0.6510 |     21.751 |   0.8859 |     28.686 |     2.1
   37 |   0.6400 |     21.504 |   0.8852 |     28.526 |     2.1
   38 |   0.6061 |     20.058 |   0.8869 |     27.949 |     2.2
   39 |   0.5943 |     19.560 |   0.8802 |     28.526 |     2.3
   40 |   0.5798 |     19.039 |   0.8736 |     27.212 |     2.3
   41 |   0.5466 |     17.834 |   0.8915 |     28.494 |     2.4
   42 |   0.5349 |     17.483 |   0.8620 |     26.538 |     2.4
   43 |   0.5309 |     17.132 |   0.8768 |     27.212 |     2.5
   44 |   0.4931 |     15.922 |   0.8726 |     27.244 |     2.5
   45 |   0.4792 |     15.270 |   0.9012 |     26.538 |     2.6
   46 |   0.4756 |     15.423 |   0.8948 |     25.769 |     2.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 326,434

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3541 |     60.925 |   1.7955 |     48.558 |     0.0
    2 |   1.5963 |     47.628 |   1.4677 |     45.801 |     0.0
    3 |   1.4135 |     45.787 |   1.3685 |     44.744 |     0.1
    4 |   1.3483 |     45.020 |   1.3216 |     44.263 |     0.1
    5 |   1.3166 |     44.680 |   1.2966 |     44.135 |     0.1
    6 |   1.2978 |     44.428 |   1.2783 |     45.096 |     0.1
    7 |   1.2681 |     44.061 |   1.2470 |     43.397 |     0.1
    8 |   1.2516 |     43.584 |   1.2296 |     43.301 |     0.2
    9 |   1.2354 |     43.436 |   1.2126 |     42.724 |     0.2
   10 |   1.2193 |     42.795 |   1.1937 |     41.699 |     0.2
   11 |   1.2019 |     42.176 |   1.1840 |     41.442 |     0.2
   12 |   1.1901 |     41.995 |   1.1660 |     40.737 |     0.2
   13 |   1.1698 |     41.595 |   1.1543 |     40.160 |     0.3
   14 |   1.1580 |     40.982 |   1.1368 |     40.000 |     0.3
   15 |   1.1412 |     40.390 |   1.1205 |     38.910 |     0.3
   16 |   1.1291 |     39.508 |   1.1183 |     39.359 |     0.3
   17 |   1.1169 |     39.519 |   1.1032 |     38.590 |     0.3
   18 |   1.0990 |     38.604 |   1.0884 |     37.628 |     0.4
   19 |   1.0851 |     38.111 |   1.0671 |     37.019 |     0.4
   20 |   1.0745 |     37.519 |   1.0570 |     36.891 |     0.4
   21 |   1.0567 |     36.462 |   1.0441 |     35.897 |     0.4
   22 |   1.0395 |     35.854 |   1.0363 |     35.801 |     0.4
   23 |   1.0298 |     35.832 |   1.0239 |     35.577 |     0.5
   24 |   1.0175 |     35.152 |   1.0102 |     34.904 |     0.5
   25 |   1.0051 |     34.654 |   1.0141 |     35.449 |     0.5
   26 |   0.9872 |     33.931 |   0.9930 |     34.583 |     0.5
   27 |   0.9685 |     33.081 |   0.9802 |     33.365 |     0.5
   28 |   0.9599 |     32.917 |   0.9728 |     34.006 |     0.6
   29 |   0.9361 |     31.991 |   0.9599 |     33.045 |     0.6
   30 |   0.9204 |     31.218 |   0.9541 |     32.981 |     0.6
   31 |   0.9069 |     30.835 |   0.9386 |     32.468 |     0.6
   32 |   0.8866 |     29.805 |   0.9240 |     32.244 |     0.6
   33 |   0.8730 |     29.493 |   0.9209 |     31.410 |     0.6
   34 |   0.8566 |     28.528 |   0.9217 |     32.019 |     0.7
   35 |   0.8386 |     28.101 |   0.9157 |     30.897 |     0.7
   36 |   0.8208 |     27.674 |   0.9122 |     30.705 |     0.7
   37 |   0.8167 |     27.482 |   0.8999 |     30.897 |     0.7
   38 |   0.8068 |     26.896 |   0.8964 |     30.513 |     0.7
   39 |   0.7946 |     26.507 |   0.8967 |     30.545 |     0.8
   40 |   0.7650 |     25.701 |   0.8849 |     29.391 |     0.8
   41 |   0.7438 |     24.693 |   0.8808 |     29.936 |     0.8
   42 |   0.7372 |     24.480 |   0.8778 |     28.974 |     0.8
   43 |   0.7194 |     24.030 |   0.8795 |     29.263 |     0.8
   44 |   0.7259 |     24.167 |   0.8714 |     29.231 |     0.9
   45 |   0.7012 |     23.006 |   0.8652 |     28.301 |     0.9
   46 |   0.6844 |     22.518 |   0.8661 |     28.974 |     0.9
   47 |   0.6687 |     21.735 |   0.8797 |     28.301 |     0.9
   48 |   0.6678 |     21.603 |   0.8673 |     28.045 |     0.9
   49 |   0.6485 |     21.137 |   0.8798 |     28.269 |     1.0
   50 |   0.6337 |     21.017 |   0.8617 |     27.436 |     1.0
   51 |   0.6217 |     20.403 |   0.8688 |     27.692 |     1.0
   52 |   0.6096 |     19.839 |   0.8582 |     27.564 |     1.0
   53 |   0.6496 |     21.384 |   0.8674 |     27.821 |     1.0
   54 |   0.6116 |     20.283 |   0.8681 |     27.949 |     1.1
   55 |   0.6014 |     19.494 |   0.8610 |     27.308 |     1.1
   56 |   0.5916 |     19.220 |   0.8711 |     27.596 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 820,002

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3403 |     61.703 |   1.7935 |     48.558 |     0.0
    2 |   1.5950 |     47.348 |   1.4818 |     46.346 |     0.1
    3 |   1.4206 |     45.825 |   1.3834 |     46.250 |     0.1
    4 |   1.3609 |     45.584 |   1.3439 |     45.288 |     0.2
    5 |   1.3293 |     45.014 |   1.3158 |     44.679 |     0.2
    6 |   1.3097 |     44.833 |   1.3004 |     43.910 |     0.2
    7 |   1.2907 |     44.154 |   1.2727 |     43.077 |     0.3
    8 |   1.2758 |     43.820 |   1.2615 |     42.981 |     0.3
    9 |   1.2610 |     43.343 |   1.2466 |     42.532 |     0.4
   10 |   1.2427 |     43.031 |   1.2319 |     42.244 |     0.4
   11 |   1.2286 |     42.521 |   1.2182 |     42.564 |     0.5
   12 |   1.2127 |     42.302 |   1.1957 |     41.571 |     0.5
   13 |   1.1945 |     41.716 |   1.1799 |     41.058 |     0.5
   14 |   1.1754 |     40.790 |   1.1593 |     40.801 |     0.6
   15 |   1.1494 |     40.067 |   1.1520 |     40.192 |     0.6
   16 |   1.1242 |     38.949 |   1.1328 |     38.526 |     0.7
   17 |   1.1051 |     37.963 |   1.1172 |     38.237 |     0.7
   18 |   1.0793 |     36.818 |   1.0925 |     37.083 |     0.7
   19 |   1.0470 |     35.377 |   1.0743 |     36.891 |     0.8
   20 |   1.0197 |     34.391 |   1.0503 |     35.705 |     0.8
   21 |   0.9918 |     32.972 |   1.0303 |     34.071 |     0.9
   22 |   0.9598 |     31.925 |   1.0260 |     34.583 |     0.9
   23 |   0.9195 |     30.314 |   0.9994 |     33.654 |     1.0
   24 |   0.8861 |     29.263 |   0.9880 |     33.205 |     1.0
   25 |   0.8634 |     28.413 |   0.9710 |     32.372 |     1.0
   26 |   0.8217 |     26.896 |   0.9637 |     31.218 |     1.1
   27 |   0.7841 |     25.318 |   0.9482 |     30.737 |     1.1
   28 |   0.7649 |     24.518 |   0.9469 |     30.962 |     1.2
   29 |   0.7456 |     24.211 |   0.9361 |     30.353 |     1.2
   30 |   0.6921 |     22.091 |   0.9293 |     28.814 |     1.3
   31 |   0.6627 |     20.940 |   0.9286 |     28.974 |     1.3
   32 |   0.6362 |     19.817 |   0.9458 |     28.910 |     1.3
   33 |   0.6163 |     19.521 |   0.9526 |     28.814 |     1.4
   34 |   0.5831 |     18.256 |   0.9384 |     28.429 |     1.4
   35 |   0.5566 |     17.488 |   0.9426 |     28.141 |     1.5
   36 |   0.5374 |     16.880 |   0.9244 |     27.853 |     1.5
   37 |   0.5053 |     15.691 |   0.9383 |     27.372 |     1.5
   38 |   0.4847 |     14.804 |   0.9483 |     27.628 |     1.6
   39 |   0.4580 |     14.163 |   0.9315 |     26.859 |     1.6
   40 |   0.4419 |     13.670 |   0.9498 |     27.532 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 307,874

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4009 |     64.250 |   1.8500 |     48.750 |     0.0
    2 |   1.6324 |     47.622 |   1.5049 |     46.218 |     0.0
    3 |   1.4427 |     45.929 |   1.4085 |     46.667 |     0.1
    4 |   1.3737 |     45.628 |   1.3547 |     46.250 |     0.1
    5 |   1.3380 |     45.042 |   1.3218 |     44.583 |     0.1
    6 |   1.3167 |     44.833 |   1.2995 |     44.391 |     0.1
    7 |   1.2974 |     44.417 |   1.2848 |     43.974 |     0.1
    8 |   1.2773 |     44.247 |   1.2659 |     43.494 |     0.2
    9 |   1.2649 |     43.782 |   1.2555 |     42.853 |     0.2
   10 |   1.2483 |     43.398 |   1.2264 |     41.827 |     0.2
   11 |   1.2341 |     42.801 |   1.2231 |     42.051 |     0.2
   12 |   1.2196 |     42.527 |   1.2170 |     43.301 |     0.2
   13 |   1.2122 |     42.538 |   1.1974 |     42.147 |     0.3
   14 |   1.1970 |     42.154 |   1.1887 |     41.827 |     0.3
   15 |   1.1922 |     41.705 |   1.1763 |     40.609 |     0.3
   16 |   1.1785 |     41.234 |   1.1632 |     40.545 |     0.3
   17 |   1.1637 |     40.889 |   1.1541 |     39.647 |     0.3
   18 |   1.1511 |     40.434 |   1.1405 |     39.423 |     0.4
   19 |   1.1412 |     39.853 |   1.1258 |     39.391 |     0.4
   20 |   1.1235 |     39.097 |   1.1248 |     38.333 |     0.4
   21 |   1.1162 |     38.829 |   1.1009 |     38.269 |     0.4
   22 |   1.0972 |     38.346 |   1.0967 |     37.756 |     0.4
   23 |   1.0852 |     37.475 |   1.0760 |     37.308 |     0.5
   24 |   1.0693 |     37.218 |   1.0742 |     37.436 |     0.5
   25 |   1.0560 |     36.473 |   1.0579 |     35.929 |     0.5
   26 |   1.0387 |     35.854 |   1.0455 |     36.282 |     0.5
   27 |   1.0224 |     35.065 |   1.0444 |     36.090 |     0.5
   28 |   1.0099 |     34.835 |   1.0394 |     35.288 |     0.5
   29 |   0.9992 |     34.380 |   1.0217 |     35.321 |     0.6
   30 |   0.9824 |     33.772 |   1.0113 |     34.487 |     0.6
   31 |   0.9738 |     33.394 |   0.9973 |     34.583 |     0.6
   32 |   0.9588 |     32.588 |   0.9938 |     33.782 |     0.6
   33 |   0.9404 |     31.953 |   0.9891 |     34.263 |     0.6
   34 |   0.9254 |     31.158 |   0.9772 |     33.494 |     0.7
   35 |   0.9019 |     30.484 |   0.9623 |     32.724 |     0.7
   36 |   0.8929 |     30.150 |   0.9648 |     32.500 |     0.7
   37 |   0.8790 |     29.520 |   0.9544 |     32.788 |     0.7
   38 |   0.8564 |     28.945 |   0.9434 |     32.468 |     0.7
   39 |   0.8344 |     27.679 |   0.9490 |     32.051 |     0.8
   40 |   0.8297 |     27.416 |   0.9288 |     30.481 |     0.8
   41 |   0.8112 |     27.071 |   0.9222 |     30.929 |     0.8
   42 |   0.7941 |     26.183 |   0.9303 |     30.128 |     0.8
   43 |   0.7806 |     25.773 |   0.9236 |     30.545 |     0.8
   44 |   0.7622 |     25.312 |   0.9165 |     29.744 |     0.9
   45 |   0.7453 |     24.710 |   0.9182 |     30.032 |     0.9
   46 |   0.7357 |     23.849 |   0.9043 |     29.103 |     0.9
   47 |   0.7190 |     23.537 |   0.9103 |     29.295 |     0.9
   48 |   0.7033 |     22.781 |   0.9130 |     29.519 |     0.9
   49 |   0.7009 |     22.869 |   0.9018 |     28.750 |     1.0
   50 |   0.6741 |     22.030 |   0.8980 |     28.718 |     1.0
   51 |   0.6559 |     21.148 |   0.8947 |     28.462 |     1.0
   52 |   0.6387 |     20.726 |   0.8873 |     28.013 |     1.0
   53 |   0.6280 |     20.206 |   0.8998 |     28.686 |     1.0
   54 |   0.6233 |     19.921 |   0.8952 |     27.660 |     1.1
   55 |   0.6020 |     19.275 |   0.9087 |     28.365 |     1.1
   56 |   0.5874 |     18.666 |   0.9010 |     27.692 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 557,858

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3704 |     63.215 |   1.7938 |     50.673 |     0.0
    2 |   1.5766 |     47.414 |   1.4503 |     45.769 |     0.1
    3 |   1.4071 |     45.710 |   1.3734 |     45.769 |     0.1
    4 |   1.3540 |     45.600 |   1.3283 |     45.321 |     0.1
    5 |   1.3204 |     44.631 |   1.2954 |     43.750 |     0.2
    6 |   1.2989 |     44.138 |   1.2697 |     43.814 |     0.2
    7 |   1.2712 |     43.831 |   1.2449 |     43.205 |     0.3
    8 |   1.2516 |     43.699 |   1.2220 |     42.628 |     0.3
    9 |   1.2312 |     42.899 |   1.2037 |     43.013 |     0.3
   10 |   1.2112 |     42.445 |   1.1843 |     41.186 |     0.4
   11 |   1.1912 |     41.710 |   1.1657 |     41.218 |     0.4
   12 |   1.1728 |     40.993 |   1.1448 |     39.904 |     0.4
   13 |   1.1517 |     40.242 |   1.1263 |     39.679 |     0.5
   14 |   1.1276 |     39.168 |   1.1078 |     38.141 |     0.5
   15 |   1.1095 |     38.396 |   1.0877 |     37.404 |     0.6
   16 |   1.0934 |     37.826 |   1.0632 |     36.891 |     0.6
   17 |   1.0689 |     36.862 |   1.0490 |     35.737 |     0.6
   18 |   1.0397 |     35.536 |   1.0376 |     36.154 |     0.7
   19 |   1.0190 |     35.037 |   1.0153 |     35.064 |     0.7
   20 |   0.9902 |     34.018 |   1.0051 |     34.006 |     0.7
   21 |   0.9687 |     32.901 |   0.9904 |     33.750 |     0.8
   22 |   0.9449 |     31.909 |   0.9642 |     33.077 |     0.8
   23 |   0.9195 |     30.660 |   0.9647 |     33.301 |     0.9
   24 |   0.8976 |     30.172 |   0.9393 |     31.635 |     0.9
   25 |   0.8774 |     29.410 |   0.9333 |     31.667 |     0.9
   26 |   0.8577 |     28.545 |   0.9213 |     31.090 |     1.0
   27 |   0.8315 |     27.739 |   0.9031 |     30.064 |     1.0
   28 |   0.8024 |     26.479 |   0.8895 |     29.808 |     1.0
   29 |   0.7701 |     25.258 |   0.8813 |     29.359 |     1.1
   30 |   0.7534 |     24.584 |   0.8914 |     29.712 |     1.1
   31 |   0.7474 |     24.430 |   0.8877 |     29.455 |     1.2
   32 |   0.7274 |     23.888 |   0.8717 |     28.013 |     1.2
   33 |   0.6923 |     22.441 |   0.8644 |     28.269 |     1.2
   34 |   0.6815 |     22.113 |   0.8696 |     28.141 |     1.3
   35 |   0.6558 |     21.187 |   0.8675 |     27.596 |     1.3
   36 |   0.6438 |     20.787 |   0.8632 |     27.532 |     1.3
   37 |   0.6338 |     20.370 |   0.8612 |     27.340 |     1.4
   38 |   0.6086 |     19.861 |   0.8674 |     27.660 |     1.4
   39 |   0.5952 |     19.055 |   0.8548 |     27.115 |     1.5
   40 |   0.6206 |     20.222 |   0.8365 |     26.122 |     1.5
   41 |   0.5679 |     18.458 |   0.8656 |     26.154 |     1.5
   42 |   0.5438 |     17.439 |   0.8455 |     25.897 |     1.6
   43 |   0.5315 |     16.908 |   0.8577 |     25.801 |     1.6
   44 |   0.5306 |     17.154 |   0.8507 |     24.808 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 503,778

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4260 |     62.579 |   1.8716 |     51.603 |     0.0
    2 |   1.6580 |     48.066 |   1.5130 |     46.186 |     0.1
    3 |   1.4477 |     45.858 |   1.4139 |     46.346 |     0.1
    4 |   1.3795 |     45.732 |   1.3601 |     45.769 |     0.1
    5 |   1.3434 |     45.359 |   1.3248 |     45.224 |     0.2
    6 |   1.3235 |     45.168 |   1.3084 |     44.712 |     0.2
    7 |   1.2997 |     44.981 |   1.2838 |     44.167 |     0.2
    8 |   1.2874 |     44.614 |   1.2726 |     44.295 |     0.3
    9 |   1.2745 |     44.138 |   1.2594 |     44.135 |     0.3
   10 |   1.2580 |     43.946 |   1.2468 |     42.917 |     0.4
   11 |   1.2492 |     43.398 |   1.2320 |     43.494 |     0.4
   12 |   1.2311 |     43.031 |   1.2131 |     41.538 |     0.4
   13 |   1.2121 |     42.384 |   1.1985 |     40.962 |     0.5
   14 |   1.1963 |     41.826 |   1.1831 |     41.090 |     0.5
   15 |   1.1740 |     40.965 |   1.1623 |     40.224 |     0.5
   16 |   1.1611 |     40.746 |   1.1484 |     39.872 |     0.6
   17 |   1.1480 |     40.001 |   1.1422 |     39.615 |     0.6
   18 |   1.1315 |     39.497 |   1.1179 |     39.423 |     0.6
   19 |   1.1123 |     38.916 |   1.1088 |     38.814 |     0.7
   20 |   1.0955 |     38.155 |   1.0903 |     38.173 |     0.7
   21 |   1.0748 |     37.037 |   1.0723 |     37.436 |     0.7
   22 |   1.0515 |     36.111 |   1.0535 |     35.769 |     0.8
   23 |   1.0359 |     35.426 |   1.0352 |     35.353 |     0.8
   24 |   1.0118 |     34.402 |   1.0235 |     35.000 |     0.8
   25 |   0.9923 |     33.815 |   1.0045 |     34.455 |     0.9
   26 |   0.9654 |     32.774 |   0.9863 |     32.949 |     0.9
   27 |   0.9476 |     32.068 |   0.9848 |     33.750 |     0.9
   28 |   0.9193 |     31.065 |   0.9573 |     32.756 |     1.0
   29 |   0.9023 |     29.860 |   0.9496 |     32.564 |     1.0
   30 |   0.8728 |     28.956 |   0.9361 |     31.474 |     1.1
   31 |   0.8579 |     28.534 |   0.9249 |     30.994 |     1.1
   32 |   0.8360 |     27.630 |   0.9133 |     30.545 |     1.1
   33 |   0.8080 |     26.616 |   0.9188 |     30.321 |     1.2
   34 |   0.7944 |     25.975 |   0.9010 |     30.032 |     1.2
   35 |   0.7686 |     24.929 |   0.8990 |     29.519 |     1.2
   36 |   0.7554 |     24.627 |   0.8884 |     28.750 |     1.3
   37 |   0.7361 |     23.406 |   0.8893 |     28.429 |     1.3
   38 |   0.7193 |     23.236 |   0.8851 |     28.269 |     1.3
   39 |   0.6927 |     22.200 |   0.8726 |     28.141 |     1.4
   40 |   0.6760 |     21.735 |   0.8723 |     27.564 |     1.4
   41 |   0.7115 |     23.345 |   0.8972 |     28.526 |     1.4
   42 |   0.6524 |     21.324 |   0.8880 |     27.756 |     1.5
   43 |   0.6385 |     20.809 |   0.8773 |     27.532 |     1.5
   44 |   0.6073 |     19.587 |   0.8861 |     27.500 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,529,762

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0301 |     55.325 |   1.5036 |     46.154 |     0.1
    2 |   1.4089 |     45.874 |   1.3673 |     45.801 |     0.1
    3 |   1.3358 |     45.337 |   1.3106 |     45.224 |     0.2
    4 |   1.2951 |     44.609 |   1.2734 |     44.103 |     0.2
    5 |   1.2657 |     43.913 |   1.2448 |     43.397 |     0.3
    6 |   1.2364 |     43.475 |   1.2107 |     42.564 |     0.4
    7 |   1.2092 |     42.834 |   1.1910 |     41.667 |     0.4
    8 |   1.1859 |     42.083 |   1.1704 |     41.314 |     0.5
    9 |   1.1645 |     41.453 |   1.1454 |     40.833 |     0.6
   10 |   1.1409 |     40.505 |   1.1262 |     40.000 |     0.6
   11 |   1.1188 |     39.535 |   1.1059 |     39.551 |     0.7
   12 |   1.0966 |     38.401 |   1.0804 |     38.077 |     0.7
   13 |   1.0713 |     37.585 |   1.0636 |     37.532 |     0.8
   14 |   1.0538 |     36.577 |   1.0378 |     36.058 |     0.9
   15 |   1.0296 |     36.106 |   1.0252 |     35.962 |     0.9
   16 |   1.0183 |     35.185 |   1.0215 |     35.609 |     1.0
   17 |   0.9896 |     33.980 |   0.9963 |     34.359 |     1.1
   18 |   0.9604 |     33.290 |   0.9763 |     33.750 |     1.1
   19 |   0.9430 |     32.353 |   0.9624 |     33.782 |     1.2
   20 |   0.9200 |     31.712 |   0.9390 |     32.628 |     1.2
   21 |   0.8931 |     30.561 |   0.9383 |     32.917 |     1.3
   22 |   0.8753 |     29.536 |   0.9202 |     30.929 |     1.4
   23 |   0.8449 |     28.304 |   0.9007 |     30.609 |     1.4
   24 |   0.8221 |     27.887 |   0.9100 |     31.250 |     1.5
   25 |   0.8035 |     27.033 |   0.8868 |     29.455 |     1.6
   26 |   0.7805 |     26.474 |   0.8747 |     28.141 |     1.6
   27 |   0.7434 |     24.808 |   0.8837 |     28.942 |     1.7
   28 |   0.7314 |     24.408 |   0.8648 |     28.301 |     1.7
   29 |   0.7005 |     23.214 |   0.8363 |     26.955 |     1.8
   30 |   0.6838 |     22.507 |   0.8440 |     27.468 |     1.9
   31 |   0.6697 |     22.365 |   0.8392 |     27.821 |     1.9
   32 |   0.6278 |     20.639 |   0.8338 |     27.083 |     2.0
   33 |   0.6175 |     20.283 |   0.8338 |     26.891 |     2.1
   34 |   0.5945 |     19.439 |   0.8315 |     26.763 |     2.1
   35 |   0.5653 |     18.612 |   0.8374 |     26.538 |     2.2
   36 |   0.5455 |     17.795 |   0.8265 |     25.994 |     2.2
   37 |   0.5398 |     17.598 |   0.8333 |     25.449 |     2.3
   38 |   0.5156 |     16.749 |   0.8501 |     25.897 |     2.4
   39 |   0.5006 |     16.256 |   0.8509 |     25.801 |     2.4
   40 |   0.4647 |     14.963 |   0.8192 |     24.391 |     2.5
   41 |   0.4455 |     14.272 |   0.8301 |     24.615 |     2.6
   42 |   0.4458 |     14.415 |   0.8353 |     24.776 |     2.6
   43 |   0.4132 |     13.412 |   0.8457 |     25.160 |     2.7
   44 |   0.4174 |     13.385 |   0.8413 |     24.103 |     2.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,593,570

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1934 |     59.681 |   1.5940 |     46.346 |     0.1
    2 |   1.4641 |     46.088 |   1.4221 |     46.827 |     0.1
    3 |   1.3907 |     46.198 |   1.3825 |     46.827 |     0.2
    4 |   1.3576 |     45.858 |   1.3409 |     45.545 |     0.2
    5 |   1.3265 |     45.721 |   1.3103 |     45.673 |     0.3
    6 |   1.3036 |     45.447 |   1.2954 |     44.583 |     0.4
    7 |   1.2902 |     45.195 |   1.2796 |     45.192 |     0.4
    8 |   1.2793 |     44.883 |   1.2626 |     44.712 |     0.5
    9 |   1.2645 |     44.735 |   1.2547 |     44.551 |     0.6
   10 |   1.2495 |     44.329 |   1.2292 |     44.519 |     0.6
   11 |   1.2254 |     43.809 |   1.2056 |     42.564 |     0.7
   12 |   1.2039 |     43.354 |   1.1945 |     42.596 |     0.7
   13 |   1.1886 |     42.406 |   1.1693 |     41.122 |     0.8
   14 |   1.1701 |     41.820 |   1.1527 |     40.641 |     0.9
   15 |   1.1454 |     40.483 |   1.1331 |     40.385 |     0.9
   16 |   1.1257 |     39.859 |   1.1022 |     38.558 |     1.0
   17 |   1.1035 |     39.124 |   1.0844 |     37.372 |     1.1
   18 |   1.0823 |     38.374 |   1.0703 |     37.756 |     1.1
   19 |   1.0560 |     37.152 |   1.0585 |     37.019 |     1.2
   20 |   1.0384 |     36.500 |   1.0548 |     37.244 |     1.2
   21 |   1.0164 |     35.651 |   1.0341 |     35.385 |     1.3
   22 |   0.9931 |     34.583 |   1.0257 |     35.353 |     1.4
   23 |   0.9701 |     33.980 |   1.0021 |     35.000 |     1.4
   24 |   0.9543 |     32.988 |   0.9912 |     34.295 |     1.5
   25 |   0.9302 |     32.292 |   0.9759 |     33.333 |     1.5
   26 |   0.9018 |     30.961 |   0.9788 |     33.654 |     1.6
   27 |   0.8784 |     30.298 |   0.9686 |     33.237 |     1.7
   28 |   0.8544 |     29.274 |   0.9645 |     32.308 |     1.7
   29 |   0.8424 |     28.671 |   0.9398 |     31.218 |     1.8
   30 |   0.8143 |     27.537 |   0.9307 |     31.218 |     1.9
   31 |   0.8015 |     27.137 |   0.9272 |     30.769 |     1.9
   32 |   0.7639 |     25.520 |   0.9309 |     30.160 |     2.0
   33 |   0.7489 |     25.164 |   0.9280 |     30.449 |     2.0
   34 |   0.7215 |     24.091 |   0.9203 |     29.199 |     2.1
   35 |   0.6901 |     22.551 |   0.9046 |     28.814 |     2.2
   36 |   0.6679 |     22.047 |   0.9059 |     28.654 |     2.2
   37 |   0.6402 |     21.083 |   0.9069 |     28.301 |     2.3
   38 |   0.6183 |     20.129 |   0.9120 |     27.692 |     2.4
   39 |   0.6053 |     19.658 |   0.9013 |     26.923 |     2.4
   40 |   0.5596 |     17.883 |   0.9286 |     27.756 |     2.5
   41 |   0.5399 |     17.330 |   0.9089 |     27.019 |     2.5
   42 |   0.5198 |     16.716 |   0.9272 |     26.282 |     2.6
   43 |   0.5099 |     16.195 |   0.8953 |     25.545 |     2.7
   44 |   0.4898 |     15.593 |   0.9247 |     26.571 |     2.7
   45 |   0.4778 |     15.259 |   0.9157 |     25.897 |     2.8
   46 |   0.4600 |     14.766 |   0.9342 |     26.058 |     2.9
   47 |   0.4224 |     13.571 |   0.9234 |     25.577 |     2.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 507,682

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0755 |     56.131 |   1.5168 |     46.058 |     0.0
    2 |   1.4110 |     45.683 |   1.3687 |     45.737 |     0.0
    3 |   1.3322 |     45.228 |   1.3074 |     44.840 |     0.1
    4 |   1.2925 |     44.554 |   1.2767 |     44.327 |     0.1
    5 |   1.2698 |     44.302 |   1.2507 |     43.750 |     0.1
    6 |   1.2466 |     43.803 |   1.2300 |     43.526 |     0.1
    7 |   1.2269 |     43.469 |   1.2015 |     42.404 |     0.1
    8 |   1.2056 |     42.757 |   1.1911 |     42.115 |     0.2
    9 |   1.1826 |     41.930 |   1.1652 |     40.385 |     0.2
   10 |   1.1582 |     40.993 |   1.1366 |     40.513 |     0.2
   11 |   1.1377 |     40.286 |   1.1210 |     38.878 |     0.2
   12 |   1.1101 |     38.938 |   1.1121 |     39.167 |     0.2
   13 |   1.0878 |     38.182 |   1.0727 |     37.340 |     0.2
   14 |   1.0597 |     36.889 |   1.0552 |     36.955 |     0.3
   15 |   1.0403 |     36.407 |   1.0216 |     35.641 |     0.3
   16 |   1.0176 |     35.136 |   1.0030 |     34.840 |     0.3
   17 |   0.9875 |     34.292 |   1.0082 |     34.808 |     0.3
   18 |   0.9707 |     33.383 |   0.9838 |     34.327 |     0.3
   19 |   0.9440 |     32.243 |   0.9669 |     33.686 |     0.4
   20 |   0.9152 |     31.169 |   0.9508 |     32.372 |     0.4
   21 |   0.8943 |     30.424 |   0.9549 |     33.269 |     0.4
   22 |   0.8612 |     28.994 |   0.9351 |     31.827 |     0.4
   23 |   0.8386 |     28.304 |   0.9400 |     32.981 |     0.4
   24 |   0.8113 |     27.509 |   0.9167 |     31.699 |     0.5
   25 |   0.7802 |     25.986 |   0.8928 |     29.583 |     0.5
   26 |   0.7471 |     24.989 |   0.8866 |     29.519 |     0.5
   27 |   0.7321 |     24.255 |   0.8824 |     28.686 |     0.5
   28 |   0.7076 |     23.488 |   0.8691 |     28.878 |     0.5
   29 |   0.6765 |     22.140 |   0.8609 |     28.462 |     0.6
   30 |   0.6483 |     20.973 |   0.8774 |     29.199 |     0.6
   31 |   0.6309 |     20.491 |   0.8592 |     28.333 |     0.6
   32 |   0.5998 |     19.702 |   0.8516 |     27.051 |     0.6
   33 |   0.5809 |     18.727 |   0.8617 |     27.468 |     0.6
   34 |   0.5593 |     18.080 |   0.8614 |     27.308 |     0.7
   35 |   0.5408 |     17.423 |   0.8592 |     27.083 |     0.7
   36 |   0.5110 |     16.174 |   0.8527 |     26.538 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 326,434

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3371 |     61.106 |   1.7907 |     48.526 |     0.0
    2 |   1.5910 |     47.337 |   1.4651 |     45.769 |     0.0
    3 |   1.4138 |     45.743 |   1.3761 |     46.795 |     0.1
    4 |   1.3567 |     45.327 |   1.3318 |     44.744 |     0.1
    5 |   1.3219 |     44.702 |   1.2969 |     43.429 |     0.1
    6 |   1.2961 |     44.401 |   1.2747 |     43.013 |     0.1
    7 |   1.2758 |     43.957 |   1.2587 |     43.013 |     0.1
    8 |   1.2580 |     43.836 |   1.2407 |     42.404 |     0.2
    9 |   1.2444 |     43.239 |   1.2222 |     42.147 |     0.2
   10 |   1.2236 |     42.845 |   1.2097 |     42.308 |     0.2
   11 |   1.2052 |     42.236 |   1.1840 |     41.571 |     0.2
   12 |   1.1820 |     41.376 |   1.1694 |     40.577 |     0.2
   13 |   1.1603 |     40.396 |   1.1417 |     40.385 |     0.3
   14 |   1.1347 |     39.415 |   1.1294 |     39.071 |     0.3
   15 |   1.1084 |     38.248 |   1.1011 |     38.558 |     0.3
   16 |   1.0854 |     37.399 |   1.0851 |     38.269 |     0.3
   17 |   1.0543 |     36.303 |   1.0575 |     36.699 |     0.3
   18 |   1.0266 |     34.884 |   1.0445 |     35.256 |     0.4
   19 |   0.9973 |     33.580 |   1.0202 |     34.231 |     0.4
   20 |   0.9790 |     32.703 |   0.9981 |     33.814 |     0.4
   21 |   0.9498 |     31.597 |   0.9792 |     33.269 |     0.4
   22 |   0.9208 |     30.325 |   0.9861 |     33.077 |     0.4
   23 |   0.9053 |     30.112 |   0.9604 |     32.179 |     0.5
   24 |   0.8884 |     29.005 |   0.9552 |     32.212 |     0.5
   25 |   0.8763 |     28.972 |   0.9391 |     30.962 |     0.5
   26 |   0.8405 |     27.405 |   0.9356 |     30.449 |     0.5
   27 |   0.8191 |     26.677 |   0.9306 |     31.058 |     0.5
   28 |   0.7993 |     25.756 |   0.9158 |     30.256 |     0.6
   29 |   0.7782 |     25.323 |   0.9171 |     29.615 |     0.6
   30 |   0.7641 |     24.984 |   0.9164 |     29.679 |     0.6
   31 |   0.7430 |     24.277 |   0.9147 |     29.583 |     0.6
   32 |   0.7364 |     23.636 |   0.9090 |     29.135 |     0.6
   33 |   0.7205 |     23.433 |   0.9012 |     28.942 |     0.7
   34 |   0.7036 |     22.502 |   0.9031 |     28.526 |     0.7
   35 |   0.6838 |     21.954 |   0.8894 |     28.622 |     0.7
   36 |   0.6827 |     21.866 |   0.8914 |     28.558 |     0.7
   37 |   0.6650 |     21.466 |   0.8990 |     28.013 |     0.7
   38 |   0.6583 |     21.296 |   0.8882 |     27.821 |     0.8
   39 |   0.6392 |     20.590 |   0.8818 |     27.532 |     0.8
   40 |   0.6203 |     19.992 |   0.8861 |     27.628 |     0.8
   41 |   0.6033 |     19.302 |   0.8933 |     27.244 |     0.8
   42 |   0.5977 |     19.203 |   0.8928 |     26.987 |     0.8
   43 |   0.5850 |     18.655 |   0.8954 |     27.404 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 442,146

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3602 |     62.640 |   1.7608 |     48.526 |     0.0
    2 |   1.5625 |     46.921 |   1.4509 |     45.865 |     0.1
    3 |   1.4067 |     45.918 |   1.3742 |     46.154 |     0.1
    4 |   1.3584 |     45.557 |   1.3362 |     45.417 |     0.1
    5 |   1.3277 |     45.447 |   1.3120 |     45.160 |     0.1
    6 |   1.3028 |     45.190 |   1.2664 |     44.071 |     0.2
    7 |   1.2742 |     44.231 |   1.2439 |     43.686 |     0.2
    8 |   1.2490 |     43.831 |   1.2265 |     43.397 |     0.2
    9 |   1.2287 |     43.327 |   1.2061 |     42.596 |     0.3
   10 |   1.2146 |     42.784 |   1.1912 |     41.891 |     0.3
   11 |   1.1920 |     42.302 |   1.1713 |     42.051 |     0.3
   12 |   1.1735 |     41.678 |   1.1590 |     41.090 |     0.3
   13 |   1.1566 |     41.004 |   1.1341 |     39.968 |     0.4
   14 |   1.1423 |     40.116 |   1.1298 |     39.712 |     0.4
   15 |   1.1220 |     39.470 |   1.1164 |     39.359 |     0.4
   16 |   1.1139 |     38.872 |   1.0920 |     37.628 |     0.4
   17 |   1.0899 |     38.007 |   1.0821 |     37.308 |     0.5
   18 |   1.0726 |     37.388 |   1.0642 |     37.468 |     0.5
   19 |   1.0555 |     36.434 |   1.0494 |     36.250 |     0.5
   20 |   1.0378 |     35.750 |   1.0388 |     36.026 |     0.6
   21 |   1.0228 |     35.245 |   1.0253 |     35.545 |     0.6
   22 |   1.0088 |     34.796 |   1.0151 |     35.000 |     0.6
   23 |   0.9884 |     33.750 |   1.0129 |     34.487 |     0.6
   24 |   0.9743 |     33.202 |   0.9934 |     34.071 |     0.7
   25 |   0.9561 |     32.769 |   0.9861 |     33.910 |     0.7
   26 |   0.9451 |     31.827 |   0.9752 |     33.365 |     0.7
   27 |   0.9278 |     31.312 |   0.9812 |     33.109 |     0.8
   28 |   0.9123 |     30.693 |   0.9598 |     32.468 |     0.8
   29 |   0.8908 |     30.167 |   0.9633 |     32.853 |     0.8
   30 |   0.8807 |     29.383 |   0.9525 |     32.340 |     0.8
   31 |   0.8621 |     29.071 |   0.9364 |     31.378 |     0.9
   32 |   0.8462 |     28.183 |   0.9416 |     31.090 |     0.9
   33 |   0.8369 |     27.772 |   0.9415 |     31.314 |     0.9
   34 |   0.8150 |     27.181 |   0.9379 |     31.442 |     1.0
   35 |   0.8101 |     27.022 |   0.9325 |     31.795 |     1.0
   36 |   0.7852 |     26.293 |   0.9259 |     31.090 |     1.0
   37 |   0.7729 |     25.827 |   0.9215 |     30.353 |     1.0
   38 |   0.7549 |     24.945 |   0.9180 |     30.192 |     1.1
   39 |   0.7330 |     24.129 |   0.9252 |     29.776 |     1.1
   40 |   0.7239 |     24.211 |   0.9105 |     29.519 |     1.1
   41 |   0.7170 |     23.822 |   0.9193 |     29.583 |     1.1
   42 |   0.6970 |     22.825 |   0.9142 |     29.936 |     1.2
   43 |   0.7138 |     23.882 |   0.8979 |     29.199 |     1.2
   44 |   0.6889 |     22.628 |   0.8928 |     28.942 |     1.2
   45 |   0.6705 |     22.009 |   0.9090 |     28.942 |     1.3
   46 |   0.6456 |     21.389 |   0.9150 |     28.974 |     1.3
   47 |   0.6380 |     20.962 |   0.9021 |     28.590 |     1.3
   48 |   0.6252 |     20.272 |   0.9120 |     28.269 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 504,578

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3133 |     59.161 |   1.7316 |     49.615 |     0.0
    2 |   1.5476 |     46.998 |   1.4447 |     46.186 |     0.1
    3 |   1.3940 |     45.803 |   1.3575 |     45.897 |     0.1
    4 |   1.3406 |     45.277 |   1.3103 |     44.167 |     0.1
    5 |   1.3016 |     44.740 |   1.2781 |     44.231 |     0.2
    6 |   1.2750 |     43.984 |   1.2588 |     44.071 |     0.2
    7 |   1.2591 |     44.088 |   1.2365 |     42.981 |     0.2
    8 |   1.2381 |     43.453 |   1.2232 |     42.340 |     0.3
    9 |   1.2257 |     42.834 |   1.2029 |     41.731 |     0.3
   10 |   1.2079 |     42.390 |   1.1903 |     41.474 |     0.3
   11 |   1.1898 |     41.941 |   1.1783 |     41.603 |     0.4
   12 |   1.1830 |     41.579 |   1.1672 |     40.737 |     0.4
   13 |   1.1651 |     41.223 |   1.1488 |     40.609 |     0.4
   14 |   1.1668 |     41.420 |   1.1543 |     41.218 |     0.5
   15 |   1.1606 |     41.278 |   1.1335 |     40.417 |     0.5
   16 |   1.1444 |     40.615 |   1.1271 |     39.583 |     0.5
   17 |   1.1339 |     40.461 |   1.1194 |     38.878 |     0.6
   18 |   1.1209 |     39.809 |   1.0989 |     38.622 |     0.6
   19 |   1.1169 |     39.366 |   1.0929 |     38.750 |     0.6
   20 |   1.1002 |     39.053 |   1.0856 |     38.846 |     0.7
   21 |   1.0912 |     38.664 |   1.0756 |     38.750 |     0.7
   22 |   1.0822 |     38.297 |   1.0632 |     36.891 |     0.7
   23 |   1.0712 |     37.892 |   1.0559 |     37.179 |     0.8
   24 |   1.0603 |     37.514 |   1.0476 |     36.987 |     0.8
   25 |   1.0533 |     37.273 |   1.0500 |     37.051 |     0.8
   26 |   1.0475 |     36.873 |   1.0389 |     35.641 |     0.9
   27 |   1.0343 |     36.637 |   1.0361 |     35.545 |     0.9
   28 |   1.0276 |     36.314 |   1.0264 |     35.577 |     1.0
   29 |   1.0188 |     36.144 |   1.0303 |     35.994 |     1.0
   30 |   1.0130 |     35.656 |   1.0128 |     35.481 |     1.0
   31 |   0.9972 |     35.180 |   1.0085 |     35.064 |     1.1
   32 |   0.9915 |     34.851 |   1.0012 |     34.615 |     1.1
   33 |   0.9781 |     34.106 |   0.9916 |     34.744 |     1.1
   34 |   0.9693 |     33.991 |   0.9890 |     34.231 |     1.2
   35 |   0.9625 |     34.013 |   0.9818 |     33.814 |     1.2
   36 |   0.9552 |     33.432 |   0.9799 |     33.686 |     1.2
   37 |   0.9436 |     32.561 |   0.9733 |     32.885 |     1.3
   38 |   0.9389 |     32.544 |   0.9785 |     33.910 |     1.3
   39 |   0.9310 |     32.024 |   0.9622 |     33.237 |     1.3
   40 |   0.9108 |     31.481 |   0.9504 |     32.756 |     1.4
   41 |   0.9094 |     31.471 |   0.9418 |     32.821 |     1.4
   42 |   0.8921 |     30.632 |   0.9447 |     32.436 |     1.4
   43 |   0.8774 |     30.008 |   0.9386 |     31.859 |     1.5
   44 |   0.8756 |     29.542 |   0.9283 |     31.346 |     1.5
   45 |   0.8634 |     29.542 |   0.9288 |     31.218 |     1.5
   46 |   0.8369 |     28.463 |   0.9196 |     31.090 |     1.6
   47 |   0.8360 |     28.117 |   0.9242 |     30.865 |     1.6
   48 |   0.8196 |     27.723 |   0.9061 |     30.288 |     1.6
   49 |   0.8116 |     26.797 |   0.9100 |     30.128 |     1.7
   50 |   0.8000 |     26.813 |   0.9044 |     29.808 |     1.7
   51 |   0.7732 |     25.877 |   0.8999 |     29.872 |     1.7
   52 |   0.7663 |     25.614 |   0.9030 |     29.519 |     1.8
   53 |   0.7537 |     25.318 |   0.8927 |     29.519 |     1.8
   54 |   0.7375 |     24.540 |   0.8977 |     30.353 |     1.8
   55 |   0.7241 |     23.997 |   0.8941 |     29.647 |     1.9
   56 |   0.7328 |     24.184 |   0.8897 |     28.910 |     1.9
   57 |   0.7016 |     23.060 |   0.8912 |     29.776 |     1.9
   58 |   0.6880 |     22.556 |   0.8965 |     29.135 |     2.0
   59 |   0.6726 |     22.020 |   0.8865 |     28.878 |     2.0
   60 |   0.6619 |     21.669 |   0.8919 |     29.199 |     2.0
   61 |   0.6434 |     21.088 |   0.8953 |     28.686 |     2.1
   62 |   0.6239 |     20.365 |   0.8930 |     28.526 |     2.1
   63 |   0.6153 |     19.981 |   0.8845 |     27.788 |     2.1
   64 |   0.6078 |     19.740 |   0.9055 |     28.333 |     2.2
   65 |   0.6237 |     20.710 |   0.8962 |     28.814 |     2.2
   66 |   0.5769 |     18.842 |   0.9004 |     28.333 |     2.2
   67 |   0.5615 |     17.872 |   0.8884 |     27.404 |     2.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 639,778

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1601 |     58.657 |   1.5926 |     46.346 |     0.0
    2 |   1.4552 |     46.028 |   1.4275 |     46.346 |     0.0
    3 |   1.3849 |     46.099 |   1.3724 |     46.346 |     0.1
    4 |   1.3539 |     45.529 |   1.3387 |     45.096 |     0.1
    5 |   1.3266 |     45.392 |   1.3329 |     45.833 |     0.1
    6 |   1.3072 |     44.948 |   1.3050 |     45.064 |     0.1
    7 |   1.2907 |     44.653 |   1.2861 |     44.038 |     0.1
    8 |   1.2722 |     44.423 |   1.2709 |     44.615 |     0.2
    9 |   1.2586 |     44.291 |   1.2446 |     43.814 |     0.2
   10 |   1.2412 |     43.672 |   1.2433 |     43.205 |     0.2
   11 |   1.2253 |     42.998 |   1.2088 |     42.340 |     0.2
   12 |   1.2017 |     42.034 |   1.1936 |     41.506 |     0.2
   13 |   1.1779 |     41.530 |   1.1738 |     41.026 |     0.3
   14 |   1.1579 |     40.757 |   1.1652 |     40.673 |     0.3
   15 |   1.1404 |     39.755 |   1.1406 |     39.840 |     0.3
   16 |   1.1107 |     38.894 |   1.1305 |     38.654 |     0.3
   17 |   1.0902 |     38.281 |   1.1131 |     38.301 |     0.3
   18 |   1.0576 |     36.517 |   1.0677 |     37.179 |     0.4
   19 |   1.0180 |     34.856 |   1.0508 |     36.218 |     0.4
   20 |   0.9832 |     33.454 |   1.0329 |     35.128 |     0.4
   21 |   0.9527 |     32.095 |   1.0105 |     34.391 |     0.4
   22 |   0.9192 |     30.682 |   1.0158 |     34.359 |     0.4
   23 |   0.8712 |     28.824 |   0.9867 |     32.564 |     0.5
   24 |   0.8352 |     27.301 |   0.9704 |     32.179 |     0.5
   25 |   0.8098 |     26.288 |   0.9565 |     31.090 |     0.5
   26 |   0.7887 |     25.515 |   0.9453 |     30.705 |     0.5
   27 |   0.7503 |     24.425 |   0.9445 |     30.353 |     0.5
   28 |   0.7143 |     23.110 |   0.9435 |     29.776 |     0.6
   29 |   0.6787 |     21.625 |   0.9469 |     29.904 |     0.6
   30 |   0.6382 |     20.283 |   0.9534 |     29.263 |     0.6
   31 |   0.6108 |     19.324 |   0.9587 |     29.167 |     0.6
   32 |   0.5934 |     18.962 |   0.9646 |     29.679 |     0.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 771,874

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2503 |     62.130 |   1.6638 |     49.135 |     0.0
    2 |   1.4941 |     46.154 |   1.4361 |     46.346 |     0.1
    3 |   1.4070 |     46.094 |   1.3972 |     46.346 |     0.1
    4 |   1.3687 |     45.770 |   1.3465 |     46.186 |     0.1
    5 |   1.3323 |     45.584 |   1.3124 |     45.609 |     0.1
    6 |   1.3062 |     44.970 |   1.2898 |     44.199 |     0.2
    7 |   1.2836 |     44.620 |   1.2699 |     44.391 |     0.2
    8 |   1.2693 |     44.077 |   1.2523 |     44.071 |     0.2
    9 |   1.2504 |     44.017 |   1.2541 |     43.654 |     0.3
   10 |   1.2386 |     43.497 |   1.2247 |     43.237 |     0.3
   11 |   1.2211 |     43.294 |   1.2033 |     42.596 |     0.3
   12 |   1.2051 |     42.943 |   1.1872 |     41.923 |     0.3
   13 |   1.1951 |     42.390 |   1.1795 |     41.506 |     0.4
   14 |   1.1807 |     41.847 |   1.1593 |     40.705 |     0.4
   15 |   1.1669 |     41.480 |   1.1487 |     40.673 |     0.4
   16 |   1.1583 |     41.628 |   1.1363 |     40.032 |     0.5
   17 |   1.1500 |     41.048 |   1.1275 |     39.487 |     0.5
   18 |   1.1434 |     40.768 |   1.1241 |     39.327 |     0.5
   19 |   1.1332 |     40.648 |   1.1115 |     39.359 |     0.5
   20 |   1.1183 |     40.204 |   1.1047 |     39.551 |     0.6
   21 |   1.1152 |     39.946 |   1.0963 |     39.071 |     0.6
   22 |   1.1091 |     39.870 |   1.0925 |     38.814 |     0.6
   23 |   1.1026 |     39.574 |   1.0784 |     38.397 |     0.7
   24 |   1.0999 |     39.524 |   1.0748 |     38.622 |     0.7
   25 |   1.0939 |     39.409 |   1.0746 |     39.167 |     0.7
   26 |   1.0884 |     39.103 |   1.0718 |     38.942 |     0.7
   27 |   1.0824 |     39.103 |   1.0607 |     38.814 |     0.8
   28 |   1.0791 |     39.108 |   1.0651 |     38.333 |     0.8
   29 |   1.0664 |     38.571 |   1.0536 |     38.365 |     0.8
   30 |   1.0623 |     38.401 |   1.0549 |     38.045 |     0.9
   31 |   1.0536 |     38.259 |   1.0447 |     38.462 |     0.9
   32 |   1.0533 |     38.089 |   1.0367 |     37.532 |     0.9
   33 |   1.0452 |     37.826 |   1.0303 |     36.571 |     0.9
   34 |   1.0368 |     37.169 |   1.0326 |     37.372 |     1.0
   35 |   1.0319 |     36.884 |   1.0385 |     37.276 |     1.0
   36 |   1.0250 |     36.544 |   1.0233 |     37.147 |     1.0
   37 |   1.0214 |     36.533 |   1.0179 |     36.763 |     1.0
   38 |   1.0107 |     36.259 |   1.0087 |     36.058 |     1.1
   39 |   1.0046 |     36.056 |   1.0079 |     35.929 |     1.1
   40 |   1.0007 |     35.908 |   1.0037 |     35.353 |     1.1
   41 |   1.0009 |     35.739 |   1.0000 |     36.122 |     1.2
   42 |   0.9945 |     35.508 |   0.9983 |     35.449 |     1.2
   43 |   0.9779 |     35.152 |   0.9952 |     35.897 |     1.2
   44 |   0.9710 |     34.593 |   0.9811 |     35.288 |     1.2
   45 |   0.9608 |     34.484 |   0.9768 |     35.160 |     1.3
   46 |   0.9538 |     33.991 |   0.9701 |     35.032 |     1.3
   47 |   0.9463 |     33.739 |   0.9549 |     34.423 |     1.3
   48 |   0.9376 |     33.399 |   0.9541 |     33.910 |     1.4
   49 |   0.9419 |     33.372 |   0.9471 |     34.327 |     1.4
   50 |   0.9271 |     32.862 |   0.9498 |     34.487 |     1.4
   51 |   0.9181 |     32.468 |   0.9446 |     33.558 |     1.4
   52 |   0.9121 |     32.133 |   0.9459 |     33.558 |     1.5
   53 |   0.9069 |     32.007 |   0.9345 |     33.429 |     1.5
   54 |   0.9046 |     32.101 |   0.9383 |     32.660 |     1.5
   55 |   0.8984 |     31.607 |   0.9386 |     33.622 |     1.6
   56 |   0.8888 |     31.449 |   0.9255 |     32.981 |     1.6
   57 |   0.8854 |     31.016 |   0.9230 |     32.532 |     1.6
   58 |   0.8644 |     30.156 |   0.9419 |     32.853 |     1.6
   59 |   0.8592 |     30.276 |   0.9072 |     31.795 |     1.7
   60 |   0.8546 |     29.854 |   0.9146 |     32.083 |     1.7
   61 |   0.8714 |     30.605 |   0.9111 |     31.699 |     1.7
   62 |   0.8407 |     29.465 |   0.8931 |     31.122 |     1.8
   63 |   0.8301 |     29.049 |   0.9072 |     31.955 |     1.8
   64 |   0.8215 |     28.874 |   0.8997 |     31.346 |     1.8
   65 |   0.8196 |     28.375 |   0.8853 |     30.801 |     1.8
   66 |   0.8097 |     28.238 |   0.8975 |     31.474 |     1.9
   67 |   0.8064 |     28.189 |   0.8809 |     30.064 |     1.9
   68 |   0.7958 |     27.416 |   0.9003 |     31.314 |     1.9
   69 |   0.7869 |     27.307 |   0.9047 |     30.994 |     2.0
   70 |   0.7760 |     26.720 |   0.8785 |     30.096 |     2.0
   71 |   0.7734 |     26.518 |   0.8763 |     29.872 |     2.0
   72 |   0.7649 |     26.315 |   0.8779 |     30.096 |     2.0
   73 |   0.7608 |     26.162 |   0.8744 |     30.064 |     2.1
   74 |   0.7610 |     26.107 |   0.8778 |     29.455 |     2.1
   75 |   0.7446 |     25.712 |   0.8707 |     29.167 |     2.1
   76 |   0.7395 |     25.252 |   0.8905 |     30.224 |     2.2
   77 |   0.7471 |     25.384 |   0.8636 |     27.917 |     2.2
   78 |   0.7247 |     24.819 |   0.8634 |     29.615 |     2.2
   79 |   0.7212 |     24.737 |   0.8786 |     29.135 |     2.2
   80 |   0.7230 |     24.699 |   0.8641 |     28.942 |     2.3
   81 |   0.7109 |     24.480 |   0.8576 |     28.301 |     2.3
   82 |   0.7114 |     24.364 |   0.8657 |     29.167 |     2.3
   83 |   0.7316 |     25.263 |   0.9327 |     30.064 |     2.4
   84 |   0.7117 |     24.359 |   0.8600 |     28.814 |     2.4
   85 |   0.6843 |     23.263 |   0.8661 |     29.199 |     2.4
   86 |   0.6774 |     22.956 |   0.8511 |     28.429 |     2.5
   87 |   0.6728 |     22.945 |   0.8570 |     27.981 |     2.5
   88 |   0.6659 |     22.551 |   0.8588 |     28.237 |     2.5
   89 |   0.6834 |     23.608 |   0.8638 |     28.045 |     2.5
   90 |   0.6626 |     22.578 |   0.8556 |     28.269 |     2.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 287,874

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4960 |     67.461 |   1.9131 |     54.167 |     0.0
    2 |   1.6635 |     47.606 |   1.5311 |     46.346 |     0.0
    3 |   1.4550 |     46.055 |   1.4364 |     46.827 |     0.0
    4 |   1.3991 |     45.951 |   1.3875 |     46.378 |     0.1
    5 |   1.3652 |     45.716 |   1.3629 |     45.929 |     0.1
    6 |   1.3359 |     45.135 |   1.3360 |     44.776 |     0.1
    7 |   1.3262 |     44.981 |   1.3174 |     45.064 |     0.1
    8 |   1.3062 |     44.658 |   1.2901 |     44.391 |     0.1
    9 |   1.2862 |     44.324 |   1.2824 |     43.974 |     0.1
   10 |   1.2730 |     44.192 |   1.2640 |     43.622 |     0.2
   11 |   1.2564 |     44.083 |   1.2576 |     43.718 |     0.2
   12 |   1.2448 |     43.918 |   1.2381 |     43.622 |     0.2
   13 |   1.2329 |     43.853 |   1.2273 |     43.782 |     0.2
   14 |   1.2254 |     43.749 |   1.2116 |     43.782 |     0.2
   15 |   1.2152 |     43.403 |   1.2078 |     43.397 |     0.2
   16 |   1.2081 |     43.190 |   1.2046 |     43.045 |     0.3
   17 |   1.1998 |     42.976 |   1.1965 |     42.917 |     0.3
   18 |   1.1925 |     42.609 |   1.1861 |     41.859 |     0.3
   19 |   1.1830 |     42.510 |   1.1778 |     42.051 |     0.3
   20 |   1.1773 |     42.028 |   1.1720 |     41.218 |     0.3
   21 |   1.1760 |     41.968 |   1.1766 |     41.987 |     0.3
   22 |   1.1649 |     41.535 |   1.1577 |     41.026 |     0.4
   23 |   1.1569 |     41.294 |   1.1595 |     41.442 |     0.4
   24 |   1.1541 |     41.261 |   1.1463 |     40.224 |     0.4
   25 |   1.1461 |     40.620 |   1.1436 |     40.385 |     0.4
   26 |   1.1410 |     40.237 |   1.1311 |     39.808 |     0.4
   27 |   1.1343 |     40.363 |   1.1325 |     40.064 |     0.4
   28 |   1.1273 |     39.837 |   1.1241 |     40.449 |     0.5
   29 |   1.1182 |     39.870 |   1.1231 |     39.776 |     0.5
   30 |   1.1133 |     39.497 |   1.1053 |     39.071 |     0.5
   31 |   1.1102 |     39.338 |   1.1069 |     39.615 |     0.5
   32 |   1.1035 |     39.492 |   1.0978 |     38.718 |     0.5
   33 |   1.1045 |     39.185 |   1.0990 |     39.071 |     0.5
   34 |   1.0917 |     38.719 |   1.0906 |     38.494 |     0.6
   35 |   1.0875 |     38.566 |   1.0970 |     39.744 |     0.6
   36 |   1.0867 |     38.834 |   1.0798 |     37.788 |     0.6
   37 |   1.0723 |     38.040 |   1.0862 |     37.756 |     0.6
   38 |   1.0661 |     37.886 |   1.0770 |     37.853 |     0.6
   39 |   1.0680 |     37.810 |   1.0688 |     37.532 |     0.6
   40 |   1.0575 |     37.377 |   1.0693 |     37.276 |     0.7
   41 |   1.0508 |     37.169 |   1.0687 |     37.724 |     0.7
   42 |   1.0455 |     37.032 |   1.0660 |     37.981 |     0.7
   43 |   1.0406 |     36.692 |   1.0582 |     37.147 |     0.7
   44 |   1.0411 |     36.889 |   1.0629 |     37.724 |     0.7
   45 |   1.0381 |     36.823 |   1.0599 |     36.538 |     0.7
   46 |   1.0268 |     36.297 |   1.0449 |     36.731 |     0.8
   47 |   1.0183 |     36.034 |   1.0417 |     36.923 |     0.8
   48 |   1.0147 |     35.892 |   1.0396 |     36.090 |     0.8
   49 |   1.0059 |     35.525 |   1.0423 |     36.410 |     0.8
   50 |   1.0041 |     35.426 |   1.0383 |     35.865 |     0.8
   51 |   0.9942 |     35.010 |   1.0368 |     35.641 |     0.8
   52 |   0.9953 |     34.845 |   1.0303 |     35.513 |     0.9
   53 |   0.9842 |     34.676 |   1.0248 |     35.449 |     0.9
   54 |   0.9840 |     34.632 |   1.0200 |     35.865 |     0.9
   55 |   0.9816 |     34.473 |   1.0154 |     35.385 |     0.9
   56 |   0.9705 |     33.744 |   1.0194 |     34.968 |     0.9
   57 |   0.9569 |     33.679 |   1.0107 |     34.776 |     0.9
   58 |   0.9529 |     33.448 |   1.0129 |     34.904 |     1.0
   59 |   0.9443 |     32.851 |   0.9906 |     34.455 |     1.0
   60 |   0.9415 |     32.648 |   1.0003 |     34.776 |     1.0
   61 |   0.9297 |     32.364 |   1.0000 |     34.840 |     1.0
   62 |   0.9241 |     31.881 |   1.0020 |     34.679 |     1.0
   63 |   0.9172 |     31.558 |   0.9751 |     33.942 |     1.1
   64 |   0.9024 |     31.191 |   0.9815 |     33.462 |     1.1
   65 |   0.9008 |     31.131 |   0.9760 |     33.397 |     1.1
   66 |   0.8857 |     30.336 |   0.9718 |     34.103 |     1.1
   67 |   0.8794 |     30.342 |   0.9586 |     33.045 |     1.1
   68 |   0.8658 |     29.641 |   0.9558 |     33.397 |     1.1
   69 |   0.8617 |     29.290 |   0.9552 |     32.468 |     1.2
   70 |   0.8537 |     28.934 |   0.9531 |     32.756 |     1.2
   71 |   0.8493 |     29.131 |   0.9492 |     32.115 |     1.2
   72 |   0.8326 |     28.643 |   0.9531 |     32.468 |     1.2
   73 |   0.8237 |     27.959 |   0.9598 |     32.692 |     1.2
   74 |   0.8260 |     28.178 |   0.9373 |     31.891 |     1.2
   75 |   0.8012 |     27.219 |   0.9305 |     30.962 |     1.3
   76 |   0.7935 |     26.907 |   0.9409 |     31.346 |     1.3
   77 |   0.7853 |     26.764 |   0.9272 |     31.474 |     1.3
   78 |   0.7781 |     26.485 |   0.9258 |     30.994 |     1.3
   79 |   0.7652 |     25.981 |   0.9155 |     30.705 |     1.3
   80 |   0.7558 |     25.636 |   0.9332 |     30.769 |     1.3
   81 |   0.7454 |     25.121 |   0.9210 |     30.673 |     1.4
   82 |   0.7414 |     25.099 |   0.9196 |     30.962 |     1.4
   83 |   0.7289 |     24.233 |   0.9209 |     29.679 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,626,914

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2121 |     60.722 |   1.6179 |     46.346 |     0.1
    2 |   1.4774 |     46.252 |   1.4323 |     46.827 |     0.1
    3 |   1.4033 |     46.126 |   1.3944 |     46.346 |     0.2
    4 |   1.3731 |     45.853 |   1.3552 |     45.577 |     0.3
    5 |   1.3338 |     45.458 |   1.3332 |     45.577 |     0.3
    6 |   1.3124 |     45.409 |   1.3011 |     45.032 |     0.4
    7 |   1.2953 |     45.157 |   1.2871 |     44.295 |     0.4
    8 |   1.2779 |     44.735 |   1.2606 |     44.744 |     0.5
    9 |   1.2630 |     44.357 |   1.2467 |     43.750 |     0.6
   10 |   1.2530 |     44.477 |   1.2400 |     43.397 |     0.6
   11 |   1.2363 |     43.924 |   1.2131 |     43.205 |     0.7
   12 |   1.2222 |     43.393 |   1.2067 |     42.788 |     0.8
   13 |   1.2089 |     43.036 |   1.1943 |     42.628 |     0.8
   14 |   1.1928 |     42.724 |   1.1863 |     42.468 |     0.9
   15 |   1.1833 |     42.072 |   1.1714 |     41.955 |     1.0
   16 |   1.1700 |     41.694 |   1.1464 |     40.545 |     1.0
   17 |   1.1485 |     41.130 |   1.1364 |     40.609 |     1.1
   18 |   1.1413 |     40.856 |   1.1298 |     40.705 |     1.1
   19 |   1.1235 |     39.744 |   1.1067 |     39.679 |     1.2
   20 |   1.0996 |     39.250 |   1.0998 |     38.910 |     1.3
   21 |   1.0835 |     38.292 |   1.0818 |     38.045 |     1.3
   22 |   1.0611 |     37.689 |   1.0625 |     37.853 |     1.4
   23 |   1.0427 |     36.555 |   1.0566 |     36.891 |     1.5
   24 |   1.0161 |     35.558 |   1.0404 |     36.538 |     1.5
   25 |   0.9957 |     34.440 |   1.0145 |     35.064 |     1.6
   26 |   0.9672 |     33.525 |   1.0106 |     35.128 |     1.6
   27 |   0.9393 |     32.457 |   0.9862 |     34.295 |     1.7
   28 |   0.9035 |     30.556 |   0.9625 |     33.045 |     1.8
   29 |   0.8779 |     29.772 |   0.9644 |     32.981 |     1.8
   30 |   0.8493 |     28.490 |   0.9628 |     32.404 |     1.9
   31 |   0.8275 |     28.030 |   0.9300 |     30.513 |     2.0
   32 |   0.8055 |     26.748 |   0.9288 |     30.737 |     2.0
   33 |   0.7712 |     25.822 |   0.9187 |     29.423 |     2.1
   34 |   0.7448 |     24.693 |   0.9237 |     29.551 |     2.1
   35 |   0.7173 |     24.112 |   0.9063 |     29.295 |     2.2
   36 |   0.6835 |     22.463 |   0.8917 |     28.494 |     2.3
   37 |   0.6585 |     21.466 |   0.8893 |     27.788 |     2.3
   38 |   0.6374 |     20.436 |   0.9045 |     28.301 |     2.4
   39 |   0.6298 |     20.321 |   0.8966 |     27.532 |     2.5
   40 |   0.5922 |     18.880 |   0.9002 |     27.853 |     2.5
   41 |   0.5738 |     18.393 |   0.9116 |     27.853 |     2.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 326,434

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3473 |     60.382 |   1.7745 |     49.359 |     0.0
    2 |   1.5855 |     47.113 |   1.4723 |     45.769 |     0.0
    3 |   1.4131 |     45.381 |   1.3865 |     45.064 |     0.1
    4 |   1.3468 |     44.675 |   1.3333 |     44.167 |     0.1
    5 |   1.3187 |     44.488 |   1.2989 |     43.910 |     0.1
    6 |   1.2914 |     44.225 |   1.2800 |     44.199 |     0.1
    7 |   1.2699 |     43.677 |   1.2464 |     42.981 |     0.1
    8 |   1.2395 |     43.217 |   1.2277 |     43.365 |     0.1
    9 |   1.2208 |     42.521 |   1.2039 |     42.596 |     0.2
   10 |   1.2011 |     42.056 |   1.1873 |     41.378 |     0.2
   11 |   1.1844 |     41.404 |   1.1707 |     40.962 |     0.2
   12 |   1.1662 |     41.108 |   1.1631 |     40.481 |     0.2
   13 |   1.1476 |     40.160 |   1.1406 |     40.385 |     0.2
   14 |   1.1305 |     39.897 |   1.1292 |     39.936 |     0.3
   15 |   1.1149 |     38.823 |   1.1115 |     38.718 |     0.3
   16 |   1.0982 |     38.374 |   1.0895 |     38.750 |     0.3
   17 |   1.0818 |     37.749 |   1.0848 |     38.333 |     0.3
   18 |   1.0650 |     36.884 |   1.0625 |     36.667 |     0.3
   19 |   1.0411 |     35.886 |   1.0555 |     36.474 |     0.4
   20 |   1.0213 |     35.180 |   1.0495 |     36.506 |     0.4
   21 |   1.0083 |     34.692 |   1.0370 |     36.218 |     0.4
   22 |   0.9893 |     33.750 |   1.0224 |     35.609 |     0.4
   23 |   0.9621 |     32.720 |   1.0094 |     34.647 |     0.4
   24 |   0.9465 |     31.865 |   0.9882 |     33.718 |     0.5
   25 |   0.9211 |     31.218 |   1.0000 |     34.295 |     0.5
   26 |   0.9007 |     30.041 |   0.9735 |     33.494 |     0.5
   27 |   0.8803 |     29.542 |   0.9517 |     32.340 |     0.5
   28 |   0.8487 |     28.446 |   0.9579 |     32.340 |     0.5
   29 |   0.8388 |     27.679 |   0.9493 |     32.083 |     0.5
   30 |   0.8106 |     26.896 |   0.9467 |     31.987 |     0.6
   31 |   0.7886 |     25.460 |   0.9253 |     30.769 |     0.6
   32 |   0.7615 |     24.940 |   0.9266 |     30.897 |     0.6
   33 |   0.7431 |     24.217 |   0.9433 |     31.506 |     0.6
   34 |   0.7195 |     23.219 |   0.9160 |     30.224 |     0.6
   35 |   0.6993 |     22.535 |   0.9113 |     29.135 |     0.7
   36 |   0.6859 |     21.872 |   0.9260 |     29.712 |     0.7
   37 |   0.6622 |     21.077 |   0.9034 |     29.263 |     0.7
   38 |   0.6330 |     20.102 |   0.9157 |     28.333 |     0.7
   39 |   0.6257 |     19.472 |   0.9161 |     28.269 |     0.7
   40 |   0.6123 |     19.121 |   0.9282 |     28.429 |     0.8
   41 |   0.5851 |     18.371 |   0.9125 |     28.365 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,461,474

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0075 |     55.451 |   1.4846 |     46.378 |     0.1
    2 |   1.4014 |     45.902 |   1.3552 |     45.737 |     0.1
    3 |   1.3303 |     45.135 |   1.2917 |     44.071 |     0.2
    4 |   1.2864 |     44.631 |   1.2612 |     43.814 |     0.2
    5 |   1.2580 |     44.132 |   1.2388 |     42.917 |     0.3
    6 |   1.2276 |     43.469 |   1.2120 |     43.365 |     0.4
    7 |   1.2069 |     42.768 |   1.1830 |     41.699 |     0.4
    8 |   1.1849 |     42.302 |   1.1710 |     41.218 |     0.5
    9 |   1.1650 |     41.256 |   1.1437 |     40.833 |     0.5
   10 |   1.1454 |     40.790 |   1.1324 |     40.353 |     0.6
   11 |   1.1263 |     40.374 |   1.1041 |     39.167 |     0.7
   12 |   1.1085 |     39.349 |   1.0898 |     38.558 |     0.7
   13 |   1.0869 |     38.511 |   1.0781 |     37.756 |     0.8
   14 |   1.0698 |     37.985 |   1.0547 |     36.506 |     0.9
   15 |   1.0565 |     37.256 |   1.0392 |     36.506 |     0.9
   16 |   1.0301 |     36.462 |   1.0270 |     36.282 |     1.0
   17 |   1.0155 |     35.552 |   1.0117 |     35.064 |     1.0
   18 |   1.0027 |     35.497 |   1.0048 |     35.288 |     1.1
   19 |   0.9824 |     34.298 |   0.9966 |     33.878 |     1.2
   20 |   0.9642 |     33.810 |   0.9747 |     33.974 |     1.2
   21 |   0.9480 |     32.906 |   0.9675 |     32.821 |     1.3
   22 |   0.9331 |     32.358 |   0.9595 |     33.173 |     1.3
   23 |   0.9136 |     31.799 |   0.9536 |     32.564 |     1.4
   24 |   0.9077 |     31.421 |   0.9388 |     32.083 |     1.5
   25 |   0.8776 |     30.304 |   0.9263 |     31.827 |     1.5
   26 |   0.8585 |     29.454 |   0.9226 |     31.506 |     1.6
   27 |   0.8376 |     28.643 |   0.9202 |     31.763 |     1.6
   28 |   0.8289 |     28.265 |   0.9075 |     31.314 |     1.7
   29 |   0.8071 |     27.542 |   0.9036 |     30.769 |     1.8
   30 |   0.7949 |     27.027 |   0.8827 |     29.647 |     1.8
   31 |   0.7807 |     26.737 |   0.8745 |     29.295 |     1.9
   32 |   0.7569 |     25.438 |   0.8782 |     29.295 |     2.0
   33 |   0.7450 |     25.422 |   0.8793 |     29.712 |     2.0
   34 |   0.7213 |     24.255 |   0.8711 |     28.910 |     2.1
   35 |   0.7144 |     24.134 |   0.8672 |     28.558 |     2.1
   36 |   0.6876 |     23.132 |   0.8631 |     28.846 |     2.2
   37 |   0.6639 |     22.321 |   0.8687 |     28.622 |     2.3
   38 |   0.6480 |     22.151 |   0.8549 |     27.981 |     2.3
   39 |   0.6373 |     21.280 |   0.8416 |     27.756 |     2.4
   40 |   0.6135 |     20.546 |   0.8465 |     27.212 |     2.4
   41 |   0.5955 |     19.883 |   0.8357 |     27.404 |     2.5
   42 |   0.5765 |     19.083 |   0.8492 |     27.468 |     2.6
   43 |   0.5580 |     18.557 |   0.8443 |     26.635 |     2.6
   44 |   0.5380 |     17.494 |   0.8647 |     27.179 |     2.7
   45 |   0.5332 |     17.576 |   0.8704 |     26.955 |     2.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 305,634

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5202 |     68.376 |   1.9645 |     53.718 |     0.0
    2 |   1.7186 |     48.663 |   1.5591 |     46.346 |     0.0
    3 |   1.4775 |     46.126 |   1.4496 |     46.346 |     0.1
    4 |   1.4131 |     46.050 |   1.4261 |     46.603 |     0.1
    5 |   1.3827 |     46.000 |   1.3879 |     46.026 |     0.1
    6 |   1.3638 |     45.748 |   1.3606 |     46.026 |     0.1
    7 |   1.3494 |     45.622 |   1.3516 |     46.282 |     0.1
    8 |   1.3379 |     45.474 |   1.3354 |     45.705 |     0.1
    9 |   1.3294 |     45.496 |   1.3284 |     45.064 |     0.2
   10 |   1.3168 |     45.222 |   1.3160 |     44.808 |     0.2
   11 |   1.3011 |     45.020 |   1.3039 |     44.583 |     0.2
   12 |   1.2918 |     44.883 |   1.2953 |     44.744 |     0.2
   13 |   1.2717 |     44.488 |   1.2648 |     44.231 |     0.2
   14 |   1.2542 |     43.853 |   1.2467 |     42.628 |     0.2
   15 |   1.2375 |     42.982 |   1.2292 |     42.372 |     0.3
   16 |   1.2165 |     42.576 |   1.2155 |     42.853 |     0.3
   17 |   1.2055 |     42.094 |   1.1941 |     40.641 |     0.3
   18 |   1.1851 |     41.206 |   1.1843 |     41.026 |     0.3
   19 |   1.1731 |     40.730 |   1.1652 |     40.224 |     0.3
   20 |   1.1590 |     40.220 |   1.1514 |     40.064 |     0.3
   21 |   1.1426 |     39.360 |   1.1386 |     39.167 |     0.4
   22 |   1.1247 |     39.070 |   1.1240 |     38.526 |     0.4
   23 |   1.1109 |     38.588 |   1.1089 |     38.462 |     0.4
   24 |   1.0886 |     37.831 |   1.0937 |     37.596 |     0.4
   25 |   1.0752 |     37.327 |   1.0774 |     36.603 |     0.4
   26 |   1.0527 |     36.292 |   1.0655 |     36.795 |     0.4
   27 |   1.0424 |     35.788 |   1.0696 |     36.923 |     0.5
   28 |   1.0212 |     35.054 |   1.0416 |     35.545 |     0.5
   29 |   0.9958 |     34.243 |   1.0400 |     35.962 |     0.5
   30 |   0.9779 |     33.361 |   1.0270 |     34.744 |     0.5
   31 |   0.9599 |     32.725 |   1.0189 |     34.647 |     0.5
   32 |   0.9389 |     32.051 |   1.0164 |     34.199 |     0.5
   33 |   0.9166 |     31.147 |   0.9995 |     34.071 |     0.6
   34 |   0.8895 |     29.997 |   0.9986 |     33.590 |     0.6
   35 |   0.8703 |     29.147 |   0.9936 |     33.365 |     0.6
   36 |   0.8465 |     28.353 |   0.9786 |     33.237 |     0.6
   37 |   0.8257 |     27.663 |   0.9750 |     32.115 |     0.6
   38 |   0.8068 |     26.709 |   0.9808 |     31.827 |     0.6
   39 |   0.7859 |     25.844 |   0.9715 |     30.897 |     0.7
   40 |   0.7775 |     25.449 |   0.9778 |     31.410 |     0.7
   41 |   0.7607 |     24.885 |   0.9581 |     30.769 |     0.7
   42 |   0.7361 |     23.751 |   0.9781 |     30.417 |     0.7
   43 |   0.7122 |     23.154 |   0.9714 |     30.962 |     0.7
   44 |   0.7000 |     22.452 |   0.9618 |     30.288 |     0.7
   45 |   0.6976 |     22.392 |   0.9578 |     30.000 |     0.8
   46 |   0.6709 |     21.252 |   0.9645 |     30.192 |     0.8
   47 |   0.6525 |     20.946 |   0.9672 |     29.327 |     0.8
   48 |   0.6282 |     19.828 |   0.9565 |     29.744 |     0.8
   49 |   0.6134 |     19.461 |   0.9489 |     28.333 |     0.8
   50 |   0.6077 |     19.362 |   0.9522 |     28.205 |     0.9
   51 |   0.5828 |     18.393 |   0.9749 |     28.558 |     0.9
   52 |   0.5665 |     17.510 |   0.9805 |     28.494 |     0.9
   53 |   0.5615 |     17.675 |   0.9783 |     28.622 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 903,394

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2216 |     60.613 |   1.6208 |     46.346 |     0.0
    2 |   1.4735 |     46.159 |   1.4344 |     46.827 |     0.1
    3 |   1.3909 |     46.094 |   1.3880 |     46.346 |     0.1
    4 |   1.3533 |     45.710 |   1.3325 |     45.833 |     0.1
    5 |   1.3195 |     45.447 |   1.3182 |     46.154 |     0.2
    6 |   1.2992 |     45.146 |   1.2824 |     44.327 |     0.2
    7 |   1.2771 |     44.455 |   1.2717 |     44.359 |     0.2
    8 |   1.2555 |     43.951 |   1.2426 |     43.173 |     0.3
    9 |   1.2376 |     44.028 |   1.2271 |     43.301 |     0.3
   10 |   1.2233 |     43.420 |   1.2088 |     42.885 |     0.3
   11 |   1.2066 |     43.228 |   1.1997 |     42.981 |     0.3
   12 |   1.1962 |     42.850 |   1.1918 |     43.077 |     0.4
   13 |   1.1818 |     42.379 |   1.1670 |     42.083 |     0.4
   14 |   1.1670 |     41.815 |   1.1488 |     41.731 |     0.4
   15 |   1.1513 |     41.349 |   1.1406 |     40.192 |     0.5
   16 |   1.1417 |     40.850 |   1.1282 |     40.224 |     0.5
   17 |   1.1301 |     40.615 |   1.1353 |     41.474 |     0.5
   18 |   1.1225 |     40.374 |   1.1074 |     40.417 |     0.6
   19 |   1.1087 |     40.078 |   1.1048 |     39.904 |     0.6
   20 |   1.0989 |     39.870 |   1.0918 |     38.494 |     0.6
   21 |   1.0928 |     39.398 |   1.0814 |     39.231 |     0.7
   22 |   1.0818 |     39.344 |   1.0745 |     38.205 |     0.7
   23 |   1.0730 |     39.360 |   1.0728 |     38.237 |     0.7
   24 |   1.0646 |     38.555 |   1.0642 |     38.397 |     0.8
   25 |   1.0579 |     38.242 |   1.0547 |     37.468 |     0.8
   26 |   1.0533 |     38.237 |   1.0617 |     37.821 |     0.8
   27 |   1.0452 |     37.990 |   1.0643 |     38.269 |     0.9
   28 |   1.0358 |     37.705 |   1.0353 |     37.404 |     0.9
   29 |   1.0216 |     37.136 |   1.0406 |     37.276 |     0.9
   30 |   1.0232 |     37.026 |   1.0312 |     37.692 |     0.9
   31 |   1.0033 |     36.034 |   1.0307 |     37.212 |     1.0
   32 |   0.9973 |     36.089 |   1.0304 |     36.923 |     1.0
   33 |   0.9872 |     35.448 |   1.0194 |     36.410 |     1.0
   34 |   0.9821 |     35.421 |   1.0033 |     35.609 |     1.1
   35 |   0.9687 |     34.785 |   0.9983 |     36.250 |     1.1
   36 |   0.9584 |     34.259 |   0.9956 |     35.481 |     1.1
   37 |   0.9461 |     33.470 |   0.9850 |     35.128 |     1.2
   38 |   0.9321 |     33.065 |   0.9904 |     34.744 |     1.2
   39 |   0.9253 |     32.780 |   0.9799 |     34.679 |     1.2
   40 |   0.9126 |     32.177 |   0.9744 |     34.423 |     1.3
   41 |   0.9043 |     31.536 |   0.9691 |     33.622 |     1.3
   42 |   0.8977 |     31.202 |   0.9582 |     33.750 |     1.3
   43 |   0.8960 |     31.421 |   0.9633 |     33.558 |     1.4
   44 |   0.8735 |     30.413 |   0.9541 |     33.173 |     1.4
   45 |   0.8640 |     30.046 |   0.9447 |     33.333 |     1.4
   46 |   0.8534 |     29.920 |   0.9463 |     33.109 |     1.5
   47 |   0.8407 |     29.011 |   0.9321 |     31.923 |     1.5
   48 |   0.8303 |     28.649 |   0.9369 |     32.628 |     1.5
   49 |   0.8176 |     28.320 |   0.9284 |     32.115 |     1.5
   50 |   0.8084 |     28.090 |   0.9232 |     31.667 |     1.6
   51 |   0.8074 |     28.096 |   0.9369 |     32.660 |     1.6
   52 |   0.7878 |     27.109 |   0.9154 |     31.763 |     1.6
   53 |   0.7743 |     26.720 |   0.9224 |     31.250 |     1.7
   54 |   0.7766 |     26.616 |   0.9153 |     30.994 |     1.7
   55 |   0.7690 |     26.408 |   0.9048 |     30.801 |     1.7
   56 |   0.7365 |     25.049 |   0.9054 |     29.904 |     1.8
   57 |   0.7230 |     24.595 |   0.8986 |     29.455 |     1.8
   58 |   0.7171 |     24.189 |   0.9073 |     29.744 |     1.8
   59 |   0.7124 |     24.195 |   0.8904 |     28.878 |     1.9
   60 |   0.7026 |     23.619 |   0.9110 |     29.359 |     1.9
   61 |   0.6861 |     23.351 |   0.8986 |     28.365 |     1.9
   62 |   0.6666 |     22.535 |   0.8949 |     29.038 |     2.0
   63 |   0.6575 |     22.146 |   0.8906 |     28.173 |     2.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 43 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,213,026

Training started
X_train.shape: torch.Size([3042, 702])
Y_train.shape: torch.Size([3042, 7])
X_dev.shape: torch.Size([520, 467])
Y_dev.shape: torch.Size([520, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4885 |     65.582 |   1.9379 |     53.045 |     0.1
    2 |   1.7381 |     49.310 |   1.5811 |     46.378 |     0.1
    3 |   1.4910 |     46.044 |   1.4571 |     46.378 |     0.2
    4 |   1.4153 |     46.094 |   1.3922 |     46.346 |     0.2
    5 |   1.3715 |     45.743 |   1.3444 |     45.801 |     0.3
    6 |   1.3339 |     45.464 |   1.3156 |     45.353 |     0.4
    7 |   1.3084 |     44.987 |   1.2920 |     44.808 |     0.4
    8 |   1.2892 |     44.675 |   1.2646 |     43.878 |     0.5
    9 |   1.2670 |     44.368 |   1.2444 |     44.038 |     0.5
   10 |   1.2484 |     43.787 |   1.2327 |     43.141 |     0.6
   11 |   1.2331 |     43.519 |   1.2176 |     42.821 |     0.7
   12 |   1.2248 |     43.212 |   1.2038 |     41.699 |     0.7
   13 |   1.2076 |     42.598 |   1.1922 |     42.115 |     0.8
   14 |   1.2018 |     42.615 |   1.1810 |     42.051 |     0.9
   15 |   1.1914 |     42.198 |   1.1815 |     41.987 |     0.9
   16 |   1.1876 |     41.886 |   1.1681 |     41.218 |     1.0
   17 |   1.1745 |     41.995 |   1.1606 |     40.833 |     1.0
   18 |   1.1701 |     41.261 |   1.1509 |     40.577 |     1.1
   19 |   1.1620 |     41.415 |   1.1567 |     40.994 |     1.2
   20 |   1.1534 |     41.075 |   1.1473 |     40.032 |     1.2
   21 |   1.1597 |     41.311 |   1.1337 |     40.128 |     1.3
   22 |   1.1523 |     41.059 |   1.1325 |     40.673 |     1.3
   23 |   1.1410 |     40.587 |   1.1212 |     39.455 |     1.4
   24 |   1.1332 |     40.242 |   1.1163 |     39.647 |     1.5
   25 |   1.1299 |     40.083 |   1.1127 |     39.455 |     1.5
   26 |   1.1231 |     40.039 |   1.1118 |     38.878 |     1.6
   27 |   1.1183 |     39.694 |   1.1060 |     38.750 |     1.6
   28 |   1.1155 |     39.722 |   1.0981 |     38.782 |     1.7
   29 |   1.1095 |     39.639 |   1.1041 |     39.006 |     1.8
   30 |   1.1067 |     39.464 |   1.1007 |     39.551 |     1.8
   31 |   1.0995 |     39.448 |   1.0842 |     38.429 |     1.9
   32 |   1.0990 |     39.349 |   1.0883 |     38.462 |     1.9
   33 |   1.0942 |     39.064 |   1.0832 |     38.237 |     2.0
   34 |   1.0897 |     38.998 |   1.0803 |     38.429 |     2.1
   35 |   1.0790 |     38.796 |   1.0701 |     38.205 |     2.1
   36 |   1.0803 |     39.031 |   1.0701 |     37.724 |     2.2
   37 |   1.0766 |     38.735 |   1.0660 |     37.853 |     2.2
   38 |   1.0707 |     38.412 |   1.0653 |     38.013 |     2.3
   39 |   1.0705 |     38.483 |   1.0611 |     37.724 |     2.4
   40 |   1.0641 |     38.385 |   1.0568 |     37.468 |     2.4
   41 |   1.0594 |     37.864 |   1.0640 |     37.821 |     2.5
   42 |   1.0634 |     38.511 |   1.0544 |     37.628 |     2.6
   43 |   1.0538 |     37.930 |   1.0537 |     37.308 |     2.6
   44 |   1.0489 |     37.656 |   1.0504 |     37.564 |     2.7
   45 |   1.0458 |     37.831 |   1.0483 |     37.532 |     2.7
   46 |   1.0418 |     37.596 |   1.0581 |     37.885 |     2.8
   47 |   1.0393 |     37.322 |   1.0433 |     37.212 |     2.9
   48 |   1.0390 |     37.158 |   1.0435 |     37.564 |     2.9
   49 |   1.0304 |     36.796 |   1.0414 |     36.923 |     3.0
   50 |   1.0238 |     36.867 |   1.0319 |     36.731 |     3.0
   51 |   1.0224 |     36.977 |   1.0276 |     35.929 |     3.1
   52 |   1.0206 |     37.108 |   1.0271 |     36.378 |     3.2
   53 |   1.0125 |     36.571 |   1.0277 |     36.603 |     3.2
   54 |   1.0099 |     36.166 |   1.0178 |     35.449 |     3.3
   55 |   1.0080 |     36.215 |   1.0265 |     36.378 |     3.3
   56 |   1.0045 |     35.925 |   1.0254 |     36.122 |     3.4
   57 |   0.9964 |     35.366 |   1.0199 |     36.250 |     3.5
   58 |   0.9893 |     35.514 |   1.0157 |     36.923 |     3.5
   59 |   0.9847 |     35.191 |   1.0154 |     36.731 |     3.6
   60 |   0.9803 |     35.147 |   1.0116 |     35.513 |     3.6
   61 |   0.9792 |     35.130 |   1.0099 |     35.609 |     3.7
   62 |   0.9722 |     34.561 |   1.0070 |     36.122 |     3.8
   63 |   0.9683 |     34.495 |   1.0103 |     36.378 |     3.8
   64 |   0.9587 |     34.500 |   1.0092 |     36.154 |     3.9
   65 |   0.9567 |     33.958 |   0.9993 |     35.673 |     3.9
   66 |   0.9537 |     33.996 |   0.9941 |     34.712 |     4.0
   67 |   0.9459 |     33.755 |   1.0099 |     35.609 |     4.1
   68 |   0.9415 |     33.750 |   0.9960 |     35.481 |     4.1
   69 |   0.9347 |     33.131 |   1.0000 |     34.904 |     4.2
   70 |   0.9290 |     32.742 |   0.9982 |     35.737 |     4.2
   71 |   0.9270 |     33.092 |   0.9850 |     35.256 |     4.3
   72 |   0.9244 |     32.638 |   0.9871 |     34.936 |     4.4
   73 |   0.9326 |     33.229 |   0.9859 |     34.776 |     4.4
   74 |   0.9193 |     32.544 |   0.9861 |     35.385 |     4.5
   75 |   0.9075 |     32.166 |   0.9823 |     34.872 |     4.5
   76 |   0.9045 |     31.870 |   0.9682 |     33.814 |     4.6
   77 |   0.8918 |     31.257 |   0.9663 |     33.718 |     4.7
   78 |   0.8888 |     31.218 |   0.9715 |     33.397 |     4.7
   79 |   0.8851 |     30.884 |   0.9591 |     33.397 |     4.8
   80 |   0.8793 |     30.879 |   0.9711 |     33.590 |     4.9
   81 |   0.8689 |     30.216 |   0.9544 |     33.782 |     4.9
   82 |   0.8826 |     30.868 |   0.9700 |     33.590 |     5.0
   83 |   0.8686 |     30.260 |   0.9703 |     33.846 |     5.0
   84 |   0.8607 |     30.123 |   0.9528 |     32.885 |     5.1
   85 |   0.8579 |     29.843 |   0.9573 |     32.981 |     5.2
   86 |   0.8484 |     29.230 |   0.9491 |     32.276 |     5.2
   87 |   0.8481 |     29.772 |   0.9368 |     32.115 |     5.3
   88 |   0.8408 |     29.421 |   0.9453 |     32.372 |     5.3
   89 |   0.8334 |     29.038 |   0.9419 |     32.532 |     5.4
   90 |   0.8453 |     29.076 |   0.9558 |     32.532 |     5.5
   91 |   0.8363 |     28.961 |   0.9399 |     32.051 |     5.5
Early stopping

