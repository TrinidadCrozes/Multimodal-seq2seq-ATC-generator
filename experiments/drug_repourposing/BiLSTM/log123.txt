Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 503,906

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4515 |     65.004 |   1.8836 |     51.092 |     0.0
    2 |   1.6686 |     46.824 |   1.5130 |     45.350 |     0.1
    3 |   1.4635 |     46.191 |   1.4142 |     45.350 |     0.1
    4 |   1.3901 |     45.872 |   1.3601 |     45.693 |     0.1
    5 |   1.3476 |     45.052 |   1.3218 |     44.070 |     0.1
    6 |   1.3172 |     44.408 |   1.2959 |     43.976 |     0.2
    7 |   1.2942 |     44.182 |   1.2815 |     43.914 |     0.2
    8 |   1.2771 |     43.984 |   1.2654 |     43.508 |     0.2
    9 |   1.2567 |     43.819 |   1.2503 |     43.009 |     0.3
   10 |   1.2417 |     43.632 |   1.2342 |     42.603 |     0.3
   11 |   1.2250 |     43.257 |   1.2128 |     42.665 |     0.3
   12 |   1.2114 |     42.817 |   1.2055 |     41.979 |     0.4
   13 |   1.1987 |     42.388 |   1.1930 |     41.448 |     0.4
   14 |   1.1853 |     41.469 |   1.1826 |     42.104 |     0.4
   15 |   1.1715 |     41.210 |   1.1666 |     41.011 |     0.4
   16 |   1.1581 |     40.445 |   1.1642 |     40.824 |     0.5
   17 |   1.1440 |     39.977 |   1.1540 |     39.638 |     0.5
   18 |   1.1290 |     39.393 |   1.1414 |     39.700 |     0.5
   19 |   1.1154 |     39.168 |   1.1282 |     38.764 |     0.6
   20 |   1.1041 |     38.601 |   1.1203 |     39.045 |     0.6
   21 |   1.0913 |     38.315 |   1.1065 |     38.483 |     0.6
   22 |   1.0790 |     37.742 |   1.0984 |     37.391 |     0.6
   23 |   1.0619 |     36.983 |   1.0874 |     37.859 |     0.7
   24 |   1.0502 |     36.669 |   1.0773 |     36.423 |     0.7
   25 |   1.0348 |     35.634 |   1.0708 |     36.704 |     0.7
   26 |   1.0196 |     35.128 |   1.0628 |     35.893 |     0.8
   27 |   1.0081 |     34.759 |   1.0588 |     36.642 |     0.8
   28 |   0.9964 |     34.368 |   1.0489 |     36.111 |     0.8
   29 |   0.9795 |     33.515 |   1.0402 |     34.519 |     0.9
   30 |   0.9622 |     32.932 |   1.0139 |     34.114 |     0.9
   31 |   0.9490 |     32.150 |   1.0253 |     34.675 |     0.9
   32 |   0.9374 |     31.622 |   1.0130 |     34.114 |     0.9
   33 |   0.9173 |     31.126 |   1.0030 |     33.895 |     1.0
   34 |   0.9038 |     30.658 |   0.9917 |     33.521 |     1.0
   35 |   0.8908 |     29.706 |   0.9871 |     33.489 |     1.0
   36 |   0.8686 |     28.969 |   0.9800 |     32.803 |     1.1
   37 |   0.8519 |     28.556 |   0.9831 |     33.084 |     1.1
   38 |   0.8372 |     27.774 |   0.9675 |     32.615 |     1.1
   39 |   0.8330 |     27.752 |   0.9776 |     32.803 |     1.1
   40 |   0.8177 |     27.218 |   0.9638 |     32.959 |     1.2
   41 |   0.7907 |     26.123 |   0.9605 |     32.022 |     1.2
   42 |   0.7768 |     25.672 |   0.9509 |     32.022 |     1.2
   43 |   0.7607 |     25.154 |   0.9476 |     31.554 |     1.3
   44 |   0.7431 |     24.328 |   0.9405 |     30.774 |     1.3
   45 |   0.7293 |     23.965 |   0.9450 |     30.899 |     1.3
   46 |   0.7130 |     23.079 |   0.9413 |     30.868 |     1.4
   47 |   0.7012 |     22.699 |   0.9451 |     30.618 |     1.4
   48 |   0.6780 |     21.725 |   0.9333 |     30.712 |     1.4
   49 |   0.6597 |     20.910 |   0.9444 |     30.212 |     1.4
   50 |   0.6402 |     20.162 |   0.9317 |     30.337 |     1.5
   51 |   0.6270 |     19.551 |   0.9483 |     30.056 |     1.5
   52 |   0.6155 |     19.837 |   0.9348 |     29.089 |     1.5
   53 |   0.5963 |     18.615 |   0.9324 |     29.151 |     1.6
   54 |   0.5800 |     18.257 |   0.9574 |     29.775 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 537,922

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4730 |     66.364 |   1.8870 |     47.940 |     0.0
    2 |   1.6757 |     48.217 |   1.5209 |     45.350 |     0.1
    3 |   1.4681 |     46.120 |   1.4282 |     45.350 |     0.1
    4 |   1.4121 |     46.120 |   1.3870 |     45.006 |     0.1
    5 |   1.3795 |     45.943 |   1.3558 |     44.694 |     0.2
    6 |   1.3541 |     45.349 |   1.3316 |     44.538 |     0.2
    7 |   1.3356 |     45.167 |   1.3158 |     44.164 |     0.2
    8 |   1.3185 |     44.975 |   1.3046 |     44.164 |     0.2
    9 |   1.3040 |     44.573 |   1.2901 |     43.633 |     0.3
   10 |   1.2904 |     44.204 |   1.2784 |     43.414 |     0.3
   11 |   1.2736 |     44.309 |   1.2578 |     43.134 |     0.3
   12 |   1.2551 |     43.434 |   1.2507 |     43.227 |     0.4
   13 |   1.2385 |     43.411 |   1.2297 |     42.603 |     0.4
   14 |   1.2228 |     43.015 |   1.2256 |     42.603 |     0.4
   15 |   1.2088 |     42.602 |   1.2058 |     41.916 |     0.5
   16 |   1.1895 |     41.628 |   1.1919 |     40.793 |     0.5
   17 |   1.1717 |     40.891 |   1.1806 |     40.200 |     0.5
   18 |   1.1597 |     40.544 |   1.1767 |     40.543 |     0.6
   19 |   1.1411 |     39.966 |   1.1561 |     38.702 |     0.6
   20 |   1.1244 |     39.025 |   1.1430 |     38.983 |     0.6
   21 |   1.1066 |     38.502 |   1.1378 |     38.983 |     0.7
   22 |   1.0924 |     37.880 |   1.1366 |     38.421 |     0.7
   23 |   1.0725 |     37.230 |   1.1179 |     38.670 |     0.7
   24 |   1.0560 |     36.630 |   1.1101 |     38.421 |     0.8
   25 |   1.0362 |     35.651 |   1.0947 |     37.547 |     0.8
   26 |   1.0133 |     34.880 |   1.0813 |     37.609 |     0.8
   27 |   0.9947 |     34.175 |   1.0809 |     37.235 |     0.8
   28 |   0.9730 |     33.454 |   1.0828 |     37.453 |     0.9
   29 |   0.9574 |     32.739 |   1.0745 |     36.860 |     0.9
   30 |   0.9354 |     32.238 |   1.0619 |     36.579 |     0.9
   31 |   0.9132 |     31.275 |   1.0672 |     35.705 |     1.0
   32 |   0.8875 |     30.036 |   1.0658 |     35.737 |     1.0
   33 |   0.8656 |     29.167 |   1.0518 |     35.424 |     1.0
   34 |   0.8456 |     28.418 |   1.0737 |     35.362 |     1.1
   35 |   0.8292 |     27.868 |   1.0534 |     34.519 |     1.1
   36 |   0.7995 |     26.536 |   1.0555 |     34.395 |     1.1
   37 |   0.7723 |     25.501 |   1.0510 |     34.363 |     1.2
   38 |   0.7515 |     24.686 |   1.0644 |     33.895 |     1.2
   39 |   0.7309 |     23.745 |   1.0567 |     33.240 |     1.2
   40 |   0.7083 |     22.815 |   1.0647 |     33.552 |     1.3
   41 |   0.6780 |     21.664 |   1.0655 |     33.084 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,662,114

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1905 |     60.122 |   1.5835 |     45.350 |     0.1
    2 |   1.4721 |     46.131 |   1.4102 |     45.350 |     0.1
    3 |   1.3905 |     46.026 |   1.3573 |     44.725 |     0.2
    4 |   1.3463 |     45.701 |   1.3173 |     44.413 |     0.3
    5 |   1.3146 |     45.134 |   1.2909 |     44.351 |     0.3
    6 |   1.2883 |     44.672 |   1.2799 |     43.883 |     0.4
    7 |   1.2671 |     44.705 |   1.2411 |     43.789 |     0.4
    8 |   1.2426 |     43.830 |   1.2300 |     43.165 |     0.5
    9 |   1.2227 |     43.345 |   1.1996 |     42.353 |     0.6
   10 |   1.2054 |     42.823 |   1.1868 |     40.886 |     0.6
   11 |   1.1904 |     42.162 |   1.1800 |     42.197 |     0.7
   12 |   1.1759 |     42.008 |   1.1748 |     41.011 |     0.8
   13 |   1.1643 |     41.419 |   1.1477 |     40.106 |     0.8
   14 |   1.1511 |     41.083 |   1.1393 |     40.325 |     0.9
   15 |   1.1384 |     40.825 |   1.1243 |     40.387 |     1.0
   16 |   1.1256 |     40.236 |   1.1247 |     39.763 |     1.0
   17 |   1.1169 |     40.021 |   1.1234 |     39.888 |     1.1
   18 |   1.1031 |     39.806 |   1.1198 |     39.513 |     1.1
   19 |   1.0918 |     39.234 |   1.0969 |     38.889 |     1.2
   20 |   1.0804 |     38.926 |   1.0878 |     38.296 |     1.3
   21 |   1.0710 |     38.364 |   1.0850 |     37.765 |     1.3
   22 |   1.0536 |     37.539 |   1.0666 |     37.984 |     1.4
   23 |   1.0428 |     37.274 |   1.0612 |     37.422 |     1.5
   24 |   1.0307 |     36.707 |   1.0606 |     38.140 |     1.5
   25 |   1.0187 |     36.306 |   1.0476 |     37.203 |     1.6
   26 |   1.0081 |     35.860 |   1.0476 |     35.955 |     1.7
   27 |   0.9990 |     35.320 |   1.0340 |     36.454 |     1.7
   28 |   0.9828 |     34.847 |   1.0222 |     35.393 |     1.8
   29 |   0.9717 |     34.495 |   1.0232 |     35.893 |     1.8
   30 |   0.9599 |     34.126 |   1.0047 |     34.863 |     1.9
   31 |   0.9453 |     33.482 |   1.0028 |     35.237 |     2.0
   32 |   0.9379 |     32.827 |   0.9971 |     34.332 |     2.0
   33 |   0.9227 |     32.541 |   0.9951 |     34.176 |     2.1
   34 |   0.9148 |     31.803 |   0.9834 |     34.051 |     2.2
   35 |   0.9002 |     31.583 |   0.9785 |     33.895 |     2.2
   36 |   0.8914 |     30.972 |   0.9733 |     33.489 |     2.3
   37 |   0.8901 |     31.330 |   0.9683 |     33.302 |     2.4
   38 |   0.8692 |     30.196 |   0.9646 |     32.522 |     2.4
   39 |   0.8546 |     29.194 |   0.9557 |     32.615 |     2.5
   40 |   0.8462 |     29.057 |   0.9509 |     32.491 |     2.5
   41 |   0.8304 |     28.699 |   0.9374 |     31.898 |     2.6
   42 |   0.8220 |     28.198 |   0.9535 |     32.335 |     2.7
   43 |   0.8122 |     27.994 |   0.9472 |     31.866 |     2.7
   44 |   0.7950 |     27.202 |   0.9398 |     31.648 |     2.8
   45 |   0.7875 |     26.789 |   0.9489 |     31.461 |     2.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 572,834

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5407 |     68.098 |   1.9702 |     58.583 |     0.0
    2 |   1.7471 |     49.961 |   1.5524 |     45.350 |     0.1
    3 |   1.4936 |     46.175 |   1.4411 |     45.350 |     0.1
    4 |   1.4293 |     46.224 |   1.4043 |     45.350 |     0.1
    5 |   1.4041 |     46.092 |   1.3871 |     45.350 |     0.2
    6 |   1.3867 |     46.042 |   1.3663 |     45.006 |     0.2
    7 |   1.3673 |     46.054 |   1.3436 |     44.944 |     0.3
    8 |   1.3395 |     45.723 |   1.3194 |     45.100 |     0.3
    9 |   1.3188 |     44.986 |   1.3015 |     44.694 |     0.3
   10 |   1.3050 |     44.903 |   1.2874 |     44.164 |     0.4
   11 |   1.2880 |     44.540 |   1.2618 |     44.257 |     0.4
   12 |   1.2744 |     44.402 |   1.2551 |     44.101 |     0.4
   13 |   1.2604 |     44.232 |   1.2375 |     43.633 |     0.5
   14 |   1.2507 |     44.254 |   1.2294 |     43.009 |     0.5
   15 |   1.2395 |     43.736 |   1.2238 |     43.071 |     0.6
   16 |   1.2289 |     43.659 |   1.2050 |     42.665 |     0.6
   17 |   1.2210 |     43.483 |   1.2023 |     42.572 |     0.6
   18 |   1.2140 |     43.345 |   1.1957 |     42.072 |     0.7
   19 |   1.2068 |     42.988 |   1.1906 |     42.509 |     0.7
   20 |   1.1976 |     42.580 |   1.1890 |     42.135 |     0.7
   21 |   1.1931 |     42.371 |   1.1772 |     42.353 |     0.8
   22 |   1.1872 |     42.278 |   1.1733 |     41.261 |     0.8
   23 |   1.1814 |     42.063 |   1.1652 |     41.042 |     0.9
   24 |   1.1747 |     42.041 |   1.1646 |     41.011 |     0.9
   25 |   1.1692 |     41.936 |   1.1587 |     41.167 |     0.9
   26 |   1.1644 |     41.755 |   1.1507 |     40.824 |     1.0
   27 |   1.1589 |     41.650 |   1.1474 |     40.637 |     1.0
   28 |   1.1535 |     41.347 |   1.1526 |     41.042 |     1.1
   29 |   1.1480 |     41.298 |   1.1421 |     40.512 |     1.1
   30 |   1.1445 |     40.968 |   1.1387 |     40.793 |     1.1
   31 |   1.1375 |     40.858 |   1.1418 |     41.105 |     1.2
   32 |   1.1325 |     40.819 |   1.1257 |     40.044 |     1.2
   33 |   1.1259 |     40.313 |   1.1267 |     39.856 |     1.3
   34 |   1.1222 |     39.812 |   1.1208 |     39.825 |     1.3
   35 |   1.1205 |     40.252 |   1.1162 |     39.669 |     1.3
   36 |   1.1136 |     39.812 |   1.1082 |     39.326 |     1.4
   37 |   1.1082 |     39.355 |   1.1104 |     39.950 |     1.4
   38 |   1.1030 |     39.025 |   1.1014 |     39.544 |     1.4
   39 |   1.0941 |     38.893 |   1.0981 |     39.232 |     1.5
   40 |   1.0932 |     39.085 |   1.0997 |     39.201 |     1.5
   41 |   1.0861 |     38.661 |   1.0936 |     38.733 |     1.6
   42 |   1.0850 |     38.557 |   1.0872 |     38.608 |     1.6
   43 |   1.0789 |     38.403 |   1.0918 |     39.139 |     1.6
   44 |   1.0761 |     38.205 |   1.0796 |     38.483 |     1.7
   45 |   1.0719 |     38.364 |   1.0820 |     38.889 |     1.7
   46 |   1.0674 |     37.940 |   1.0754 |     38.795 |     1.7
   47 |   1.0624 |     37.555 |   1.0788 |     38.639 |     1.8
   48 |   1.0602 |     37.583 |   1.0702 |     37.797 |     1.8
   49 |   1.0522 |     37.351 |   1.0702 |     37.828 |     1.9
   50 |   1.0489 |     36.906 |   1.0602 |     37.953 |     1.9
   51 |   1.0425 |     36.713 |   1.0622 |     37.547 |     1.9
   52 |   1.0414 |     36.988 |   1.0614 |     37.110 |     2.0
   53 |   1.0364 |     36.724 |   1.0537 |     37.984 |     2.0
   54 |   1.0323 |     36.718 |   1.0538 |     38.046 |     2.0
   55 |   1.0264 |     36.416 |   1.0454 |     37.453 |     2.1
   56 |   1.0210 |     36.311 |   1.0448 |     36.798 |     2.1
   57 |   1.0160 |     35.915 |   1.0448 |     37.110 |     2.1
   58 |   1.0122 |     35.832 |   1.0456 |     36.735 |     2.2
   59 |   1.0102 |     35.452 |   1.0478 |     36.767 |     2.2
   60 |   1.0055 |     35.596 |   1.0462 |     36.548 |     2.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,495,074

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0743 |     56.792 |   1.5288 |     44.975 |     0.1
    2 |   1.4405 |     45.850 |   1.3858 |     44.944 |     0.1
    3 |   1.3622 |     45.608 |   1.3324 |     44.320 |     0.2
    4 |   1.3158 |     44.887 |   1.2944 |     43.695 |     0.2
    5 |   1.2926 |     44.446 |   1.2796 |     43.820 |     0.3
    6 |   1.2677 |     44.105 |   1.2499 |     43.227 |     0.4
    7 |   1.2431 |     43.301 |   1.2339 |     41.823 |     0.4
    8 |   1.2196 |     42.311 |   1.2098 |     41.230 |     0.5
    9 |   1.1880 |     41.441 |   1.1949 |     40.949 |     0.6
   10 |   1.1560 |     40.450 |   1.1613 |     40.106 |     0.6
   11 |   1.1147 |     38.496 |   1.1212 |     37.640 |     0.7
   12 |   1.0814 |     37.054 |   1.1101 |     37.047 |     0.8
   13 |   1.0399 |     35.563 |   1.0860 |     37.047 |     0.8
   14 |   0.9953 |     33.818 |   1.0391 |     34.519 |     0.9
   15 |   0.9450 |     31.616 |   1.0225 |     34.270 |     0.9
   16 |   0.9008 |     30.053 |   1.0103 |     33.177 |     1.0
   17 |   0.8509 |     27.956 |   0.9761 |     32.210 |     1.1
   18 |   0.7934 |     25.501 |   0.9583 |     30.524 |     1.1
   19 |   0.7410 |     23.811 |   0.9419 |     30.400 |     1.2
   20 |   0.6909 |     21.720 |   0.9285 |     29.057 |     1.3
   21 |   0.6387 |     20.195 |   0.9487 |     30.181 |     1.3
   22 |   0.5851 |     18.131 |   0.9543 |     29.526 |     1.4
   23 |   0.5332 |     16.661 |   0.9328 |     29.026 |     1.4
   24 |   0.4825 |     14.790 |   0.9372 |     27.653 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 718,658

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2112 |     60.304 |   1.5846 |     45.350 |     0.0
    2 |   1.4724 |     46.257 |   1.4111 |     45.662 |     0.1
    3 |   1.3933 |     46.252 |   1.3638 |     44.694 |     0.1
    4 |   1.3590 |     45.921 |   1.3313 |     44.788 |     0.1
    5 |   1.3329 |     45.564 |   1.3055 |     44.444 |     0.1
    6 |   1.3092 |     45.619 |   1.2936 |     44.288 |     0.2
    7 |   1.2952 |     45.024 |   1.2796 |     44.444 |     0.2
    8 |   1.2833 |     45.024 |   1.2704 |     43.633 |     0.2
    9 |   1.2705 |     44.859 |   1.2568 |     43.727 |     0.2
   10 |   1.2531 |     44.496 |   1.2361 |     43.227 |     0.3
   11 |   1.2349 |     44.006 |   1.2114 |     43.602 |     0.3
   12 |   1.2208 |     43.615 |   1.2046 |     42.385 |     0.3
   13 |   1.2043 |     43.065 |   1.1855 |     42.478 |     0.3
   14 |   1.1938 |     42.701 |   1.1902 |     42.541 |     0.4
   15 |   1.1839 |     42.729 |   1.1786 |     42.385 |     0.4
   16 |   1.1722 |     41.947 |   1.1622 |     41.479 |     0.4
   17 |   1.1639 |     41.650 |   1.1583 |     40.481 |     0.4
   18 |   1.1550 |     41.215 |   1.1484 |     40.637 |     0.5
   19 |   1.1454 |     41.012 |   1.1483 |     40.012 |     0.5
   20 |   1.1364 |     40.549 |   1.1397 |     40.356 |     0.5
   21 |   1.1303 |     40.384 |   1.1335 |     39.950 |     0.6
   22 |   1.1222 |     40.197 |   1.1302 |     40.449 |     0.6
   23 |   1.1151 |     40.219 |   1.1244 |     40.137 |     0.6
   24 |   1.1100 |     40.125 |   1.1139 |     39.295 |     0.6
   25 |   1.1011 |     39.421 |   1.1105 |     39.482 |     0.7
   26 |   1.0941 |     39.432 |   1.1037 |     39.419 |     0.7
   27 |   1.0864 |     39.146 |   1.1000 |     39.045 |     0.7
   28 |   1.0798 |     39.195 |   1.0914 |     39.326 |     0.7
   29 |   1.0714 |     38.887 |   1.0816 |     38.546 |     0.8
   30 |   1.0624 |     38.326 |   1.0729 |     38.920 |     0.8
   31 |   1.0556 |     38.100 |   1.0785 |     38.702 |     0.8
   32 |   1.0515 |     38.034 |   1.0640 |     38.390 |     0.9
   33 |   1.0452 |     37.770 |   1.0593 |     37.516 |     0.9
   34 |   1.0349 |     37.627 |   1.0494 |     38.202 |     0.9
   35 |   1.0277 |     37.082 |   1.0546 |     37.484 |     0.9
   36 |   1.0217 |     37.137 |   1.0334 |     36.486 |     1.0
   37 |   1.0101 |     36.597 |   1.0368 |     36.704 |     1.0
   38 |   1.0078 |     36.306 |   1.0294 |     36.642 |     1.0
   39 |   0.9954 |     35.920 |   1.0328 |     36.860 |     1.0
   40 |   0.9902 |     35.942 |   1.0197 |     35.986 |     1.1
   41 |   0.9845 |     35.557 |   1.0109 |     36.080 |     1.1
   42 |   0.9724 |     35.392 |   1.0040 |     35.612 |     1.1
   43 |   0.9670 |     35.056 |   1.0064 |     35.331 |     1.1
   44 |   0.9596 |     34.632 |   1.0056 |     35.674 |     1.2
   45 |   0.9542 |     34.522 |   0.9952 |     35.362 |     1.2
   46 |   0.9464 |     34.220 |   0.9966 |     35.768 |     1.2
   47 |   0.9349 |     33.686 |   0.9959 |     35.331 |     1.3
   48 |   0.9295 |     33.339 |   1.0001 |     34.582 |     1.3
   49 |   0.9221 |     32.871 |   0.9886 |     34.426 |     1.3
   50 |   0.9185 |     32.915 |   0.9751 |     34.145 |     1.3
   51 |   0.9153 |     32.794 |   0.9841 |     34.800 |     1.4
   52 |   0.8993 |     32.177 |   0.9619 |     33.365 |     1.4
   53 |   0.8953 |     31.704 |   0.9756 |     34.082 |     1.4
   54 |   0.8882 |     31.528 |   0.9960 |     34.644 |     1.4
   55 |   0.8843 |     31.500 |   0.9547 |     33.677 |     1.5
   56 |   0.8702 |     31.033 |   0.9548 |     33.333 |     1.5
   57 |   0.8615 |     30.983 |   0.9409 |     33.458 |     1.5
   58 |   0.8557 |     30.526 |   0.9327 |     32.397 |     1.6
   59 |   0.8448 |     29.970 |   0.9400 |     32.491 |     1.6
   60 |   0.8422 |     29.783 |   0.9413 |     32.678 |     1.6
   61 |   0.8340 |     29.657 |   0.9395 |     31.648 |     1.6
   62 |   0.8265 |     29.051 |   0.9358 |     31.336 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,392,450

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1241 |     57.689 |   1.5402 |     45.350 |     0.1
    2 |   1.4450 |     46.065 |   1.3784 |     45.849 |     0.1
    3 |   1.3572 |     45.404 |   1.3212 |     44.195 |     0.2
    4 |   1.3157 |     44.848 |   1.2961 |     44.132 |     0.3
    5 |   1.2885 |     44.606 |   1.2700 |     44.007 |     0.3
    6 |   1.2665 |     44.441 |   1.2448 |     43.414 |     0.4
    7 |   1.2407 |     43.610 |   1.2255 |     42.541 |     0.5
    8 |   1.2168 |     42.894 |   1.2096 |     41.823 |     0.5
    9 |   1.1964 |     42.322 |   1.1807 |     41.011 |     0.6
   10 |   1.1737 |     41.419 |   1.1568 |     41.011 |     0.7
   11 |   1.1496 |     40.610 |   1.1415 |     39.045 |     0.7
   12 |   1.1239 |     39.366 |   1.1185 |     39.482 |     0.8
   13 |   1.0937 |     38.348 |   1.0909 |     37.609 |     0.8
   14 |   1.0658 |     36.768 |   1.0907 |     37.953 |     0.9
   15 |   1.0371 |     36.157 |   1.0538 |     35.643 |     1.0
   16 |   1.0057 |     34.665 |   1.0317 |     34.488 |     1.0
   17 |   0.9695 |     33.333 |   1.0230 |     35.081 |     1.1
   18 |   0.9381 |     32.216 |   1.0002 |     33.146 |     1.2
   19 |   0.9106 |     30.834 |   0.9845 |     33.396 |     1.2
   20 |   0.8775 |     29.783 |   0.9669 |     32.272 |     1.3
   21 |   0.8390 |     28.088 |   0.9420 |     31.835 |     1.4
   22 |   0.8062 |     26.965 |   0.9387 |     30.993 |     1.4
   23 |   0.7693 |     25.655 |   0.9301 |     30.774 |     1.5
   24 |   0.7473 |     24.593 |   0.9206 |     29.900 |     1.6
   25 |   0.7038 |     23.211 |   0.9232 |     30.400 |     1.6
   26 |   0.7119 |     23.723 |   0.9123 |     29.682 |     1.7
   27 |   0.6524 |     21.439 |   0.9035 |     28.995 |     1.8
   28 |   0.6139 |     20.277 |   0.9074 |     29.307 |     1.8
   29 |   0.5749 |     18.632 |   0.9007 |     27.871 |     1.9
   30 |   0.5418 |     17.542 |   0.9177 |     28.246 |     2.0
   31 |   0.5175 |     16.843 |   0.9062 |     26.966 |     2.0
   32 |   0.4802 |     15.494 |   0.9114 |     26.685 |     2.1
   33 |   0.4604 |     14.828 |   0.9245 |     26.529 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,606,978

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4913 |     66.369 |   1.9147 |     53.246 |     0.1
    2 |   1.7129 |     48.806 |   1.5377 |     45.350 |     0.2
    3 |   1.4851 |     46.142 |   1.4398 |     45.350 |     0.3
    4 |   1.4292 |     46.142 |   1.4067 |     45.350 |     0.3
    5 |   1.4061 |     46.158 |   1.3880 |     45.350 |     0.4
    6 |   1.3888 |     46.092 |   1.3692 |     44.944 |     0.5
    7 |   1.3689 |     45.569 |   1.3455 |     44.070 |     0.6
    8 |   1.3521 |     45.107 |   1.3303 |     44.444 |     0.7
    9 |   1.3327 |     44.997 |   1.3088 |     43.914 |     0.8
   10 |   1.3115 |     44.705 |   1.2951 |     44.195 |     0.9
   11 |   1.2921 |     44.265 |   1.2778 |     43.727 |     0.9
   12 |   1.2724 |     44.061 |   1.2488 |     43.633 |     1.0
   13 |   1.2546 |     43.808 |   1.2349 |     42.821 |     1.1
   14 |   1.2373 |     43.224 |   1.2198 |     42.697 |     1.2
   15 |   1.2209 |     42.872 |   1.2039 |     42.634 |     1.3
   16 |   1.2041 |     42.283 |   1.1877 |     41.261 |     1.4
   17 |   1.1880 |     41.700 |   1.1748 |     40.762 |     1.5
   18 |   1.1698 |     41.083 |   1.1558 |     39.981 |     1.5
   19 |   1.1537 |     40.412 |   1.1449 |     39.451 |     1.6
   20 |   1.1370 |     39.746 |   1.1353 |     38.951 |     1.7
   21 |   1.1233 |     39.333 |   1.1186 |     38.639 |     1.8
   22 |   1.1046 |     38.656 |   1.1051 |     38.109 |     1.9
   23 |   1.0928 |     38.089 |   1.1075 |     38.265 |     2.0
   24 |   1.0789 |     37.483 |   1.1048 |     38.358 |     2.1
   25 |   1.0623 |     36.972 |   1.0830 |     37.734 |     2.1
   26 |   1.0509 |     36.504 |   1.0793 |     37.360 |     2.2
   27 |   1.0374 |     36.377 |   1.0700 |     37.453 |     2.3
   28 |   1.0210 |     35.722 |   1.0672 |     36.985 |     2.4
   29 |   1.0044 |     34.715 |   1.0607 |     36.392 |     2.5
   30 |   0.9912 |     34.357 |   1.0544 |     36.049 |     2.6
   31 |   0.9727 |     33.565 |   1.0484 |     35.674 |     2.7
   32 |   0.9518 |     32.695 |   1.0418 |     36.142 |     2.7
   33 |   0.9394 |     32.513 |   1.0377 |     35.861 |     2.8
   34 |   0.9182 |     31.473 |   1.0247 |     34.925 |     2.9
   35 |   0.8994 |     30.807 |   1.0305 |     34.894 |     3.0
   36 |   0.8855 |     30.433 |   1.0161 |     34.145 |     3.1
   37 |   0.8562 |     29.442 |   1.0169 |     33.458 |     3.2
   38 |   0.8451 |     28.930 |   1.0296 |     34.332 |     3.3
   39 |   0.8243 |     28.033 |   1.0190 |     33.770 |     3.3
   40 |   0.8048 |     27.191 |   1.0134 |     33.458 |     3.4
   41 |   0.7828 |     26.299 |   1.0139 |     32.647 |     3.5
   42 |   0.7665 |     26.117 |   1.0196 |     32.553 |     3.6
   43 |   0.7483 |     25.105 |   1.0125 |     32.022 |     3.7
   44 |   0.7276 |     24.334 |   1.0294 |     31.898 |     3.8
   45 |   0.7105 |     23.558 |   1.0343 |     31.429 |     3.9
   46 |   0.6881 |     22.562 |   1.0328 |     31.086 |     4.0
   47 |   0.6684 |     21.758 |   1.0271 |     31.024 |     4.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 766,562

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5742 |     69.050 |   2.0269 |     57.740 |     0.0
    2 |   1.8257 |     52.064 |   1.6262 |     45.350 |     0.1
    3 |   1.5364 |     46.230 |   1.4666 |     45.350 |     0.1
    4 |   1.4489 |     46.180 |   1.4240 |     45.350 |     0.2
    5 |   1.4210 |     46.235 |   1.4086 |     45.350 |     0.2
    6 |   1.4054 |     46.175 |   1.3871 |     45.350 |     0.2
    7 |   1.3852 |     46.087 |   1.3605 |     45.194 |     0.3
    8 |   1.3582 |     45.877 |   1.3292 |     44.632 |     0.3
    9 |   1.3288 |     45.123 |   1.3125 |     44.819 |     0.4
   10 |   1.3092 |     44.766 |   1.2873 |     43.695 |     0.4
   11 |   1.2934 |     44.457 |   1.2715 |     43.820 |     0.5
   12 |   1.2786 |     44.077 |   1.2684 |     43.290 |     0.5
   13 |   1.2662 |     43.841 |   1.2545 |     43.009 |     0.5
   14 |   1.2541 |     43.802 |   1.2332 |     42.447 |     0.6
   15 |   1.2427 |     43.566 |   1.2281 |     42.665 |     0.6
   16 |   1.2336 |     43.279 |   1.2166 |     42.322 |     0.7
   17 |   1.2260 |     43.263 |   1.2138 |     42.697 |     0.7
   18 |   1.2190 |     43.092 |   1.2043 |     42.291 |     0.8
   19 |   1.2122 |     42.993 |   1.2022 |     41.823 |     0.8
   20 |   1.2064 |     42.999 |   1.1976 |     42.135 |     0.8
   21 |   1.2000 |     43.010 |   1.1890 |     41.823 |     0.9
   22 |   1.1946 |     42.745 |   1.1877 |     42.884 |     0.9
   23 |   1.1883 |     42.701 |   1.1833 |     42.572 |     1.0
   24 |   1.1838 |     42.630 |   1.1807 |     42.322 |     1.0
   25 |   1.1805 |     42.410 |   1.1827 |     42.665 |     1.0
   26 |   1.1766 |     42.459 |   1.1660 |     41.604 |     1.1
   27 |   1.1702 |     42.349 |   1.1632 |     41.698 |     1.1
   28 |   1.1666 |     42.245 |   1.1618 |     41.479 |     1.2
   29 |   1.1640 |     42.256 |   1.1596 |     41.105 |     1.2
   30 |   1.1596 |     41.727 |   1.1605 |     41.511 |     1.3
   31 |   1.1569 |     41.914 |   1.1532 |     41.448 |     1.3
   32 |   1.1525 |     41.590 |   1.1526 |     41.355 |     1.3
   33 |   1.1514 |     41.298 |   1.1465 |     40.325 |     1.4
   34 |   1.1474 |     41.458 |   1.1415 |     40.855 |     1.4
   35 |   1.1414 |     41.017 |   1.1373 |     41.167 |     1.5
   36 |   1.1371 |     40.681 |   1.1384 |     41.011 |     1.5
   37 |   1.1340 |     40.847 |   1.1293 |     40.637 |     1.6
   38 |   1.1331 |     40.714 |   1.1305 |     40.200 |     1.6
   39 |   1.1283 |     40.588 |   1.1207 |     40.231 |     1.6
   40 |   1.1232 |     40.318 |   1.1259 |     39.919 |     1.7
   41 |   1.1185 |     39.922 |   1.1157 |     40.325 |     1.7
   42 |   1.1164 |     40.065 |   1.1186 |     40.325 |     1.8
   43 |   1.1112 |     39.900 |   1.1142 |     40.481 |     1.8
   44 |   1.1078 |     39.729 |   1.1057 |     40.075 |     1.9
   45 |   1.1024 |     39.702 |   1.1043 |     40.387 |     1.9
   46 |   1.0987 |     39.476 |   1.0989 |     39.732 |     1.9
   47 |   1.0965 |     39.531 |   1.0969 |     39.732 |     2.0
   48 |   1.0949 |     39.443 |   1.0915 |     39.794 |     2.0
   49 |   1.0917 |     39.614 |   1.0923 |     39.419 |     2.1
   50 |   1.0873 |     39.223 |   1.0913 |     39.794 |     2.1
   51 |   1.0831 |     39.349 |   1.0967 |     39.513 |     2.2
   52 |   1.0796 |     39.008 |   1.0847 |     39.263 |     2.2
   53 |   1.0790 |     38.849 |   1.0808 |     39.139 |     2.2
   54 |   1.0749 |     38.815 |   1.0757 |     39.045 |     2.3
   55 |   1.0695 |     38.623 |   1.0767 |     39.326 |     2.3
   56 |   1.0694 |     38.804 |   1.0796 |     39.014 |     2.4
   57 |   1.0649 |     38.551 |   1.0721 |     38.951 |     2.4
   58 |   1.0636 |     38.392 |   1.0747 |     38.858 |     2.4
   59 |   1.0582 |     38.320 |   1.0664 |     38.889 |     2.5
   60 |   1.0573 |     38.452 |   1.0785 |     39.357 |     2.5
   61 |   1.0562 |     38.293 |   1.0636 |     38.546 |     2.6
   62 |   1.0514 |     38.072 |   1.0614 |     38.296 |     2.6
   63 |   1.0462 |     37.737 |   1.0609 |     38.639 |     2.7
   64 |   1.0458 |     37.803 |   1.0619 |     38.826 |     2.7
   65 |   1.0446 |     37.935 |   1.0606 |     38.358 |     2.8
   66 |   1.0402 |     37.775 |   1.0600 |     38.390 |     2.8
   67 |   1.0391 |     37.671 |   1.0581 |     38.452 |     2.8
   68 |   1.0361 |     37.461 |   1.0575 |     38.514 |     2.9
   69 |   1.0339 |     37.539 |   1.0544 |     38.390 |     2.9
   70 |   1.0313 |     37.412 |   1.0505 |     38.390 |     3.0
   71 |   1.0271 |     37.395 |   1.0474 |     37.422 |     3.0
   72 |   1.0291 |     37.423 |   1.0446 |     38.015 |     3.1
   73 |   1.0241 |     37.654 |   1.0420 |     38.296 |     3.1
   74 |   1.0184 |     37.175 |   1.0449 |     38.514 |     3.1
   75 |   1.0132 |     36.707 |   1.0406 |     38.140 |     3.2
   76 |   1.0121 |     36.619 |   1.0415 |     36.954 |     3.2
   77 |   1.0105 |     36.691 |   1.0365 |     37.110 |     3.3
   78 |   1.0060 |     36.509 |   1.0330 |     36.860 |     3.3
   79 |   1.0030 |     36.069 |   1.0391 |     37.797 |     3.4
   80 |   1.0024 |     35.992 |   1.0298 |     36.860 |     3.4
   81 |   1.0015 |     36.129 |   1.0262 |     36.454 |     3.4
   82 |   0.9931 |     35.986 |   1.0244 |     36.423 |     3.5
   83 |   0.9902 |     35.485 |   1.0262 |     36.829 |     3.5
   84 |   0.9875 |     35.629 |   1.0248 |     36.735 |     3.6
   85 |   0.9845 |     35.326 |   1.0241 |     36.673 |     3.6
   86 |   0.9852 |     35.485 |   1.0162 |     36.111 |     3.7
   87 |   0.9764 |     35.062 |   1.0175 |     36.361 |     3.7
   88 |   0.9766 |     35.298 |   1.0227 |     37.203 |     3.7
   89 |   0.9741 |     35.018 |   1.0147 |     35.705 |     3.8
   90 |   0.9730 |     34.462 |   1.0108 |     36.361 |     3.8
   91 |   0.9682 |     34.517 |   1.0085 |     35.830 |     3.9
   92 |   0.9633 |     34.297 |   1.0050 |     35.674 |     3.9
   93 |   0.9595 |     34.379 |   1.0040 |     35.830 |     4.0
   94 |   0.9582 |     34.462 |   1.0052 |     35.737 |     4.0
   95 |   0.9605 |     34.253 |   1.0014 |     35.861 |     4.1
   96 |   0.9497 |     33.873 |   1.0014 |     35.175 |     4.1
   97 |   0.9456 |     33.691 |   1.0014 |     36.298 |     4.1
   98 |   0.9447 |     33.548 |   1.0011 |     35.612 |     4.2
   99 |   0.9431 |     33.994 |   1.0067 |     36.298 |     4.2
  100 |   0.9385 |     33.531 |   1.0006 |     35.456 |     4.3
  101 |   0.9394 |     33.443 |   0.9924 |     35.674 |     4.3
  102 |   0.9352 |     33.443 |   0.9874 |     35.081 |     4.4
  103 |   0.9304 |     33.113 |   0.9871 |     35.112 |     4.4
  104 |   0.9258 |     33.047 |   0.9932 |     35.175 |     4.4
  105 |   0.9215 |     32.954 |   0.9814 |     34.863 |     4.5
  106 |   0.9216 |     32.750 |   0.9805 |     35.237 |     4.5
  107 |   0.9151 |     32.337 |   0.9839 |     35.237 |     4.6
  108 |   0.9140 |     32.486 |   0.9757 |     34.863 |     4.6
  109 |   0.9073 |     32.392 |   0.9805 |     35.206 |     4.7
  110 |   0.9101 |     32.535 |   0.9750 |     34.988 |     4.7
  111 |   0.9050 |     32.227 |   0.9686 |     34.238 |     4.8
  112 |   0.9060 |     32.260 |   0.9685 |     34.675 |     4.8
  113 |   0.8965 |     31.853 |   0.9715 |     34.644 |     4.8
  114 |   0.8942 |     31.836 |   0.9586 |     34.082 |     4.9
  115 |   0.8884 |     31.677 |   0.9606 |     34.863 |     4.9
  116 |   0.8875 |     31.390 |   0.9665 |     34.738 |     5.0
  117 |   0.8861 |     31.522 |   0.9624 |     34.395 |     5.0
  118 |   0.8797 |     31.550 |   0.9623 |     33.521 |     5.1
  119 |   0.8913 |     31.616 |   0.9568 |     34.051 |     5.1
  120 |   0.8792 |     31.137 |   0.9648 |     34.582 |     5.1
  121 |   0.8735 |     30.796 |   0.9555 |     33.864 |     5.2
  122 |   0.8645 |     30.609 |   0.9564 |     34.238 |     5.2
  123 |   0.8663 |     30.774 |   0.9565 |     33.989 |     5.3
  124 |   0.8605 |     30.653 |   0.9528 |     32.928 |     5.3
  125 |   0.8558 |     30.466 |   0.9556 |     33.770 |     5.4
  126 |   0.8558 |     30.168 |   0.9476 |     33.583 |     5.4
  127 |   0.8551 |     30.438 |   0.9469 |     33.833 |     5.4
  128 |   0.8423 |     29.651 |   0.9448 |     33.521 |     5.5
  129 |   0.8407 |     29.695 |   0.9489 |     33.271 |     5.5
  130 |   0.8436 |     29.657 |   0.9470 |     33.489 |     5.6
  131 |   0.8403 |     29.684 |   0.9500 |     33.177 |     5.6
  132 |   0.8295 |     28.864 |   0.9379 |     32.647 |     5.7
  133 |   0.8291 |     29.238 |   0.9383 |     32.959 |     5.7
  134 |   0.8205 |     28.809 |   0.9302 |     32.085 |     5.8
  135 |   0.8259 |     28.891 |   0.9401 |     32.865 |     5.8
  136 |   0.8149 |     28.594 |   0.9558 |     33.084 |     5.8
  137 |   0.8211 |     28.781 |   0.9438 |     32.803 |     5.9
  138 |   0.8124 |     28.649 |   0.9332 |     33.146 |     5.9
  139 |   0.8009 |     28.104 |   0.9247 |     31.835 |     6.0
  140 |   0.8063 |     28.170 |   0.9327 |     32.459 |     6.0
  141 |   0.8003 |     28.077 |   0.9305 |     32.615 |     6.1
  142 |   0.7928 |     27.631 |   0.9226 |     32.085 |     6.1
  143 |   0.7967 |     28.181 |   0.9530 |     33.427 |     6.1
  144 |   0.8029 |     28.066 |   0.9281 |     31.898 |     6.2
  145 |   0.7917 |     27.708 |   0.9278 |     31.960 |     6.2
  146 |   0.7883 |     27.637 |   0.9220 |     31.773 |     6.3
  147 |   0.7831 |     27.466 |   0.9243 |     31.554 |     6.3
  148 |   0.7759 |     26.822 |   0.9280 |     31.991 |     6.4
  149 |   0.7723 |     27.070 |   0.9209 |     31.835 |     6.4
  150 |   0.7655 |     26.739 |   0.9174 |     30.899 |     6.5
  151 |   0.7698 |     26.684 |   0.9362 |     32.335 |     6.5
  152 |   0.7620 |     26.525 |   0.9168 |     31.336 |     6.5
  153 |   0.7504 |     25.820 |   0.9198 |     31.086 |     6.6
  154 |   0.7498 |     26.035 |   0.9214 |     31.086 |     6.6
  155 |   0.7434 |     25.484 |   0.9361 |     31.336 |     6.7
  156 |   0.7498 |     26.079 |   0.9340 |     31.367 |     6.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,853,250

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1050 |     56.798 |   1.5811 |     45.350 |     0.1
    2 |   1.4656 |     46.136 |   1.3926 |     45.318 |     0.2
    3 |   1.3729 |     45.762 |   1.3383 |     44.351 |     0.3
    4 |   1.3295 |     45.162 |   1.3061 |     44.600 |     0.3
    5 |   1.3032 |     44.953 |   1.2820 |     43.789 |     0.4
    6 |   1.2778 |     44.331 |   1.2632 |     43.383 |     0.5
    7 |   1.2538 |     43.604 |   1.2390 |     43.571 |     0.6
    8 |   1.2306 |     43.290 |   1.2105 |     42.634 |     0.7
    9 |   1.2098 |     42.558 |   1.1870 |     41.042 |     0.8
   10 |   1.1824 |     41.535 |   1.1724 |     40.387 |     0.9
   11 |   1.1567 |     40.362 |   1.1443 |     39.669 |     1.0
   12 |   1.1326 |     39.691 |   1.1281 |     39.513 |     1.0
   13 |   1.1027 |     38.705 |   1.1114 |     38.670 |     1.1
   14 |   1.0823 |     38.172 |   1.0974 |     38.233 |     1.2
   15 |   1.0503 |     37.098 |   1.0695 |     36.891 |     1.3
   16 |   1.0178 |     35.744 |   1.0365 |     35.144 |     1.4
   17 |   0.9844 |     34.093 |   1.0350 |     34.831 |     1.5
   18 |   0.9489 |     32.810 |   1.0043 |     34.301 |     1.6
   19 |   0.9184 |     31.539 |   0.9913 |     34.301 |     1.7
   20 |   0.8856 |     29.970 |   0.9725 |     32.803 |     1.8
   21 |   0.8455 |     28.380 |   0.9558 |     32.147 |     1.9
   22 |   0.8092 |     27.350 |   0.9397 |     31.398 |     1.9
   23 |   0.7709 |     25.402 |   0.9308 |     30.556 |     2.0
   24 |   0.7331 |     24.196 |   0.9251 |     29.588 |     2.1
   25 |   0.6997 |     22.947 |   0.8966 |     28.933 |     2.2
   26 |   0.6477 |     21.103 |   0.8938 |     27.809 |     2.3
   27 |   0.6162 |     20.019 |   0.9097 |     28.745 |     2.4
   28 |   0.5995 |     19.452 |   0.8745 |     27.122 |     2.5
   29 |   0.5444 |     17.476 |   0.8785 |     27.154 |     2.6
   30 |   0.5009 |     15.891 |   0.9040 |     26.966 |     2.7
   31 |   0.4734 |     15.269 |   0.8927 |     26.124 |     2.7
   32 |   0.4423 |     13.997 |   0.8950 |     26.186 |     2.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 718,658

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0949 |     57.398 |   1.5143 |     44.975 |     0.0
    2 |   1.4183 |     45.773 |   1.3605 |     45.194 |     0.1
    3 |   1.3373 |     45.140 |   1.3079 |     44.070 |     0.1
    4 |   1.2988 |     44.386 |   1.2751 |     43.789 |     0.1
    5 |   1.2717 |     44.089 |   1.2641 |     43.883 |     0.2
    6 |   1.2463 |     43.478 |   1.2317 |     42.915 |     0.2
    7 |   1.2253 |     42.823 |   1.2126 |     42.603 |     0.2
    8 |   1.2052 |     42.487 |   1.1997 |     41.948 |     0.3
    9 |   1.1843 |     42.035 |   1.1784 |     41.042 |     0.3
   10 |   1.1626 |     41.166 |   1.1571 |     41.292 |     0.3
   11 |   1.1404 |     40.555 |   1.1441 |     39.856 |     0.4
   12 |   1.1233 |     39.680 |   1.1197 |     39.357 |     0.4
   13 |   1.1018 |     39.140 |   1.1136 |     39.388 |     0.4
   14 |   1.0846 |     38.298 |   1.0931 |     38.327 |     0.5
   15 |   1.0659 |     37.693 |   1.0837 |     37.172 |     0.5
   16 |   1.0490 |     37.164 |   1.0641 |     37.235 |     0.5
   17 |   1.0297 |     36.311 |   1.0664 |     37.328 |     0.6
   18 |   1.0144 |     35.502 |   1.0498 |     36.642 |     0.6
   19 |   0.9982 |     35.023 |   1.0306 |     35.830 |     0.6
   20 |   0.9811 |     34.093 |   1.0230 |     35.175 |     0.7
   21 |   0.9609 |     33.333 |   1.0303 |     35.518 |     0.7
   22 |   0.9440 |     32.959 |   1.0002 |     34.831 |     0.7
   23 |   0.9325 |     32.552 |   1.0054 |     35.019 |     0.8
   24 |   0.9148 |     31.842 |   0.9919 |     33.677 |     0.8
   25 |   0.8975 |     31.005 |   0.9916 |     33.458 |     0.8
   26 |   0.8776 |     30.317 |   0.9806 |     33.677 |     0.9
   27 |   0.8615 |     29.469 |   0.9733 |     33.208 |     0.9
   28 |   0.8470 |     29.117 |   0.9618 |     32.803 |     0.9
   29 |   0.8315 |     28.451 |   0.9802 |     33.240 |     1.0
   30 |   0.8161 |     27.725 |   0.9606 |     32.241 |     1.0
   31 |   0.7916 |     26.844 |   0.9391 |     32.147 |     1.0
   32 |   0.7732 |     26.013 |   0.9441 |     31.679 |     1.1
   33 |   0.7657 |     25.952 |   0.9419 |     31.367 |     1.1
   34 |   0.7446 |     24.923 |   0.9299 |     30.649 |     1.1
   35 |   0.7256 |     24.191 |   0.9306 |     30.400 |     1.2
   36 |   0.7033 |     23.508 |   0.9236 |     29.744 |     1.2
   37 |   0.6894 |     23.041 |   0.9374 |     29.588 |     1.2
   38 |   0.6735 |     22.237 |   0.9097 |     29.120 |     1.3
   39 |   0.6546 |     21.780 |   0.9241 |     30.243 |     1.3
   40 |   0.6385 |     21.252 |   0.9478 |     30.400 |     1.3
   41 |   0.6208 |     20.828 |   0.9261 |     29.713 |     1.4
   42 |   0.5995 |     19.903 |   0.9300 |     29.245 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 486,082

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3858 |     63.061 |   1.7765 |     48.752 |     0.0
    2 |   1.5967 |     47.721 |   1.4643 |     45.350 |     0.1
    3 |   1.4317 |     45.976 |   1.3922 |     45.194 |     0.1
    4 |   1.3851 |     45.745 |   1.3623 |     44.351 |     0.1
    5 |   1.3569 |     45.448 |   1.3351 |     44.382 |     0.2
    6 |   1.3333 |     45.387 |   1.3067 |     43.851 |     0.2
    7 |   1.3051 |     44.397 |   1.2885 |     43.633 |     0.3
    8 |   1.2846 |     43.984 |   1.2639 |     42.728 |     0.3
    9 |   1.2645 |     43.813 |   1.2457 |     42.853 |     0.3
   10 |   1.2435 |     42.949 |   1.2261 |     42.166 |     0.4
   11 |   1.2266 |     42.679 |   1.2049 |     42.135 |     0.4
   12 |   1.2033 |     41.920 |   1.1900 |     40.855 |     0.4
   13 |   1.1855 |     41.661 |   1.1686 |     40.855 |     0.5
   14 |   1.1680 |     41.089 |   1.1478 |     40.325 |     0.5
   15 |   1.1469 |     39.905 |   1.1360 |     39.014 |     0.6
   16 |   1.1330 |     39.658 |   1.1300 |     39.981 |     0.6
   17 |   1.1105 |     38.436 |   1.1024 |     38.514 |     0.6
   18 |   1.0945 |     37.957 |   1.0968 |     38.109 |     0.7
   19 |   1.0780 |     37.093 |   1.0798 |     37.172 |     0.7
   20 |   1.0556 |     36.564 |   1.0716 |     37.235 |     0.7
   21 |   1.0375 |     35.959 |   1.0481 |     35.799 |     0.8
   22 |   1.0153 |     34.930 |   1.0414 |     35.955 |     0.8
   23 |   0.9986 |     34.555 |   1.0380 |     35.393 |     0.9
   24 |   0.9777 |     33.713 |   1.0220 |     35.268 |     0.9
   25 |   0.9576 |     33.053 |   1.0086 |     34.238 |     0.9
   26 |   0.9326 |     31.655 |   0.9924 |     33.801 |     1.0
   27 |   0.9344 |     31.941 |   0.9962 |     33.583 |     1.0
   28 |   0.9113 |     30.785 |   0.9810 |     33.489 |     1.0
   29 |   0.8887 |     30.003 |   0.9680 |     32.366 |     1.1
   30 |   0.8628 |     28.600 |   0.9683 |     32.678 |     1.1
   31 |   0.8442 |     27.939 |   0.9609 |     32.085 |     1.1
   32 |   0.8198 |     27.229 |   0.9533 |     31.866 |     1.2
   33 |   0.8007 |     26.563 |   0.9407 |     30.587 |     1.2
   34 |   0.7765 |     25.594 |   0.9390 |     30.587 |     1.3
   35 |   0.7594 |     25.077 |   0.9352 |     30.243 |     1.3
   36 |   0.7657 |     25.231 |   0.9364 |     30.431 |     1.3
   37 |   0.7340 |     24.031 |   0.9270 |     30.556 |     1.4
   38 |   0.7104 |     22.914 |   0.9177 |     29.713 |     1.4
   39 |   0.6927 |     22.666 |   0.9339 |     29.963 |     1.4
   40 |   0.6689 |     21.681 |   0.9254 |     28.870 |     1.5
   41 |   0.6551 |     21.048 |   0.9403 |     30.431 |     1.5
   42 |   0.6473 |     20.789 |   0.9314 |     29.838 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 542,882

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0140 |     55.069 |   1.4565 |     44.944 |     0.0
    2 |   1.3855 |     45.608 |   1.3209 |     44.320 |     0.0
    3 |   1.3084 |     44.518 |   1.2713 |     43.539 |     0.1
    4 |   1.2646 |     43.786 |   1.2462 |     43.633 |     0.1
    5 |   1.2346 |     43.213 |   1.2059 |     41.573 |     0.1
    6 |   1.2049 |     42.481 |   1.1947 |     41.635 |     0.1
    7 |   1.1774 |     41.287 |   1.1529 |     39.856 |     0.2
    8 |   1.1518 |     40.825 |   1.1370 |     39.576 |     0.2
    9 |   1.1204 |     39.674 |   1.1084 |     38.577 |     0.2
   10 |   1.0851 |     38.116 |   1.0810 |     37.640 |     0.2
   11 |   1.0555 |     36.762 |   1.0632 |     37.110 |     0.2
   12 |   1.0222 |     35.469 |   1.0347 |     35.081 |     0.3
   13 |   0.9901 |     34.489 |   1.0241 |     35.393 |     0.3
   14 |   0.9517 |     32.728 |   1.0044 |     33.926 |     0.3
   15 |   0.9237 |     31.291 |   0.9713 |     32.678 |     0.3
   16 |   0.8955 |     29.888 |   0.9661 |     31.991 |     0.4
   17 |   0.8582 |     28.820 |   0.9419 |     30.993 |     0.4
   18 |   0.8344 |     27.912 |   0.9366 |     31.991 |     0.4
   19 |   0.8042 |     27.009 |   0.9337 |     30.556 |     0.4
   20 |   0.7762 |     25.881 |   0.9043 |     29.588 |     0.4
   21 |   0.7494 |     24.664 |   0.8954 |     29.463 |     0.5
   22 |   0.7204 |     23.795 |   0.9094 |     29.619 |     0.5
   23 |   0.6964 |     23.068 |   0.8843 |     28.620 |     0.5
   24 |   0.6775 |     22.408 |   0.8979 |     28.808 |     0.5
   25 |   0.6510 |     21.384 |   0.8931 |     28.027 |     0.5
   26 |   0.6273 |     20.470 |   0.8920 |     28.215 |     0.6
   27 |   0.6077 |     19.766 |   0.8869 |     28.620 |     0.6
   28 |   0.5853 |     18.890 |   0.8837 |     27.809 |     0.6
   29 |   0.5628 |     18.384 |   0.8996 |     28.340 |     0.6
   30 |   0.5465 |     17.845 |   0.8794 |     27.122 |     0.7
   31 |   0.5279 |     17.168 |   0.9016 |     27.622 |     0.7
   32 |   0.5300 |     17.437 |   0.8960 |     28.121 |     0.7
   33 |   0.4980 |     16.248 |   0.8959 |     27.747 |     0.7
   34 |   0.4795 |     15.709 |   0.8967 |     27.216 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,985,346

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1889 |     60.458 |   1.5900 |     45.350 |     0.1
    2 |   1.4816 |     46.175 |   1.4231 |     45.662 |     0.2
    3 |   1.4132 |     46.285 |   1.3913 |     45.350 |     0.2
    4 |   1.3817 |     45.921 |   1.3713 |     44.663 |     0.3
    5 |   1.3521 |     45.459 |   1.3207 |     44.725 |     0.4
    6 |   1.3188 |     45.272 |   1.3120 |     45.225 |     0.5
    7 |   1.3018 |     44.837 |   1.2841 |     44.132 |     0.6
    8 |   1.2797 |     44.391 |   1.2596 |     43.727 |     0.7
    9 |   1.2575 |     43.907 |   1.2451 |     43.414 |     0.7
   10 |   1.2410 |     43.626 |   1.2263 |     42.385 |     0.8
   11 |   1.2252 |     43.279 |   1.2050 |     43.196 |     0.9
   12 |   1.2083 |     42.949 |   1.1946 |     41.792 |     1.0
   13 |   1.1942 |     42.459 |   1.1821 |     42.166 |     1.1
   14 |   1.1819 |     42.311 |   1.1713 |     42.572 |     1.2
   15 |   1.1737 |     42.140 |   1.1656 |     41.823 |     1.2
   16 |   1.1650 |     41.854 |   1.1560 |     41.948 |     1.3
   17 |   1.1568 |     41.590 |   1.1575 |     41.386 |     1.4
   18 |   1.1474 |     41.419 |   1.1412 |     41.042 |     1.5
   19 |   1.1421 |     41.188 |   1.1418 |     40.918 |     1.6
   20 |   1.1331 |     41.133 |   1.1300 |     41.105 |     1.7
   21 |   1.1291 |     40.863 |   1.1329 |     40.730 |     1.7
   22 |   1.1225 |     41.045 |   1.1200 |     40.387 |     1.8
   23 |   1.1169 |     40.758 |   1.1201 |     41.199 |     1.9
   24 |   1.1106 |     40.615 |   1.1145 |     41.136 |     2.0
   25 |   1.1065 |     40.632 |   1.1034 |     40.387 |     2.1
   26 |   1.1029 |     40.285 |   1.1050 |     40.418 |     2.2
   27 |   1.0972 |     40.230 |   1.1069 |     40.044 |     2.2
   28 |   1.0947 |     40.203 |   1.1011 |     40.169 |     2.3
   29 |   1.0899 |     40.351 |   1.1044 |     40.824 |     2.4
   30 |   1.0875 |     39.999 |   1.0992 |     40.387 |     2.5
   31 |   1.0823 |     39.823 |   1.1005 |     40.200 |     2.6
   32 |   1.0814 |     40.037 |   1.0896 |     40.044 |     2.7
   33 |   1.0788 |     40.087 |   1.0904 |     40.262 |     2.7
   34 |   1.0781 |     39.647 |   1.0890 |     39.888 |     2.8
   35 |   1.0760 |     39.795 |   1.0919 |     39.981 |     2.9
   36 |   1.0743 |     39.971 |   1.0908 |     39.732 |     3.0
   37 |   1.0705 |     39.817 |   1.0798 |     39.544 |     3.1
   38 |   1.0682 |     39.702 |   1.0820 |     40.106 |     3.1
   39 |   1.0674 |     39.696 |   1.0844 |     39.763 |     3.2
   40 |   1.0664 |     39.548 |   1.0860 |     40.387 |     3.3
   41 |   1.0681 |     39.762 |   1.0866 |     39.825 |     3.4
   42 |   1.0646 |     39.531 |   1.0789 |     39.950 |     3.5
   43 |   1.0638 |     39.542 |   1.0782 |     39.794 |     3.6
   44 |   1.0622 |     39.647 |   1.0841 |     39.544 |     3.6
   45 |   1.0610 |     39.702 |   1.0775 |     40.137 |     3.7
   46 |   1.0613 |     39.757 |   1.0769 |     39.888 |     3.8
   47 |   1.0602 |     39.586 |   1.0774 |     39.919 |     3.9
   48 |   1.0591 |     39.575 |   1.0750 |     39.763 |     4.0
   49 |   1.0589 |     39.630 |   1.0823 |     39.763 |     4.0
   50 |   1.0578 |     39.691 |   1.0731 |     40.356 |     4.1
   51 |   1.0567 |     39.454 |   1.0766 |     40.106 |     4.2
   52 |   1.0554 |     39.680 |   1.0739 |     39.607 |     4.3
   53 |   1.0554 |     39.448 |   1.0743 |     39.607 |     4.4
   54 |   1.0548 |     39.443 |   1.0755 |     39.919 |     4.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 753,602

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2057 |     60.436 |   1.6121 |     45.350 |     0.0
    2 |   1.4762 |     46.224 |   1.3987 |     45.350 |     0.1
    3 |   1.3841 |     45.976 |   1.3460 |     44.913 |     0.1
    4 |   1.3418 |     45.597 |   1.3149 |     44.819 |     0.1
    5 |   1.3136 |     45.101 |   1.2899 |     45.006 |     0.1
    6 |   1.2863 |     44.710 |   1.2616 |     44.226 |     0.2
    7 |   1.2581 |     43.973 |   1.2293 |     43.196 |     0.2
    8 |   1.2358 |     43.549 |   1.2175 |     42.385 |     0.2
    9 |   1.2183 |     43.186 |   1.2064 |     42.509 |     0.2
   10 |   1.2050 |     42.894 |   1.1922 |     41.792 |     0.3
   11 |   1.1900 |     42.575 |   1.1806 |     41.542 |     0.3
   12 |   1.1770 |     42.124 |   1.1668 |     41.948 |     0.3
   13 |   1.1664 |     41.859 |   1.1486 |     40.980 |     0.4
   14 |   1.1554 |     41.744 |   1.1511 |     41.760 |     0.4
   15 |   1.1455 |     41.446 |   1.1556 |     41.511 |     0.4
   16 |   1.1426 |     41.281 |   1.1328 |     40.855 |     0.4
   17 |   1.1325 |     40.913 |   1.1328 |     40.543 |     0.5
   18 |   1.1290 |     41.083 |   1.1249 |     40.668 |     0.5
   19 |   1.1199 |     40.357 |   1.1229 |     41.074 |     0.5
   20 |   1.1148 |     40.681 |   1.1221 |     40.449 |     0.5
   21 |   1.1130 |     40.588 |   1.1109 |     40.699 |     0.6
   22 |   1.1055 |     40.566 |   1.1069 |     40.169 |     0.6
   23 |   1.0999 |     40.131 |   1.1042 |     40.325 |     0.6
   24 |   1.0948 |     40.280 |   1.1036 |     40.075 |     0.7
   25 |   1.0909 |     39.971 |   1.0975 |     40.012 |     0.7
   26 |   1.0856 |     39.828 |   1.0957 |     41.323 |     0.7
   27 |   1.0814 |     39.707 |   1.0904 |     40.356 |     0.7
   28 |   1.0777 |     39.927 |   1.0842 |     40.169 |     0.8
   29 |   1.0726 |     39.603 |   1.0827 |     39.856 |     0.8
   30 |   1.0669 |     39.399 |   1.0761 |     39.139 |     0.8
   31 |   1.0632 |     39.102 |   1.0747 |     38.327 |     0.8
   32 |   1.0553 |     38.562 |   1.0661 |     38.233 |     0.9
   33 |   1.0549 |     38.535 |   1.0628 |     38.514 |     0.9
   34 |   1.0501 |     38.331 |   1.0558 |     37.921 |     0.9
   35 |   1.0429 |     37.891 |   1.0592 |     38.046 |     1.0
   36 |   1.0346 |     37.439 |   1.0482 |     37.578 |     1.0
   37 |   1.0304 |     37.274 |   1.0420 |     37.266 |     1.0
   38 |   1.0266 |     37.494 |   1.0413 |     37.203 |     1.0
   39 |   1.0219 |     37.203 |   1.0439 |     37.640 |     1.1
   40 |   1.0136 |     36.917 |   1.0340 |     36.548 |     1.1
   41 |   1.0121 |     36.685 |   1.0318 |     35.986 |     1.1
   42 |   1.0066 |     36.520 |   1.0268 |     36.767 |     1.1
   43 |   0.9989 |     36.085 |   1.0258 |     36.017 |     1.2
   44 |   0.9924 |     35.656 |   1.0243 |     36.860 |     1.2
   45 |   0.9857 |     35.761 |   1.0209 |     36.174 |     1.2
   46 |   0.9814 |     35.507 |   1.0217 |     36.548 |     1.3
   47 |   0.9786 |     35.309 |   1.0520 |     37.953 |     1.3
   48 |   0.9764 |     35.194 |   1.0122 |     36.486 |     1.3
   49 |   0.9664 |     34.830 |   1.0124 |     36.610 |     1.3
   50 |   0.9622 |     34.539 |   1.0005 |     35.206 |     1.4
   51 |   0.9562 |     34.566 |   1.0031 |     36.017 |     1.4
   52 |   0.9534 |     34.291 |   1.0119 |     36.735 |     1.4
   53 |   0.9451 |     34.082 |   1.0015 |     35.612 |     1.4
   54 |   0.9410 |     33.878 |   1.0019 |     35.549 |     1.5
   55 |   0.9392 |     33.779 |   0.9872 |     35.112 |     1.5
   56 |   0.9338 |     33.647 |   0.9901 |     35.050 |     1.5
   57 |   0.9296 |     33.493 |   0.9878 |     35.331 |     1.6
   58 |   0.9206 |     32.954 |   0.9914 |     35.643 |     1.6
   59 |   0.9218 |     33.152 |   0.9831 |     34.800 |     1.6
   60 |   0.9103 |     32.794 |   0.9855 |     34.582 |     1.6
   61 |   0.9086 |     32.480 |   0.9808 |     34.707 |     1.7
   62 |   0.8998 |     32.106 |   0.9797 |     34.270 |     1.7
   63 |   0.8987 |     31.996 |   0.9648 |     34.738 |     1.7
   64 |   0.8939 |     32.040 |   0.9638 |     34.020 |     1.8
   65 |   0.8828 |     31.511 |   0.9636 |     33.677 |     1.8
   66 |   0.8793 |     31.291 |   0.9627 |     33.583 |     1.8
   67 |   0.8751 |     31.165 |   0.9575 |     33.833 |     1.8
   68 |   0.8705 |     30.867 |   0.9551 |     33.521 |     1.9
   69 |   0.8647 |     30.488 |   0.9466 |     33.427 |     1.9
   70 |   0.8528 |     30.020 |   0.9482 |     32.615 |     1.9
   71 |   0.8488 |     29.877 |   0.9412 |     32.335 |     1.9
   72 |   0.8395 |     29.447 |   0.9446 |     33.146 |     2.0
   73 |   0.8342 |     29.519 |   0.9377 |     32.303 |     2.0
   74 |   0.8276 |     29.079 |   0.9481 |     32.772 |     2.0
   75 |   0.8254 |     28.776 |   0.9444 |     32.335 |     2.1
   76 |   0.8171 |     28.748 |   0.9191 |     31.586 |     2.1
   77 |   0.8094 |     28.330 |   0.9311 |     32.397 |     2.1
   78 |   0.8012 |     27.846 |   0.9203 |     31.586 |     2.1
   79 |   0.7946 |     27.956 |   0.9362 |     32.054 |     2.2
   80 |   0.7905 |     28.005 |   0.9210 |     30.993 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 489,410

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0483 |     57.332 |   1.4760 |     45.100 |     0.0
    2 |   1.4047 |     45.877 |   1.3440 |     44.881 |     0.0
    3 |   1.3244 |     44.727 |   1.2909 |     44.039 |     0.1
    4 |   1.2772 |     43.846 |   1.2476 |     43.383 |     0.1
    5 |   1.2396 |     43.175 |   1.2197 |     42.010 |     0.1
    6 |   1.2127 |     42.663 |   1.1953 |     41.105 |     0.1
    7 |   1.1861 |     41.667 |   1.1653 |     40.918 |     0.1
    8 |   1.1616 |     41.127 |   1.1445 |     39.888 |     0.2
    9 |   1.1349 |     40.032 |   1.1345 |     39.607 |     0.2
   10 |   1.1129 |     39.283 |   1.1108 |     38.951 |     0.2
   11 |   1.0873 |     38.414 |   1.0892 |     38.046 |     0.2
   12 |   1.0578 |     37.318 |   1.0668 |     37.016 |     0.2
   13 |   1.0328 |     36.201 |   1.0528 |     36.829 |     0.2
   14 |   1.0091 |     35.552 |   1.0376 |     36.080 |     0.3
   15 |   0.9835 |     34.495 |   1.0220 |     35.581 |     0.3
   16 |   0.9583 |     33.240 |   1.0167 |     34.988 |     0.3
   17 |   0.9334 |     32.673 |   0.9994 |     34.363 |     0.3
   18 |   0.9097 |     31.765 |   0.9780 |     33.021 |     0.3
   19 |   0.8864 |     30.812 |   0.9818 |     33.271 |     0.4
   20 |   0.8676 |     29.866 |   0.9919 |     33.084 |     0.4
   21 |   0.8479 |     28.737 |   0.9486 |     31.273 |     0.4
   22 |   0.8246 |     28.126 |   0.9594 |     32.335 |     0.4
   23 |   0.7948 |     26.767 |   0.9479 |     30.993 |     0.4
   24 |   0.7747 |     25.947 |   0.9426 |     30.243 |     0.5
   25 |   0.7495 |     25.198 |   0.9216 |     29.213 |     0.5
   26 |   0.7344 |     24.675 |   0.9362 |     30.087 |     0.5
   27 |   0.7128 |     23.773 |   0.9187 |     29.463 |     0.5
   28 |   0.6890 |     22.815 |   0.9255 |     29.057 |     0.5
   29 |   0.6677 |     22.193 |   0.9198 |     28.839 |     0.5
   30 |   0.6495 |     21.538 |   0.9196 |     29.276 |     0.6
   31 |   0.6346 |     21.059 |   0.9302 |     28.683 |     0.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 537,922

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4759 |     65.599 |   1.9061 |     54.744 |     0.0
    2 |   1.6974 |     48.635 |   1.5223 |     45.350 |     0.1
    3 |   1.4688 |     46.114 |   1.4236 |     45.350 |     0.1
    4 |   1.4157 |     46.147 |   1.3905 |     45.350 |     0.1
    5 |   1.3875 |     45.971 |   1.3693 |     45.194 |     0.2
    6 |   1.3658 |     45.602 |   1.3425 |     44.913 |     0.2
    7 |   1.3473 |     45.057 |   1.3327 |     44.694 |     0.3
    8 |   1.3342 |     44.942 |   1.3134 |     43.664 |     0.3
    9 |   1.3172 |     44.606 |   1.2931 |     43.976 |     0.3
   10 |   1.2969 |     44.276 |   1.2797 |     43.633 |     0.4
   11 |   1.2827 |     44.171 |   1.2666 |     43.134 |     0.4
   12 |   1.2645 |     43.720 |   1.2491 |     42.978 |     0.4
   13 |   1.2469 |     43.615 |   1.2300 |     43.040 |     0.5
   14 |   1.2282 |     43.373 |   1.2075 |     42.509 |     0.5
   15 |   1.2109 |     42.734 |   1.1916 |     41.854 |     0.6
   16 |   1.1922 |     42.035 |   1.1813 |     41.074 |     0.6
   17 |   1.1755 |     40.968 |   1.1708 |     40.449 |     0.6
   18 |   1.1594 |     40.511 |   1.1529 |     39.825 |     0.7
   19 |   1.1428 |     39.966 |   1.1406 |     39.170 |     0.7
   20 |   1.1261 |     39.349 |   1.1265 |     38.795 |     0.7
   21 |   1.1084 |     38.386 |   1.1190 |     38.233 |     0.8
   22 |   1.0942 |     38.039 |   1.1153 |     38.951 |     0.8
   23 |   1.0795 |     37.577 |   1.0994 |     37.453 |     0.8
   24 |   1.0643 |     36.911 |   1.0926 |     38.077 |     0.9
   25 |   1.0525 |     36.295 |   1.0860 |     37.609 |     0.9
   26 |   1.0351 |     35.722 |   1.0825 |     37.672 |     1.0
   27 |   1.0202 |     35.260 |   1.0757 |     37.360 |     1.0
   28 |   1.0081 |     35.078 |   1.0698 |     37.734 |     1.0
   29 |   0.9932 |     34.313 |   1.0707 |     37.765 |     1.1
   30 |   0.9762 |     33.719 |   1.0648 |     36.923 |     1.1
   31 |   0.9645 |     33.355 |   1.0641 |     36.330 |     1.1
   32 |   0.9518 |     32.744 |   1.0517 |     35.893 |     1.2
   33 |   0.9309 |     32.172 |   1.0555 |     36.174 |     1.2
   34 |   0.9124 |     31.291 |   1.0353 |     35.144 |     1.3
   35 |   0.9016 |     30.889 |   1.0391 |     34.769 |     1.3
   36 |   0.8885 |     30.229 |   1.0489 |     35.861 |     1.3
   37 |   0.8768 |     29.519 |   1.0271 |     34.395 |     1.4
   38 |   0.8605 |     29.354 |   1.0349 |     34.831 |     1.4
   39 |   0.8439 |     28.649 |   1.0269 |     33.708 |     1.4
   40 |   0.8325 |     28.033 |   1.0196 |     34.051 |     1.5
   41 |   0.8109 |     27.020 |   1.0309 |     33.396 |     1.5
   42 |   0.8011 |     26.827 |   1.0360 |     34.457 |     1.5
   43 |   0.7840 |     26.464 |   1.0331 |     32.928 |     1.6
   44 |   0.7776 |     25.793 |   1.0270 |     33.427 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 834,978

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5537 |     67.993 |   1.9875 |     58.708 |     0.0
    2 |   1.7583 |     50.132 |   1.5780 |     45.350 |     0.1
    3 |   1.5080 |     46.120 |   1.4573 |     45.350 |     0.1
    4 |   1.4380 |     46.120 |   1.4179 |     45.350 |     0.2
    5 |   1.4092 |     46.208 |   1.3858 |     45.350 |     0.2
    6 |   1.3798 |     46.136 |   1.3613 |     45.350 |     0.3
    7 |   1.3564 |     45.525 |   1.3464 |     44.881 |     0.3
    8 |   1.3344 |     45.145 |   1.3141 |     44.195 |     0.3
    9 |   1.3109 |     44.766 |   1.2912 |     44.164 |     0.4
   10 |   1.2908 |     44.430 |   1.2744 |     44.039 |     0.4
   11 |   1.2768 |     44.116 |   1.2595 |     43.727 |     0.5
   12 |   1.2661 |     43.945 |   1.2525 |     43.258 |     0.5
   13 |   1.2544 |     43.566 |   1.2555 |     43.539 |     0.6
   14 |   1.2428 |     43.423 |   1.2280 |     43.134 |     0.6
   15 |   1.2322 |     42.806 |   1.2262 |     42.416 |     0.6
   16 |   1.2183 |     42.630 |   1.2115 |     42.228 |     0.7
   17 |   1.2112 |     42.272 |   1.2041 |     41.511 |     0.7
   18 |   1.2003 |     42.113 |   1.1957 |     41.261 |     0.8
   19 |   1.1928 |     41.771 |   1.1801 |     40.793 |     0.8
   20 |   1.1807 |     41.375 |   1.1810 |     40.730 |     0.9
   21 |   1.1720 |     41.006 |   1.1696 |     40.793 |     0.9
   22 |   1.1637 |     40.769 |   1.1619 |     40.106 |     0.9
   23 |   1.1560 |     40.494 |   1.1660 |     40.169 |     1.0
   24 |   1.1504 |     40.461 |   1.1518 |     40.106 |     1.0
   25 |   1.1406 |     40.059 |   1.1434 |     39.388 |     1.1
   26 |   1.1328 |     39.850 |   1.1433 |     39.919 |     1.1
   27 |   1.1234 |     39.652 |   1.1289 |     39.045 |     1.2
   28 |   1.1171 |     39.234 |   1.1242 |     39.014 |     1.2
   29 |   1.1077 |     39.047 |   1.1191 |     39.045 |     1.2
   30 |   1.0989 |     38.727 |   1.1157 |     38.421 |     1.3
   31 |   1.0932 |     38.375 |   1.1172 |     37.890 |     1.3
   32 |   1.0803 |     37.698 |   1.1054 |     38.327 |     1.4
   33 |   1.0737 |     37.384 |   1.0935 |     37.235 |     1.4
   34 |   1.0631 |     37.098 |   1.0888 |     37.921 |     1.5
   35 |   1.0573 |     36.939 |   1.0853 |     37.203 |     1.5
   36 |   1.0460 |     36.542 |   1.0804 |     36.798 |     1.5
   37 |   1.0357 |     36.025 |   1.0706 |     36.891 |     1.6
   38 |   1.0231 |     35.629 |   1.0711 |     36.642 |     1.6
   39 |   1.0144 |     35.100 |   1.0546 |     36.111 |     1.7
   40 |   1.0088 |     35.045 |   1.0519 |     36.298 |     1.7
   41 |   1.0001 |     34.814 |   1.0472 |     36.486 |     1.8
   42 |   0.9890 |     34.302 |   1.0394 |     35.986 |     1.8
   43 |   0.9883 |     34.269 |   1.0474 |     36.017 |     1.8
   44 |   0.9767 |     33.867 |   1.0394 |     35.268 |     1.9
   45 |   0.9691 |     33.642 |   1.0365 |     35.456 |     1.9
   46 |   0.9585 |     33.036 |   1.0249 |     34.800 |     2.0
   47 |   0.9473 |     32.634 |   1.0211 |     34.769 |     2.0
   48 |   0.9357 |     32.288 |   1.0220 |     34.769 |     2.1
   49 |   0.9257 |     31.611 |   1.0077 |     34.426 |     2.1
   50 |   0.9195 |     31.330 |   1.0067 |     34.051 |     2.2
   51 |   0.9096 |     30.928 |   1.0016 |     34.426 |     2.2
   52 |   0.8969 |     30.592 |   0.9968 |     33.895 |     2.2
   53 |   0.8914 |     29.965 |   1.0087 |     34.176 |     2.3
   54 |   0.8786 |     29.761 |   0.9930 |     33.833 |     2.3
   55 |   0.8690 |     29.200 |   0.9858 |     32.990 |     2.4
   56 |   0.8571 |     28.930 |   0.9919 |     33.583 |     2.4
   57 |   0.8548 |     28.792 |   0.9765 |     33.084 |     2.5
   58 |   0.8446 |     28.677 |   0.9853 |     32.865 |     2.5
   59 |   0.8339 |     28.082 |   0.9884 |     32.834 |     2.5
   60 |   0.8200 |     27.554 |   0.9881 |     32.615 |     2.6
   61 |   0.8133 |     27.251 |   0.9718 |     32.553 |     2.6
   62 |   0.8055 |     26.888 |   0.9948 |     33.302 |     2.7
   63 |   0.7931 |     26.194 |   0.9730 |     32.428 |     2.7
   64 |   0.7892 |     26.266 |   0.9998 |     33.084 |     2.8
   65 |   0.7831 |     26.387 |   0.9678 |     31.804 |     2.8
   66 |   0.7594 |     24.983 |   0.9670 |     31.648 |     2.9
   67 |   0.7518 |     24.873 |   0.9683 |     32.022 |     2.9
   68 |   0.7441 |     24.461 |   0.9908 |     31.679 |     2.9
   69 |   0.7360 |     24.290 |   0.9754 |     31.617 |     3.0
   70 |   0.7187 |     23.332 |   0.9795 |     31.617 |     3.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,000,802

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0474 |     56.710 |   1.4717 |     44.975 |     0.0
    2 |   1.3966 |     45.817 |   1.3321 |     43.914 |     0.1
    3 |   1.3199 |     44.551 |   1.2914 |     43.758 |     0.1
    4 |   1.2836 |     44.259 |   1.2630 |     43.945 |     0.2
    5 |   1.2519 |     43.560 |   1.2328 |     41.948 |     0.2
    6 |   1.2234 |     42.949 |   1.2202 |     41.667 |     0.3
    7 |   1.1919 |     41.777 |   1.1768 |     40.012 |     0.3
    8 |   1.1621 |     40.681 |   1.1559 |     39.700 |     0.3
    9 |   1.1315 |     39.691 |   1.1274 |     39.107 |     0.4
   10 |   1.0995 |     38.331 |   1.1020 |     37.953 |     0.4
   11 |   1.0662 |     37.241 |   1.0768 |     36.111 |     0.5
   12 |   1.0309 |     35.733 |   1.0651 |     36.142 |     0.5
   13 |   0.9918 |     34.065 |   1.0427 |     35.268 |     0.6
   14 |   0.9617 |     32.954 |   1.0152 |     34.769 |     0.6
   15 |   0.9248 |     32.018 |   0.9940 |     33.677 |     0.7
   16 |   0.8851 |     30.124 |   0.9926 |     33.958 |     0.7
   17 |   0.8578 |     28.858 |   0.9805 |     32.772 |     0.7
   18 |   0.8189 |     27.400 |   0.9555 |     31.617 |     0.8
   19 |   0.7757 |     25.787 |   0.9406 |     31.586 |     0.8
   20 |   0.7405 |     24.593 |   0.9350 |     30.306 |     0.9
   21 |   0.7084 |     23.079 |   0.9260 |     29.432 |     0.9
   22 |   0.6741 |     22.033 |   0.9228 |     29.432 |     1.0
   23 |   0.6325 |     20.641 |   0.9219 |     28.995 |     1.0
   24 |   0.6075 |     19.749 |   0.9086 |     28.777 |     1.1
   25 |   0.5766 |     18.544 |   0.9119 |     28.808 |     1.1
   26 |   0.5412 |     17.492 |   0.9188 |     28.246 |     1.2
   27 |   0.5100 |     16.507 |   0.9246 |     27.403 |     1.2
   28 |   0.4888 |     15.841 |   0.9142 |     27.372 |     1.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,606,978

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5167 |     67.421 |   1.8996 |     53.246 |     0.1
    2 |   1.6929 |     48.448 |   1.5245 |     45.350 |     0.2
    3 |   1.4756 |     46.257 |   1.4282 |     45.350 |     0.3
    4 |   1.4192 |     46.109 |   1.3938 |     45.350 |     0.3
    5 |   1.3881 |     46.114 |   1.3653 |     45.350 |     0.4
    6 |   1.3591 |     45.806 |   1.3367 |     44.164 |     0.5
    7 |   1.3332 |     45.151 |   1.3121 |     44.070 |     0.6
    8 |   1.3116 |     44.760 |   1.2906 |     43.727 |     0.7
    9 |   1.2902 |     44.375 |   1.2706 |     44.164 |     0.8
   10 |   1.2743 |     44.177 |   1.2555 |     43.633 |     0.9
   11 |   1.2601 |     43.956 |   1.2452 |     43.539 |     1.0
   12 |   1.2498 |     43.670 |   1.2319 |     43.258 |     1.0
   13 |   1.2382 |     43.378 |   1.2239 |     42.509 |     1.1
   14 |   1.2277 |     43.257 |   1.2135 |     43.134 |     1.2
   15 |   1.2161 |     42.938 |   1.2075 |     42.790 |     1.3
   16 |   1.2074 |     43.081 |   1.1943 |     42.572 |     1.4
   17 |   1.1966 |     42.679 |   1.1884 |     42.291 |     1.5
   18 |   1.1873 |     42.388 |   1.1815 |     41.916 |     1.6
   19 |   1.1825 |     42.498 |   1.1755 |     41.760 |     1.6
   20 |   1.1743 |     42.041 |   1.1685 |     42.197 |     1.7
   21 |   1.1696 |     41.859 |   1.1636 |     41.729 |     1.8
   22 |   1.1639 |     41.694 |   1.1604 |     42.291 |     1.9
   23 |   1.1592 |     41.463 |   1.1624 |     41.979 |     2.0
   24 |   1.1532 |     41.705 |   1.1500 |     41.542 |     2.1
   25 |   1.1465 |     41.314 |   1.1476 |     41.698 |     2.2
   26 |   1.1444 |     41.413 |   1.1482 |     41.698 |     2.3
   27 |   1.1390 |     41.116 |   1.1437 |     41.042 |     2.3
   28 |   1.1339 |     41.056 |   1.1373 |     41.355 |     2.4
   29 |   1.1315 |     40.995 |   1.1383 |     40.949 |     2.5
   30 |   1.1272 |     40.891 |   1.1283 |     41.011 |     2.6
   31 |   1.1211 |     40.571 |   1.1250 |     40.668 |     2.7
   32 |   1.1188 |     40.423 |   1.1226 |     40.637 |     2.8
   33 |   1.1150 |     40.434 |   1.1193 |     40.886 |     2.9
   34 |   1.1100 |     39.839 |   1.1206 |     40.387 |     3.0
   35 |   1.1081 |     40.269 |   1.1120 |     40.574 |     3.0
   36 |   1.1041 |     39.878 |   1.1149 |     40.418 |     3.1
   37 |   1.1008 |     39.828 |   1.1092 |     39.794 |     3.2
   38 |   1.0981 |     39.927 |   1.1060 |     39.326 |     3.3
   39 |   1.0929 |     39.443 |   1.1081 |     40.169 |     3.4
   40 |   1.0902 |     39.520 |   1.1025 |     39.888 |     3.5
   41 |   1.0893 |     39.454 |   1.1010 |     39.825 |     3.6
   42 |   1.0878 |     39.179 |   1.0943 |     39.576 |     3.6
   43 |   1.0811 |     39.327 |   1.0919 |     39.888 |     3.7
   44 |   1.0829 |     39.267 |   1.0868 |     39.295 |     3.8
   45 |   1.0798 |     39.025 |   1.0900 |     39.357 |     3.9
   46 |   1.0756 |     39.190 |   1.0869 |     39.357 |     4.0
   47 |   1.0713 |     38.843 |   1.0864 |     39.295 |     4.1
   48 |   1.0718 |     38.986 |   1.0813 |     39.638 |     4.2
   49 |   1.0653 |     38.970 |   1.0810 |     38.826 |     4.3
   50 |   1.0648 |     38.672 |   1.0791 |     39.295 |     4.3
   51 |   1.0629 |     38.430 |   1.0763 |     39.388 |     4.4
   52 |   1.0601 |     38.408 |   1.0790 |     38.764 |     4.5
   53 |   1.0572 |     38.381 |   1.0760 |     38.889 |     4.6
   54 |   1.0563 |     38.557 |   1.0764 |     39.357 |     4.7
   55 |   1.0540 |     38.238 |   1.0739 |     39.107 |     4.8
   56 |   1.0518 |     38.320 |   1.0790 |     39.232 |     4.9
   57 |   1.0484 |     38.034 |   1.0675 |     39.045 |     5.0
   58 |   1.0453 |     38.127 |   1.0657 |     38.296 |     5.0
   59 |   1.0432 |     38.105 |   1.0607 |     38.202 |     5.1
   60 |   1.0401 |     37.830 |   1.0631 |     38.421 |     5.2
   61 |   1.0403 |     38.089 |   1.0685 |     38.795 |     5.3
   62 |   1.0403 |     37.990 |   1.0691 |     38.826 |     5.4
   63 |   1.0371 |     38.056 |   1.0599 |     38.358 |     5.5
   64 |   1.0337 |     37.472 |   1.0653 |     38.390 |     5.6
   65 |   1.0332 |     37.555 |   1.0650 |     38.608 |     5.7
   66 |   1.0293 |     37.483 |   1.0575 |     38.764 |     5.7
   67 |   1.0250 |     37.197 |   1.0634 |     38.733 |     5.8
   68 |   1.0242 |     37.340 |   1.0584 |     38.140 |     5.9
   69 |   1.0222 |     37.395 |   1.0572 |     37.765 |     6.0
   70 |   1.0200 |     37.192 |   1.0589 |     38.202 |     6.1
   71 |   1.0206 |     37.082 |   1.0586 |     38.608 |     6.2
   72 |   1.0160 |     36.828 |   1.0583 |     38.670 |     6.3
   73 |   1.0145 |     37.203 |   1.0584 |     38.233 |     6.3
   74 |   1.0156 |     37.027 |   1.0537 |     38.171 |     6.4
   75 |   1.0161 |     36.972 |   1.0547 |     38.046 |     6.5
   76 |   1.0095 |     36.636 |   1.0478 |     38.109 |     6.6
   77 |   1.0076 |     36.537 |   1.0508 |     38.140 |     6.7
   78 |   1.0054 |     36.482 |   1.0573 |     38.421 |     6.8
   79 |   1.0028 |     36.383 |   1.0548 |     38.015 |     6.9
   80 |   0.9990 |     36.135 |   1.0459 |     37.640 |     7.0
   81 |   0.9947 |     36.278 |   1.0536 |     37.890 |     7.0
   82 |   0.9948 |     36.196 |   1.0500 |     38.077 |     7.1
   83 |   0.9953 |     35.915 |   1.0555 |     37.797 |     7.2
   84 |   0.9929 |     36.003 |   1.0487 |     37.703 |     7.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 820,258

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3990 |     63.001 |   1.8168 |     48.502 |     0.0
    2 |   1.6065 |     47.187 |   1.4612 |     45.037 |     0.1
    3 |   1.4159 |     45.894 |   1.3635 |     45.225 |     0.1
    4 |   1.3507 |     45.096 |   1.3182 |     44.070 |     0.2
    5 |   1.3159 |     44.490 |   1.2958 |     43.571 |     0.2
    6 |   1.2918 |     44.089 |   1.2673 |     43.352 |     0.3
    7 |   1.2657 |     43.857 |   1.2431 |     42.946 |     0.3
    8 |   1.2413 |     43.395 |   1.2239 |     42.853 |     0.3
    9 |   1.2204 |     43.032 |   1.2009 |     42.197 |     0.4
   10 |   1.2019 |     42.168 |   1.1860 |     41.074 |     0.4
   11 |   1.1831 |     41.325 |   1.1728 |     41.199 |     0.5
   12 |   1.1655 |     40.896 |   1.1493 |     40.293 |     0.5
   13 |   1.1495 |     40.159 |   1.1425 |     39.388 |     0.6
   14 |   1.1343 |     39.476 |   1.1201 |     39.544 |     0.6
   15 |   1.1140 |     38.628 |   1.1136 |     38.514 |     0.7
   16 |   1.0983 |     38.326 |   1.0979 |     37.484 |     0.7
   17 |   1.0849 |     37.616 |   1.0928 |     37.360 |     0.8
   18 |   1.0674 |     36.873 |   1.0909 |     37.765 |     0.8
   19 |   1.0567 |     36.465 |   1.0729 |     36.860 |     0.8
   20 |   1.0381 |     35.799 |   1.0576 |     37.079 |     0.9
   21 |   1.0200 |     35.155 |   1.0378 |     35.518 |     0.9
   22 |   1.0071 |     34.638 |   1.0426 |     35.924 |     1.0
   23 |   0.9964 |     34.352 |   1.0296 |     35.206 |     1.0
   24 |   0.9765 |     33.350 |   1.0259 |     35.986 |     1.1
   25 |   0.9737 |     33.344 |   1.0135 |     34.395 |     1.1
   26 |   0.9510 |     32.541 |   1.0068 |     34.051 |     1.2
   27 |   0.9328 |     31.792 |   1.0060 |     34.238 |     1.2
   28 |   0.9324 |     31.737 |   1.0108 |     34.363 |     1.2
   29 |   0.9119 |     31.159 |   0.9813 |     33.146 |     1.3
   30 |   0.8930 |     30.064 |   0.9785 |     33.021 |     1.3
   31 |   0.8785 |     29.376 |   0.9706 |     32.615 |     1.4
   32 |   0.8669 |     29.101 |   0.9856 |     33.677 |     1.4
   33 |   0.8446 |     28.347 |   0.9602 |     31.710 |     1.5
   34 |   0.8241 |     27.714 |   0.9452 |     30.993 |     1.5
   35 |   0.8078 |     26.811 |   0.9429 |     31.586 |     1.6
   36 |   0.7895 |     26.255 |   0.9491 |     31.710 |     1.6
   37 |   0.7700 |     25.336 |   0.9394 |     30.743 |     1.7
   38 |   0.7551 |     24.895 |   0.9397 |     30.868 |     1.7
   39 |   0.7360 |     24.119 |   0.9376 |     30.524 |     1.7
   40 |   0.7119 |     23.107 |   0.9303 |     29.432 |     1.8
   41 |   0.7006 |     22.892 |   0.9309 |     30.025 |     1.8
   42 |   0.6774 |     21.896 |   0.9237 |     29.089 |     1.9
   43 |   0.6525 |     21.015 |   0.9290 |     29.401 |     1.9
   44 |   0.6353 |     20.332 |   0.9134 |     28.371 |     2.0
   45 |   0.6104 |     19.474 |   0.9170 |     28.714 |     2.0
   46 |   0.5961 |     19.314 |   0.9227 |     28.277 |     2.1
   47 |   0.5708 |     18.340 |   0.9184 |     27.622 |     2.1
   48 |   0.5504 |     17.492 |   0.9350 |     28.121 |     2.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 801,698

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3444 |     63.254 |   1.7777 |     48.814 |     0.0
    2 |   1.5826 |     47.297 |   1.4617 |     45.350 |     0.1
    3 |   1.4179 |     45.905 |   1.3755 |     44.975 |     0.1
    4 |   1.3565 |     45.261 |   1.3345 |     44.600 |     0.2
    5 |   1.3198 |     44.430 |   1.3026 |     43.477 |     0.2
    6 |   1.2978 |     44.039 |   1.2819 |     43.290 |     0.3
    7 |   1.2746 |     43.780 |   1.2602 |     42.728 |     0.3
    8 |   1.2538 |     43.307 |   1.2466 |     42.197 |     0.3
    9 |   1.2334 |     42.894 |   1.2260 |     41.448 |     0.4
   10 |   1.2120 |     42.272 |   1.2130 |     41.167 |     0.4
   11 |   1.1896 |     41.402 |   1.2016 |     40.918 |     0.5
   12 |   1.1681 |     40.362 |   1.1766 |     40.200 |     0.5
   13 |   1.1451 |     39.531 |   1.1744 |     39.544 |     0.6
   14 |   1.1209 |     38.210 |   1.1528 |     38.577 |     0.6
   15 |   1.0924 |     37.016 |   1.1292 |     37.203 |     0.7
   16 |   1.0605 |     35.640 |   1.1108 |     36.361 |     0.7
   17 |   1.0371 |     34.902 |   1.1004 |     36.517 |     0.7
   18 |   0.9927 |     32.987 |   1.0766 |     36.142 |     0.8
   19 |   0.9503 |     30.829 |   1.0612 |     34.395 |     0.8
   20 |   0.9080 |     29.502 |   1.0495 |     32.584 |     0.9
   21 |   0.8663 |     27.901 |   1.0358 |     33.021 |     0.9
   22 |   0.8218 |     26.073 |   1.0299 |     32.303 |     1.0
   23 |   0.7787 |     24.477 |   1.0054 |     30.961 |     1.0
   24 |   0.7333 |     22.628 |   1.0078 |     31.024 |     1.1
   25 |   0.6907 |     21.433 |   1.0012 |     30.524 |     1.1
   26 |   0.6477 |     19.584 |   0.9929 |     30.493 |     1.1
   27 |   0.6092 |     18.549 |   0.9955 |     29.806 |     1.2
   28 |   0.5632 |     16.920 |   1.0113 |     29.682 |     1.2
   29 |   0.5353 |     15.951 |   0.9989 |     28.527 |     1.3
   30 |   0.4944 |     14.652 |   1.0168 |     28.745 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 586,562

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2403 |     61.096 |   1.6298 |     48.283 |     0.0
    2 |   1.4919 |     46.241 |   1.4180 |     45.662 |     0.0
    3 |   1.4062 |     46.219 |   1.3781 |     45.318 |     0.1
    4 |   1.3750 |     45.811 |   1.3439 |     44.164 |     0.1
    5 |   1.3437 |     45.272 |   1.3223 |     44.757 |     0.1
    6 |   1.3251 |     45.068 |   1.3090 |     44.694 |     0.1
    7 |   1.3002 |     44.793 |   1.2897 |     44.070 |     0.1
    8 |   1.2825 |     44.430 |   1.2549 |     43.258 |     0.1
    9 |   1.2606 |     44.072 |   1.2449 |     43.664 |     0.2
   10 |   1.2414 |     43.681 |   1.2289 |     42.915 |     0.2
   11 |   1.2275 |     43.516 |   1.2105 |     41.948 |     0.2
   12 |   1.2158 |     42.927 |   1.2142 |     43.134 |     0.2
   13 |   1.2065 |     42.674 |   1.2013 |     42.853 |     0.2
   14 |   1.1944 |     42.448 |   1.1894 |     43.196 |     0.3
   15 |   1.1830 |     41.958 |   1.1726 |     42.353 |     0.3
   16 |   1.1751 |     41.975 |   1.1705 |     41.479 |     0.3
   17 |   1.1650 |     41.799 |   1.1696 |     42.166 |     0.3
   18 |   1.1556 |     41.298 |   1.1555 |     41.292 |     0.3
   19 |   1.1422 |     41.342 |   1.1520 |     41.292 |     0.3
   20 |   1.1342 |     41.248 |   1.1314 |     40.418 |     0.4
   21 |   1.1224 |     40.439 |   1.1246 |     40.262 |     0.4
   22 |   1.1194 |     40.753 |   1.1373 |     40.044 |     0.4
   23 |   1.1052 |     39.707 |   1.1170 |     40.169 |     0.4
   24 |   1.0932 |     39.443 |   1.1063 |     39.732 |     0.4
   25 |   1.0831 |     39.036 |   1.1015 |     39.170 |     0.5
   26 |   1.0713 |     38.348 |   1.1071 |     38.826 |     0.5
   27 |   1.0625 |     38.138 |   1.0820 |     39.326 |     0.5
   28 |   1.0514 |     37.483 |   1.0743 |     37.953 |     0.5
   29 |   1.0399 |     37.208 |   1.0607 |     37.890 |     0.5
   30 |   1.0310 |     36.906 |   1.0718 |     38.202 |     0.5
   31 |   1.0214 |     36.647 |   1.0580 |     36.860 |     0.6
   32 |   1.0100 |     36.504 |   1.0447 |     37.235 |     0.6
   33 |   1.0027 |     36.091 |   1.0417 |     37.079 |     0.6
   34 |   0.9903 |     35.678 |   1.0453 |     37.422 |     0.6
   35 |   0.9847 |     35.397 |   1.0267 |     35.986 |     0.6
   36 |   0.9714 |     34.847 |   1.0471 |     36.985 |     0.7
   37 |   0.9660 |     34.594 |   1.0227 |     36.111 |     0.7
   38 |   0.9553 |     34.148 |   1.0210 |     35.799 |     0.7
   39 |   0.9475 |     33.889 |   1.0212 |     36.049 |     0.7
   40 |   0.9393 |     33.471 |   1.0029 |     35.581 |     0.7
   41 |   0.9301 |     32.871 |   1.0105 |     35.487 |     0.7
   42 |   0.9186 |     32.453 |   1.0171 |     36.111 |     0.8
   43 |   0.9166 |     32.524 |   0.9983 |     34.551 |     0.8
   44 |   0.9034 |     31.963 |   1.0066 |     34.582 |     0.8
   45 |   0.8972 |     31.622 |   0.9952 |     34.738 |     0.8
   46 |   0.8894 |     31.099 |   0.9846 |     33.271 |     0.8
   47 |   0.8806 |     30.967 |   0.9861 |     33.614 |     0.9
   48 |   0.8647 |     30.234 |   0.9830 |     33.739 |     0.9
   49 |   0.8589 |     30.201 |   0.9978 |     33.614 |     0.9
   50 |   0.8569 |     30.157 |   0.9773 |     33.271 |     0.9
   51 |   0.8369 |     29.288 |   0.9819 |     34.363 |     0.9
   52 |   0.8344 |     29.244 |   0.9836 |     32.740 |     0.9
   53 |   0.8172 |     28.578 |   0.9793 |     32.772 |     1.0
   54 |   0.8081 |     28.071 |   0.9738 |     32.491 |     1.0
   55 |   0.8034 |     28.093 |   0.9679 |     32.303 |     1.0
   56 |   0.7862 |     27.449 |   0.9707 |     32.522 |     1.0
   57 |   0.7762 |     27.020 |   0.9748 |     32.553 |     1.0
   58 |   0.7676 |     26.833 |   0.9665 |     31.960 |     1.1
   59 |   0.7548 |     26.343 |   0.9774 |     32.366 |     1.1
   60 |   0.7481 |     25.754 |   0.9707 |     31.866 |     1.1
   61 |   0.7446 |     26.172 |   0.9848 |     32.740 |     1.1
   62 |   0.7365 |     25.578 |   0.9780 |     32.022 |     1.1
   63 |   0.7206 |     25.050 |   0.9608 |     31.523 |     1.2
   64 |   0.7112 |     24.829 |   0.9623 |     31.117 |     1.2
   65 |   0.6970 |     24.147 |   0.9544 |     31.710 |     1.2
   66 |   0.6828 |     23.596 |   0.9583 |     31.086 |     1.2
   67 |   0.6802 |     23.662 |   0.9547 |     31.180 |     1.2
   68 |   0.6621 |     23.018 |   0.9605 |     31.586 |     1.2
   69 |   0.6639 |     22.985 |   0.9661 |     31.429 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 2,054,498

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2010 |     59.957 |   1.6063 |     45.350 |     0.1
    2 |   1.4819 |     46.158 |   1.4198 |     45.350 |     0.2
    3 |   1.4106 |     46.301 |   1.3971 |     45.350 |     0.3
    4 |   1.3856 |     46.136 |   1.3659 |     44.694 |     0.4
    5 |   1.3551 |     45.778 |   1.3274 |     44.694 |     0.5
    6 |   1.3305 |     45.729 |   1.3094 |     44.257 |     0.5
    7 |   1.3080 |     45.184 |   1.2808 |     44.195 |     0.6
    8 |   1.2850 |     44.677 |   1.2701 |     43.602 |     0.7
    9 |   1.2640 |     44.441 |   1.2430 |     43.134 |     0.8
   10 |   1.2416 |     43.406 |   1.2281 |     43.633 |     0.9
   11 |   1.2229 |     42.817 |   1.2081 |     42.697 |     1.0
   12 |   1.2009 |     42.256 |   1.1837 |     41.698 |     1.1
   13 |   1.1786 |     41.645 |   1.1644 |     40.543 |     1.2
   14 |   1.1581 |     41.078 |   1.1514 |     40.574 |     1.3
   15 |   1.1403 |     40.555 |   1.1404 |     40.169 |     1.3
   16 |   1.1224 |     39.977 |   1.1183 |     39.357 |     1.4
   17 |   1.1023 |     38.931 |   1.1253 |     39.451 |     1.5
   18 |   1.0899 |     38.370 |   1.1026 |     38.733 |     1.6
   19 |   1.0739 |     38.006 |   1.0916 |     38.951 |     1.7
   20 |   1.0579 |     37.401 |   1.0819 |     38.140 |     1.8
   21 |   1.0404 |     36.961 |   1.0701 |     38.296 |     1.9
   22 |   1.0230 |     36.229 |   1.0703 |     36.642 |     2.0
   23 |   1.0062 |     35.309 |   1.0528 |     36.829 |     2.1
   24 |   0.9887 |     34.797 |   1.0366 |     36.174 |     2.1
   25 |   0.9669 |     34.005 |   1.0298 |     35.487 |     2.2
   26 |   0.9462 |     32.915 |   1.0162 |     34.207 |     2.3
   27 |   0.9216 |     32.023 |   1.0132 |     34.644 |     2.4
   28 |   0.9002 |     30.989 |   0.9903 |     33.989 |     2.5
   29 |   0.8777 |     30.179 |   0.9915 |     33.677 |     2.6
   30 |   0.8526 |     29.238 |   0.9805 |     33.489 |     2.7
   31 |   0.8322 |     28.523 |   0.9868 |     33.021 |     2.8
   32 |   0.8112 |     27.510 |   0.9829 |     32.428 |     2.9
   33 |   0.7837 |     26.734 |   0.9583 |     31.710 |     2.9
   34 |   0.7541 |     25.655 |   0.9517 |     31.211 |     3.0
   35 |   0.7365 |     24.895 |   0.9582 |     30.431 |     3.1
   36 |   0.7143 |     24.031 |   0.9652 |     30.587 |     3.2
   37 |   0.6853 |     22.974 |   0.9460 |     29.931 |     3.3
   38 |   0.6695 |     22.209 |   0.9442 |     30.025 |     3.4
   39 |   0.6429 |     21.323 |   0.9515 |     30.119 |     3.5
   40 |   0.6161 |     20.041 |   0.9364 |     28.933 |     3.6
   41 |   0.5981 |     19.600 |   0.9418 |     29.370 |     3.7
   42 |   0.5705 |     18.571 |   0.9565 |     28.433 |     3.7
   43 |   0.5519 |     17.872 |   0.9563 |     28.496 |     3.8
   44 |   0.5246 |     17.013 |   0.9504 |     28.184 |     3.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,641,186

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5485 |     66.513 |   1.9881 |     58.708 |     0.1
    2 |   1.7758 |     49.642 |   1.5995 |     45.350 |     0.2
    3 |   1.5210 |     46.120 |   1.4619 |     45.350 |     0.2
    4 |   1.4421 |     46.120 |   1.4153 |     45.350 |     0.3
    5 |   1.4069 |     46.125 |   1.3834 |     45.350 |     0.4
    6 |   1.3784 |     45.943 |   1.3725 |     44.320 |     0.5
    7 |   1.3595 |     45.564 |   1.3483 |     44.757 |     0.6
    8 |   1.3418 |     45.156 |   1.3194 |     44.600 |     0.7
    9 |   1.3226 |     44.854 |   1.3075 |     44.600 |     0.7
   10 |   1.3077 |     44.380 |   1.2938 |     43.695 |     0.8
   11 |   1.2952 |     43.813 |   1.2854 |     42.978 |     0.9
   12 |   1.2847 |     43.940 |   1.2712 |     43.633 |     1.0
   13 |   1.2715 |     43.714 |   1.2643 |     42.291 |     1.1
   14 |   1.2591 |     43.615 |   1.2425 |     42.260 |     1.2
   15 |   1.2416 |     43.076 |   1.2318 |     42.135 |     1.2
   16 |   1.2259 |     42.657 |   1.2167 |     41.760 |     1.3
   17 |   1.2062 |     42.002 |   1.1985 |     40.886 |     1.4
   18 |   1.1871 |     41.254 |   1.1787 |     40.949 |     1.5
   19 |   1.1681 |     40.769 |   1.1651 |     40.762 |     1.6
   20 |   1.1496 |     40.252 |   1.1576 |     39.981 |     1.7
   21 |   1.1315 |     39.581 |   1.1481 |     39.607 |     1.7
   22 |   1.1135 |     38.815 |   1.1336 |     39.201 |     1.8
   23 |   1.0995 |     38.463 |   1.1286 |     38.920 |     1.9
   24 |   1.0806 |     37.720 |   1.1128 |     38.858 |     2.0
   25 |   1.0643 |     37.225 |   1.1081 |     38.358 |     2.1
   26 |   1.0460 |     36.669 |   1.1091 |     37.609 |     2.1
   27 |   1.0252 |     35.425 |   1.0982 |     37.079 |     2.2
   28 |   1.0085 |     34.709 |   1.0942 |     38.452 |     2.3
   29 |   0.9862 |     34.175 |   1.0966 |     37.953 |     2.4
   30 |   0.9700 |     33.361 |   1.0728 |     36.486 |     2.5
   31 |   0.9479 |     32.425 |   1.0689 |     36.392 |     2.6
   32 |   0.9265 |     31.913 |   1.0709 |     36.673 |     2.6
   33 |   0.8995 |     30.911 |   1.0591 |     35.518 |     2.7
   34 |   0.8742 |     29.827 |   1.0502 |     34.551 |     2.8
   35 |   0.8442 |     28.842 |   1.0387 |     35.019 |     2.9
   36 |   0.8279 |     28.280 |   1.0435 |     34.831 |     3.0
   37 |   0.7995 |     26.745 |   1.0358 |     33.739 |     3.1
   38 |   0.7694 |     25.644 |   1.0303 |     33.052 |     3.1
   39 |   0.7381 |     24.042 |   1.0354 |     31.960 |     3.2
   40 |   0.7131 |     23.112 |   1.0380 |     32.459 |     3.3
   41 |   0.6888 |     22.248 |   1.0194 |     31.149 |     3.4
   42 |   0.6539 |     20.938 |   1.0404 |     31.180 |     3.5
   43 |   0.6238 |     19.628 |   1.0367 |     30.618 |     3.6
   44 |   0.6021 |     18.984 |   1.0267 |     29.900 |     3.6
   45 |   0.6536 |     20.910 |   1.0311 |     30.181 |     3.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 639,330

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0329 |     54.425 |   1.4865 |     44.944 |     0.0
    2 |   1.4041 |     45.679 |   1.3363 |     44.507 |     0.1
    3 |   1.3182 |     44.534 |   1.2772 |     43.789 |     0.1
    4 |   1.2782 |     44.358 |   1.2473 |     42.634 |     0.1
    5 |   1.2465 |     43.312 |   1.2197 |     42.821 |     0.1
    6 |   1.2193 |     42.955 |   1.2115 |     41.916 |     0.2
    7 |   1.1937 |     41.876 |   1.1679 |     41.386 |     0.2
    8 |   1.1700 |     41.034 |   1.1595 |     40.075 |     0.2
    9 |   1.1511 |     40.814 |   1.1327 |     39.888 |     0.2
   10 |   1.1292 |     39.845 |   1.1219 |     39.700 |     0.3
   11 |   1.1137 |     39.344 |   1.1116 |     39.482 |     0.3
   12 |   1.0933 |     38.612 |   1.0934 |     39.107 |     0.3
   13 |   1.0756 |     38.149 |   1.0772 |     38.296 |     0.4
   14 |   1.0606 |     37.280 |   1.0720 |     37.578 |     0.4
   15 |   1.0434 |     37.027 |   1.0577 |     36.985 |     0.4
   16 |   1.0299 |     36.542 |   1.0326 |     35.737 |     0.4
   17 |   1.0103 |     35.722 |   1.0239 |     35.518 |     0.5
   18 |   0.9896 |     34.830 |   1.0137 |     35.112 |     0.5
   19 |   0.9759 |     34.467 |   1.0124 |     35.331 |     0.5
   20 |   0.9569 |     33.895 |   0.9993 |     34.613 |     0.5
   21 |   0.9419 |     33.168 |   0.9893 |     34.051 |     0.6
   22 |   0.9215 |     32.370 |   0.9798 |     33.427 |     0.6
   23 |   0.9077 |     31.627 |   0.9652 |     33.583 |     0.6
   24 |   0.8833 |     30.576 |   0.9615 |     32.366 |     0.7
   25 |   0.8760 |     30.234 |   0.9564 |     32.865 |     0.7
   26 |   0.8602 |     29.849 |   0.9487 |     32.335 |     0.7
   27 |   0.8464 |     28.842 |   0.9428 |     31.679 |     0.7
   28 |   0.8242 |     28.550 |   0.9155 |     30.868 |     0.8
   29 |   0.8010 |     27.345 |   0.9154 |     29.900 |     0.8
   30 |   0.7894 |     26.976 |   0.9255 |     31.523 |     0.8
   31 |   0.7763 |     26.161 |   0.9036 |     30.337 |     0.9
   32 |   0.7568 |     25.672 |   0.9184 |     30.337 |     0.9
   33 |   0.7438 |     25.000 |   0.9004 |     29.806 |     0.9
   34 |   0.7324 |     24.626 |   0.9051 |     29.900 |     0.9
   35 |   0.7106 |     23.690 |   0.8997 |     29.619 |     1.0
   36 |   0.6897 |     22.936 |   0.8972 |     28.964 |     1.0
   37 |   0.6707 |     22.380 |   0.9116 |     29.713 |     1.0
   38 |   0.6597 |     22.066 |   0.8966 |     28.777 |     1.0
   39 |   0.6405 |     21.400 |   0.8915 |     27.965 |     1.1
   40 |   0.6248 |     20.624 |   0.8975 |     28.184 |     1.1
   41 |   0.6109 |     20.454 |   0.8950 |     28.340 |     1.1
   42 |   0.5979 |     19.628 |   0.9014 |     28.464 |     1.2
   43 |   0.5973 |     19.909 |   0.8971 |     27.497 |     1.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,691,042

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5169 |     67.195 |   1.9533 |     58.146 |     0.1
    2 |   1.7505 |     49.461 |   1.5729 |     45.350 |     0.2
    3 |   1.5061 |     46.120 |   1.4508 |     45.350 |     0.3
    4 |   1.4360 |     46.114 |   1.4146 |     45.350 |     0.4
    5 |   1.4085 |     46.136 |   1.3973 |     45.350 |     0.4
    6 |   1.3893 |     46.142 |   1.3715 |     45.350 |     0.5
    7 |   1.3680 |     45.608 |   1.3536 |     44.694 |     0.6
    8 |   1.3461 |     44.892 |   1.3295 |     44.538 |     0.7
    9 |   1.3291 |     44.892 |   1.3085 |     43.695 |     0.8
   10 |   1.3110 |     44.160 |   1.2966 |     43.383 |     0.9
   11 |   1.2913 |     43.896 |   1.2765 |     42.759 |     1.0
   12 |   1.2715 |     43.560 |   1.2476 |     42.478 |     1.1
   13 |   1.2527 |     43.290 |   1.2298 |     42.353 |     1.2
   14 |   1.2353 |     42.927 |   1.2181 |     41.448 |     1.2
   15 |   1.2197 |     42.553 |   1.2016 |     41.355 |     1.3
   16 |   1.2018 |     42.024 |   1.1893 |     41.105 |     1.4
   17 |   1.1862 |     41.738 |   1.1801 |     40.699 |     1.5
   18 |   1.1717 |     40.946 |   1.1664 |     40.668 |     1.6
   19 |   1.1532 |     40.577 |   1.1559 |     39.232 |     1.7
   20 |   1.1345 |     39.421 |   1.1395 |     39.576 |     1.8
   21 |   1.1185 |     39.074 |   1.1279 |     38.889 |     1.9
   22 |   1.1033 |     38.298 |   1.1213 |     38.733 |     2.0
   23 |   1.0860 |     37.825 |   1.1094 |     38.296 |     2.0
   24 |   1.0695 |     37.197 |   1.1092 |     38.015 |     2.1
   25 |   1.0546 |     36.603 |   1.0931 |     37.391 |     2.2
   26 |   1.0338 |     35.827 |   1.0903 |     37.391 |     2.3
   27 |   1.0147 |     34.665 |   1.0779 |     36.454 |     2.4
   28 |   0.9958 |     34.247 |   1.0766 |     35.955 |     2.5
   29 |   0.9748 |     33.047 |   1.0684 |     36.142 |     2.6
   30 |   0.9491 |     32.045 |   1.0663 |     35.581 |     2.7
   31 |   0.9290 |     31.710 |   1.0405 |     34.831 |     2.8
   32 |   0.9028 |     30.471 |   1.0580 |     33.958 |     2.8
   33 |   0.8772 |     29.486 |   1.0348 |     33.396 |     2.9
   34 |   0.8547 |     28.220 |   1.0414 |     33.271 |     3.0
   35 |   0.8253 |     27.229 |   1.0351 |     32.772 |     3.1
   36 |   0.8131 |     26.662 |   1.0267 |     32.272 |     3.2
   37 |   0.7824 |     25.655 |   1.0223 |     31.710 |     3.3
   38 |   0.7503 |     24.152 |   1.0258 |     32.179 |     3.4
   39 |   0.7293 |     23.718 |   1.0288 |     30.868 |     3.5
   40 |   0.7176 |     23.272 |   1.0497 |     31.648 |     3.6
   41 |   0.6894 |     22.121 |   1.0373 |     31.617 |     3.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,990,818

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0355 |     55.460 |   1.4942 |     44.975 |     0.8
    2 |   1.4206 |     45.729 |   1.3736 |     45.381 |     1.8
    3 |   1.3520 |     45.399 |   1.3181 |     44.819 |     2.7
    4 |   1.2963 |     44.468 |   1.2655 |     44.007 |     3.7
    5 |   1.2572 |     43.791 |   1.2345 |     42.853 |     4.6
    6 |   1.2302 |     43.252 |   1.2149 |     41.199 |     5.5
    7 |   1.2031 |     42.090 |   1.1945 |     41.386 |     6.5
    8 |   1.1732 |     41.034 |   1.1693 |     40.012 |     7.6
    9 |   1.1426 |     39.927 |   1.1481 |     39.856 |     8.5
   10 |   1.1047 |     38.551 |   1.1030 |     37.859 |     9.4
   11 |   1.0624 |     36.707 |   1.0777 |     37.360 |    10.4
   12 |   1.0162 |     34.869 |   1.0572 |     35.924 |    11.4
   13 |   0.9721 |     33.196 |   1.0175 |     33.770 |    12.4
   14 |   0.9212 |     31.187 |   0.9789 |     32.740 |    13.3
   15 |   0.8645 |     28.688 |   0.9693 |     31.960 |    14.3
   16 |   0.8085 |     26.448 |   0.9512 |     31.336 |    15.3
   17 |   0.7637 |     24.796 |   0.9264 |     31.024 |    16.2
   18 |   0.7049 |     22.490 |   0.9035 |     28.433 |    17.1
   19 |   0.6505 |     20.657 |   0.9052 |     28.246 |    18.1
   20 |   0.6016 |     19.044 |   0.9075 |     27.684 |    19.0
   21 |   0.5492 |     17.338 |   0.9156 |     28.371 |    19.9
   22 |   0.4993 |     15.670 |   0.9028 |     26.311 |    20.9
   23 |   0.4585 |     14.404 |   0.9114 |     27.091 |    21.8
   24 |   0.4086 |     12.698 |   0.9227 |     25.936 |    22.8
   25 |   0.3689 |     11.179 |   0.9524 |     25.936 |    23.7
   26 |   0.3369 |     10.249 |   0.9753 |     26.342 |    24.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 475,682

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6407 |     71.312 |   2.0989 |     58.708 |     0.0
    2 |   1.8430 |     51.982 |   1.6096 |     45.350 |     0.1
    3 |   1.5259 |     46.257 |   1.4586 |     45.350 |     0.1
    4 |   1.4324 |     46.125 |   1.3951 |     45.350 |     0.1
    5 |   1.3779 |     45.960 |   1.3499 |     45.225 |     0.1
    6 |   1.3441 |     45.558 |   1.3188 |     44.507 |     0.2
    7 |   1.3186 |     44.804 |   1.2905 |     44.007 |     0.2
    8 |   1.2958 |     44.402 |   1.2724 |     43.664 |     0.2
    9 |   1.2778 |     44.100 |   1.2573 |     43.758 |     0.2
   10 |   1.2605 |     43.995 |   1.2373 |     43.571 |     0.3
   11 |   1.2471 |     43.725 |   1.2241 |     42.728 |     0.3
   12 |   1.2330 |     43.588 |   1.2134 |     42.416 |     0.3
   13 |   1.2204 |     43.577 |   1.2030 |     42.197 |     0.3
   14 |   1.2079 |     42.723 |   1.1885 |     41.604 |     0.4
   15 |   1.1962 |     42.212 |   1.1926 |     42.166 |     0.4
   16 |   1.1860 |     41.953 |   1.1756 |     41.479 |     0.4
   17 |   1.1752 |     41.562 |   1.1694 |     40.949 |     0.4
   18 |   1.1669 |     41.402 |   1.1655 |     41.042 |     0.5
   19 |   1.1587 |     41.028 |   1.1503 |     40.481 |     0.5
   20 |   1.1465 |     40.808 |   1.1448 |     40.605 |     0.5
   21 |   1.1410 |     40.747 |   1.1450 |     39.825 |     0.5
   22 |   1.1311 |     40.203 |   1.1341 |     40.293 |     0.6
   23 |   1.1261 |     39.878 |   1.1252 |     39.700 |     0.6
   24 |   1.1170 |     39.592 |   1.1282 |     39.888 |     0.6
   25 |   1.1107 |     39.201 |   1.1163 |     39.045 |     0.6
   26 |   1.1042 |     38.937 |   1.1099 |     38.889 |     0.7
   27 |   1.0984 |     38.656 |   1.1045 |     39.170 |     0.7
   28 |   1.0910 |     38.485 |   1.1004 |     38.920 |     0.7
   29 |   1.0839 |     38.403 |   1.0995 |     39.139 |     0.7
   30 |   1.0761 |     37.896 |   1.0878 |     38.327 |     0.8
   31 |   1.0729 |     38.094 |   1.0861 |     38.109 |     0.8
   32 |   1.0656 |     37.467 |   1.0806 |     38.639 |     0.8
   33 |   1.0597 |     37.269 |   1.0754 |     38.109 |     0.8
   34 |   1.0514 |     36.994 |   1.0640 |     37.203 |     0.9
   35 |   1.0463 |     36.856 |   1.0666 |     37.484 |     0.9
   36 |   1.0422 |     36.355 |   1.0659 |     37.360 |     0.9
   37 |   1.0322 |     35.986 |   1.0539 |     36.548 |     0.9
   38 |   1.0292 |     36.069 |   1.0529 |     36.735 |     1.0
   39 |   1.0194 |     35.557 |   1.0467 |     36.579 |     1.0
   40 |   1.0138 |     35.425 |   1.0432 |     36.423 |     1.0
   41 |   1.0100 |     35.227 |   1.0349 |     35.331 |     1.0
   42 |   1.0029 |     35.012 |   1.0372 |     35.643 |     1.1
   43 |   0.9971 |     34.676 |   1.0263 |     35.144 |     1.1
   44 |   0.9889 |     34.390 |   1.0272 |     35.612 |     1.1
   45 |   0.9809 |     34.010 |   1.0171 |     35.081 |     1.2
   46 |   0.9788 |     33.768 |   1.0258 |     35.268 |     1.2
   47 |   0.9669 |     33.427 |   1.0112 |     33.895 |     1.2
   48 |   0.9629 |     33.108 |   1.0037 |     34.051 |     1.2
   49 |   0.9526 |     32.893 |   1.0004 |     34.488 |     1.3
   50 |   0.9427 |     32.370 |   0.9942 |     33.958 |     1.3
   51 |   0.9521 |     32.783 |   1.0075 |     34.800 |     1.3
   52 |   0.9428 |     32.370 |   0.9933 |     33.895 |     1.3
   53 |   0.9314 |     32.001 |   0.9863 |     33.614 |     1.4
   54 |   0.9247 |     31.693 |   0.9996 |     33.864 |     1.4
   55 |   0.9134 |     30.978 |   0.9792 |     33.895 |     1.4
   56 |   0.9057 |     30.801 |   0.9686 |     32.678 |     1.4
   57 |   0.9022 |     30.664 |   0.9616 |     32.459 |     1.5
   58 |   0.8953 |     30.477 |   0.9713 |     33.271 |     1.5
   59 |   0.9060 |     31.176 |   0.9683 |     32.865 |     1.5
   60 |   0.8860 |     30.064 |   0.9636 |     32.272 |     1.5
   61 |   0.8746 |     29.794 |   0.9650 |     32.022 |     1.6
   62 |   0.8739 |     29.651 |   0.9580 |     31.742 |     1.6
   63 |   0.8658 |     29.095 |   0.9596 |     31.336 |     1.6
   64 |   0.8574 |     29.150 |   0.9574 |     31.742 |     1.6
   65 |   0.8468 |     28.413 |   0.9559 |     31.429 |     1.7
   66 |   0.8469 |     28.330 |   0.9487 |     31.242 |     1.7
   67 |   0.8378 |     27.945 |   0.9543 |     30.836 |     1.7
   68 |   0.8277 |     27.774 |   0.9561 |     31.866 |     1.7
   69 |   0.8260 |     27.736 |   0.9584 |     31.367 |     1.8
   70 |   0.8190 |     27.279 |   0.9428 |     30.587 |     1.8
   71 |   0.8125 |     27.196 |   0.9461 |     30.556 |     1.8
   72 |   0.8029 |     26.684 |   0.9531 |     30.212 |     1.9
   73 |   0.7965 |     26.321 |   0.9435 |     30.712 |     1.9
   74 |   0.7964 |     26.833 |   0.9467 |     30.805 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,063,746

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2214 |     60.447 |   1.6071 |     45.350 |     0.4
    2 |   1.4775 |     46.241 |   1.4139 |     45.350 |     0.9
    3 |   1.3996 |     46.147 |   1.3692 |     45.662 |     1.4
    4 |   1.3639 |     45.575 |   1.3319 |     44.663 |     1.9
    5 |   1.3329 |     45.487 |   1.3047 |     44.164 |     2.3
    6 |   1.3097 |     45.283 |   1.2870 |     43.945 |     2.8
    7 |   1.2981 |     45.244 |   1.2774 |     44.164 |     3.3
    8 |   1.2865 |     44.705 |   1.2692 |     43.851 |     3.8
    9 |   1.2733 |     44.364 |   1.2554 |     43.664 |     4.2
   10 |   1.2615 |     44.122 |   1.2436 |     43.727 |     4.7
   11 |   1.2458 |     43.868 |   1.2259 |     42.821 |     5.2
   12 |   1.2274 |     43.555 |   1.2060 |     41.916 |     5.7
   13 |   1.2122 |     43.136 |   1.1928 |     42.603 |     6.2
   14 |   1.1933 |     42.663 |   1.1780 |     41.916 |     6.6
   15 |   1.1795 |     42.294 |   1.1792 |     41.417 |     7.1
   16 |   1.1615 |     41.078 |   1.1531 |     40.075 |     7.6
   17 |   1.1473 |     40.830 |   1.1380 |     39.482 |     8.1
   18 |   1.1293 |     40.137 |   1.1250 |     39.263 |     8.5
   19 |   1.1147 |     39.702 |   1.1135 |     39.513 |     9.0
   20 |   1.1019 |     39.338 |   1.1039 |     38.983 |     9.5
   21 |   1.0873 |     39.047 |   1.0988 |     39.076 |    10.0
   22 |   1.0711 |     38.116 |   1.0866 |     38.046 |    10.4
   23 |   1.0611 |     37.632 |   1.0817 |     38.421 |    10.9
   24 |   1.0434 |     36.768 |   1.0689 |     37.110 |    11.4
   25 |   1.0312 |     36.762 |   1.0637 |     37.516 |    11.9
   26 |   1.0113 |     35.799 |   1.0569 |     36.330 |    12.3
   27 |   0.9979 |     35.232 |   1.0443 |     35.768 |    12.8
   28 |   0.9831 |     34.671 |   1.0372 |     35.737 |    13.3
   29 |   0.9660 |     34.032 |   1.0258 |     35.737 |    13.8
   30 |   0.9498 |     33.322 |   1.0163 |     35.300 |    14.2
   31 |   0.9338 |     32.645 |   1.0108 |     34.738 |    14.7
   32 |   0.9165 |     32.199 |   1.0110 |     35.393 |    15.2
   33 |   0.8936 |     31.181 |   0.9991 |     34.488 |    15.7
   34 |   0.8742 |     30.256 |   0.9867 |     33.458 |    16.2
   35 |   0.8491 |     29.194 |   0.9736 |     31.929 |    16.6
   36 |   0.8238 |     28.297 |   0.9825 |     33.021 |    17.1
   37 |   0.8077 |     27.543 |   0.9636 |     32.303 |    17.6
   38 |   0.7811 |     26.475 |   0.9472 |     30.712 |    18.1
   39 |   0.7631 |     25.842 |   0.9434 |     30.712 |    18.5
   40 |   0.7383 |     24.609 |   0.9516 |     30.993 |    19.0
   41 |   0.7121 |     23.718 |   0.9386 |     29.806 |    19.5
   42 |   0.6957 |     23.343 |   0.9645 |     30.805 |    20.0
   43 |   0.6811 |     22.809 |   0.9314 |     29.713 |    20.4
   44 |   0.6517 |     21.417 |   0.9360 |     29.588 |    20.9
   45 |   0.6329 |     20.723 |   0.9471 |     28.870 |    21.4
   46 |   0.6135 |     20.294 |   0.9329 |     29.213 |    21.9
   47 |   0.5986 |     19.826 |   0.9501 |     28.995 |    22.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 604,386

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0893 |     56.798 |   1.5039 |     45.350 |     0.0
    2 |   1.4123 |     45.789 |   1.3502 |     44.039 |     0.1
    3 |   1.3268 |     44.595 |   1.2953 |     43.789 |     0.1
    4 |   1.2863 |     44.248 |   1.2613 |     43.321 |     0.1
    5 |   1.2566 |     43.665 |   1.2377 |     42.509 |     0.1
    6 |   1.2338 |     43.026 |   1.2160 |     42.166 |     0.2
    7 |   1.2122 |     42.377 |   1.1961 |     42.416 |     0.2
    8 |   1.1920 |     41.947 |   1.1818 |     41.667 |     0.2
    9 |   1.1691 |     41.078 |   1.1630 |     40.762 |     0.2
   10 |   1.1477 |     40.577 |   1.1403 |     39.950 |     0.3
   11 |   1.1276 |     39.900 |   1.1262 |     39.451 |     0.3
   12 |   1.1070 |     39.085 |   1.0968 |     38.639 |     0.3
   13 |   1.0843 |     38.430 |   1.0882 |     37.921 |     0.3
   14 |   1.0695 |     37.654 |   1.0771 |     37.141 |     0.4
   15 |   1.0488 |     37.076 |   1.0557 |     36.142 |     0.4
   16 |   1.0285 |     36.185 |   1.0534 |     36.236 |     0.4
   17 |   1.0126 |     35.436 |   1.0468 |     36.517 |     0.5
   18 |   0.9902 |     34.726 |   1.0344 |     35.737 |     0.5
   19 |   0.9749 |     33.933 |   1.0238 |     34.956 |     0.5
   20 |   0.9544 |     33.289 |   1.0192 |     34.738 |     0.5
   21 |   0.9376 |     32.673 |   0.9973 |     34.114 |     0.6
   22 |   0.9144 |     31.550 |   0.9889 |     33.489 |     0.6
   23 |   0.8972 |     30.917 |   0.9831 |     33.645 |     0.6
   24 |   0.8789 |     30.091 |   0.9775 |     33.864 |     0.6
   25 |   0.8583 |     29.266 |   0.9580 |     32.740 |     0.7
   26 |   0.8346 |     28.473 |   0.9589 |     32.116 |     0.7
   27 |   0.8195 |     28.121 |   0.9485 |     31.773 |     0.7
   28 |   0.7958 |     26.805 |   0.9319 |     31.492 |     0.7
   29 |   0.7763 |     26.211 |   0.9234 |     31.273 |     0.8
   30 |   0.7540 |     25.534 |   0.9256 |     30.868 |     0.8
   31 |   0.7339 |     24.417 |   0.9189 |     30.805 |     0.8
   32 |   0.7103 |     23.624 |   0.9080 |     30.181 |     0.9
   33 |   0.6898 |     22.881 |   0.9130 |     29.588 |     0.9
   34 |   0.6810 |     22.798 |   0.9114 |     29.838 |     0.9
   35 |   0.6587 |     21.769 |   0.9100 |     30.025 |     0.9
   36 |   0.6342 |     20.954 |   0.9121 |     29.463 |     1.0
   37 |   0.6112 |     20.002 |   0.9023 |     29.151 |     1.0
   38 |   0.5910 |     19.578 |   0.8912 |     28.558 |     1.0
   39 |   0.5735 |     18.731 |   0.9017 |     28.464 |     1.0
   40 |   0.5532 |     18.076 |   0.9183 |     28.527 |     1.1
   41 |   0.5712 |     18.736 |   0.9053 |     28.184 |     1.1
   42 |   0.5181 |     16.887 |   0.9163 |     28.277 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 291,042

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4006 |     61.685 |   1.7930 |     48.096 |     0.0
    2 |   1.6033 |     47.209 |   1.4626 |     45.006 |     0.0
    3 |   1.4212 |     45.894 |   1.3730 |     44.944 |     0.0
    4 |   1.3615 |     45.685 |   1.3355 |     45.443 |     0.1
    5 |   1.3263 |     45.085 |   1.3044 |     44.164 |     0.1
    6 |   1.2974 |     44.309 |   1.2719 |     43.508 |     0.1
    7 |   1.2716 |     43.978 |   1.2488 |     43.071 |     0.1
    8 |   1.2473 |     43.659 |   1.2283 |     43.071 |     0.1
    9 |   1.2262 |     43.197 |   1.2065 |     41.823 |     0.1
   10 |   1.2083 |     42.580 |   1.1903 |     41.885 |     0.2
   11 |   1.1905 |     41.953 |   1.1756 |     40.918 |     0.2
   12 |   1.1736 |     41.111 |   1.1577 |     39.794 |     0.2
   13 |   1.1538 |     40.043 |   1.1428 |     39.700 |     0.2
   14 |   1.1400 |     39.982 |   1.1278 |     39.732 |     0.2
   15 |   1.1226 |     38.981 |   1.1218 |     38.452 |     0.2
   16 |   1.1088 |     38.804 |   1.1053 |     38.577 |     0.3
   17 |   1.0938 |     38.309 |   1.0988 |     37.984 |     0.3
   18 |   1.0812 |     37.940 |   1.0934 |     37.609 |     0.3
   19 |   1.0698 |     37.291 |   1.0727 |     36.923 |     0.3
   20 |   1.0569 |     37.115 |   1.0649 |     36.174 |     0.3
   21 |   1.0439 |     36.515 |   1.0519 |     36.049 |     0.3
   22 |   1.0293 |     36.058 |   1.0517 |     36.049 |     0.4
   23 |   1.0196 |     35.629 |   1.0356 |     35.986 |     0.4
   24 |   1.0040 |     35.155 |   1.0324 |     35.424 |     0.4
   25 |   0.9923 |     34.473 |   1.0289 |     34.988 |     0.4
   26 |   0.9897 |     34.533 |   1.0218 |     34.769 |     0.4
   27 |   0.9675 |     33.620 |   1.0114 |     33.864 |     0.4
   28 |   0.9625 |     33.388 |   1.0049 |     34.207 |     0.5
   29 |   0.9418 |     33.003 |   1.0162 |     34.831 |     0.5
   30 |   0.9418 |     32.491 |   0.9836 |     32.959 |     0.5
   31 |   0.9222 |     32.144 |   0.9836 |     33.677 |     0.5
   32 |   0.9017 |     31.077 |   0.9700 |     32.522 |     0.5
   33 |   0.9076 |     31.390 |   0.9789 |     32.803 |     0.5
   34 |   0.8867 |     30.411 |   0.9746 |     32.647 |     0.6
   35 |   0.8707 |     29.745 |   0.9592 |     32.709 |     0.6
   36 |   0.8596 |     29.271 |   0.9681 |     32.428 |     0.6
   37 |   0.8398 |     28.341 |   0.9528 |     31.835 |     0.6
   38 |   0.8274 |     27.895 |   0.9459 |     31.773 |     0.6
   39 |   0.8126 |     27.290 |   0.9474 |     32.303 |     0.6
   40 |   0.7980 |     26.844 |   0.9498 |     31.710 |     0.7
   41 |   0.7851 |     26.404 |   0.9415 |     31.180 |     0.7
   42 |   0.7706 |     25.721 |   0.9261 |     30.556 |     0.7
   43 |   0.7574 |     25.253 |   0.9504 |     31.866 |     0.7
   44 |   0.7546 |     25.149 |   0.9330 |     30.836 |     0.7
   45 |   0.7302 |     24.048 |   0.9265 |     30.243 |     0.7
   46 |   0.7186 |     23.932 |   0.9207 |     30.368 |     0.8
   47 |   0.7092 |     23.563 |   0.9305 |     29.963 |     0.8
   48 |   0.6918 |     22.765 |   0.9471 |     30.836 |     0.8
   49 |   0.6873 |     22.688 |   0.9269 |     30.243 |     0.8
   50 |   0.6633 |     22.000 |   0.9185 |     29.401 |     0.8
   51 |   0.6455 |     21.334 |   0.9175 |     29.463 |     0.8
   52 |   0.6343 |     20.679 |   0.9167 |     29.526 |     0.9
   53 |   0.6234 |     20.305 |   0.9188 |     28.901 |     0.9
   54 |   0.6118 |     20.371 |   0.9267 |     28.901 |     0.9
   55 |   0.6132 |     20.503 |   0.9137 |     28.371 |     0.9
   56 |   0.5986 |     19.788 |   0.9288 |     28.995 |     0.9
   57 |   0.5943 |     19.523 |   0.9242 |     28.527 |     0.9
   58 |   0.5716 |     18.483 |   0.9171 |     28.090 |     0.9
   59 |   0.5606 |     18.301 |   0.9347 |     27.903 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,622,626

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5258 |     67.889 |   1.9675 |     53.246 |     0.8
    2 |   1.7257 |     49.020 |   1.5463 |     45.350 |     1.8
    3 |   1.4844 |     46.120 |   1.4424 |     45.350 |     2.7
    4 |   1.4260 |     46.120 |   1.4040 |     45.350 |     3.6
    5 |   1.3999 |     46.158 |   1.3783 |     45.350 |     4.5
    6 |   1.3791 |     46.059 |   1.3600 |     44.632 |     5.5
    7 |   1.3631 |     45.657 |   1.3465 |     44.632 |     6.4
    8 |   1.3442 |     45.542 |   1.3254 |     44.757 |     7.3
    9 |   1.3258 |     45.151 |   1.3099 |     44.600 |     8.3
   10 |   1.3070 |     44.837 |   1.2962 |     44.101 |     9.2
   11 |   1.2952 |     44.463 |   1.2831 |     44.382 |    10.1
   12 |   1.2830 |     44.375 |   1.2665 |     43.695 |    11.1
   13 |   1.2711 |     44.386 |   1.2537 |     43.477 |    12.0
   14 |   1.2565 |     43.962 |   1.2413 |     42.821 |    12.9
   15 |   1.2437 |     43.307 |   1.2285 |     42.665 |    13.8
   16 |   1.2300 |     42.911 |   1.2204 |     42.166 |    14.8
   17 |   1.2210 |     42.668 |   1.2131 |     42.135 |    15.7
   18 |   1.2112 |     42.421 |   1.1997 |     41.916 |    16.6
   19 |   1.2022 |     42.272 |   1.1954 |     41.448 |    17.6
   20 |   1.1942 |     42.162 |   1.1864 |     41.760 |    18.5
   21 |   1.1852 |     41.975 |   1.1755 |     41.230 |    19.4
   22 |   1.1773 |     41.942 |   1.1745 |     41.979 |    20.4
   23 |   1.1708 |     41.870 |   1.1655 |     41.199 |    21.3
   24 |   1.1620 |     41.579 |   1.1627 |     41.011 |    22.2
   25 |   1.1548 |     41.171 |   1.1553 |     40.418 |    23.1
   26 |   1.1487 |     41.353 |   1.1477 |     41.386 |    24.1
   27 |   1.1421 |     41.155 |   1.1420 |     40.356 |    25.0
   28 |   1.1364 |     40.555 |   1.1489 |     40.668 |    25.9
   29 |   1.1318 |     40.533 |   1.1323 |     40.543 |    26.9
   30 |   1.1223 |     40.197 |   1.1271 |     39.950 |    27.8
   31 |   1.1152 |     39.889 |   1.1186 |     40.075 |    28.7
   32 |   1.1075 |     39.680 |   1.1159 |     40.200 |    29.7
   33 |   1.0988 |     39.470 |   1.1219 |     39.326 |    30.6
   34 |   1.0922 |     39.003 |   1.1062 |     39.607 |    31.5
   35 |   1.0843 |     38.843 |   1.1074 |     39.419 |    32.4
   36 |   1.0785 |     38.320 |   1.0980 |     38.983 |    33.4
   37 |   1.0701 |     38.161 |   1.0837 |     38.514 |    34.3
   38 |   1.0627 |     37.825 |   1.0850 |     38.951 |    35.2
   39 |   1.0594 |     37.852 |   1.0789 |     38.452 |    36.2
   40 |   1.0494 |     37.555 |   1.0746 |     38.639 |    37.1
   41 |   1.0447 |     37.384 |   1.0738 |     38.109 |    38.0
   42 |   1.0363 |     36.972 |   1.0610 |     37.734 |    39.0
   43 |   1.0293 |     36.724 |   1.0627 |     36.423 |    39.9
   44 |   1.0200 |     36.229 |   1.0562 |     37.297 |    40.8
   45 |   1.0122 |     35.893 |   1.0530 |     37.047 |    41.8
   46 |   1.0084 |     35.898 |   1.0557 |     36.610 |    42.7
   47 |   1.0009 |     35.618 |   1.0439 |     36.049 |    43.6
   48 |   0.9925 |     35.232 |   1.0356 |     36.579 |    44.6
   49 |   0.9826 |     34.671 |   1.0253 |     35.768 |    45.5
   50 |   0.9785 |     34.621 |   1.0275 |     36.642 |    46.4
   51 |   0.9665 |     33.950 |   1.0312 |     35.674 |    47.3
   52 |   0.9612 |     33.983 |   1.0191 |     35.237 |    48.3
   53 |   0.9553 |     33.675 |   1.0164 |     35.737 |    49.2
   54 |   0.9461 |     33.075 |   0.9993 |     34.395 |    50.1
   55 |   0.9400 |     33.234 |   1.0119 |     35.081 |    51.1
   56 |   0.9311 |     32.480 |   0.9943 |     34.395 |    52.0
   57 |   0.9214 |     32.100 |   0.9960 |     35.050 |    52.9
   58 |   0.9178 |     31.869 |   1.0050 |     34.956 |    53.9
   59 |   0.9102 |     31.660 |   0.9904 |     34.082 |    54.8
   60 |   0.9036 |     31.759 |   0.9852 |     33.614 |    55.7
   61 |   0.8959 |     31.093 |   0.9881 |     34.114 |    56.7
   62 |   0.8904 |     30.889 |   0.9858 |     34.051 |    57.6
   63 |   0.8824 |     30.471 |   0.9849 |     34.207 |    58.5
   64 |   0.8722 |     30.020 |   0.9777 |     33.958 |    59.4
   65 |   0.8687 |     30.102 |   0.9775 |     33.989 |    60.4
   66 |   0.8591 |     29.811 |   0.9751 |     33.427 |    61.3
   67 |   0.8586 |     29.420 |   0.9670 |     33.052 |    62.2
   68 |   0.8431 |     28.946 |   0.9715 |     33.552 |    63.2
   69 |   0.8335 |     28.473 |   0.9785 |     34.020 |    64.1
   70 |   0.8436 |     29.046 |   0.9691 |     33.208 |    65.0
   71 |   0.8274 |     28.330 |   0.9614 |     32.834 |    66.0
   72 |   0.8287 |     28.528 |   0.9750 |     32.678 |    66.9
   73 |   0.8159 |     27.785 |   0.9701 |     33.021 |    67.8
   74 |   0.8064 |     27.493 |   0.9561 |     32.491 |    68.8
   75 |   0.7952 |     27.075 |   0.9607 |     32.303 |    69.7
   76 |   0.7929 |     27.130 |   0.9667 |     32.303 |    70.6
   77 |   0.7808 |     26.277 |   0.9580 |     32.491 |    71.6
   78 |   0.7752 |     26.332 |   0.9607 |     31.710 |    72.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 674,978

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0242 |     55.251 |   1.4739 |     45.037 |     0.0
    2 |   1.3925 |     45.514 |   1.3308 |     44.600 |     0.1
    3 |   1.3133 |     44.413 |   1.2790 |     43.196 |     0.1
    4 |   1.2744 |     43.659 |   1.2453 |     43.071 |     0.1
    5 |   1.2407 |     43.065 |   1.2184 |     42.478 |     0.2
    6 |   1.2115 |     42.520 |   1.1986 |     41.074 |     0.2
    7 |   1.1807 |     41.243 |   1.1709 |     40.762 |     0.2
    8 |   1.1499 |     40.593 |   1.1437 |     40.106 |     0.2
    9 |   1.1147 |     39.261 |   1.1190 |     38.202 |     0.3
   10 |   1.0773 |     37.428 |   1.0810 |     37.172 |     0.3
   11 |   1.0435 |     36.482 |   1.0828 |     37.110 |     0.3
   12 |   1.0075 |     34.930 |   1.0335 |     35.019 |     0.4
   13 |   0.9698 |     33.476 |   1.0098 |     34.051 |     0.4
   14 |   0.9337 |     32.045 |   0.9899 |     33.552 |     0.4
   15 |   0.8990 |     30.587 |   0.9770 |     32.959 |     0.5
   16 |   0.8650 |     29.271 |   0.9692 |     32.584 |     0.5
   17 |   0.8272 |     27.895 |   0.9564 |     32.179 |     0.5
   18 |   0.7830 |     25.842 |   0.9229 |     30.774 |     0.6
   19 |   0.7420 |     24.279 |   0.9100 |     30.025 |     0.6
   20 |   0.7110 |     23.184 |   0.8936 |     29.370 |     0.6
   21 |   0.6744 |     21.918 |   0.8940 |     28.433 |     0.6
   22 |   0.6388 |     20.641 |   0.8870 |     28.246 |     0.7
   23 |   0.6029 |     19.435 |   0.8984 |     27.871 |     0.7
   24 |   0.5805 |     18.456 |   0.8846 |     27.216 |     0.7
   25 |   0.5430 |     17.135 |   0.8972 |     27.653 |     0.8
   26 |   0.5185 |     16.397 |   0.8916 |     27.185 |     0.8
   27 |   0.4872 |     15.571 |   0.9168 |     27.029 |     0.8
   28 |   0.4703 |     14.806 |   0.8928 |     26.841 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 305,762

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5813 |     69.320 |   2.0183 |     58.240 |     0.0
    2 |   1.7527 |     49.862 |   1.5527 |     45.350 |     0.0
    3 |   1.4882 |     46.120 |   1.4413 |     45.350 |     0.0
    4 |   1.4271 |     46.241 |   1.4094 |     45.318 |     0.1
    5 |   1.4003 |     46.208 |   1.3799 |     45.350 |     0.1
    6 |   1.3710 |     45.800 |   1.3415 |     45.069 |     0.1
    7 |   1.3375 |     45.090 |   1.3166 |     43.758 |     0.1
    8 |   1.3134 |     44.430 |   1.2943 |     43.508 |     0.1
    9 |   1.2926 |     44.116 |   1.2718 |     43.727 |     0.1
   10 |   1.2741 |     44.160 |   1.2628 |     43.695 |     0.2
   11 |   1.2583 |     43.929 |   1.2474 |     43.227 |     0.2
   12 |   1.2469 |     43.863 |   1.2349 |     42.821 |     0.2
   13 |   1.2332 |     43.456 |   1.2229 |     42.447 |     0.2
   14 |   1.2174 |     43.153 |   1.2138 |     41.417 |     0.2
   15 |   1.2046 |     42.459 |   1.1974 |     41.667 |     0.2
   16 |   1.1895 |     41.496 |   1.1840 |     41.011 |     0.2
   17 |   1.1802 |     41.513 |   1.1699 |     40.980 |     0.3
   18 |   1.1680 |     40.885 |   1.1614 |     40.356 |     0.3
   19 |   1.1618 |     40.874 |   1.1576 |     40.637 |     0.3
   20 |   1.1517 |     40.599 |   1.1627 |     40.325 |     0.3
   21 |   1.1436 |     40.274 |   1.1534 |     40.418 |     0.3
   22 |   1.1344 |     39.900 |   1.1400 |     39.607 |     0.3
   23 |   1.1244 |     39.674 |   1.1394 |     39.607 |     0.4
   24 |   1.1190 |     39.614 |   1.1314 |     38.889 |     0.4
   25 |   1.1105 |     39.278 |   1.1205 |     38.795 |     0.4
   26 |   1.0999 |     38.953 |   1.1195 |     39.326 |     0.4
   27 |   1.0922 |     38.656 |   1.1309 |     40.012 |     0.4
   28 |   1.0842 |     38.298 |   1.1059 |     38.327 |     0.4
   29 |   1.0755 |     38.056 |   1.1131 |     38.858 |     0.5
   30 |   1.0719 |     37.979 |   1.1014 |     38.764 |     0.5
   31 |   1.0638 |     37.577 |   1.1025 |     38.702 |     0.5
   32 |   1.0566 |     37.335 |   1.0913 |     37.672 |     0.5
   33 |   1.0509 |     37.032 |   1.0976 |     38.327 |     0.5
   34 |   1.0445 |     37.131 |   1.0845 |     37.765 |     0.5
   35 |   1.0352 |     36.190 |   1.0766 |     37.141 |     0.5
   36 |   1.0268 |     36.273 |   1.0743 |     38.109 |     0.6
   37 |   1.0174 |     35.805 |   1.0693 |     38.109 |     0.6
   38 |   1.0113 |     35.563 |   1.0761 |     37.672 |     0.6
   39 |   0.9994 |     35.144 |   1.0659 |     37.484 |     0.6
   40 |   0.9948 |     34.880 |   1.0637 |     36.517 |     0.6
   41 |   0.9883 |     35.056 |   1.0602 |     37.141 |     0.6
   42 |   0.9785 |     34.445 |   1.0453 |     36.610 |     0.7
   43 |   0.9700 |     33.741 |   1.0508 |     36.767 |     0.7
   44 |   0.9588 |     33.686 |   1.0435 |     36.392 |     0.7
   45 |   0.9470 |     32.954 |   1.0411 |     36.579 |     0.7
   46 |   0.9429 |     32.755 |   1.0416 |     36.298 |     0.7
   47 |   0.9330 |     32.447 |   1.0330 |     36.330 |     0.7
   48 |   0.9226 |     32.106 |   1.0313 |     35.237 |     0.8
   49 |   0.9130 |     31.423 |   1.0334 |     35.737 |     0.8
   50 |   0.9061 |     31.600 |   1.0382 |     35.705 |     0.8
   51 |   0.8957 |     30.823 |   1.0303 |     35.362 |     0.8
   52 |   0.8797 |     30.416 |   1.0268 |     35.549 |     0.8
   53 |   0.8708 |     30.064 |   1.0275 |     35.081 |     0.8
   54 |   0.8588 |     29.491 |   1.0217 |     34.707 |     0.8
   55 |   0.8478 |     29.139 |   1.0134 |     33.989 |     0.9
   56 |   0.8407 |     28.858 |   1.0158 |     34.956 |     0.9
   57 |   0.8261 |     28.347 |   1.0225 |     34.863 |     0.9
   58 |   0.8200 |     27.923 |   1.0159 |     34.207 |     0.9
   59 |   0.8066 |     27.306 |   1.0020 |     33.427 |     0.9
   60 |   0.7909 |     26.937 |   1.0072 |     33.271 |     0.9
   61 |   0.7745 |     26.128 |   1.0171 |     33.333 |     1.0
   62 |   0.7603 |     25.694 |   0.9999 |     32.772 |     1.0
   63 |   0.7528 |     25.336 |   1.0111 |     32.740 |     1.0
   64 |   0.7397 |     24.758 |   0.9942 |     31.898 |     1.0
   65 |   0.7212 |     24.141 |   1.0181 |     32.896 |     1.0
   66 |   0.7108 |     24.009 |   1.0053 |     32.647 |     1.0
   67 |   0.6942 |     23.151 |   1.0085 |     31.960 |     1.1
   68 |   0.6844 |     22.875 |   0.9882 |     31.648 |     1.1
   69 |   0.6692 |     22.253 |   1.0200 |     31.273 |     1.1
   70 |   0.6552 |     21.494 |   0.9950 |     31.866 |     1.1
   71 |   0.6402 |     21.202 |   1.0066 |     31.617 |     1.1
   72 |   0.6265 |     20.586 |   1.0083 |     31.055 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 572,834

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5190 |     67.806 |   1.9689 |     53.433 |     0.0
    2 |   1.7536 |     49.367 |   1.5688 |     45.350 |     0.1
    3 |   1.4957 |     46.120 |   1.4482 |     45.350 |     0.1
    4 |   1.4331 |     46.120 |   1.4159 |     45.350 |     0.1
    5 |   1.4102 |     46.208 |   1.4032 |     45.350 |     0.2
    6 |   1.3986 |     46.131 |   1.3873 |     45.350 |     0.2
    7 |   1.3860 |     46.054 |   1.3748 |     45.350 |     0.2
    8 |   1.3732 |     45.916 |   1.3597 |     45.350 |     0.3
    9 |   1.3612 |     45.784 |   1.3488 |     44.944 |     0.3
   10 |   1.3434 |     45.277 |   1.3293 |     44.600 |     0.3
   11 |   1.3246 |     44.832 |   1.3071 |     44.007 |     0.4
   12 |   1.3013 |     44.655 |   1.2794 |     43.477 |     0.4
   13 |   1.2843 |     43.956 |   1.2675 |     42.790 |     0.4
   14 |   1.2637 |     43.599 |   1.2451 |     43.258 |     0.5
   15 |   1.2424 |     43.599 |   1.2325 |     43.165 |     0.5
   16 |   1.2237 |     43.202 |   1.2212 |     42.821 |     0.5
   17 |   1.2042 |     42.630 |   1.1950 |     41.885 |     0.5
   18 |   1.1851 |     41.903 |   1.1872 |     41.199 |     0.6
   19 |   1.1676 |     40.990 |   1.1672 |     40.231 |     0.6
   20 |   1.1484 |     40.296 |   1.1545 |     39.981 |     0.6
   21 |   1.1308 |     39.790 |   1.1469 |     40.169 |     0.7
   22 |   1.1142 |     38.749 |   1.1371 |     39.544 |     0.7
   23 |   1.0972 |     38.359 |   1.1296 |     39.357 |     0.7
   24 |   1.0818 |     37.660 |   1.1098 |     38.046 |     0.8
   25 |   1.0638 |     37.208 |   1.1060 |     37.797 |     0.8
   26 |   1.0461 |     36.366 |   1.0939 |     37.859 |     0.8
   27 |   1.0266 |     35.755 |   1.0855 |     37.297 |     0.9
   28 |   1.0075 |     34.935 |   1.0950 |     37.266 |     0.9
   29 |   0.9868 |     33.785 |   1.0922 |     37.484 |     0.9
   30 |   0.9679 |     33.229 |   1.0712 |     35.986 |     1.0
   31 |   0.9437 |     32.073 |   1.0667 |     35.237 |     1.0
   32 |   0.9201 |     31.038 |   1.0634 |     34.769 |     1.0
   33 |   0.8948 |     29.926 |   1.0656 |     35.081 |     1.1
   34 |   0.8698 |     28.952 |   1.0568 |     33.708 |     1.1
   35 |   0.8524 |     28.115 |   1.0534 |     34.332 |     1.1
   36 |   0.8247 |     27.064 |   1.0473 |     32.584 |     1.2
   37 |   0.8016 |     26.530 |   1.0483 |     33.365 |     1.2
   38 |   0.7759 |     25.402 |   1.0419 |     32.303 |     1.2
   39 |   0.7564 |     24.521 |   1.0446 |     32.272 |     1.3
   40 |   0.7399 |     24.152 |   1.0400 |     31.773 |     1.3
   41 |   0.7076 |     22.655 |   1.0443 |     31.523 |     1.3
   42 |   0.6847 |     21.934 |   1.0734 |     31.835 |     1.4
   43 |   0.6666 |     21.323 |   1.0622 |     31.617 |     1.4
   44 |   0.6440 |     20.316 |   1.0592 |     31.149 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 732,354

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5267 |     67.817 |   1.9676 |     53.246 |     0.4
    2 |   1.7604 |     50.275 |   1.5676 |     45.350 |     0.9
    3 |   1.5023 |     46.125 |   1.4509 |     45.350 |     1.4
    4 |   1.4364 |     46.120 |   1.4163 |     45.350 |     1.8
    5 |   1.4111 |     46.131 |   1.4066 |     45.350 |     2.3
    6 |   1.3931 |     46.224 |   1.3754 |     45.350 |     2.8
    7 |   1.3783 |     45.828 |   1.3624 |     44.850 |     3.2
    8 |   1.3618 |     45.509 |   1.3483 |     44.569 |     3.7
    9 |   1.3486 |     45.443 |   1.3363 |     44.507 |     4.2
   10 |   1.3374 |     45.382 |   1.3199 |     44.444 |     4.6
   11 |   1.3239 |     45.492 |   1.3047 |     44.351 |     5.1
   12 |   1.3044 |     45.068 |   1.2857 |     43.883 |     5.6
   13 |   1.2895 |     44.435 |   1.2689 |     43.758 |     6.0
   14 |   1.2741 |     44.243 |   1.2560 |     43.446 |     6.5
   15 |   1.2577 |     44.039 |   1.2389 |     42.634 |     7.0
   16 |   1.2443 |     43.505 |   1.2295 |     43.134 |     7.5
   17 |   1.2304 |     43.153 |   1.2123 |     42.104 |     7.9
   18 |   1.2230 |     43.296 |   1.1998 |     42.166 |     8.4
   19 |   1.2045 |     42.399 |   1.1936 |     41.729 |     8.9
   20 |   1.1939 |     42.195 |   1.1828 |     41.511 |     9.3
   21 |   1.1820 |     41.617 |   1.1768 |     41.042 |     9.8
   22 |   1.1685 |     41.265 |   1.1687 |     41.105 |    10.3
   23 |   1.1582 |     41.083 |   1.1549 |     40.231 |    10.7
   24 |   1.1526 |     40.742 |   1.1489 |     40.730 |    11.2
   25 |   1.1402 |     40.456 |   1.1376 |     39.732 |    11.7
   26 |   1.1288 |     40.026 |   1.1350 |     39.451 |    12.1
   27 |   1.1188 |     39.548 |   1.1282 |     39.669 |    12.6
   28 |   1.1079 |     38.986 |   1.1162 |     38.951 |    13.1
   29 |   1.1005 |     38.760 |   1.1192 |     39.669 |    13.6
   30 |   1.0944 |     38.480 |   1.1113 |     38.826 |    14.0
   31 |   1.0757 |     37.830 |   1.1013 |     38.702 |    14.5
   32 |   1.0643 |     37.181 |   1.0956 |     38.046 |    15.0
   33 |   1.0478 |     36.696 |   1.0842 |     38.452 |    15.4
   34 |   1.0385 |     36.218 |   1.0660 |     36.486 |    15.9
   35 |   1.0266 |     35.684 |   1.0730 |     37.266 |    16.4
   36 |   1.0199 |     35.414 |   1.0656 |     36.673 |    16.8
   37 |   1.0069 |     35.128 |   1.0622 |     36.267 |    17.3
   38 |   0.9999 |     34.786 |   1.0606 |     36.454 |    17.8
   39 |   0.9842 |     33.906 |   1.0436 |     36.049 |    18.2
   40 |   0.9710 |     33.520 |   1.0322 |     35.300 |    18.7
   41 |   0.9560 |     33.025 |   1.0306 |     34.457 |    19.2
   42 |   0.9432 |     32.304 |   1.0298 |     35.268 |    19.7
   43 |   0.9311 |     31.979 |   1.0158 |     34.738 |    20.1
   44 |   0.9185 |     31.544 |   1.0125 |     34.363 |    20.6
   45 |   0.8991 |     30.581 |   1.0078 |     33.833 |    21.1
   46 |   0.8883 |     30.350 |   1.0080 |     33.801 |    21.5
   47 |   0.8782 |     29.893 |   0.9979 |     32.740 |    22.0
   48 |   0.8604 |     29.139 |   0.9855 |     32.335 |    22.5
   49 |   0.8402 |     28.170 |   0.9876 |     32.491 |    22.9
   50 |   0.8254 |     27.989 |   0.9789 |     32.491 |    23.4
   51 |   0.8090 |     26.982 |   0.9916 |     33.115 |    23.9
   52 |   0.7999 |     27.125 |   0.9821 |     32.241 |    24.3
   53 |   0.7830 |     26.288 |   0.9642 |     31.710 |    24.8
   54 |   0.7694 |     25.859 |   0.9594 |     31.305 |    25.3
   55 |   0.7524 |     25.132 |   0.9718 |     31.024 |    25.8
   56 |   0.7358 |     24.615 |   0.9625 |     30.524 |    26.2
   57 |   0.7351 |     24.510 |   0.9686 |     31.117 |    26.7
   58 |   0.7155 |     23.789 |   0.9664 |     30.493 |    27.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,194,594

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5315 |     67.751 |   1.9820 |     54.588 |     0.6
    2 |   1.7896 |     51.029 |   1.6073 |     45.350 |     1.3
    3 |   1.5270 |     46.131 |   1.4618 |     45.350 |     2.0
    4 |   1.4393 |     46.252 |   1.4047 |     45.350 |     2.7
    5 |   1.3884 |     46.059 |   1.3651 |     44.788 |     3.5
    6 |   1.3501 |     45.811 |   1.3284 |     45.474 |     4.2
    7 |   1.3254 |     45.415 |   1.3067 |     44.538 |     4.9
    8 |   1.3086 |     45.035 |   1.2969 |     44.351 |     5.6
    9 |   1.2939 |     44.633 |   1.2861 |     44.007 |     6.3
   10 |   1.2843 |     44.435 |   1.2692 |     43.883 |     7.0
   11 |   1.2710 |     44.232 |   1.2536 |     43.414 |     7.7
   12 |   1.2554 |     44.144 |   1.2456 |     42.790 |     8.4
   13 |   1.2397 |     43.681 |   1.2275 |     42.447 |     9.1
   14 |   1.2271 |     43.373 |   1.2132 |     42.853 |     9.8
   15 |   1.2160 |     43.301 |   1.2021 |     42.790 |    10.5
   16 |   1.2062 |     43.087 |   1.2007 |     42.228 |    11.2
   17 |   1.1957 |     42.839 |   1.1888 |     41.948 |    11.9
   18 |   1.1854 |     42.228 |   1.1824 |     41.542 |    12.6
   19 |   1.1764 |     42.283 |   1.1749 |     41.136 |    13.3
   20 |   1.1688 |     41.617 |   1.1646 |     41.042 |    14.1
   21 |   1.1591 |     41.518 |   1.1587 |     40.605 |    14.8
   22 |   1.1510 |     41.083 |   1.1482 |     41.011 |    15.5
   23 |   1.1499 |     41.303 |   1.1524 |     41.479 |    16.2
   24 |   1.1439 |     40.880 |   1.1407 |     40.137 |    16.9
   25 |   1.1356 |     40.324 |   1.1361 |     40.137 |    17.6
   26 |   1.1246 |     39.949 |   1.1268 |     39.763 |    18.3
   27 |   1.1156 |     40.010 |   1.1228 |     40.012 |    19.0
   28 |   1.1127 |     39.680 |   1.1228 |     40.387 |    19.7
   29 |   1.1034 |     39.399 |   1.1163 |     40.012 |    20.4
   30 |   1.0975 |     39.036 |   1.1062 |     39.295 |    21.1
   31 |   1.0887 |     38.788 |   1.1026 |     39.107 |    21.8
   32 |   1.0843 |     38.661 |   1.0959 |     39.451 |    22.5
   33 |   1.0771 |     38.194 |   1.0937 |     38.920 |    23.2
   34 |   1.0692 |     38.177 |   1.0833 |     39.014 |    23.9
   35 |   1.0644 |     38.172 |   1.0831 |     38.733 |    24.6
   36 |   1.0590 |     37.841 |   1.0789 |     39.107 |    25.3
   37 |   1.0542 |     37.902 |   1.0828 |     38.452 |    26.0
   38 |   1.0469 |     37.329 |   1.0705 |     38.514 |    26.8
   39 |   1.0415 |     37.175 |   1.0667 |     38.452 |    27.5
   40 |   1.0357 |     36.950 |   1.0670 |     38.015 |    28.2
   41 |   1.0297 |     37.082 |   1.0585 |     37.734 |    28.9
   42 |   1.0257 |     36.795 |   1.0552 |     37.297 |    29.6
   43 |   1.0167 |     36.537 |   1.0640 |     37.703 |    30.3
   44 |   1.0159 |     36.339 |   1.0568 |     37.360 |    31.0
   45 |   1.0081 |     36.162 |   1.0512 |     37.297 |    31.7
   46 |   1.0047 |     36.058 |   1.0451 |     36.330 |    32.4
   47 |   0.9969 |     35.788 |   1.0422 |     36.985 |    33.1
   48 |   0.9939 |     35.524 |   1.0292 |     36.548 |    33.8
   49 |   0.9842 |     35.040 |   1.0330 |     36.454 |    34.5
   50 |   0.9798 |     34.709 |   1.0261 |     35.737 |    35.2
   51 |   0.9718 |     34.665 |   1.0280 |     35.737 |    35.9
   52 |   0.9671 |     34.440 |   1.0213 |     35.986 |    36.6
   53 |   0.9634 |     34.467 |   1.0199 |     35.518 |    37.4
   54 |   0.9560 |     34.043 |   1.0158 |     35.768 |    38.1
   55 |   0.9506 |     33.680 |   1.0114 |     36.080 |    38.8
   56 |   0.9428 |     33.454 |   1.0036 |     35.237 |    39.5
   57 |   0.9406 |     33.130 |   1.0053 |     35.144 |    40.2
   58 |   0.9286 |     32.629 |   0.9996 |     35.081 |    40.9
   59 |   0.9209 |     32.436 |   1.0022 |     34.925 |    41.6
   60 |   0.9166 |     32.238 |   0.9915 |     34.675 |    42.3
   61 |   0.9098 |     31.572 |   0.9990 |     35.206 |    43.0
   62 |   0.8992 |     31.699 |   0.9860 |     34.707 |    43.7
   63 |   0.8965 |     31.115 |   0.9867 |     34.332 |    44.4
   64 |   0.8859 |     31.093 |   0.9837 |     34.238 |    45.1
   65 |   0.8831 |     31.022 |   0.9976 |     34.301 |    45.8
   66 |   0.8786 |     30.477 |   0.9754 |     34.145 |    46.5
   67 |   0.8647 |     30.014 |   0.9694 |     33.614 |    47.2
   68 |   0.8590 |     30.025 |   0.9709 |     33.427 |    47.9
   69 |   0.8525 |     29.398 |   0.9690 |     33.365 |    48.7
   70 |   0.8447 |     29.200 |   0.9709 |     34.082 |    49.3
   71 |   0.8413 |     28.902 |   0.9662 |     32.553 |    50.1
   72 |   0.8283 |     28.633 |   0.9674 |     32.772 |    50.8
   73 |   0.8253 |     28.225 |   0.9626 |     32.459 |    51.5
   74 |   0.8178 |     27.972 |   0.9867 |     32.335 |    52.2
   75 |   0.8116 |     27.620 |   0.9725 |     33.021 |    52.9
   76 |   0.8035 |     27.559 |   0.9676 |     32.647 |    53.6
   77 |   0.7946 |     26.998 |   0.9574 |     31.898 |    54.3
   78 |   0.7906 |     26.932 |   0.9676 |     31.773 |    55.0
   79 |   0.7826 |     26.536 |   0.9649 |     31.742 |    55.7
   80 |   0.7705 |     26.167 |   0.9695 |     32.615 |    56.4
   81 |   0.7741 |     26.277 |   0.9552 |     31.523 |    57.1
   82 |   0.7546 |     25.418 |   0.9560 |     30.805 |    57.8
   83 |   0.7516 |     25.215 |   0.9505 |     31.336 |    58.5
   84 |   0.7409 |     25.044 |   0.9679 |     31.773 |    59.2
   85 |   0.7406 |     24.950 |   0.9503 |     31.055 |    59.9
   86 |   0.7249 |     24.268 |   0.9560 |     31.024 |    60.7
   87 |   0.7232 |     23.960 |   0.9653 |     31.367 |    61.4
   88 |   0.7173 |     24.081 |   0.9513 |     30.712 |    62.1
   89 |   0.7055 |     23.415 |   0.9639 |     30.524 |    62.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,179,874

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4064 |     64.107 |   1.8660 |     52.622 |     0.6
    2 |   1.6496 |     47.875 |   1.4798 |     44.975 |     1.3
    3 |   1.4365 |     45.982 |   1.3880 |     45.256 |     2.0
    4 |   1.3724 |     45.448 |   1.3392 |     44.226 |     2.7
    5 |   1.3323 |     44.777 |   1.3073 |     44.288 |     3.5
    6 |   1.3043 |     44.424 |   1.2789 |     44.007 |     4.2
    7 |   1.2836 |     44.314 |   1.2609 |     44.195 |     4.9
    8 |   1.2606 |     43.857 |   1.2415 |     42.884 |     5.6
    9 |   1.2410 |     43.378 |   1.2192 |     42.260 |     6.3
   10 |   1.2233 |     42.872 |   1.2039 |     41.542 |     7.0
   11 |   1.2030 |     41.771 |   1.1890 |     40.949 |     7.7
   12 |   1.1890 |     41.452 |   1.1715 |     40.637 |     8.4
   13 |   1.1727 |     40.995 |   1.1577 |     40.262 |     9.1
   14 |   1.1621 |     40.439 |   1.1526 |     39.544 |     9.8
   15 |   1.1487 |     40.335 |   1.1420 |     40.169 |    10.5
   16 |   1.1379 |     39.779 |   1.1332 |     39.732 |    11.2
   17 |   1.1259 |     39.404 |   1.1275 |     39.388 |    11.9
   18 |   1.1147 |     39.129 |   1.1138 |     39.201 |    12.6
   19 |   1.1081 |     38.860 |   1.1115 |     38.639 |    13.3
   20 |   1.0964 |     38.474 |   1.1073 |     38.483 |    14.0
   21 |   1.0890 |     37.979 |   1.0926 |     37.828 |    14.8
   22 |   1.0751 |     37.704 |   1.0914 |     38.390 |    15.5
   23 |   1.0704 |     37.494 |   1.0876 |     38.109 |    16.2
   24 |   1.0592 |     37.098 |   1.0807 |     37.453 |    16.9
   25 |   1.0512 |     36.768 |   1.0796 |     37.203 |    17.6
   26 |   1.0391 |     36.383 |   1.0748 |     37.422 |    18.3
   27 |   1.0292 |     35.827 |   1.0597 |     36.361 |    19.0
   28 |   1.0192 |     35.601 |   1.0535 |     36.080 |    19.7
   29 |   1.0053 |     35.150 |   1.0461 |     35.456 |    20.4
   30 |   0.9968 |     34.693 |   1.0491 |     35.893 |    21.1
   31 |   0.9871 |     34.539 |   1.0409 |     35.861 |    21.8
   32 |   0.9710 |     33.537 |   1.0316 |     34.457 |    22.5
   33 |   0.9658 |     33.476 |   1.0262 |     35.581 |    23.2
   34 |   0.9518 |     33.207 |   1.0168 |     35.019 |    23.9
   35 |   0.9405 |     32.513 |   1.0055 |     34.301 |    24.6
   36 |   0.9283 |     32.166 |   1.0131 |     34.582 |    25.4
   37 |   0.9169 |     31.693 |   1.0037 |     34.426 |    26.1
   38 |   0.9027 |     31.170 |   0.9971 |     34.270 |    26.8
   39 |   0.8924 |     30.735 |   0.9863 |     33.396 |    27.5
   40 |   0.8797 |     30.218 |   0.9804 |     33.770 |    28.2
   41 |   0.8645 |     29.486 |   0.9869 |     33.115 |    28.9
   42 |   0.8474 |     28.776 |   0.9735 |     32.459 |    29.6
   43 |   0.8357 |     28.330 |   0.9645 |     32.272 |    30.3
   44 |   0.8180 |     27.515 |   0.9561 |     32.335 |    31.0
   45 |   0.8036 |     27.059 |   0.9547 |     31.586 |    31.7
   46 |   0.7982 |     26.783 |   0.9463 |     30.961 |    32.4
   47 |   0.7840 |     26.404 |   0.9427 |     31.055 |    33.1
   48 |   0.7709 |     25.479 |   0.9371 |     30.805 |    33.8
   49 |   0.7484 |     24.939 |   0.9412 |     30.712 |    34.5
   50 |   0.7369 |     24.719 |   0.9325 |     29.963 |    35.2
   51 |   0.7183 |     24.009 |   0.9323 |     30.025 |    35.9
   52 |   0.7066 |     23.211 |   0.9255 |     30.150 |    36.6
   53 |   0.6903 |     23.018 |   0.9371 |     30.119 |    37.3
   54 |   0.6749 |     22.143 |   0.9200 |     29.650 |    38.1
   55 |   0.6606 |     21.411 |   0.9363 |     30.056 |    38.8
   56 |   0.6464 |     20.921 |   0.9263 |     29.307 |    39.5
   57 |   0.6279 |     20.360 |   0.9161 |     29.120 |    40.2
   58 |   0.6227 |     20.134 |   0.9193 |     29.526 |    40.9
   59 |   0.6104 |     19.589 |   0.9489 |     29.650 |    41.6
   60 |   0.5900 |     18.995 |   0.9297 |     28.777 |    42.3
   61 |   0.5834 |     18.703 |   0.9458 |     29.775 |    43.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,097,954

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1813 |     60.205 |   1.5970 |     48.283 |     0.4
    2 |   1.4815 |     46.252 |   1.4202 |     45.350 |     0.9
    3 |   1.4107 |     46.109 |   1.4003 |     45.350 |     1.4
    4 |   1.3847 |     46.202 |   1.3586 |     44.694 |     1.9
    5 |   1.3572 |     45.646 |   1.3398 |     44.881 |     2.3
    6 |   1.3361 |     45.531 |   1.3172 |     44.975 |     2.8
    7 |   1.3114 |     45.443 |   1.2955 |     44.070 |     3.3
    8 |   1.2914 |     44.755 |   1.2748 |     44.288 |     3.8
    9 |   1.2762 |     44.573 |   1.2530 |     43.602 |     4.2
   10 |   1.2548 |     44.000 |   1.2385 |     43.258 |     4.7
   11 |   1.2373 |     43.362 |   1.2118 |     42.291 |     5.2
   12 |   1.2174 |     42.927 |   1.1982 |     41.667 |     5.7
   13 |   1.2010 |     42.239 |   1.1864 |     41.355 |     6.1
   14 |   1.1866 |     41.964 |   1.1746 |     41.573 |     6.6
   15 |   1.1706 |     41.678 |   1.1560 |     40.106 |     7.1
   16 |   1.1545 |     40.924 |   1.1447 |     39.669 |     7.6
   17 |   1.1355 |     40.335 |   1.1314 |     40.200 |     8.1
   18 |   1.1205 |     39.718 |   1.1144 |     38.858 |     8.5
   19 |   1.0999 |     39.008 |   1.1029 |     38.358 |     9.0
   20 |   1.0799 |     38.227 |   1.0875 |     37.734 |     9.5
   21 |   1.0607 |     37.401 |   1.0709 |     36.610 |    10.0
   22 |   1.0393 |     36.608 |   1.0548 |     36.579 |    10.4
   23 |   1.0167 |     35.854 |   1.0414 |     36.392 |    10.9
   24 |   0.9931 |     34.665 |   1.0353 |     35.206 |    11.4
   25 |   0.9714 |     33.977 |   1.0233 |     35.300 |    11.9
   26 |   0.9487 |     33.003 |   1.0084 |     34.800 |    12.3
   27 |   0.9262 |     31.875 |   1.0122 |     34.145 |    12.8
   28 |   0.9078 |     31.242 |   0.9911 |     33.677 |    13.3
   29 |   0.8837 |     30.119 |   0.9869 |     33.645 |    13.8
   30 |   0.8565 |     29.178 |   0.9872 |     33.645 |    14.3
   31 |   0.8369 |     28.033 |   0.9849 |     33.240 |    14.7
   32 |   0.8124 |     27.295 |   0.9558 |     31.835 |    15.2
   33 |   0.7905 |     26.420 |   0.9530 |     31.804 |    15.7
   34 |   0.7687 |     25.484 |   0.9581 |     31.242 |    16.2
   35 |   0.7427 |     24.862 |   0.9624 |     31.149 |    16.6
   36 |   0.7195 |     23.503 |   0.9465 |     30.150 |    17.1
   37 |   0.6967 |     22.853 |   0.9456 |     30.087 |    17.6
   38 |   0.6889 |     22.534 |   0.9514 |     29.682 |    18.1
   39 |   0.6572 |     21.180 |   0.9447 |     29.463 |    18.5
   40 |   0.6327 |     20.690 |   0.9338 |     28.933 |    19.0
   41 |   0.6132 |     20.074 |   0.9339 |     28.402 |    19.5
   42 |   0.6042 |     19.859 |   0.9327 |     28.464 |    20.0
   43 |   0.5799 |     18.764 |   0.9437 |     28.964 |    20.5
   44 |   0.5632 |     18.461 |   0.9235 |     28.152 |    20.9
   45 |   0.5384 |     17.525 |   0.9260 |     27.278 |    21.4
   46 |   0.5167 |     16.777 |   0.9518 |     28.184 |    21.9
   47 |   0.5032 |     16.546 |   0.9560 |     27.559 |    22.4
   48 |   0.4883 |     15.924 |   0.9475 |     27.434 |    22.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,054,498

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1819 |     59.605 |   1.5983 |     45.350 |     0.8
    2 |   1.4726 |     46.153 |   1.4096 |     45.350 |     1.8
    3 |   1.3998 |     46.186 |   1.3773 |     45.006 |     2.7
    4 |   1.3667 |     45.894 |   1.3456 |     44.913 |     3.7
    5 |   1.3420 |     45.393 |   1.3312 |     44.725 |     4.6
    6 |   1.3157 |     45.041 |   1.2946 |     44.070 |     5.5
    7 |   1.2937 |     44.793 |   1.2775 |     44.132 |     6.5
    8 |   1.2779 |     44.391 |   1.2719 |     43.695 |     7.4
    9 |   1.2620 |     44.077 |   1.2449 |     43.664 |     8.4
   10 |   1.2356 |     43.758 |   1.2236 |     43.102 |     9.3
   11 |   1.2151 |     43.279 |   1.2122 |     42.010 |    10.2
   12 |   1.1920 |     42.107 |   1.1813 |     41.823 |    11.2
   13 |   1.1744 |     41.573 |   1.1746 |     40.918 |    12.1
   14 |   1.1612 |     41.133 |   1.1560 |     40.855 |    13.1
   15 |   1.1418 |     40.461 |   1.1339 |     39.419 |    14.0
   16 |   1.1165 |     39.305 |   1.1227 |     39.419 |    15.0
   17 |   1.0968 |     38.964 |   1.1085 |     39.326 |    15.9
   18 |   1.0760 |     37.841 |   1.0988 |     39.045 |    16.8
   19 |   1.0530 |     37.236 |   1.0773 |     37.703 |    17.8
   20 |   1.0325 |     36.520 |   1.0638 |     37.266 |    18.7
   21 |   1.0080 |     35.711 |   1.0459 |     36.985 |    19.7
   22 |   0.9824 |     34.726 |   1.0346 |     35.955 |    20.6
   23 |   0.9563 |     33.350 |   1.0316 |     35.768 |    21.5
   24 |   0.9268 |     32.161 |   1.0093 |     33.801 |    22.5
   25 |   0.8998 |     31.258 |   0.9986 |     34.301 |    23.4
   26 |   0.8678 |     29.838 |   0.9965 |     33.958 |    24.4
   27 |   0.8345 |     28.396 |   0.9689 |     32.491 |    25.3
   28 |   0.8034 |     26.827 |   0.9531 |     31.117 |    26.3
   29 |   0.7744 |     25.716 |   0.9474 |     30.712 |    27.2
   30 |   0.7436 |     24.444 |   0.9347 |     30.961 |    28.1
   31 |   0.7142 |     23.327 |   0.9400 |     29.900 |    29.1
   32 |   0.6796 |     22.072 |   0.9433 |     30.087 |    30.0
   33 |   0.6604 |     21.433 |   0.9350 |     29.370 |    31.0
   34 |   0.6260 |     20.013 |   0.9325 |     29.463 |    31.9
   35 |   0.5902 |     18.714 |   0.9303 |     28.184 |    32.8
   36 |   0.5550 |     17.344 |   0.9457 |     28.777 |    33.8
   37 |   0.5296 |     16.650 |   0.9579 |     28.402 |    34.7
   38 |   0.4991 |     15.538 |   0.9556 |     28.152 |    35.7
   39 |   0.4672 |     14.680 |   0.9390 |     26.592 |    36.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 472,290

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0981 |     57.893 |   1.5001 |     45.350 |     0.0
    2 |   1.4099 |     45.861 |   1.3413 |     45.069 |     0.0
    3 |   1.3233 |     45.112 |   1.2927 |     43.851 |     0.1
    4 |   1.2865 |     44.210 |   1.2654 |     43.976 |     0.1
    5 |   1.2565 |     43.923 |   1.2407 |     43.040 |     0.1
    6 |   1.2314 |     43.213 |   1.2082 |     42.228 |     0.1
    7 |   1.2084 |     42.514 |   1.1889 |     41.355 |     0.1
    8 |   1.1883 |     41.964 |   1.1690 |     41.386 |     0.2
    9 |   1.1649 |     41.127 |   1.1596 |     40.730 |     0.2
   10 |   1.1478 |     40.979 |   1.1455 |     40.356 |     0.2
   11 |   1.1331 |     40.417 |   1.1288 |     40.169 |     0.2
   12 |   1.1114 |     39.261 |   1.1141 |     39.295 |     0.3
   13 |   1.0954 |     38.661 |   1.1176 |     39.482 |     0.3
   14 |   1.0795 |     38.144 |   1.0876 |     38.233 |     0.3
   15 |   1.0605 |     37.456 |   1.0833 |     37.422 |     0.3
   16 |   1.0444 |     36.911 |   1.0687 |     37.422 |     0.3
   17 |   1.0257 |     36.036 |   1.0467 |     36.767 |     0.4
   18 |   1.0074 |     35.794 |   1.0446 |     35.986 |     0.4
   19 |   0.9921 |     34.764 |   1.0311 |     35.737 |     0.4
   20 |   0.9706 |     33.928 |   1.0181 |     34.519 |     0.4
   21 |   0.9533 |     33.273 |   1.0101 |     34.707 |     0.4
   22 |   0.9351 |     32.596 |   0.9924 |     33.427 |     0.5
   23 |   0.9138 |     31.682 |   0.9871 |     34.082 |     0.5
   24 |   0.8976 |     30.906 |   0.9871 |     33.833 |     0.5
   25 |   0.8789 |     29.926 |   0.9627 |     32.210 |     0.5
   26 |   0.8568 |     29.156 |   0.9455 |     31.804 |     0.6
   27 |   0.8418 |     28.699 |   0.9502 |     31.991 |     0.6
   28 |   0.8268 |     28.236 |   0.9377 |     30.899 |     0.6
   29 |   0.8036 |     27.279 |   0.9275 |     30.712 |     0.6
   30 |   0.7837 |     26.618 |   0.9206 |     30.150 |     0.6
   31 |   0.7696 |     25.941 |   0.9174 |     31.024 |     0.7
   32 |   0.7496 |     25.264 |   0.9054 |     29.432 |     0.7
   33 |   0.7205 |     24.378 |   0.9043 |     29.619 |     0.7
   34 |   0.7144 |     24.070 |   0.8972 |     29.151 |     0.7
   35 |   0.6961 |     23.492 |   0.8969 |     29.151 |     0.7
   36 |   0.6741 |     22.292 |   0.9000 |     29.682 |     0.8
   37 |   0.6528 |     21.951 |   0.8868 |     28.652 |     0.8
   38 |   0.6329 |     21.197 |   0.8926 |     29.057 |     0.8
   39 |   0.6122 |     20.189 |   0.8983 |     28.652 |     0.8
   40 |   0.6040 |     20.277 |   0.9010 |     28.184 |     0.9
   41 |   0.5867 |     19.309 |   0.8812 |     28.090 |     0.9
   42 |   0.5671 |     18.764 |   0.8938 |     28.589 |     0.9
   43 |   0.5532 |     18.400 |   0.9060 |     28.652 |     0.9
   44 |   0.5391 |     18.153 |   0.8909 |     28.059 |     0.9
   45 |   0.5154 |     17.030 |   0.9067 |     28.090 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,588,418

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5701 |     68.257 |   2.0252 |     53.246 |     0.8
    2 |   1.8164 |     51.822 |   1.5990 |     45.350 |     1.8
    3 |   1.5231 |     46.153 |   1.4620 |     45.350 |     2.7
    4 |   1.4460 |     46.235 |   1.4218 |     45.350 |     3.6
    5 |   1.4183 |     46.131 |   1.4040 |     45.350 |     4.6
    6 |   1.4041 |     46.164 |   1.3858 |     45.350 |     5.5
    7 |   1.3837 |     46.092 |   1.3626 |     45.350 |     6.5
    8 |   1.3597 |     45.839 |   1.3365 |     44.257 |     7.4
    9 |   1.3355 |     45.316 |   1.3138 |     44.413 |     8.3
   10 |   1.3155 |     44.980 |   1.2948 |     44.351 |     9.3
   11 |   1.2979 |     44.617 |   1.2739 |     43.571 |    10.2
   12 |   1.2792 |     44.485 |   1.2572 |     44.132 |    11.1
   13 |   1.2646 |     44.050 |   1.2439 |     42.728 |    12.1
   14 |   1.2517 |     43.725 |   1.2228 |     42.915 |    13.0
   15 |   1.2412 |     43.626 |   1.2156 |     42.385 |    14.0
   16 |   1.2273 |     42.883 |   1.2090 |     42.041 |    14.9
   17 |   1.2173 |     42.564 |   1.1980 |     41.386 |    15.8
   18 |   1.2042 |     41.986 |   1.1815 |     41.167 |    16.8
   19 |   1.1955 |     41.650 |   1.1785 |     40.730 |    17.7
   20 |   1.1818 |     41.408 |   1.1663 |     40.262 |    18.6
   21 |   1.1695 |     41.100 |   1.1645 |     40.293 |    19.6
   22 |   1.1594 |     40.885 |   1.1560 |     40.231 |    20.5
   23 |   1.1498 |     40.786 |   1.1432 |     39.763 |    21.5
   24 |   1.1377 |     40.505 |   1.1347 |     39.576 |    22.4
   25 |   1.1261 |     39.960 |   1.1246 |     39.295 |    23.3
   26 |   1.1176 |     40.037 |   1.1148 |     39.451 |    24.3
   27 |   1.1055 |     39.168 |   1.1073 |     38.983 |    25.2
   28 |   1.0943 |     38.904 |   1.1062 |     39.669 |    26.1
   29 |   1.0819 |     38.397 |   1.0976 |     38.639 |    27.1
   30 |   1.0705 |     38.028 |   1.0840 |     38.296 |    28.0
   31 |   1.0613 |     37.450 |   1.0812 |     38.265 |    29.0
   32 |   1.0483 |     37.043 |   1.0827 |     38.358 |    29.9
   33 |   1.0413 |     36.729 |   1.0732 |     37.953 |    30.8
   34 |   1.0310 |     36.399 |   1.0707 |     37.578 |    31.8
   35 |   1.0196 |     35.805 |   1.0674 |     37.516 |    32.7
   36 |   1.0120 |     35.706 |   1.0647 |     37.609 |    33.6
   37 |   1.0006 |     35.111 |   1.0695 |     37.453 |    34.6
   38 |   0.9903 |     34.726 |   1.0468 |     36.704 |    35.5
   39 |   0.9831 |     34.451 |   1.0564 |     36.860 |    36.5
   40 |   0.9682 |     34.021 |   1.0506 |     36.642 |    37.4
   41 |   0.9569 |     33.361 |   1.0448 |     35.737 |    38.3
   42 |   0.9448 |     33.014 |   1.0464 |     35.830 |    39.3
   43 |   0.9299 |     32.321 |   1.0407 |     36.080 |    40.2
   44 |   0.9215 |     31.831 |   1.0369 |     35.737 |    41.1
   45 |   0.9058 |     30.961 |   1.0359 |     35.456 |    42.1
   46 |   0.8926 |     30.603 |   1.0389 |     35.206 |    43.0
   47 |   0.8879 |     30.295 |   1.0242 |     34.551 |    44.0
   48 |   0.8683 |     29.712 |   1.0292 |     34.395 |    44.9
   49 |   0.8563 |     29.205 |   1.0296 |     33.708 |    45.8
   50 |   0.8398 |     28.402 |   1.0229 |     34.332 |    46.8
   51 |   0.8231 |     27.983 |   1.0256 |     33.084 |    47.7
   52 |   0.8114 |     27.246 |   1.0149 |     32.303 |    48.7
   53 |   0.7989 |     26.761 |   1.0108 |     32.335 |    49.6
   54 |   0.7818 |     25.859 |   1.0092 |     32.179 |    50.5
   55 |   0.7642 |     25.303 |   1.0048 |     32.553 |    51.5
   56 |   0.7482 |     24.653 |   1.0148 |     31.617 |    52.4
   57 |   0.7313 |     24.059 |   1.0104 |     31.804 |    53.3
   58 |   0.7123 |     23.310 |   1.0171 |     30.993 |    54.3
   59 |   0.6986 |     22.974 |   1.0151 |     31.149 |    55.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 522,466

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4026 |     61.509 |   1.8100 |     47.878 |     0.0
    2 |   1.6025 |     47.446 |   1.4581 |     45.256 |     0.1
    3 |   1.4222 |     45.866 |   1.3750 |     44.757 |     0.1
    4 |   1.3619 |     45.200 |   1.3274 |     44.164 |     0.1
    5 |   1.3227 |     44.353 |   1.2943 |     44.195 |     0.2
    6 |   1.2947 |     44.243 |   1.2678 |     43.414 |     0.2
    7 |   1.2690 |     44.044 |   1.2432 |     42.790 |     0.2
    8 |   1.2465 |     43.340 |   1.2303 |     42.353 |     0.2
    9 |   1.2283 |     43.081 |   1.2054 |     42.104 |     0.3
   10 |   1.2087 |     42.223 |   1.1897 |     40.918 |     0.3
   11 |   1.1942 |     41.661 |   1.1790 |     39.700 |     0.3
   12 |   1.1756 |     41.012 |   1.1606 |     39.888 |     0.4
   13 |   1.1601 |     40.401 |   1.1466 |     39.419 |     0.4
   14 |   1.1476 |     39.971 |   1.1499 |     40.325 |     0.4
   15 |   1.1333 |     39.531 |   1.1285 |     39.170 |     0.5
   16 |   1.1202 |     39.162 |   1.1169 |     39.014 |     0.5
   17 |   1.1076 |     38.849 |   1.1128 |     38.702 |     0.5
   18 |   1.0937 |     38.105 |   1.1017 |     38.452 |     0.6
   19 |   1.0840 |     38.116 |   1.0938 |     38.608 |     0.6
   20 |   1.0755 |     37.775 |   1.0858 |     37.328 |     0.6
   21 |   1.0613 |     37.236 |   1.0755 |     37.609 |     0.7
   22 |   1.0525 |     37.170 |   1.0630 |     36.985 |     0.7
   23 |   1.0360 |     36.256 |   1.0532 |     35.861 |     0.7
   24 |   1.0275 |     35.843 |   1.0452 |     35.705 |     0.7
   25 |   1.0153 |     35.634 |   1.0390 |     34.863 |     0.8
   26 |   0.9990 |     34.654 |   1.0301 |     34.863 |     0.8
   27 |   0.9909 |     34.599 |   1.0238 |     34.769 |     0.8
   28 |   0.9776 |     33.680 |   1.0074 |     34.707 |     0.9
   29 |   0.9663 |     33.609 |   1.0046 |     34.238 |     0.9
   30 |   0.9508 |     33.086 |   0.9989 |     34.270 |     0.9
   31 |   0.9461 |     32.998 |   1.0230 |     35.206 |     1.0
   32 |   0.9461 |     32.805 |   0.9888 |     33.458 |     1.0
   33 |   0.9237 |     32.221 |   0.9811 |     33.302 |     1.0
   34 |   0.9134 |     31.627 |   0.9742 |     32.990 |     1.1
   35 |   0.8982 |     30.923 |   0.9765 |     33.052 |     1.1
   36 |   0.8863 |     30.411 |   0.9642 |     32.772 |     1.1
   37 |   0.8764 |     30.378 |   0.9616 |     31.866 |     1.2
   38 |   0.8679 |     29.882 |   0.9483 |     31.710 |     1.2
   39 |   0.8576 |     29.530 |   0.9450 |     31.617 |     1.2
   40 |   0.8403 |     28.561 |   0.9393 |     31.117 |     1.2
   41 |   0.8245 |     28.033 |   0.9383 |     30.868 |     1.3
   42 |   0.8237 |     28.044 |   0.9385 |     31.679 |     1.3
   43 |   0.8165 |     27.796 |   0.9429 |     31.554 |     1.3
   44 |   0.7960 |     26.734 |   0.9203 |     31.024 |     1.4
   45 |   0.7824 |     26.475 |   0.9200 |     29.931 |     1.4
   46 |   0.7686 |     26.095 |   0.9090 |     29.900 |     1.4
   47 |   0.7634 |     25.914 |   0.9141 |     30.743 |     1.5
   48 |   0.7615 |     25.446 |   0.9231 |     30.556 |     1.5
   49 |   0.7565 |     25.275 |   0.9237 |     30.368 |     1.5
   50 |   0.7359 |     24.527 |   0.9095 |     29.869 |     1.6
   51 |   0.7186 |     23.971 |   0.9079 |     30.181 |     1.6
   52 |   0.7079 |     23.811 |   0.9096 |     29.526 |     1.6
   53 |   0.6893 |     22.919 |   0.9122 |     29.089 |     1.7
   54 |   0.6835 |     22.633 |   0.8982 |     29.057 |     1.7
   55 |   0.6691 |     22.050 |   0.9085 |     28.839 |     1.7
   56 |   0.6585 |     21.670 |   0.9084 |     29.276 |     1.7
   57 |   0.6465 |     21.433 |   0.9065 |     28.683 |     1.8
   58 |   0.6420 |     21.356 |   0.9005 |     28.496 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,853,250

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0768 |     57.216 |   1.5198 |     45.350 |     0.8
    2 |   1.4346 |     45.921 |   1.3653 |     44.757 |     1.8
    3 |   1.3512 |     45.233 |   1.3190 |     43.945 |     2.7
    4 |   1.3110 |     44.755 |   1.2793 |     43.976 |     3.7
    5 |   1.2808 |     44.452 |   1.2613 |     44.288 |     4.6
    6 |   1.2577 |     44.325 |   1.2405 |     43.258 |     5.6
    7 |   1.2333 |     43.769 |   1.2164 |     43.009 |     6.5
    8 |   1.2149 |     43.059 |   1.1937 |     41.448 |     7.4
    9 |   1.1988 |     42.470 |   1.1780 |     41.479 |     8.4
   10 |   1.1802 |     42.107 |   1.1634 |     40.980 |     9.3
   11 |   1.1668 |     41.529 |   1.1537 |     40.793 |    10.3
   12 |   1.1501 |     40.847 |   1.1400 |     40.325 |    11.2
   13 |   1.1389 |     40.357 |   1.1217 |     40.481 |    12.1
   14 |   1.1219 |     39.927 |   1.1135 |     40.169 |    13.1
   15 |   1.1091 |     39.410 |   1.1117 |     38.514 |    14.0
   16 |   1.0986 |     38.860 |   1.0977 |     38.826 |    15.0
   17 |   1.0866 |     38.882 |   1.0940 |     38.795 |    15.9
   18 |   1.0816 |     38.337 |   1.0813 |     38.109 |    16.8
   19 |   1.0656 |     38.149 |   1.0692 |     37.484 |    17.8
   20 |   1.0565 |     37.621 |   1.0608 |     37.734 |    18.7
   21 |   1.0464 |     37.225 |   1.0591 |     37.953 |    19.7
   22 |   1.0412 |     37.170 |   1.0510 |     36.891 |    20.6
   23 |   1.0262 |     36.421 |   1.0347 |     36.423 |    21.6
   24 |   1.0173 |     36.394 |   1.0333 |     36.610 |    22.5
   25 |   1.0072 |     35.986 |   1.0264 |     36.267 |    23.4
   26 |   0.9926 |     35.530 |   1.0163 |     36.517 |    24.4
   27 |   0.9778 |     34.830 |   1.0001 |     35.518 |    25.3
   28 |   0.9665 |     34.379 |   1.0102 |     35.518 |    26.3
   29 |   0.9538 |     33.972 |   1.0068 |     35.081 |    27.2
   30 |   0.9422 |     33.438 |   0.9899 |     34.426 |    28.2
   31 |   0.9352 |     33.245 |   0.9739 |     34.551 |    29.1
   32 |   0.9205 |     32.519 |   0.9763 |     34.457 |    30.0
   33 |   0.9124 |     32.315 |   0.9543 |     32.959 |    31.0
   34 |   0.9005 |     31.627 |   0.9673 |     33.365 |    31.9
   35 |   0.8979 |     31.583 |   0.9566 |     33.333 |    32.9
   36 |   0.9025 |     31.622 |   0.9657 |     33.458 |    33.8
   37 |   0.8762 |     30.862 |   0.9364 |     32.772 |    34.7
   38 |   0.8586 |     30.042 |   0.9385 |     32.397 |    35.7
   39 |   0.8609 |     30.290 |   0.9414 |     32.179 |    36.6
   40 |   0.8456 |     29.403 |   0.9201 |     31.305 |    37.6
   41 |   0.8257 |     28.704 |   0.9221 |     30.462 |    38.5
   42 |   0.8227 |     28.435 |   0.9137 |     31.398 |    39.5
   43 |   0.8043 |     28.011 |   0.9079 |     30.774 |    40.4
   44 |   0.7983 |     27.504 |   0.9065 |     30.649 |    41.3
   45 |   0.7813 |     26.921 |   0.8900 |     29.838 |    42.3
   46 |   0.7651 |     26.062 |   0.8953 |     29.806 |    43.2
   47 |   0.7548 |     25.870 |   0.9012 |     30.337 |    44.2
   48 |   0.7466 |     25.501 |   0.8925 |     29.588 |    45.1
   49 |   0.7291 |     24.912 |   0.8872 |     29.526 |    46.1
   50 |   0.7255 |     24.895 |   0.8811 |     29.276 |    47.0
   51 |   0.7116 |     24.213 |   0.8712 |     29.245 |    47.9
   52 |   0.6962 |     23.734 |   0.8933 |     30.025 |    48.9
   53 |   0.6861 |     23.184 |   0.8805 |     28.933 |    49.8
   54 |   0.6753 |     22.826 |   0.8825 |     28.995 |    50.8
   55 |   0.6583 |     22.160 |   0.8781 |     28.527 |    51.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,229,730

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4365 |     64.834 |   1.8930 |     48.627 |     0.6
    2 |   1.6606 |     48.085 |   1.4940 |     45.350 |     1.3
    3 |   1.4554 |     46.169 |   1.4099 |     45.350 |     2.0
    4 |   1.3985 |     46.065 |   1.3666 |     45.006 |     2.8
    5 |   1.3611 |     45.855 |   1.3420 |     44.819 |     3.5
    6 |   1.3313 |     45.184 |   1.3096 |     43.914 |     4.2
    7 |   1.3071 |     44.793 |   1.2830 |     43.789 |     4.9
    8 |   1.2872 |     44.111 |   1.2729 |     43.571 |     5.6
    9 |   1.2703 |     43.736 |   1.2564 |     43.352 |     6.3
   10 |   1.2573 |     43.389 |   1.2401 |     42.478 |     7.0
   11 |   1.2456 |     43.202 |   1.2281 |     41.916 |     7.7
   12 |   1.2304 |     42.701 |   1.2233 |     41.635 |     8.4
   13 |   1.2177 |     42.410 |   1.2092 |     41.511 |     9.1
   14 |   1.2048 |     41.612 |   1.2045 |     41.479 |     9.8
   15 |   1.1908 |     41.083 |   1.1847 |     40.699 |    10.5
   16 |   1.1795 |     40.648 |   1.1788 |     40.387 |    11.2
   17 |   1.1654 |     39.993 |   1.1668 |     39.981 |    12.0
   18 |   1.1551 |     39.779 |   1.1528 |     38.826 |    12.7
   19 |   1.1407 |     39.625 |   1.1524 |     38.826 |    13.4
   20 |   1.1274 |     39.074 |   1.1424 |     38.577 |    14.1
   21 |   1.1173 |     38.678 |   1.1366 |     38.546 |    14.8
   22 |   1.1044 |     38.502 |   1.1242 |     38.046 |    15.5
   23 |   1.0912 |     37.874 |   1.1183 |     38.046 |    16.2
   24 |   1.0797 |     37.500 |   1.1042 |     37.079 |    16.9
   25 |   1.0654 |     37.285 |   1.0872 |     36.423 |    17.6
   26 |   1.0475 |     36.278 |   1.0851 |     37.110 |    18.3
   27 |   1.0352 |     36.008 |   1.0724 |     36.111 |    19.0
   28 |   1.0233 |     35.474 |   1.0635 |     36.111 |    19.7
   29 |   1.0067 |     34.930 |   1.0677 |     36.267 |    20.4
   30 |   0.9872 |     33.999 |   1.0399 |     34.707 |    21.1
   31 |   0.9719 |     33.355 |   1.0318 |     34.426 |    21.8
   32 |   0.9551 |     32.480 |   1.0163 |     34.301 |    22.5
   33 |   0.9343 |     31.902 |   1.0159 |     34.363 |    23.3
   34 |   0.9128 |     30.735 |   1.0107 |     33.895 |    24.0
   35 |   0.9020 |     30.367 |   0.9989 |     33.521 |    24.7
   36 |   0.8804 |     29.607 |   0.9961 |     33.739 |    25.4
   37 |   0.8605 |     29.117 |   0.9890 |     33.396 |    26.1
   38 |   0.8426 |     28.269 |   0.9740 |     32.803 |    26.8
   39 |   0.8254 |     27.323 |   0.9746 |     32.366 |    27.5
   40 |   0.8110 |     26.965 |   0.9680 |     32.428 |    28.2
   41 |   0.7969 |     26.343 |   0.9757 |     31.710 |    28.9
   42 |   0.7701 |     25.286 |   0.9646 |     31.960 |    29.6
   43 |   0.7576 |     25.088 |   0.9475 |     31.461 |    30.3
   44 |   0.7361 |     24.037 |   0.9605 |     30.712 |    31.0
   45 |   0.7100 |     23.233 |   0.9626 |     31.117 |    31.7
   46 |   0.7018 |     22.897 |   0.9472 |     30.836 |    32.4
   47 |   0.6867 |     22.308 |   0.9413 |     29.931 |    33.2
   48 |   0.6555 |     21.329 |   0.9540 |     29.963 |    33.9
   49 |   0.6401 |     20.844 |   0.9580 |     29.432 |    34.6
   50 |   0.6244 |     20.079 |   0.9630 |     30.587 |    35.3
   51 |   0.6100 |     19.947 |   0.9616 |     30.087 |    36.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 853,538

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4962 |     66.661 |   1.9300 |     55.306 |     0.4
    2 |   1.7203 |     49.246 |   1.5388 |     45.350 |     0.9
    3 |   1.4792 |     46.296 |   1.4317 |     45.350 |     1.4
    4 |   1.4141 |     46.290 |   1.3790 |     45.350 |     1.8
    5 |   1.3674 |     46.048 |   1.3392 |     45.287 |     2.3
    6 |   1.3363 |     45.740 |   1.3110 |     44.632 |     2.8
    7 |   1.3131 |     45.068 |   1.2877 |     44.288 |     3.2
    8 |   1.2937 |     44.567 |   1.2698 |     43.914 |     3.7
    9 |   1.2741 |     43.934 |   1.2478 |     43.352 |     4.2
   10 |   1.2598 |     43.676 |   1.2399 |     42.135 |     4.6
   11 |   1.2485 |     43.290 |   1.2262 |     42.291 |     5.1
   12 |   1.2357 |     42.988 |   1.2189 |     42.197 |     5.6
   13 |   1.2264 |     42.784 |   1.2087 |     41.854 |     6.1
   14 |   1.2183 |     42.619 |   1.1994 |     41.355 |     6.5
   15 |   1.2080 |     42.190 |   1.1956 |     41.042 |     7.0
   16 |   1.2020 |     42.338 |   1.1842 |     41.042 |     7.5
   17 |   1.1899 |     42.074 |   1.1789 |     41.105 |     7.9
   18 |   1.1828 |     41.606 |   1.1685 |     40.543 |     8.4
   19 |   1.1751 |     41.507 |   1.1606 |     40.262 |     8.9
   20 |   1.1642 |     41.034 |   1.1604 |     40.980 |     9.3
   21 |   1.1565 |     41.105 |   1.1495 |     40.137 |     9.8
   22 |   1.1498 |     40.709 |   1.1388 |     39.919 |    10.3
   23 |   1.1400 |     40.450 |   1.1371 |     40.262 |    10.7
   24 |   1.1359 |     40.070 |   1.1269 |     39.419 |    11.2
   25 |   1.1258 |     39.685 |   1.1269 |     39.232 |    11.7
   26 |   1.1192 |     39.586 |   1.1214 |     39.544 |    12.2
   27 |   1.1106 |     39.223 |   1.1149 |     39.045 |    12.6
   28 |   1.1061 |     39.003 |   1.1117 |     38.670 |    13.1
   29 |   1.0995 |     38.793 |   1.1048 |     39.076 |    13.6
   30 |   1.0908 |     38.546 |   1.0999 |     38.733 |    14.0
   31 |   1.0847 |     38.199 |   1.0936 |     38.358 |    14.5
   32 |   1.0768 |     37.869 |   1.1056 |     38.889 |    15.0
   33 |   1.0736 |     37.726 |   1.0907 |     37.984 |    15.4
   34 |   1.0604 |     37.351 |   1.0791 |     37.953 |    15.9
   35 |   1.0544 |     37.142 |   1.0818 |     38.327 |    16.4
   36 |   1.0458 |     36.790 |   1.0798 |     37.984 |    16.9
   37 |   1.0391 |     36.366 |   1.0758 |     37.266 |    17.3
   38 |   1.0376 |     36.515 |   1.0636 |     36.767 |    17.8
   39 |   1.0234 |     36.229 |   1.0656 |     37.235 |    18.3
   40 |   1.0151 |     35.722 |   1.0605 |     36.610 |    18.7
   41 |   1.0069 |     35.408 |   1.0487 |     36.610 |    19.2
   42 |   1.0018 |     35.084 |   1.0487 |     36.236 |    19.7
   43 |   0.9916 |     34.814 |   1.0408 |     36.454 |    20.1
   44 |   0.9836 |     34.599 |   1.0398 |     35.737 |    20.6
   45 |   0.9738 |     33.966 |   1.0382 |     36.142 |    21.1
   46 |   0.9641 |     33.790 |   1.0261 |     36.142 |    21.6
   47 |   0.9556 |     33.554 |   1.0285 |     35.986 |    22.0
   48 |   0.9438 |     33.042 |   1.0324 |     35.737 |    22.5
   49 |   0.9385 |     32.899 |   1.0208 |     35.643 |    23.0
   50 |   0.9241 |     32.166 |   1.0166 |     34.956 |    23.4
   51 |   0.9188 |     31.990 |   1.0085 |     34.114 |    23.9
   52 |   0.9113 |     31.682 |   1.0003 |     35.050 |    24.4
   53 |   0.8987 |     31.198 |   1.0062 |     33.926 |    24.8
   54 |   0.8900 |     30.895 |   0.9932 |     34.176 |    25.3
   55 |   0.8865 |     30.548 |   0.9968 |     34.114 |    25.8
   56 |   0.8738 |     30.064 |   0.9913 |     33.926 |    26.3
   57 |   0.8669 |     29.684 |   0.9902 |     33.302 |    26.7
   58 |   0.8551 |     29.734 |   0.9743 |     33.552 |    27.2
   59 |   0.8423 |     28.721 |   0.9750 |     32.584 |    27.7
   60 |   0.8348 |     29.029 |   0.9761 |     32.366 |    28.1
   61 |   0.8239 |     28.330 |   0.9657 |     32.147 |    28.6
   62 |   0.8156 |     27.846 |   0.9754 |     32.335 |    29.1
   63 |   0.8086 |     27.670 |   0.9710 |     32.116 |    29.6
   64 |   0.7960 |     26.959 |   0.9596 |     32.553 |    30.0
   65 |   0.7834 |     26.415 |   0.9599 |     31.866 |    30.5
   66 |   0.7788 |     26.293 |   0.9642 |     31.835 |    31.0
   67 |   0.7687 |     26.079 |   0.9607 |     31.648 |    31.4
   68 |   0.7581 |     25.275 |   0.9550 |     30.930 |    31.9
   69 |   0.7498 |     25.154 |   0.9633 |     31.367 |    32.4
   70 |   0.7363 |     24.741 |   0.9411 |     30.743 |    32.8
   71 |   0.7329 |     24.549 |   0.9412 |     30.056 |    33.3
   72 |   0.7166 |     23.679 |   0.9545 |     30.337 |    33.8
   73 |   0.7042 |     23.833 |   0.9618 |     30.556 |    34.2
   74 |   0.7026 |     23.580 |   0.9647 |     30.181 |    34.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 324,322

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5166 |     64.476 |   1.9608 |     53.340 |     0.0
    2 |   1.7410 |     49.510 |   1.5462 |     45.350 |     0.0
    3 |   1.4843 |     46.202 |   1.4323 |     45.350 |     0.0
    4 |   1.4206 |     46.191 |   1.3938 |     45.350 |     0.1
    5 |   1.3929 |     46.131 |   1.3683 |     45.350 |     0.1
    6 |   1.3680 |     45.674 |   1.3449 |     44.757 |     0.1
    7 |   1.3481 |     45.310 |   1.3292 |     44.569 |     0.1
    8 |   1.3342 |     45.118 |   1.3183 |     44.725 |     0.1
    9 |   1.3194 |     44.876 |   1.3013 |     43.976 |     0.1
   10 |   1.3023 |     44.562 |   1.2837 |     43.851 |     0.2
   11 |   1.2860 |     44.413 |   1.2704 |     44.288 |     0.2
   12 |   1.2735 |     44.226 |   1.2527 |     43.071 |     0.2
   13 |   1.2615 |     44.122 |   1.2428 |     43.664 |     0.2
   14 |   1.2472 |     44.039 |   1.2226 |     42.790 |     0.2
   15 |   1.2310 |     43.351 |   1.2115 |     42.790 |     0.2
   16 |   1.2141 |     42.679 |   1.2007 |     41.511 |     0.3
   17 |   1.2013 |     42.107 |   1.1892 |     40.824 |     0.3
   18 |   1.1855 |     41.529 |   1.1726 |     40.824 |     0.3
   19 |   1.1722 |     41.039 |   1.1541 |     39.856 |     0.3
   20 |   1.1575 |     40.665 |   1.1540 |     40.512 |     0.3
   21 |   1.1455 |     39.960 |   1.1359 |     39.700 |     0.3
   22 |   1.1301 |     39.548 |   1.1283 |     38.639 |     0.4
   23 |   1.1142 |     38.920 |   1.1093 |     37.953 |     0.4
   24 |   1.1008 |     38.326 |   1.0987 |     38.171 |     0.4
   25 |   1.0846 |     37.770 |   1.0825 |     36.704 |     0.4
   26 |   1.0646 |     36.851 |   1.0771 |     36.454 |     0.4
   27 |   1.0495 |     36.251 |   1.0599 |     35.893 |     0.4
   28 |   1.0378 |     35.717 |   1.0569 |     35.456 |     0.5
   29 |   1.0245 |     35.309 |   1.0441 |     35.144 |     0.5
   30 |   1.0061 |     34.429 |   1.0359 |     35.799 |     0.5
   31 |   0.9963 |     34.170 |   1.0288 |     35.144 |     0.5
   32 |   0.9803 |     33.339 |   1.0261 |     34.457 |     0.5
   33 |   0.9608 |     32.887 |   1.0038 |     34.270 |     0.5
   34 |   0.9518 |     32.530 |   1.0141 |     34.675 |     0.6
   35 |   0.9429 |     32.051 |   1.0032 |     34.082 |     0.6
   36 |   0.9234 |     31.280 |   0.9885 |     32.990 |     0.6
   37 |   0.9052 |     30.609 |   0.9835 |     32.896 |     0.6
   38 |   0.8881 |     30.080 |   0.9828 |     32.647 |     0.6
   39 |   0.8755 |     29.194 |   0.9754 |     32.584 |     0.7
   40 |   0.8618 |     28.682 |   0.9864 |     33.146 |     0.7
   41 |   0.8541 |     28.847 |   0.9729 |     32.054 |     0.7
   42 |   0.8325 |     27.802 |   0.9694 |     32.241 |     0.7
   43 |   0.8226 |     27.207 |   0.9663 |     31.586 |     0.7
   44 |   0.8141 |     27.059 |   0.9527 |     31.461 |     0.7
   45 |   0.7966 |     26.277 |   0.9592 |     31.086 |     0.8
   46 |   0.7864 |     26.338 |   0.9563 |     30.680 |     0.8
   47 |   0.7690 |     25.264 |   0.9637 |     31.586 |     0.8
   48 |   0.7588 |     25.044 |   0.9522 |     30.618 |     0.8
   49 |   0.7556 |     24.851 |   0.9657 |     31.086 |     0.8
   50 |   0.7433 |     24.736 |   0.9535 |     30.524 |     0.8
   51 |   0.7341 |     24.384 |   0.9502 |     30.337 |     0.9
   52 |   0.7158 |     23.486 |   0.9551 |     30.150 |     0.9
   53 |   0.7085 |     23.085 |   0.9480 |     29.869 |     0.9
   54 |   0.6992 |     23.101 |   0.9580 |     29.713 |     0.9
   55 |   0.6845 |     22.578 |   0.9530 |     29.213 |     0.9
   56 |   0.6775 |     22.105 |   0.9521 |     29.838 |     0.9
   57 |   0.6661 |     21.764 |   0.9458 |     29.089 |     1.0
   58 |   0.6539 |     21.373 |   0.9443 |     29.089 |     1.0
   59 |   0.6634 |     21.609 |   0.9451 |     29.338 |     1.0
   60 |   0.6449 |     21.087 |   0.9320 |     28.308 |     1.0
   61 |   0.6293 |     20.354 |   0.9477 |     28.808 |     1.0
   62 |   0.6229 |     20.459 |   0.9704 |     29.619 |     1.0
   63 |   0.6136 |     19.639 |   0.9360 |     28.589 |     1.1
   64 |   0.6073 |     19.677 |   0.9649 |     28.340 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 772,130

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2146 |     60.673 |   1.6034 |     45.350 |     0.0
    2 |   1.4827 |     46.136 |   1.4204 |     45.350 |     0.1
    3 |   1.4040 |     46.285 |   1.3792 |     45.787 |     0.1
    4 |   1.3705 |     45.657 |   1.3437 |     45.381 |     0.1
    5 |   1.3434 |     45.866 |   1.3186 |     44.507 |     0.2
    6 |   1.3209 |     45.674 |   1.3015 |     43.695 |     0.2
    7 |   1.3030 |     44.843 |   1.2928 |     44.288 |     0.2
    8 |   1.2881 |     44.672 |   1.2902 |     44.195 |     0.3
    9 |   1.2737 |     44.309 |   1.2588 |     43.477 |     0.3
   10 |   1.2569 |     43.951 |   1.2499 |     43.414 |     0.3
   11 |   1.2383 |     43.439 |   1.2224 |     41.573 |     0.4
   12 |   1.2188 |     42.839 |   1.2035 |     42.010 |     0.4
   13 |   1.1986 |     42.366 |   1.1915 |     41.011 |     0.4
   14 |   1.1750 |     41.573 |   1.1605 |     40.293 |     0.4
   15 |   1.1434 |     40.368 |   1.1549 |     40.012 |     0.5
   16 |   1.1138 |     38.876 |   1.1145 |     37.547 |     0.5
   17 |   1.0856 |     38.045 |   1.0981 |     38.109 |     0.5
   18 |   1.0505 |     36.537 |   1.0769 |     36.080 |     0.6
   19 |   1.0214 |     35.552 |   1.0615 |     36.111 |     0.6
   20 |   0.9927 |     34.049 |   1.0498 |     35.487 |     0.6
   21 |   0.9609 |     32.899 |   1.0373 |     34.894 |     0.7
   22 |   0.9270 |     31.231 |   1.0223 |     34.363 |     0.7
   23 |   0.8974 |     30.207 |   1.0080 |     33.583 |     0.7
   24 |   0.8598 |     28.578 |   0.9972 |     32.834 |     0.8
   25 |   0.8293 |     27.081 |   0.9906 |     32.272 |     0.8
   26 |   0.7945 |     25.837 |   0.9880 |     31.898 |     0.8
   27 |   0.7556 |     24.295 |   0.9922 |     31.461 |     0.9
   28 |   0.7301 |     23.536 |   1.0024 |     31.929 |     0.9
   29 |   0.7009 |     22.534 |   0.9749 |     31.149 |     0.9
   30 |   0.6633 |     21.120 |   0.9798 |     30.462 |     1.0
   31 |   0.6274 |     19.909 |   0.9897 |     30.649 |     1.0
   32 |   0.6079 |     19.358 |   0.9871 |     30.649 |     1.0
   33 |   0.5753 |     18.037 |   0.9946 |     29.463 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 423,842

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4037 |     63.105 |   1.8360 |     49.220 |     0.0
    2 |   1.6259 |     46.857 |   1.4761 |     45.350 |     0.1
    3 |   1.4342 |     45.982 |   1.3883 |     45.256 |     0.1
    4 |   1.3741 |     45.624 |   1.3463 |     44.476 |     0.1
    5 |   1.3371 |     45.178 |   1.3117 |     44.413 |     0.1
    6 |   1.3128 |     44.881 |   1.2928 |     44.132 |     0.2
    7 |   1.2956 |     44.006 |   1.2789 |     43.227 |     0.2
    8 |   1.2818 |     43.912 |   1.2667 |     43.539 |     0.2
    9 |   1.2667 |     43.637 |   1.2521 |     42.790 |     0.2
   10 |   1.2533 |     43.224 |   1.2388 |     42.322 |     0.3
   11 |   1.2376 |     42.977 |   1.2281 |     42.541 |     0.3
   12 |   1.2254 |     43.257 |   1.2203 |     42.072 |     0.3
   13 |   1.2105 |     42.443 |   1.2015 |     41.136 |     0.3
   14 |   1.1986 |     41.760 |   1.1877 |     41.136 |     0.4
   15 |   1.1855 |     41.270 |   1.1763 |     40.605 |     0.4
   16 |   1.1713 |     41.023 |   1.1692 |     40.824 |     0.4
   17 |   1.1565 |     40.170 |   1.1528 |     40.044 |     0.4
   18 |   1.1452 |     39.977 |   1.1477 |     40.044 |     0.5
   19 |   1.1302 |     39.371 |   1.1284 |     39.388 |     0.5
   20 |   1.1202 |     39.382 |   1.1196 |     39.201 |     0.5
   21 |   1.1030 |     38.601 |   1.1101 |     38.390 |     0.6
   22 |   1.0893 |     37.962 |   1.1197 |     39.201 |     0.6
   23 |   1.0747 |     37.274 |   1.1005 |     37.734 |     0.6
   24 |   1.0539 |     36.515 |   1.0930 |     37.703 |     0.6
   25 |   1.0353 |     35.744 |   1.0794 |     37.360 |     0.7
   26 |   1.0216 |     35.249 |   1.0733 |     36.548 |     0.7
   27 |   1.0045 |     34.385 |   1.0616 |     36.049 |     0.7
   28 |   0.9901 |     34.142 |   1.0468 |     35.737 |     0.7
   29 |   0.9669 |     33.113 |   1.0385 |     34.988 |     0.8
   30 |   0.9486 |     32.343 |   1.0253 |     34.738 |     0.8
   31 |   0.9337 |     31.710 |   1.0230 |     34.238 |     0.8
   32 |   0.9099 |     30.598 |   1.0221 |     33.958 |     0.8
   33 |   0.8936 |     30.196 |   0.9976 |     33.302 |     0.9
   34 |   0.8754 |     29.348 |   1.0031 |     33.864 |     0.9
   35 |   0.8583 |     28.616 |   1.0009 |     32.990 |     0.9
   36 |   0.8366 |     27.769 |   0.9757 |     31.835 |     1.0
   37 |   0.8127 |     27.009 |   0.9796 |     31.804 |     1.0
   38 |   0.7971 |     26.260 |   0.9792 |     32.210 |     1.0
   39 |   0.7761 |     25.473 |   0.9767 |     32.553 |     1.0
   40 |   0.7556 |     24.686 |   0.9674 |     32.147 |     1.1
   41 |   0.7405 |     24.251 |   0.9617 |     31.149 |     1.1
   42 |   0.7207 |     23.321 |   0.9698 |     31.149 |     1.1
   43 |   0.7019 |     22.611 |   0.9529 |     31.117 |     1.1
   44 |   0.6807 |     21.659 |   0.9517 |     31.086 |     1.2
   45 |   0.6604 |     21.109 |   0.9522 |     30.743 |     1.2
   46 |   0.6446 |     20.756 |   0.9567 |     30.337 |     1.2
   47 |   0.6373 |     20.492 |   0.9537 |     30.243 |     1.2
   48 |   0.6174 |     19.898 |   0.9595 |     30.400 |     1.3
Early stopping

