Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,213,218

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5300 |     69.208 |   1.9877 |     58.654 |     0.1
    2 |   1.7705 |     50.199 |   1.5535 |     45.963 |     0.1
    3 |   1.4964 |     46.105 |   1.4267 |     45.963 |     0.2
    4 |   1.4171 |     46.067 |   1.3752 |     45.872 |     0.2
    5 |   1.3759 |     45.752 |   1.3377 |     45.321 |     0.3
    6 |   1.3315 |     45.437 |   1.3112 |     45.199 |     0.4
    7 |   1.3056 |     45.039 |   1.2911 |     44.618 |     0.4
    8 |   1.2847 |     44.647 |   1.2686 |     44.312 |     0.5
    9 |   1.2644 |     44.106 |   1.2594 |     43.547 |     0.5
   10 |   1.2477 |     44.023 |   1.2355 |     43.761 |     0.6
   11 |   1.2321 |     43.481 |   1.2278 |     43.578 |     0.6
   12 |   1.2172 |     43.310 |   1.2156 |     42.813 |     0.7
   13 |   1.2081 |     43.161 |   1.2135 |     43.150 |     0.8
   14 |   1.1986 |     42.752 |   1.1969 |     42.783 |     0.8
   15 |   1.1906 |     42.614 |   1.1952 |     41.927 |     0.9
   16 |   1.1826 |     42.244 |   1.1842 |     41.988 |     0.9
   17 |   1.1735 |     42.161 |   1.1761 |     42.080 |     1.0
   18 |   1.1639 |     41.592 |   1.1762 |     41.590 |     1.1
   19 |   1.1558 |     41.437 |   1.1644 |     40.734 |     1.1
   20 |   1.1508 |     40.962 |   1.1557 |     41.407 |     1.2
   21 |   1.1394 |     40.509 |   1.1566 |     40.979 |     1.2
   22 |   1.1325 |     40.288 |   1.1485 |     40.979 |     1.3
   23 |   1.1261 |     39.946 |   1.1462 |     40.734 |     1.4
   24 |   1.1161 |     39.637 |   1.1334 |     40.061 |     1.4
   25 |   1.1127 |     39.631 |   1.1326 |     39.511 |     1.5
   26 |   1.1078 |     39.449 |   1.1300 |     39.755 |     1.5
   27 |   1.1005 |     39.112 |   1.1268 |     39.602 |     1.6
   28 |   1.0937 |     38.913 |   1.1275 |     39.786 |     1.7
   29 |   1.0910 |     38.946 |   1.1078 |     39.266 |     1.7
   30 |   1.0841 |     38.537 |   1.1115 |     38.991 |     1.8
   31 |   1.0825 |     38.427 |   1.1095 |     38.869 |     1.8
   32 |   1.0725 |     38.090 |   1.0952 |     37.768 |     1.9
   33 |   1.0663 |     37.963 |   1.0983 |     37.706 |     2.0
   34 |   1.0655 |     37.863 |   1.0949 |     38.654 |     2.0
   35 |   1.0610 |     37.642 |   1.0816 |     37.615 |     2.1
   36 |   1.0542 |     37.499 |   1.0830 |     38.135 |     2.1
   37 |   1.0483 |     37.477 |   1.0777 |     37.554 |     2.2
   38 |   1.0404 |     37.145 |   1.0756 |     37.125 |     2.3
   39 |   1.0357 |     36.930 |   1.0778 |     37.768 |     2.3
   40 |   1.0344 |     37.079 |   1.0653 |     37.401 |     2.4
   41 |   1.0333 |     36.941 |   1.0751 |     37.768 |     2.4
   42 |   1.0305 |     36.941 |   1.0709 |     37.309 |     2.5
   43 |   1.0235 |     36.488 |   1.0661 |     37.064 |     2.6
   44 |   1.0198 |     36.482 |   1.0631 |     36.606 |     2.6
   45 |   1.0108 |     36.118 |   1.0571 |     36.911 |     2.7
   46 |   1.0086 |     35.863 |   1.0542 |     36.544 |     2.7
   47 |   1.0045 |     35.731 |   1.0532 |     36.728 |     2.8
   48 |   0.9986 |     35.549 |   1.0499 |     36.697 |     2.8
   49 |   0.9987 |     35.493 |   1.0485 |     36.300 |     2.9
   50 |   0.9949 |     35.344 |   1.0489 |     36.606 |     3.0
   51 |   0.9913 |     35.394 |   1.0509 |     36.361 |     3.0
   52 |   0.9868 |     35.278 |   1.0510 |     36.544 |     3.1
   53 |   0.9812 |     34.919 |   1.0461 |     36.086 |     3.1
   54 |   0.9739 |     34.996 |   1.0481 |     36.606 |     3.2
   55 |   0.9738 |     34.714 |   1.0465 |     36.911 |     3.3
   56 |   0.9671 |     34.571 |   1.0424 |     36.269 |     3.3
   57 |   0.9602 |     34.107 |   1.0532 |     36.361 |     3.4
   58 |   0.9596 |     34.620 |   1.0396 |     35.810 |     3.4
   59 |   0.9635 |     34.455 |   1.0485 |     35.749 |     3.5
   60 |   0.9539 |     34.046 |   1.0335 |     36.239 |     3.6
   61 |   0.9458 |     33.692 |   1.0337 |     35.780 |     3.6
   62 |   0.9390 |     33.709 |   1.0348 |     35.291 |     3.7
   63 |   0.9318 |     33.328 |   1.0351 |     35.780 |     3.7
   64 |   0.9272 |     33.052 |   1.0292 |     35.596 |     3.8
   65 |   0.9260 |     32.908 |   1.0253 |     35.596 |     3.9
   66 |   0.9168 |     32.427 |   1.0363 |     35.535 |     3.9
   67 |   0.9093 |     32.389 |   1.0274 |     34.709 |     4.0
   68 |   0.9049 |     32.052 |   1.0223 |     34.924 |     4.0
   69 |   0.8982 |     32.041 |   1.0150 |     34.709 |     4.1
   70 |   0.8941 |     31.704 |   1.0191 |     34.648 |     4.2
   71 |   0.8837 |     31.256 |   1.0144 |     34.587 |     4.2
   72 |   0.8812 |     31.427 |   1.0284 |     34.893 |     4.3
   73 |   0.8734 |     31.041 |   1.0064 |     33.670 |     4.3
   74 |   0.8635 |     30.477 |   0.9946 |     33.547 |     4.4
   75 |   0.8531 |     30.024 |   1.0026 |     33.853 |     4.5
   76 |   0.8510 |     29.919 |   1.0046 |     33.945 |     4.5
   77 |   0.8399 |     29.334 |   0.9932 |     32.691 |     4.6
   78 |   0.8357 |     29.350 |   0.9922 |     32.783 |     4.6
   79 |   0.8228 |     28.765 |   1.0017 |     33.150 |     4.7
   80 |   0.8129 |     28.273 |   1.0025 |     33.119 |     4.8
   81 |   0.8036 |     27.737 |   1.0109 |     32.599 |     4.8
   82 |   0.7977 |     27.616 |   0.9927 |     32.630 |     4.9
   83 |   0.7890 |     27.168 |   0.9885 |     32.263 |     4.9
   84 |   0.7767 |     26.450 |   1.0032 |     32.202 |     5.0
   85 |   0.7711 |     26.378 |   0.9917 |     32.232 |     5.1
   86 |   0.7655 |     26.102 |   1.0132 |     31.957 |     5.1
   87 |   0.7550 |     25.986 |   0.9952 |     31.988 |     5.2
   88 |   0.7460 |     25.445 |   0.9857 |     31.560 |     5.2
   89 |   0.7320 |     24.716 |   0.9886 |     31.009 |     5.3
   90 |   0.7260 |     24.367 |   0.9888 |     30.917 |     5.4
   91 |   0.7132 |     24.141 |   1.0082 |     31.743 |     5.4
   92 |   0.7051 |     23.677 |   0.9998 |     31.346 |     5.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 621,538

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1929 |     59.629 |   1.5541 |     45.963 |     0.0
    2 |   1.4537 |     46.177 |   1.3837 |     45.963 |     0.0
    3 |   1.3752 |     45.890 |   1.3379 |     45.810 |     0.1
    4 |   1.3376 |     45.630 |   1.3090 |     45.291 |     0.1
    5 |   1.3106 |     44.940 |   1.2903 |     45.107 |     0.1
    6 |   1.2851 |     44.409 |   1.2791 |     44.373 |     0.1
    7 |   1.2654 |     43.956 |   1.2629 |     44.281 |     0.1
    8 |   1.2474 |     43.758 |   1.2435 |     43.731 |     0.1
    9 |   1.2308 |     43.305 |   1.2251 |     42.630 |     0.2
   10 |   1.2070 |     42.691 |   1.2140 |     42.355 |     0.2
   11 |   1.1892 |     42.106 |   1.1940 |     41.407 |     0.2
   12 |   1.1649 |     41.194 |   1.1862 |     40.795 |     0.2
   13 |   1.1428 |     40.228 |   1.1605 |     39.786 |     0.2
   14 |   1.1205 |     39.311 |   1.1412 |     39.572 |     0.2
   15 |   1.0966 |     38.581 |   1.1285 |     38.410 |     0.3
   16 |   1.0694 |     37.294 |   1.1234 |     38.287 |     0.3
   17 |   1.0445 |     36.333 |   1.0966 |     36.422 |     0.3
   18 |   1.0239 |     35.670 |   1.0938 |     37.248 |     0.3
   19 |   1.0000 |     34.565 |   1.0597 |     35.994 |     0.3
   20 |   0.9736 |     33.853 |   1.0482 |     35.260 |     0.3
   21 |   0.9589 |     33.405 |   1.0495 |     35.749 |     0.4
   22 |   0.9329 |     32.262 |   1.0227 |     35.229 |     0.4
   23 |   0.9079 |     31.356 |   1.0164 |     34.954 |     0.4
   24 |   0.8780 |     30.190 |   1.0045 |     33.792 |     0.4
   25 |   0.8482 |     28.831 |   1.0058 |     33.486 |     0.4
   26 |   0.8254 |     27.975 |   0.9853 |     32.080 |     0.4
   27 |   0.7986 |     27.003 |   0.9693 |     31.651 |     0.5
   28 |   0.7738 |     26.306 |   0.9970 |     32.141 |     0.5
   29 |   0.7797 |     26.235 |   0.9639 |     30.612 |     0.5
   30 |   0.7211 |     24.130 |   0.9491 |     31.040 |     0.5
   31 |   0.6943 |     23.119 |   0.9633 |     31.040 |     0.5
   32 |   0.6678 |     22.147 |   0.9652 |     30.520 |     0.5
   33 |   0.6388 |     21.020 |   0.9761 |     30.520 |     0.6
   34 |   0.6163 |     20.009 |   0.9969 |     29.878 |     0.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,524,578

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2237 |     60.463 |   1.6471 |     48.746 |     0.1
    2 |   1.5101 |     46.387 |   1.4080 |     45.963 |     0.1
    3 |   1.4144 |     46.249 |   1.3820 |     45.963 |     0.2
    4 |   1.3951 |     46.139 |   1.3686 |     45.963 |     0.2
    5 |   1.3733 |     46.045 |   1.3424 |     45.352 |     0.3
    6 |   1.3452 |     45.575 |   1.3288 |     46.024 |     0.4
    7 |   1.3223 |     45.514 |   1.2933 |     45.138 |     0.4
    8 |   1.3002 |     45.150 |   1.2944 |     45.138 |     0.5
    9 |   1.2841 |     44.757 |   1.2684 |     44.954 |     0.6
   10 |   1.2647 |     44.349 |   1.2533 |     44.343 |     0.6
   11 |   1.2465 |     43.879 |   1.2436 |     43.761 |     0.7
   12 |   1.2297 |     43.553 |   1.2236 |     42.722 |     0.7
   13 |   1.2157 |     42.907 |   1.2141 |     42.752 |     0.8
   14 |   1.2013 |     42.443 |   1.2120 |     42.232 |     0.9
   15 |   1.1866 |     41.951 |   1.1931 |     41.957 |     0.9
   16 |   1.1762 |     41.741 |   1.1868 |     41.621 |     1.0
   17 |   1.1571 |     41.089 |   1.1770 |     41.346 |     1.0
   18 |   1.1368 |     40.117 |   1.1714 |     40.000 |     1.1
   19 |   1.1218 |     39.802 |   1.1461 |     39.908 |     1.2
   20 |   1.1046 |     39.178 |   1.1329 |     40.153 |     1.2
   21 |   1.0856 |     38.587 |   1.1259 |     39.327 |     1.3
   22 |   1.0678 |     37.642 |   1.1196 |     38.563 |     1.4
   23 |   1.0451 |     36.642 |   1.0993 |     37.798 |     1.4
   24 |   1.0203 |     35.543 |   1.0915 |     36.972 |     1.5
   25 |   0.9994 |     34.488 |   1.0612 |     36.300 |     1.5
   26 |   0.9751 |     33.720 |   1.0674 |     36.300 |     1.6
   27 |   0.9512 |     32.715 |   1.0376 |     35.015 |     1.7
   28 |   0.9203 |     31.394 |   1.0234 |     34.557 |     1.7
   29 |   0.9034 |     30.637 |   1.0314 |     33.853 |     1.8
   30 |   0.8720 |     29.516 |   1.0159 |     34.434 |     1.8
   31 |   0.8497 |     28.411 |   0.9881 |     33.119 |     1.9
   32 |   0.8208 |     27.428 |   1.0023 |     32.875 |     2.0
   33 |   0.7956 |     26.677 |   0.9947 |     32.446 |     2.0
   34 |   0.7714 |     25.677 |   0.9801 |     31.927 |     2.1
   35 |   0.7439 |     24.533 |   0.9723 |     31.560 |     2.2
   36 |   0.7145 |     23.401 |   0.9824 |     31.346 |     2.2
   37 |   0.6922 |     22.793 |   0.9657 |     30.856 |     2.3
   38 |   0.6695 |     21.931 |   0.9743 |     31.009 |     2.3
   39 |   0.6361 |     20.694 |   0.9679 |     29.602 |     2.4
   40 |   0.6114 |     19.882 |   0.9936 |     30.612 |     2.5
   41 |   0.5948 |     19.230 |   1.0043 |     29.969 |     2.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 733,346

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4057 |     63.794 |   1.8444 |     53.364 |     0.0
    2 |   1.6483 |     48.503 |   1.4826 |     45.963 |     0.1
    3 |   1.4505 |     45.984 |   1.3933 |     45.963 |     0.1
    4 |   1.3910 |     45.818 |   1.3597 |     45.596 |     0.2
    5 |   1.3588 |     45.559 |   1.3308 |     44.343 |     0.2
    6 |   1.3318 |     44.746 |   1.3162 |     44.190 |     0.2
    7 |   1.3166 |     44.304 |   1.3014 |     44.220 |     0.3
    8 |   1.3022 |     44.139 |   1.2891 |     43.945 |     0.3
    9 |   1.2868 |     43.918 |   1.2826 |     44.190 |     0.4
   10 |   1.2715 |     43.752 |   1.2593 |     43.700 |     0.4
   11 |   1.2571 |     43.244 |   1.2504 |     43.425 |     0.4
   12 |   1.2463 |     43.133 |   1.2409 |     42.722 |     0.5
   13 |   1.2327 |     42.962 |   1.2389 |     42.936 |     0.5
   14 |   1.2184 |     42.575 |   1.2179 |     42.416 |     0.6
   15 |   1.2025 |     42.410 |   1.2083 |     42.171 |     0.6
   16 |   1.1894 |     41.852 |   1.1954 |     41.774 |     0.6
   17 |   1.1726 |     41.111 |   1.1906 |     41.743 |     0.7
   18 |   1.1572 |     40.587 |   1.1721 |     40.398 |     0.7
   19 |   1.1392 |     39.482 |   1.1672 |     40.489 |     0.8
   20 |   1.1279 |     39.244 |   1.1541 |     39.358 |     0.8
   21 |   1.1078 |     38.344 |   1.1477 |     39.205 |     0.8
   22 |   1.0968 |     38.073 |   1.1397 |     39.297 |     0.9
   23 |   1.0793 |     37.112 |   1.1244 |     39.083 |     0.9
   24 |   1.0575 |     36.648 |   1.1091 |     37.370 |     1.0
   25 |   1.0290 |     35.438 |   1.0984 |     36.850 |     1.0
   26 |   1.0114 |     34.637 |   1.0859 |     36.758 |     1.0
   27 |   0.9843 |     33.090 |   1.0676 |     35.657 |     1.1
   28 |   0.9595 |     32.455 |   1.0664 |     35.566 |     1.1
   29 |   0.9382 |     31.593 |   1.0546 |     35.535 |     1.2
   30 |   0.9035 |     30.229 |   1.0389 |     34.251 |     1.2
   31 |   0.8753 |     29.107 |   1.0317 |     33.731 |     1.2
   32 |   0.8459 |     28.251 |   1.0346 |     34.098 |     1.3
   33 |   0.8160 |     27.041 |   1.0327 |     32.905 |     1.3
   34 |   0.7918 |     26.086 |   1.0094 |     32.110 |     1.4
   35 |   0.7613 |     24.876 |   1.0242 |     32.752 |     1.4
   36 |   0.7293 |     23.793 |   1.0201 |     31.713 |     1.5
   37 |   0.7014 |     22.909 |   1.0061 |     30.856 |     1.5
   38 |   0.6751 |     21.567 |   1.0134 |     31.346 |     1.5
   39 |   0.6601 |     20.953 |   1.0110 |     31.162 |     1.6
   40 |   0.6200 |     19.782 |   1.0231 |     31.407 |     1.6
   41 |   0.5941 |     18.865 |   1.0261 |     30.367 |     1.7
   42 |   0.5625 |     17.865 |   0.9982 |     29.969 |     1.7
   43 |   0.5529 |     17.451 |   1.0140 |     30.459 |     1.7
   44 |   0.5296 |     16.667 |   1.0115 |     29.235 |     1.8
   45 |   0.5020 |     15.794 |   1.0206 |     29.694 |     1.8
   46 |   0.4774 |     14.910 |   1.0290 |     30.275 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 604,450

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1005 |     56.419 |   1.5051 |     45.657 |     0.0
    2 |   1.4273 |     45.818 |   1.3601 |     45.657 |     0.1
    3 |   1.3365 |     45.255 |   1.3088 |     44.312 |     0.1
    4 |   1.2963 |     44.316 |   1.2740 |     43.884 |     0.1
    5 |   1.2678 |     43.763 |   1.2607 |     43.976 |     0.1
    6 |   1.2428 |     43.034 |   1.2379 |     43.180 |     0.2
    7 |   1.2163 |     42.824 |   1.2171 |     42.569 |     0.2
    8 |   1.1956 |     42.095 |   1.2001 |     41.835 |     0.2
    9 |   1.1704 |     40.979 |   1.1866 |     41.009 |     0.2
   10 |   1.1403 |     39.697 |   1.1676 |     39.969 |     0.3
   11 |   1.1160 |     38.858 |   1.1586 |     39.633 |     0.3
   12 |   1.0809 |     37.587 |   1.1385 |     39.174 |     0.3
   13 |   1.0491 |     35.941 |   1.1059 |     38.104 |     0.3
   14 |   1.0227 |     35.399 |   1.0888 |     37.125 |     0.4
   15 |   0.9870 |     33.930 |   1.0785 |     36.391 |     0.4
   16 |   0.9503 |     32.400 |   1.0517 |     35.413 |     0.4
   17 |   0.9145 |     31.013 |   1.0375 |     34.985 |     0.4
   18 |   0.8790 |     29.798 |   1.0160 |     34.281 |     0.5
   19 |   0.8446 |     28.472 |   1.0132 |     33.884 |     0.5
   20 |   0.8074 |     26.920 |   0.9784 |     32.294 |     0.5
   21 |   0.7794 |     25.964 |   1.0166 |     34.434 |     0.6
   22 |   0.7665 |     25.605 |   0.9706 |     32.049 |     0.6
   23 |   0.7095 |     23.290 |   0.9932 |     31.498 |     0.6
   24 |   0.6760 |     22.340 |   0.9591 |     30.703 |     0.6
   25 |   0.6425 |     21.025 |   0.9652 |     30.489 |     0.7
   26 |   0.6129 |     20.053 |   0.9689 |     29.847 |     0.7
   27 |   0.5877 |     19.191 |   0.9495 |     30.183 |     0.7
   28 |   0.5509 |     17.678 |   0.9572 |     29.908 |     0.7
   29 |   0.5296 |     16.959 |   0.9351 |     29.083 |     0.8
   30 |   0.5029 |     15.915 |   0.9686 |     29.450 |     0.8
   31 |   0.4760 |     15.247 |   0.9515 |     27.890 |     0.8
   32 |   0.4518 |     14.507 |   0.9573 |     29.297 |     0.8
   33 |   0.4301 |     13.667 |   0.9463 |     28.104 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,194,658

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5282 |     67.981 |   1.9646 |     58.654 |     0.1
    2 |   1.7755 |     50.994 |   1.5763 |     45.963 |     0.1
    3 |   1.5164 |     46.139 |   1.4443 |     45.994 |     0.2
    4 |   1.4410 |     46.161 |   1.4075 |     45.994 |     0.2
    5 |   1.4138 |     46.183 |   1.3827 |     45.963 |     0.3
    6 |   1.3883 |     46.100 |   1.3578 |     45.872 |     0.4
    7 |   1.3622 |     45.824 |   1.3339 |     45.963 |     0.4
    8 |   1.3373 |     45.708 |   1.3123 |     45.474 |     0.5
    9 |   1.3169 |     45.536 |   1.3006 |     45.535 |     0.5
   10 |   1.3018 |     45.133 |   1.2911 |     45.902 |     0.6
   11 |   1.2895 |     44.862 |   1.2799 |     44.557 |     0.7
   12 |   1.2759 |     44.581 |   1.2626 |     44.465 |     0.7
   13 |   1.2627 |     44.360 |   1.2526 |     44.220 |     0.8
   14 |   1.2513 |     44.095 |   1.2418 |     44.220 |     0.8
   15 |   1.2415 |     44.161 |   1.2392 |     44.343 |     0.9
   16 |   1.2320 |     43.951 |   1.2265 |     44.159 |     1.0
   17 |   1.2245 |     43.785 |   1.2202 |     43.394 |     1.0
   18 |   1.2186 |     43.354 |   1.2146 |     43.547 |     1.1
   19 |   1.2115 |     43.062 |   1.2070 |     42.599 |     1.1
   20 |   1.2056 |     42.968 |   1.2055 |     42.783 |     1.2
   21 |   1.1982 |     42.658 |   1.1920 |     42.569 |     1.3
   22 |   1.1925 |     42.669 |   1.1924 |     42.385 |     1.3
   23 |   1.1883 |     42.741 |   1.1886 |     41.988 |     1.4
   24 |   1.1821 |     42.167 |   1.1781 |     42.722 |     1.4
   25 |   1.1760 |     42.145 |   1.1771 |     42.477 |     1.5
   26 |   1.1706 |     41.946 |   1.1713 |     41.560 |     1.6
   27 |   1.1696 |     41.808 |   1.1706 |     41.437 |     1.6
   28 |   1.1641 |     41.647 |   1.1675 |     41.162 |     1.7
   29 |   1.1587 |     41.371 |   1.1674 |     41.040 |     1.7
   30 |   1.1543 |     41.288 |   1.1638 |     40.856 |     1.8
   31 |   1.1459 |     40.797 |   1.1561 |     41.346 |     1.9
   32 |   1.1456 |     40.670 |   1.1527 |     41.131 |     1.9
   33 |   1.1423 |     40.758 |   1.1527 |     40.214 |     2.0
   34 |   1.1379 |     40.509 |   1.1494 |     40.642 |     2.0
   35 |   1.1346 |     40.194 |   1.1410 |     40.428 |     2.1
   36 |   1.1302 |     40.310 |   1.1359 |     40.061 |     2.1
   37 |   1.1249 |     39.962 |   1.1366 |     39.694 |     2.2
   38 |   1.1231 |     40.239 |   1.1405 |     40.275 |     2.3
   39 |   1.1177 |     39.979 |   1.1392 |     39.847 |     2.3
   40 |   1.1146 |     40.007 |   1.1295 |     39.786 |     2.4
   41 |   1.1108 |     39.857 |   1.1311 |     39.511 |     2.4
   42 |   1.1089 |     39.846 |   1.1283 |     39.755 |     2.5
   43 |   1.1009 |     39.775 |   1.1171 |     39.235 |     2.6
   44 |   1.1006 |     39.393 |   1.1240 |     39.725 |     2.6
   45 |   1.0946 |     39.565 |   1.1189 |     39.694 |     2.7
   46 |   1.0931 |     39.167 |   1.1158 |     39.205 |     2.7
   47 |   1.0908 |     39.056 |   1.1087 |     38.807 |     2.8
   48 |   1.0865 |     38.963 |   1.1050 |     39.450 |     2.9
   49 |   1.0853 |     39.239 |   1.1045 |     38.960 |     2.9
   50 |   1.0827 |     38.891 |   1.0971 |     38.532 |     3.0
   51 |   1.0792 |     38.996 |   1.1012 |     38.654 |     3.0
   52 |   1.0767 |     38.747 |   1.1006 |     38.532 |     3.1
   53 |   1.0716 |     38.206 |   1.0973 |     38.930 |     3.2
   54 |   1.0694 |     38.664 |   1.1084 |     39.021 |     3.2
   55 |   1.0683 |     38.366 |   1.0912 |     38.532 |     3.3
   56 |   1.0633 |     38.095 |   1.0982 |     38.379 |     3.3
   57 |   1.0636 |     38.333 |   1.0844 |     38.196 |     3.4
   58 |   1.0605 |     38.117 |   1.0840 |     37.554 |     3.5
   59 |   1.0573 |     38.001 |   1.0842 |     38.593 |     3.5
   60 |   1.0515 |     38.073 |   1.0754 |     37.615 |     3.6
   61 |   1.0491 |     37.499 |   1.0808 |     38.073 |     3.6
   62 |   1.0422 |     37.256 |   1.0719 |     37.859 |     3.7
   63 |   1.0461 |     37.598 |   1.0759 |     37.829 |     3.8
   64 |   1.0421 |     37.488 |   1.0742 |     38.104 |     3.8
   65 |   1.0378 |     37.438 |   1.0674 |     37.492 |     3.9
   66 |   1.0344 |     37.267 |   1.0617 |     36.422 |     3.9
   67 |   1.0284 |     36.753 |   1.0634 |     37.278 |     4.0
   68 |   1.0280 |     37.040 |   1.0607 |     37.125 |     4.1
   69 |   1.0227 |     36.742 |   1.0531 |     36.483 |     4.1
   70 |   1.0180 |     36.455 |   1.0549 |     36.606 |     4.2
   71 |   1.0155 |     36.482 |   1.0550 |     36.911 |     4.2
   72 |   1.0088 |     36.167 |   1.0468 |     36.422 |     4.3
   73 |   1.0133 |     36.565 |   1.1027 |     38.777 |     4.4
   74 |   1.0208 |     36.742 |   1.0531 |     36.942 |     4.4
   75 |   1.0105 |     36.156 |   1.0444 |     36.697 |     4.5
   76 |   1.0021 |     35.946 |   1.0438 |     36.636 |     4.5
   77 |   1.0016 |     35.687 |   1.0414 |     36.147 |     4.6
   78 |   0.9967 |     35.631 |   1.0350 |     35.994 |     4.7
   79 |   0.9908 |     35.245 |   1.0282 |     36.116 |     4.7
   80 |   0.9893 |     35.350 |   1.0353 |     36.361 |     4.8
   81 |   0.9852 |     35.239 |   1.0391 |     35.963 |     4.8
   82 |   0.9795 |     35.040 |   1.0281 |     36.300 |     4.9
   83 |   0.9813 |     35.162 |   1.0244 |     35.963 |     5.0
   84 |   0.9746 |     34.775 |   1.0265 |     35.780 |     5.0
   85 |   0.9743 |     34.814 |   1.0251 |     36.086 |     5.1
   86 |   0.9646 |     34.306 |   1.0301 |     35.596 |     5.1
   87 |   0.9622 |     34.179 |   1.0244 |     35.474 |     5.2
   88 |   0.9586 |     34.129 |   1.0260 |     36.086 |     5.3
   89 |   0.9565 |     34.068 |   1.0317 |     35.963 |     5.3
   90 |   0.9484 |     33.598 |   1.0189 |     35.841 |     5.4
   91 |   0.9467 |     33.643 |   1.0336 |     36.024 |     5.4
   92 |   0.9433 |     33.184 |   1.0151 |     35.352 |     5.5
   93 |   0.9401 |     33.587 |   1.0174 |     35.535 |     5.6
   94 |   0.9358 |     33.250 |   1.0142 |     34.924 |     5.6
   95 |   0.9316 |     33.123 |   1.0086 |     34.709 |     5.7
   96 |   0.9323 |     33.173 |   1.0107 |     34.924 |     5.7
   97 |   0.9262 |     32.693 |   1.0041 |     34.067 |     5.8
   98 |   0.9180 |     32.543 |   1.0052 |     34.618 |     5.9
   99 |   0.9144 |     32.466 |   1.0031 |     34.251 |     5.9
  100 |   0.9091 |     32.146 |   1.0030 |     34.587 |     6.0
  101 |   0.9047 |     32.079 |   1.0219 |     34.648 |     6.0
  102 |   0.9041 |     31.930 |   1.0130 |     34.098 |     6.1
  103 |   0.8931 |     31.488 |   1.0006 |     34.557 |     6.2
  104 |   0.8871 |     31.328 |   1.0083 |     34.159 |     6.2
  105 |   0.8794 |     31.019 |   1.0163 |     34.220 |     6.3
  106 |   0.8760 |     30.963 |   1.0073 |     34.067 |     6.3
  107 |   0.8805 |     31.002 |   1.0022 |     33.089 |     6.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 699,106

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4053 |     65.275 |   1.8624 |     50.795 |     0.0
    2 |   1.6678 |     48.094 |   1.4925 |     45.963 |     0.1
    3 |   1.4570 |     46.028 |   1.3988 |     45.963 |     0.1
    4 |   1.3991 |     45.989 |   1.3601 |     45.841 |     0.2
    5 |   1.3566 |     45.779 |   1.3284 |     45.382 |     0.2
    6 |   1.3258 |     45.078 |   1.3085 |     44.954 |     0.2
    7 |   1.3057 |     44.885 |   1.2894 |     45.260 |     0.3
    8 |   1.2871 |     44.498 |   1.2775 |     44.404 |     0.3
    9 |   1.2720 |     44.216 |   1.2652 |     43.945 |     0.3
   10 |   1.2605 |     43.890 |   1.2585 |     43.364 |     0.4
   11 |   1.2479 |     43.360 |   1.2417 |     42.813 |     0.4
   12 |   1.2329 |     42.769 |   1.2320 |     42.875 |     0.5
   13 |   1.2184 |     42.470 |   1.2209 |     42.691 |     0.5
   14 |   1.2012 |     42.029 |   1.2102 |     42.110 |     0.5
   15 |   1.1875 |     41.780 |   1.2003 |     41.529 |     0.6
   16 |   1.1728 |     41.437 |   1.1924 |     42.080 |     0.6
   17 |   1.1549 |     40.647 |   1.1706 |     40.703 |     0.7
   18 |   1.1375 |     39.686 |   1.1695 |     39.664 |     0.7
   19 |   1.1173 |     39.090 |   1.1611 |     38.991 |     0.7
   20 |   1.1048 |     38.322 |   1.1431 |     38.165 |     0.8
   21 |   1.0773 |     37.106 |   1.1436 |     37.554 |     0.8
   22 |   1.0515 |     36.046 |   1.1211 |     36.728 |     0.8
   23 |   1.0264 |     34.814 |   1.0975 |     36.024 |     0.9
   24 |   1.0001 |     33.737 |   1.1082 |     35.933 |     0.9
   25 |   0.9705 |     32.610 |   1.0938 |     35.015 |     1.0
   26 |   0.9438 |     31.041 |   1.0986 |     35.291 |     1.0
   27 |   0.9043 |     29.709 |   1.0837 |     33.761 |     1.0
   28 |   0.8725 |     28.654 |   1.0731 |     32.844 |     1.1
   29 |   0.8471 |     27.334 |   1.0766 |     33.578 |     1.1
   30 |   0.8071 |     26.406 |   1.0538 |     33.517 |     1.2
   31 |   0.7671 |     24.749 |   1.0438 |     31.835 |     1.2
   32 |   0.7316 |     23.279 |   1.0289 |     32.202 |     1.2
   33 |   0.7003 |     22.119 |   1.0351 |     32.141 |     1.3
   34 |   0.6632 |     20.594 |   1.0182 |     31.223 |     1.3
   35 |   0.6399 |     19.843 |   1.0324 |     31.223 |     1.4
   36 |   0.5970 |     18.291 |   1.0399 |     30.642 |     1.4
   37 |   0.5605 |     17.153 |   1.0022 |     31.040 |     1.4
   38 |   0.5269 |     15.932 |   0.9968 |     30.948 |     1.5
   39 |   0.5063 |     15.313 |   0.9949 |     29.235 |     1.5
   40 |   0.4658 |     13.595 |   1.0262 |     29.358 |     1.5
   41 |   0.4443 |     13.258 |   1.0466 |     29.235 |     1.6
   42 |   0.4257 |     12.496 |   1.0286 |     29.664 |     1.6
   43 |   0.3870 |     11.413 |   1.0864 |     29.419 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 442,530

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4001 |     63.313 |   1.7906 |     48.502 |     0.0
    2 |   1.6068 |     47.448 |   1.4523 |     45.841 |     0.1
    3 |   1.4184 |     45.757 |   1.3657 |     45.596 |     0.1
    4 |   1.3564 |     45.155 |   1.3255 |     44.495 |     0.1
    5 |   1.3229 |     44.266 |   1.3053 |     44.465 |     0.1
    6 |   1.2987 |     44.006 |   1.2937 |     44.343 |     0.2
    7 |   1.2774 |     43.581 |   1.2636 |     43.945 |     0.2
    8 |   1.2555 |     43.360 |   1.2449 |     43.303 |     0.2
    9 |   1.2349 |     42.791 |   1.2358 |     42.599 |     0.2
   10 |   1.2198 |     42.542 |   1.2156 |     41.713 |     0.3
   11 |   1.1987 |     41.785 |   1.2002 |     41.621 |     0.3
   12 |   1.1830 |     41.288 |   1.1846 |     40.459 |     0.3
   13 |   1.1647 |     40.642 |   1.1836 |     40.703 |     0.4
   14 |   1.1504 |     40.393 |   1.1656 |     40.703 |     0.4
   15 |   1.1369 |     39.990 |   1.1593 |     40.306 |     0.4
   16 |   1.1219 |     39.349 |   1.1498 |     39.572 |     0.4
   17 |   1.1094 |     38.891 |   1.1315 |     38.502 |     0.5
   18 |   1.0905 |     37.979 |   1.1176 |     38.012 |     0.5
   19 |   1.0731 |     37.548 |   1.1086 |     37.462 |     0.5
   20 |   1.0577 |     36.841 |   1.1028 |     37.370 |     0.6
   21 |   1.0414 |     36.029 |   1.0818 |     36.606 |     0.6
   22 |   1.0233 |     35.090 |   1.0875 |     36.453 |     0.6
   23 |   1.0049 |     34.554 |   1.0719 |     35.902 |     0.6
   24 |   0.9893 |     33.947 |   1.0670 |     35.657 |     0.7
   25 |   0.9685 |     32.858 |   1.0498 |     34.618 |     0.7
   26 |   0.9500 |     32.521 |   1.0444 |     34.373 |     0.7
   27 |   0.9304 |     31.582 |   1.0341 |     33.823 |     0.7
   28 |   0.9143 |     30.715 |   1.0328 |     34.190 |     0.8
   29 |   0.8976 |     30.212 |   1.0254 |     33.364 |     0.8
   30 |   0.8712 |     29.223 |   1.0080 |     32.905 |     0.8
   31 |   0.8555 |     28.582 |   1.0055 |     32.538 |     0.9
   32 |   0.8388 |     27.953 |   0.9918 |     31.896 |     0.9
   33 |   0.8183 |     27.417 |   1.0133 |     31.988 |     0.9
   34 |   0.8004 |     26.373 |   0.9839 |     31.376 |     0.9
   35 |   0.7778 |     25.776 |   0.9777 |     31.590 |     1.0
   36 |   0.7738 |     25.356 |   0.9809 |     30.673 |     1.0
   37 |   0.7435 |     24.528 |   0.9814 |     30.612 |     1.0
   38 |   0.7327 |     23.953 |   0.9644 |     30.183 |     1.1
   39 |   0.7084 |     23.423 |   0.9570 |     30.856 |     1.1
   40 |   0.6910 |     22.406 |   0.9540 |     29.388 |     1.1
   41 |   0.6765 |     21.964 |   0.9764 |     30.520 |     1.1
   42 |   0.6597 |     21.020 |   0.9650 |     29.725 |     1.2
   43 |   0.6508 |     21.092 |   0.9654 |     29.878 |     1.2
   44 |   0.6341 |     20.451 |   0.9546 |     28.991 |     1.2
   45 |   0.6094 |     19.578 |   0.9330 |     28.930 |     1.2
   46 |   0.6792 |     22.080 |   0.9406 |     29.113 |     1.3
   47 |   0.6071 |     19.523 |   0.9361 |     29.083 |     1.3
   48 |   0.5709 |     18.208 |   0.9267 |     28.716 |     1.3
   49 |   0.5478 |     17.346 |   0.9448 |     28.991 |     1.4
   50 |   0.5348 |     16.871 |   0.9533 |     27.768 |     1.4
   51 |   0.5260 |     16.628 |   0.9497 |     28.716 |     1.4
   52 |   0.5128 |     16.081 |   0.9529 |     28.318 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 454,498

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1033 |     57.425 |   1.5047 |     45.657 |     0.0
    2 |   1.4173 |     45.746 |   1.3524 |     45.229 |     0.0
    3 |   1.3351 |     45.078 |   1.3096 |     45.413 |     0.0
    4 |   1.2918 |     44.255 |   1.2795 |     44.251 |     0.1
    5 |   1.2677 |     44.100 |   1.2597 |     43.945 |     0.1
    6 |   1.2456 |     43.542 |   1.2516 |     43.242 |     0.1
    7 |   1.2287 |     43.249 |   1.2320 |     43.425 |     0.1
    8 |   1.2080 |     42.669 |   1.2187 |     43.394 |     0.1
    9 |   1.1898 |     42.051 |   1.1998 |     41.865 |     0.1
   10 |   1.1687 |     41.791 |   1.1988 |     42.171 |     0.2
   11 |   1.1516 |     40.725 |   1.1857 |     41.284 |     0.2
   12 |   1.1364 |     40.471 |   1.1624 |     40.398 |     0.2
   13 |   1.1153 |     39.708 |   1.1538 |     40.183 |     0.2
   14 |   1.0981 |     38.559 |   1.1406 |     40.031 |     0.2
   15 |   1.0820 |     37.880 |   1.1240 |     38.593 |     0.2
   16 |   1.0647 |     37.250 |   1.0990 |     37.706 |     0.3
   17 |   1.0447 |     36.935 |   1.0908 |     37.462 |     0.3
   18 |   1.0348 |     36.377 |   1.0784 |     37.095 |     0.3
   19 |   1.0130 |     35.642 |   1.0775 |     36.514 |     0.3
   20 |   0.9992 |     35.013 |   1.0707 |     36.208 |     0.3
   21 |   0.9853 |     34.665 |   1.0462 |     36.269 |     0.3
   22 |   0.9673 |     33.880 |   1.0479 |     35.382 |     0.4
   23 |   0.9499 |     33.311 |   1.0349 |     34.557 |     0.4
   24 |   0.9337 |     32.505 |   0.9980 |     34.190 |     0.4
   25 |   0.9175 |     31.963 |   1.0156 |     34.312 |     0.4
   26 |   0.8974 |     31.334 |   0.9813 |     33.180 |     0.4
   27 |   0.8786 |     30.389 |   0.9962 |     33.547 |     0.4
   28 |   0.8653 |     29.848 |   0.9627 |     33.486 |     0.5
   29 |   0.8421 |     28.964 |   0.9734 |     33.180 |     0.5
   30 |   0.8195 |     27.881 |   0.9611 |     32.416 |     0.5
   31 |   0.8022 |     27.235 |   0.9441 |     32.416 |     0.5
   32 |   0.7850 |     26.196 |   0.9453 |     31.621 |     0.5
   33 |   0.7645 |     25.561 |   0.9372 |     31.774 |     0.5
   34 |   0.7433 |     24.671 |   0.9554 |     31.529 |     0.6
   35 |   0.7248 |     23.981 |   0.9186 |     30.703 |     0.6
   36 |   0.7004 |     23.218 |   0.9266 |     30.673 |     0.6
   37 |   0.6815 |     22.544 |   0.9285 |     30.367 |     0.6
   38 |   0.6596 |     21.495 |   0.9152 |     29.786 |     0.6
   39 |   0.6427 |     21.296 |   0.9172 |     29.388 |     0.6
   40 |   0.6192 |     19.976 |   0.8963 |     29.174 |     0.7
   41 |   0.6055 |     19.777 |   0.9345 |     29.419 |     0.7
   42 |   0.5859 |     18.843 |   0.9172 |     29.083 |     0.7
   43 |   0.5685 |     18.329 |   0.9217 |     28.593 |     0.7
   44 |   0.5529 |     18.042 |   0.9125 |     28.869 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,098,018

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2033 |     60.667 |   1.5883 |     45.994 |     0.0
    2 |   1.4783 |     46.255 |   1.4028 |     45.994 |     0.1
    3 |   1.3987 |     46.188 |   1.3600 |     46.575 |     0.1
    4 |   1.3586 |     45.719 |   1.3336 |     45.596 |     0.2
    5 |   1.3295 |     45.641 |   1.3060 |     45.076 |     0.2
    6 |   1.3088 |     45.144 |   1.2910 |     44.618 |     0.2
    7 |   1.2880 |     45.100 |   1.2752 |     44.434 |     0.3
    8 |   1.2654 |     44.498 |   1.2670 |     44.098 |     0.3
    9 |   1.2523 |     44.316 |   1.2503 |     44.006 |     0.4
   10 |   1.2406 |     43.962 |   1.2301 |     43.150 |     0.4
   11 |   1.2268 |     43.283 |   1.2230 |     43.333 |     0.4
   12 |   1.2140 |     42.824 |   1.2188 |     43.333 |     0.5
   13 |   1.2015 |     42.233 |   1.2123 |     42.355 |     0.5
   14 |   1.1868 |     42.100 |   1.2098 |     42.813 |     0.6
   15 |   1.1815 |     41.703 |   1.2066 |     42.569 |     0.6
   16 |   1.1731 |     41.819 |   1.1914 |     42.416 |     0.6
   17 |   1.1617 |     41.443 |   1.1804 |     41.193 |     0.7
   18 |   1.1503 |     40.962 |   1.1856 |     41.682 |     0.7
   19 |   1.1456 |     40.929 |   1.1728 |     40.550 |     0.8
   20 |   1.1362 |     40.366 |   1.1566 |     40.856 |     0.8
   21 |   1.1268 |     40.294 |   1.1600 |     40.428 |     0.8
   22 |   1.1210 |     39.957 |   1.1540 |     40.856 |     0.9
   23 |   1.1074 |     39.399 |   1.1398 |     39.511 |     0.9
   24 |   1.1038 |     39.482 |   1.1362 |     39.664 |     1.0
   25 |   1.0969 |     39.211 |   1.1309 |     39.388 |     1.0
   26 |   1.0880 |     39.001 |   1.1256 |     39.419 |     1.0
   27 |   1.0788 |     38.499 |   1.1254 |     39.083 |     1.1
   28 |   1.0719 |     38.228 |   1.1145 |     38.593 |     1.1
   29 |   1.0653 |     37.670 |   1.1371 |     40.336 |     1.2
   30 |   1.0565 |     37.659 |   1.1071 |     38.349 |     1.2
   31 |   1.0468 |     37.361 |   1.0924 |     36.850 |     1.3
   32 |   1.0365 |     36.874 |   1.0893 |     37.737 |     1.3
   33 |   1.0271 |     36.405 |   1.0886 |     37.401 |     1.3
   34 |   1.0230 |     36.532 |   1.0762 |     36.667 |     1.4
   35 |   1.0126 |     35.996 |   1.0602 |     36.544 |     1.4
   36 |   1.0059 |     35.814 |   1.0598 |     37.462 |     1.5
   37 |   0.9972 |     35.526 |   1.0549 |     37.187 |     1.5
   38 |   0.9915 |     35.311 |   1.0456 |     35.902 |     1.5
   39 |   0.9804 |     34.913 |   1.0405 |     37.248 |     1.6
   40 |   0.9703 |     34.709 |   1.0245 |     35.627 |     1.6
   41 |   0.9664 |     34.377 |   1.0325 |     35.963 |     1.7
   42 |   0.9616 |     34.090 |   1.0080 |     34.985 |     1.7
   43 |   0.9494 |     33.643 |   1.0283 |     35.688 |     1.7
   44 |   0.9468 |     33.781 |   1.0219 |     35.260 |     1.8
   45 |   0.9485 |     33.698 |   1.0156 |     35.321 |     1.8
   46 |   0.9413 |     33.179 |   0.9941 |     33.823 |     1.9
   47 |   0.9179 |     32.245 |   1.0099 |     34.373 |     1.9
   48 |   0.9151 |     32.571 |   1.0062 |     34.893 |     1.9
   49 |   0.9055 |     32.124 |   0.9819 |     33.976 |     2.0
   50 |   0.9015 |     31.566 |   0.9848 |     34.251 |     2.0
   51 |   0.8941 |     31.140 |   0.9851 |     34.679 |     2.1
   52 |   0.8802 |     30.903 |   0.9855 |     34.037 |     2.1
   53 |   0.8727 |     30.754 |   0.9670 |     33.456 |     2.1
   54 |   0.8632 |     30.008 |   0.9554 |     32.752 |     2.2
   55 |   0.8543 |     29.709 |   0.9664 |     33.639 |     2.2
   56 |   0.8443 |     29.383 |   0.9611 |     33.394 |     2.3
   57 |   0.8337 |     28.831 |   0.9690 |     33.211 |     2.3
   58 |   0.8374 |     29.240 |   0.9805 |     33.211 |     2.4
   59 |   0.8351 |     28.986 |   0.9513 |     32.569 |     2.4
   60 |   0.8203 |     28.417 |   0.9559 |     32.294 |     2.4
   61 |   0.8029 |     27.903 |   0.9460 |     32.538 |     2.5
   62 |   0.7915 |     27.047 |   0.9497 |     32.080 |     2.5
   63 |   0.7895 |     27.240 |   0.9598 |     32.752 |     2.6
   64 |   0.7746 |     26.765 |   0.9515 |     31.498 |     2.6
   65 |   0.7703 |     26.428 |   0.9483 |     31.437 |     2.6
   66 |   0.7566 |     25.859 |   0.9230 |     30.948 |     2.7
   67 |   0.7495 |     25.754 |   0.9311 |     31.101 |     2.7
   68 |   0.7447 |     25.517 |   0.9336 |     31.254 |     2.8
   69 |   0.7276 |     24.782 |   0.9401 |     31.529 |     2.8
   70 |   0.7177 |     24.577 |   0.9213 |     30.031 |     2.8
   71 |   0.7071 |     24.119 |   0.9143 |     30.489 |     2.9
   72 |   0.6987 |     23.837 |   0.9282 |     29.939 |     2.9
   73 |   0.6918 |     23.456 |   0.9199 |     30.092 |     3.0
   74 |   0.6804 |     23.224 |   0.9275 |     30.734 |     3.0
   75 |   0.6754 |     22.793 |   0.9209 |     29.969 |     3.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 287,970

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5703 |     68.539 |   1.9689 |     55.963 |     0.0
    2 |   1.7398 |     49.729 |   1.5334 |     45.963 |     0.0
    3 |   1.4819 |     46.083 |   1.4203 |     45.963 |     0.1
    4 |   1.4170 |     46.161 |   1.3825 |     45.963 |     0.1
    5 |   1.3871 |     45.945 |   1.3591 |     45.688 |     0.1
    6 |   1.3643 |     45.724 |   1.3398 |     45.229 |     0.1
    7 |   1.3421 |     44.962 |   1.3329 |     45.260 |     0.1
    8 |   1.3281 |     44.879 |   1.3131 |     44.954 |     0.1
    9 |   1.3148 |     44.697 |   1.2997 |     44.648 |     0.2
   10 |   1.3014 |     44.813 |   1.2915 |     44.343 |     0.2
   11 |   1.2885 |     44.332 |   1.2855 |     44.251 |     0.2
   12 |   1.2737 |     43.901 |   1.2662 |     44.006 |     0.2
   13 |   1.2628 |     43.852 |   1.2568 |     43.639 |     0.2
   14 |   1.2470 |     43.631 |   1.2467 |     43.639 |     0.2
   15 |   1.2357 |     43.078 |   1.2355 |     42.569 |     0.3
   16 |   1.2209 |     42.702 |   1.2383 |     42.905 |     0.3
   17 |   1.2082 |     42.493 |   1.2203 |     41.835 |     0.3
   18 |   1.1967 |     42.045 |   1.2098 |     41.590 |     0.3
   19 |   1.1832 |     41.360 |   1.2048 |     40.826 |     0.3
   20 |   1.1685 |     40.835 |   1.1860 |     40.214 |     0.4
   21 |   1.1545 |     40.620 |   1.1780 |     40.550 |     0.4
   22 |   1.1381 |     40.056 |   1.1739 |     40.398 |     0.4
   23 |   1.1207 |     39.587 |   1.1638 |     39.786 |     0.4
   24 |   1.1069 |     39.034 |   1.1565 |     39.052 |     0.4
   25 |   1.0908 |     38.399 |   1.1426 |     38.287 |     0.4
   26 |   1.0766 |     37.670 |   1.1466 |     38.685 |     0.5
   27 |   1.0597 |     36.653 |   1.1418 |     38.593 |     0.5
   28 |   1.0435 |     36.200 |   1.1346 |     37.676 |     0.5
   29 |   1.0320 |     35.803 |   1.1336 |     37.370 |     0.5
   30 |   1.0230 |     35.526 |   1.1232 |     37.034 |     0.5
   31 |   1.0082 |     35.035 |   1.1154 |     37.003 |     0.5
   32 |   0.9836 |     33.969 |   1.1133 |     37.248 |     0.6
   33 |   0.9719 |     33.074 |   1.1025 |     36.086 |     0.6
   34 |   0.9527 |     32.438 |   1.1006 |     36.239 |     0.6
   35 |   0.9396 |     31.897 |   1.0986 |     35.688 |     0.6
   36 |   0.9219 |     31.184 |   1.0843 |     35.566 |     0.6
   37 |   0.9109 |     30.825 |   1.0897 |     34.740 |     0.6
   38 |   0.8892 |     30.157 |   1.0932 |     34.740 |     0.7
   39 |   0.8766 |     29.604 |   1.0870 |     35.046 |     0.7
   40 |   0.8663 |     28.964 |   1.0714 |     34.771 |     0.7
   41 |   0.8457 |     28.411 |   1.0700 |     33.884 |     0.7
   42 |   0.8320 |     28.013 |   1.0744 |     33.853 |     0.7
   43 |   0.8168 |     27.323 |   1.0791 |     33.609 |     0.8
   44 |   0.8377 |     28.047 |   1.0609 |     33.333 |     0.8
   45 |   0.7986 |     26.516 |   1.0502 |     33.211 |     0.8
   46 |   0.7828 |     26.041 |   1.0618 |     33.639 |     0.8
   47 |   0.7609 |     25.235 |   1.0700 |     33.089 |     0.8
   48 |   0.7457 |     24.699 |   1.0709 |     33.028 |     0.8
   49 |   0.7296 |     23.997 |   1.0643 |     32.385 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,000,866

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0343 |     55.574 |   1.4691 |     45.505 |     0.0
    2 |   1.3907 |     45.398 |   1.3285 |     44.679 |     0.1
    3 |   1.3065 |     44.349 |   1.2924 |     44.495 |     0.1
    4 |   1.2693 |     43.879 |   1.2630 |     43.945 |     0.2
    5 |   1.2415 |     43.117 |   1.2404 |     42.905 |     0.2
    6 |   1.2149 |     42.680 |   1.2199 |     42.355 |     0.2
    7 |   1.1896 |     41.796 |   1.2055 |     41.988 |     0.3
    8 |   1.1582 |     40.565 |   1.1736 |     39.755 |     0.3
    9 |   1.1280 |     39.101 |   1.1466 |     39.174 |     0.4
   10 |   1.0908 |     38.195 |   1.1168 |     38.869 |     0.4
   11 |   1.0518 |     36.571 |   1.0995 |     37.370 |     0.4
   12 |   1.0112 |     34.792 |   1.0548 |     36.575 |     0.5
   13 |   0.9673 |     33.134 |   1.0390 |     35.107 |     0.5
   14 |   0.9223 |     31.328 |   1.0233 |     34.526 |     0.5
   15 |   0.8819 |     29.748 |   0.9958 |     33.792 |     0.6
   16 |   0.8271 |     27.356 |   0.9776 |     32.263 |     0.6
   17 |   0.7812 |     25.892 |   0.9643 |     31.927 |     0.7
   18 |   0.7362 |     24.285 |   0.9090 |     30.398 |     0.7
   19 |   0.6822 |     22.351 |   0.9014 |     28.807 |     0.7
   20 |   0.6384 |     20.528 |   0.9362 |     29.419 |     0.8
   21 |   0.5920 |     18.937 |   0.9035 |     28.532 |     0.8
   22 |   0.5439 |     16.943 |   0.9290 |     28.287 |     0.9
   23 |   0.5113 |     15.965 |   0.8991 |     28.012 |     0.9
   24 |   0.4642 |     14.440 |   0.9267 |     28.654 |     0.9
   25 |   0.4353 |     13.595 |   0.9427 |     28.043 |     1.0
   26 |   0.4035 |     12.651 |   0.9437 |     28.073 |     1.0
   27 |   0.3769 |     11.557 |   0.9443 |     27.737 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,248,418

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4139 |     63.866 |   1.8257 |     48.410 |     0.1
    2 |   1.6100 |     46.967 |   1.4614 |     45.994 |     0.1
    3 |   1.4262 |     45.934 |   1.3793 |     45.719 |     0.2
    4 |   1.3647 |     45.564 |   1.3406 |     45.076 |     0.2
    5 |   1.3275 |     45.150 |   1.3164 |     44.832 |     0.3
    6 |   1.3004 |     44.581 |   1.2946 |     44.924 |     0.4
    7 |   1.2802 |     44.200 |   1.2888 |     43.761 |     0.4
    8 |   1.2591 |     43.531 |   1.2659 |     44.495 |     0.5
    9 |   1.2409 |     43.238 |   1.2580 |     43.578 |     0.5
   10 |   1.2236 |     42.691 |   1.2329 |     43.364 |     0.6
   11 |   1.2087 |     42.261 |   1.2216 |     42.508 |     0.6
   12 |   1.1915 |     41.647 |   1.2165 |     42.049 |     0.7
   13 |   1.1736 |     41.106 |   1.1992 |     41.407 |     0.8
   14 |   1.1559 |     40.062 |   1.1844 |     40.826 |     0.8
   15 |   1.1311 |     39.438 |   1.1821 |     39.817 |     0.9
   16 |   1.1026 |     38.112 |   1.1486 |     37.951 |     0.9
   17 |   1.0763 |     36.548 |   1.1289 |     37.676 |     1.0
   18 |   1.0437 |     35.587 |   1.1167 |     36.697 |     1.1
   19 |   1.0104 |     34.068 |   1.0986 |     36.544 |     1.1
   20 |   0.9786 |     32.891 |   1.0915 |     36.208 |     1.2
   21 |   0.9394 |     31.317 |   1.0766 |     35.872 |     1.2
   22 |   0.8951 |     29.411 |   1.0613 |     34.801 |     1.3
   23 |   0.8496 |     27.406 |   1.0527 |     33.578 |     1.4
   24 |   0.8115 |     25.925 |   1.0504 |     33.058 |     1.4
   25 |   0.7678 |     24.622 |   1.0269 |     33.639 |     1.5
   26 |   0.7229 |     22.351 |   1.0424 |     32.202 |     1.5
   27 |   0.6769 |     20.904 |   1.0190 |     31.468 |     1.6
   28 |   0.6404 |     19.467 |   1.0047 |     30.703 |     1.6
   29 |   0.5900 |     17.738 |   1.0134 |     30.367 |     1.7
   30 |   0.5579 |     16.584 |   1.0285 |     30.183 |     1.8
   31 |   0.5293 |     15.855 |   1.0240 |     30.122 |     1.8
   32 |   0.4890 |     14.407 |   1.0119 |     29.144 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 766,626

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5548 |     68.749 |   1.9643 |     57.125 |     0.0
    2 |   1.7613 |     50.492 |   1.5618 |     45.994 |     0.1
    3 |   1.5092 |     46.116 |   1.4450 |     45.994 |     0.1
    4 |   1.4375 |     46.177 |   1.4006 |     45.994 |     0.2
    5 |   1.4077 |     46.122 |   1.3792 |     45.963 |     0.2
    6 |   1.3871 |     45.984 |   1.3566 |     45.841 |     0.2
    7 |   1.3580 |     45.564 |   1.3252 |     45.321 |     0.3
    8 |   1.3297 |     45.271 |   1.3077 |     44.648 |     0.3
    9 |   1.3114 |     44.851 |   1.2897 |     44.557 |     0.4
   10 |   1.2955 |     44.647 |   1.2764 |     44.190 |     0.4
   11 |   1.2836 |     44.448 |   1.2632 |     43.914 |     0.4
   12 |   1.2694 |     44.133 |   1.2537 |     43.700 |     0.5
   13 |   1.2568 |     43.658 |   1.2513 |     43.945 |     0.5
   14 |   1.2477 |     43.465 |   1.2381 |     43.058 |     0.6
   15 |   1.2382 |     43.178 |   1.2328 |     43.364 |     0.6
   16 |   1.2303 |     43.200 |   1.2273 |     42.813 |     0.6
   17 |   1.2240 |     42.874 |   1.2273 |     42.691 |     0.7
   18 |   1.2161 |     42.824 |   1.2171 |     42.385 |     0.7
   19 |   1.2115 |     42.575 |   1.2175 |     41.804 |     0.8
   20 |   1.2039 |     42.388 |   1.2136 |     42.385 |     0.8
   21 |   1.1977 |     42.117 |   1.2033 |     41.835 |     0.8
   22 |   1.1886 |     41.868 |   1.2037 |     42.263 |     0.9
   23 |   1.1843 |     41.653 |   1.1902 |     41.315 |     0.9
   24 |   1.1767 |     41.548 |   1.1936 |     42.202 |     1.0
   25 |   1.1729 |     41.680 |   1.1892 |     41.927 |     1.0
   26 |   1.1680 |     41.277 |   1.1859 |     41.193 |     1.0
   27 |   1.1631 |     41.288 |   1.1769 |     41.009 |     1.1
   28 |   1.1576 |     41.001 |   1.1723 |     40.642 |     1.1
   29 |   1.1542 |     40.791 |   1.1630 |     41.193 |     1.2
   30 |   1.1488 |     40.714 |   1.1653 |     40.459 |     1.2
   31 |   1.1457 |     40.526 |   1.1609 |     40.489 |     1.2
   32 |   1.1407 |     40.382 |   1.1639 |     40.856 |     1.3
   33 |   1.1341 |     40.217 |   1.1599 |     41.101 |     1.3
   34 |   1.1332 |     40.233 |   1.1557 |     40.031 |     1.4
   35 |   1.1265 |     40.106 |   1.1549 |     40.887 |     1.4
   36 |   1.1225 |     39.808 |   1.1485 |     39.908 |     1.4
   37 |   1.1180 |     40.023 |   1.1474 |     39.786 |     1.5
   38 |   1.1177 |     39.869 |   1.1451 |     39.878 |     1.5
   39 |   1.1140 |     39.764 |   1.1397 |     40.153 |     1.6
   40 |   1.1080 |     39.454 |   1.1393 |     38.991 |     1.6
   41 |   1.1019 |     39.228 |   1.1365 |     39.113 |     1.6
   42 |   1.1000 |     39.239 |   1.1351 |     38.624 |     1.7
   43 |   1.0971 |     38.808 |   1.1362 |     38.991 |     1.7
   44 |   1.0940 |     38.968 |   1.1156 |     39.021 |     1.8
   45 |   1.0897 |     38.797 |   1.1297 |     38.930 |     1.8
   46 |   1.0881 |     38.521 |   1.1234 |     38.899 |     1.8
   47 |   1.0802 |     38.399 |   1.1234 |     38.685 |     1.9
   48 |   1.0744 |     38.217 |   1.1213 |     38.654 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 736,546

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2140 |     60.706 |   1.5977 |     45.963 |     0.0
    2 |   1.4853 |     46.105 |   1.4030 |     45.963 |     0.1
    3 |   1.4065 |     46.122 |   1.3676 |     45.963 |     0.1
    4 |   1.3718 |     45.757 |   1.3480 |     45.596 |     0.1
    5 |   1.3439 |     45.420 |   1.3238 |     45.046 |     0.1
    6 |   1.3251 |     45.177 |   1.3064 |     44.801 |     0.2
    7 |   1.3035 |     44.636 |   1.2859 |     44.557 |     0.2
    8 |   1.2893 |     44.316 |   1.2818 |     44.343 |     0.2
    9 |   1.2759 |     44.216 |   1.2621 |     43.731 |     0.2
   10 |   1.2547 |     43.918 |   1.2541 |     44.098 |     0.3
   11 |   1.2371 |     43.404 |   1.2310 |     42.997 |     0.3
   12 |   1.2155 |     43.034 |   1.2291 |     43.517 |     0.3
   13 |   1.1980 |     42.238 |   1.2164 |     41.804 |     0.3
   14 |   1.1725 |     41.421 |   1.1958 |     41.346 |     0.4
   15 |   1.1522 |     40.620 |   1.1691 |     40.489 |     0.4
   16 |   1.1282 |     39.775 |   1.1610 |     40.245 |     0.4
   17 |   1.1052 |     39.167 |   1.1432 |     38.257 |     0.5
   18 |   1.0794 |     38.178 |   1.1308 |     38.807 |     0.5
   19 |   1.0581 |     36.996 |   1.1322 |     38.349 |     0.5
   20 |   1.0314 |     36.140 |   1.1235 |     37.706 |     0.5
   21 |   1.0017 |     34.731 |   1.1119 |     36.728 |     0.6
   22 |   0.9817 |     34.295 |   1.1082 |     36.086 |     0.6
   23 |   0.9518 |     32.621 |   1.0922 |     35.933 |     0.6
   24 |   0.9267 |     31.682 |   1.0853 |     34.740 |     0.6
   25 |   0.9040 |     31.102 |   1.0735 |     34.526 |     0.7
   26 |   0.8790 |     29.814 |   1.0348 |     34.159 |     0.7
   27 |   0.8521 |     28.251 |   1.0476 |     33.823 |     0.7
   28 |   0.8294 |     27.660 |   1.0081 |     33.731 |     0.7
   29 |   0.8004 |     26.533 |   1.0205 |     33.914 |     0.8
   30 |   0.7793 |     25.931 |   1.0038 |     32.997 |     0.8
   31 |   0.7547 |     24.848 |   0.9920 |     32.508 |     0.8
   32 |   0.7233 |     23.578 |   0.9713 |     31.743 |     0.8
   33 |   0.7015 |     23.042 |   0.9701 |     32.049 |     0.9
   34 |   0.6680 |     22.014 |   0.9749 |     31.560 |     0.9
   35 |   0.6539 |     21.550 |   1.0113 |     30.550 |     0.9
   36 |   0.6322 |     20.832 |   0.9823 |     30.856 |     1.0
   37 |   0.6174 |     20.009 |   0.9898 |     30.061 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 771,490

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0335 |     56.198 |   1.4714 |     45.841 |     0.0
    2 |   1.4030 |     45.675 |   1.3324 |     44.281 |     0.1
    3 |   1.3148 |     44.741 |   1.2870 |     44.037 |     0.1
    4 |   1.2713 |     43.956 |   1.2606 |     43.761 |     0.1
    5 |   1.2371 |     43.564 |   1.2330 |     42.691 |     0.2
    6 |   1.2040 |     42.625 |   1.2046 |     42.202 |     0.2
    7 |   1.1733 |     41.460 |   1.1826 |     41.223 |     0.2
    8 |   1.1435 |     40.581 |   1.1579 |     40.336 |     0.3
    9 |   1.1104 |     39.117 |   1.1451 |     39.450 |     0.3
   10 |   1.0759 |     37.780 |   1.1193 |     38.563 |     0.3
   11 |   1.0435 |     36.057 |   1.0955 |     36.758 |     0.4
   12 |   1.0093 |     34.941 |   1.0617 |     35.688 |     0.4
   13 |   0.9802 |     34.013 |   1.0488 |     35.780 |     0.4
   14 |   0.9407 |     32.118 |   1.0499 |     35.138 |     0.5
   15 |   0.9054 |     30.792 |   1.0027 |     34.312 |     0.5
   16 |   0.8698 |     29.317 |   0.9689 |     32.752 |     0.6
   17 |   0.8344 |     27.958 |   0.9746 |     32.355 |     0.6
   18 |   0.7934 |     26.378 |   0.9757 |     30.948 |     0.6
   19 |   0.7574 |     24.997 |   0.9446 |     30.550 |     0.7
   20 |   0.7285 |     23.859 |   0.9342 |     29.572 |     0.7
   21 |   0.6950 |     22.622 |   0.9090 |     29.878 |     0.7
   22 |   0.6733 |     21.987 |   0.9290 |     29.694 |     0.8
   23 |   0.6292 |     20.500 |   0.9138 |     28.869 |     0.8
   24 |   0.6065 |     19.722 |   0.9273 |     29.144 |     0.8
   25 |   0.5750 |     18.550 |   0.9308 |     28.685 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 440,098

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5096 |     67.749 |   1.9207 |     56.483 |     0.0
    2 |   1.7129 |     48.569 |   1.5340 |     45.963 |     0.0
    3 |   1.4904 |     46.067 |   1.4371 |     46.361 |     0.1
    4 |   1.4357 |     46.083 |   1.4052 |     45.963 |     0.1
    5 |   1.4114 |     46.199 |   1.3849 |     45.963 |     0.1
    6 |   1.3971 |     46.122 |   1.3765 |     45.963 |     0.1
    7 |   1.3814 |     46.139 |   1.3658 |     45.963 |     0.2
    8 |   1.3724 |     46.023 |   1.3514 |     45.719 |     0.2
    9 |   1.3589 |     45.962 |   1.3357 |     45.321 |     0.2
   10 |   1.3279 |     45.249 |   1.2999 |     44.404 |     0.2
   11 |   1.2998 |     44.288 |   1.2829 |     44.190 |     0.3
   12 |   1.2804 |     44.061 |   1.2743 |     43.853 |     0.3
   13 |   1.2583 |     43.216 |   1.2555 |     43.089 |     0.3
   14 |   1.2411 |     43.310 |   1.2435 |     43.242 |     0.3
   15 |   1.2276 |     42.730 |   1.2254 |     42.446 |     0.4
   16 |   1.2099 |     41.946 |   1.2152 |     42.355 |     0.4
   17 |   1.1928 |     41.609 |   1.2007 |     41.988 |     0.4
   18 |   1.1801 |     41.205 |   1.1882 |     41.376 |     0.4
   19 |   1.1635 |     41.167 |   1.1740 |     41.284 |     0.5
   20 |   1.1459 |     40.250 |   1.1633 |     41.223 |     0.5
   21 |   1.1287 |     40.123 |   1.1602 |     40.765 |     0.5
   22 |   1.1121 |     39.366 |   1.1482 |     40.428 |     0.5
   23 |   1.0955 |     38.780 |   1.1398 |     40.031 |     0.6
   24 |   1.0811 |     38.128 |   1.1284 |     40.122 |     0.6
   25 |   1.0601 |     37.305 |   1.1230 |     39.694 |     0.6
   26 |   1.0466 |     36.769 |   1.1113 |     38.532 |     0.6
   27 |   1.0315 |     36.118 |   1.1084 |     38.257 |     0.6
   28 |   1.0119 |     35.333 |   1.1039 |     37.859 |     0.7
   29 |   0.9960 |     34.670 |   1.0940 |     37.798 |     0.7
   30 |   0.9778 |     33.880 |   1.0989 |     37.706 |     0.7
   31 |   0.9591 |     33.173 |   1.0986 |     36.544 |     0.7
   32 |   0.9405 |     32.146 |   1.0946 |     36.055 |     0.8
   33 |   0.9181 |     31.383 |   1.0896 |     35.749 |     0.8
   34 |   0.9052 |     30.731 |   1.0791 |     35.688 |     0.8
   35 |   0.8823 |     29.864 |   1.0817 |     35.780 |     0.8
   36 |   0.8806 |     30.190 |   1.0696 |     35.382 |     0.9
   37 |   0.8566 |     29.113 |   1.0654 |     34.495 |     0.9
   38 |   0.8322 |     28.174 |   1.0790 |     34.618 |     0.9
   39 |   0.8074 |     27.019 |   1.0859 |     34.159 |     0.9
   40 |   0.7869 |     26.439 |   1.0811 |     34.618 |     1.0
   41 |   0.7701 |     25.489 |   1.0773 |     33.517 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,127,138

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4528 |     65.186 |   1.9042 |     53.792 |     0.1
    2 |   1.6995 |     48.901 |   1.5165 |     45.963 |     0.1
    3 |   1.4755 |     46.089 |   1.4170 |     45.963 |     0.2
    4 |   1.4013 |     46.061 |   1.3611 |     46.239 |     0.2
    5 |   1.3594 |     45.697 |   1.3402 |     44.801 |     0.3
    6 |   1.3304 |     45.072 |   1.3160 |     44.832 |     0.4
    7 |   1.3122 |     44.625 |   1.2931 |     45.168 |     0.4
    8 |   1.2982 |     44.432 |   1.2863 |     44.343 |     0.5
    9 |   1.2816 |     44.045 |   1.2778 |     44.312 |     0.5
   10 |   1.2667 |     43.780 |   1.2635 |     43.761 |     0.6
   11 |   1.2577 |     43.675 |   1.2566 |     43.823 |     0.7
   12 |   1.2455 |     43.305 |   1.2518 |     43.456 |     0.7
   13 |   1.2356 |     43.200 |   1.2424 |     43.609 |     0.8
   14 |   1.2238 |     42.841 |   1.2231 |     42.783 |     0.9
   15 |   1.2125 |     42.332 |   1.2229 |     42.722 |     0.9
   16 |   1.2013 |     42.189 |   1.2108 |     42.141 |     1.0
   17 |   1.1893 |     41.476 |   1.2102 |     41.804 |     1.0
   18 |   1.1767 |     41.062 |   1.1975 |     41.223 |     1.1
   19 |   1.1596 |     40.399 |   1.1870 |     40.398 |     1.2
   20 |   1.1413 |     39.007 |   1.1819 |     39.511 |     1.2
   21 |   1.1224 |     38.559 |   1.1683 |     39.021 |     1.3
   22 |   1.0991 |     37.769 |   1.1563 |     39.266 |     1.3
   23 |   1.0757 |     37.057 |   1.1439 |     38.287 |     1.4
   24 |   1.0525 |     35.891 |   1.1370 |     38.226 |     1.5
   25 |   1.0194 |     34.394 |   1.1207 |     36.544 |     1.5
   26 |   0.9873 |     33.002 |   1.1131 |     36.330 |     1.6
   27 |   0.9587 |     31.842 |   1.0981 |     35.474 |     1.6
   28 |   0.9260 |     30.571 |   1.0895 |     35.138 |     1.7
   29 |   0.8972 |     29.798 |   1.0795 |     34.679 |     1.8
   30 |   0.8579 |     28.306 |   1.1219 |     36.116 |     1.8
   31 |   0.8260 |     26.986 |   1.0716 |     33.303 |     1.9
   32 |   0.7859 |     25.511 |   1.0547 |     33.058 |     2.0
   33 |   0.7579 |     24.533 |   1.0679 |     33.150 |     2.0
   34 |   0.7180 |     23.445 |   1.0573 |     32.232 |     2.1
   35 |   0.6768 |     21.898 |   1.0479 |     31.713 |     2.1
   36 |   0.6414 |     20.081 |   1.0382 |     31.774 |     2.2
   37 |   0.6160 |     19.550 |   1.0320 |     30.765 |     2.3
   38 |   0.5857 |     18.401 |   1.0519 |     31.988 |     2.3
   39 |   0.5606 |     17.534 |   1.0502 |     31.957 |     2.4
   40 |   0.5225 |     16.225 |   1.0420 |     31.162 |     2.4
   41 |   0.4978 |     15.380 |   1.0612 |     31.346 |     2.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,622,690

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5140 |     67.048 |   1.9367 |     54.832 |     0.1
    2 |   1.7415 |     49.216 |   1.5417 |     45.994 |     0.2
    3 |   1.4941 |     46.089 |   1.4248 |     45.963 |     0.2
    4 |   1.4246 |     46.122 |   1.3880 |     45.872 |     0.3
    5 |   1.3928 |     46.150 |   1.3635 |     45.474 |     0.4
    6 |   1.3630 |     45.807 |   1.3343 |     45.138 |     0.5
    7 |   1.3356 |     45.023 |   1.3113 |     44.954 |     0.6
    8 |   1.3188 |     44.757 |   1.2987 |     44.924 |     0.6
    9 |   1.3018 |     44.592 |   1.2768 |     44.343 |     0.7
   10 |   1.2827 |     44.095 |   1.2642 |     44.404 |     0.8
   11 |   1.2665 |     43.968 |   1.2496 |     43.853 |     0.9
   12 |   1.2495 |     43.840 |   1.2359 |     43.823 |     1.0
   13 |   1.2364 |     43.316 |   1.2324 |     43.364 |     1.0
   14 |   1.2275 |     43.111 |   1.2167 |     43.028 |     1.1
   15 |   1.2142 |     42.410 |   1.2126 |     42.599 |     1.2
   16 |   1.2062 |     42.200 |   1.2028 |     42.416 |     1.3
   17 |   1.1989 |     42.161 |   1.1966 |     41.835 |     1.4
   18 |   1.1886 |     41.835 |   1.1963 |     41.896 |     1.4
   19 |   1.1816 |     41.697 |   1.1878 |     41.804 |     1.5
   20 |   1.1736 |     41.692 |   1.1838 |     42.018 |     1.6
   21 |   1.1651 |     41.294 |   1.1850 |     41.896 |     1.7
   22 |   1.1598 |     41.227 |   1.1758 |     41.437 |     1.8
   23 |   1.1526 |     40.984 |   1.1706 |     41.193 |     1.8
   24 |   1.1458 |     40.852 |   1.1703 |     40.642 |     1.9
   25 |   1.1370 |     40.366 |   1.1687 |     41.040 |     2.0
   26 |   1.1288 |     40.244 |   1.1579 |     40.673 |     2.1
   27 |   1.1262 |     40.327 |   1.1638 |     40.245 |     2.2
   28 |   1.1365 |     40.885 |   1.1646 |     40.887 |     2.2
   29 |   1.1270 |     40.559 |   1.1541 |     40.367 |     2.3
   30 |   1.1136 |     39.957 |   1.1510 |     39.969 |     2.4
   31 |   1.1091 |     39.769 |   1.1494 |     40.489 |     2.5
   32 |   1.1047 |     39.515 |   1.1419 |     38.930 |     2.6
   33 |   1.0984 |     39.128 |   1.1362 |     39.235 |     2.6
   34 |   1.0955 |     39.184 |   1.1370 |     38.960 |     2.7
   35 |   1.0904 |     39.195 |   1.1278 |     38.838 |     2.8
   36 |   1.0841 |     38.786 |   1.1319 |     39.083 |     2.9
   37 |   1.0866 |     38.841 |   1.1344 |     38.777 |     3.0
   38 |   1.0801 |     38.719 |   1.1273 |     38.807 |     3.0
   39 |   1.0738 |     38.482 |   1.1230 |     38.624 |     3.1
   40 |   1.0712 |     38.399 |   1.1218 |     38.135 |     3.2
   41 |   1.0647 |     38.145 |   1.1239 |     38.593 |     3.3
   42 |   1.0646 |     38.211 |   1.1203 |     38.838 |     3.4
   43 |   1.0588 |     38.184 |   1.1222 |     38.073 |     3.4
   44 |   1.0522 |     37.742 |   1.1153 |     38.257 |     3.5
   45 |   1.0496 |     37.609 |   1.1167 |     38.716 |     3.6
   46 |   1.0583 |     38.112 |   1.1042 |     37.737 |     3.7
   47 |   1.0453 |     37.609 |   1.1233 |     38.440 |     3.8
   48 |   1.0466 |     37.609 |   1.1058 |     37.676 |     3.8
   49 |   1.0396 |     37.416 |   1.1008 |     38.104 |     3.9
   50 |   1.0366 |     37.521 |   1.1054 |     38.532 |     4.0
   51 |   1.0299 |     36.935 |   1.0979 |     37.615 |     4.1
   52 |   1.0282 |     37.112 |   1.0965 |     37.951 |     4.2
   53 |   1.0192 |     36.648 |   1.0876 |     37.584 |     4.2
   54 |   1.0174 |     36.626 |   1.1052 |     37.982 |     4.3
   55 |   1.0127 |     36.482 |   1.0913 |     37.768 |     4.4
   56 |   1.0084 |     36.189 |   1.0843 |     37.676 |     4.5
   57 |   1.0181 |     36.642 |   1.0851 |     37.645 |     4.6
   58 |   0.9993 |     35.687 |   1.0794 |     37.248 |     4.6
   59 |   0.9917 |     35.482 |   1.0847 |     37.064 |     4.7
   60 |   0.9910 |     35.504 |   1.0809 |     36.820 |     4.8
   61 |   0.9821 |     34.902 |   1.0698 |     36.208 |     4.9
   62 |   0.9766 |     34.714 |   1.0710 |     35.566 |     5.0
   63 |   0.9729 |     34.328 |   1.0700 |     35.810 |     5.0
   64 |   0.9629 |     34.156 |   1.0637 |     35.933 |     5.1
   65 |   0.9594 |     34.085 |   1.0711 |     36.575 |     5.2
   66 |   0.9541 |     33.858 |   1.0587 |     35.260 |     5.3
   67 |   0.9478 |     33.853 |   1.0761 |     35.474 |     5.4
   68 |   0.9418 |     33.488 |   1.0655 |     36.024 |     5.4
   69 |   0.9473 |     33.405 |   1.0702 |     35.841 |     5.5
   70 |   0.9577 |     34.223 |   1.0700 |     36.239 |     5.6
   71 |   0.9332 |     32.842 |   1.0578 |     35.352 |     5.7
   72 |   0.9228 |     32.577 |   1.0616 |     35.688 |     5.8
   73 |   0.9149 |     32.350 |   1.0674 |     34.985 |     5.8
   74 |   0.9119 |     31.842 |   1.0579 |     35.260 |     5.9
   75 |   0.9032 |     31.919 |   1.0554 |     34.801 |     6.0
   76 |   0.8997 |     31.770 |   1.0542 |     35.107 |     6.1
   77 |   0.8915 |     31.079 |   1.0557 |     34.465 |     6.2
   78 |   0.8860 |     30.853 |   1.0521 |     34.771 |     6.2
   79 |   0.8822 |     30.720 |   1.0525 |     34.404 |     6.3
   80 |   0.8815 |     30.549 |   1.0371 |     33.914 |     6.4
   81 |   0.8675 |     30.162 |   1.0416 |     34.067 |     6.5
   82 |   0.8545 |     29.483 |   1.0419 |     33.272 |     6.6
   83 |   0.8486 |     29.146 |   1.0393 |     32.844 |     6.6
   84 |   0.8385 |     28.947 |   1.0360 |     32.997 |     6.7
   85 |   0.8339 |     28.511 |   1.0218 |     32.355 |     6.8
   86 |   0.8294 |     28.505 |   1.0393 |     32.783 |     6.9
   87 |   0.8239 |     28.373 |   1.0458 |     33.333 |     7.0
   88 |   0.8262 |     28.234 |   1.0235 |     33.089 |     7.0
   89 |   0.8073 |     27.417 |   1.0352 |     31.835 |     7.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,530,146

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0434 |     56.126 |   1.4837 |     45.963 |     0.1
    2 |   1.4153 |     45.973 |   1.3536 |     46.361 |     0.1
    3 |   1.3344 |     44.912 |   1.3002 |     44.465 |     0.2
    4 |   1.2888 |     44.111 |   1.2723 |     44.924 |     0.2
    5 |   1.2585 |     43.840 |   1.2543 |     43.517 |     0.3
    6 |   1.2386 |     43.807 |   1.2360 |     43.578 |     0.4
    7 |   1.2103 |     42.443 |   1.2096 |     42.232 |     0.4
    8 |   1.1863 |     41.692 |   1.1891 |     41.498 |     0.5
    9 |   1.1684 |     41.283 |   1.1790 |     40.703 |     0.6
   10 |   1.1465 |     40.686 |   1.1611 |     40.428 |     0.6
   11 |   1.1200 |     39.874 |   1.1423 |     39.327 |     0.7
   12 |   1.0999 |     38.747 |   1.1263 |     37.798 |     0.7
   13 |   1.0743 |     37.836 |   1.1042 |     38.226 |     0.8
   14 |   1.0530 |     36.786 |   1.0789 |     37.584 |     0.9
   15 |   1.0306 |     36.046 |   1.0682 |     36.544 |     0.9
   16 |   1.0109 |     35.322 |   1.0454 |     36.483 |     1.0
   17 |   0.9882 |     34.328 |   1.0172 |     35.352 |     1.0
   18 |   0.9676 |     33.576 |   1.0119 |     34.648 |     1.1
   19 |   0.9476 |     33.157 |   1.0097 |     34.985 |     1.2
   20 |   0.9255 |     32.079 |   0.9845 |     34.067 |     1.2
   21 |   0.9003 |     31.113 |   0.9733 |     33.761 |     1.3
   22 |   0.8740 |     29.792 |   0.9561 |     33.119 |     1.3
   23 |   0.8468 |     28.903 |   0.9503 |     32.997 |     1.4
   24 |   0.8217 |     27.682 |   0.9469 |     31.835 |     1.5
   25 |   0.8016 |     26.947 |   0.9413 |     31.927 |     1.5
   26 |   0.7716 |     25.815 |   0.9122 |     31.223 |     1.6
   27 |   0.7502 |     25.025 |   0.9220 |     30.856 |     1.6
   28 |   0.7236 |     24.185 |   0.9182 |     30.550 |     1.7
   29 |   0.7083 |     23.467 |   0.8938 |     29.327 |     1.8
   30 |   0.6711 |     21.970 |   0.8918 |     29.235 |     1.8
   31 |   0.6485 |     21.302 |   0.8933 |     29.327 |     1.9
   32 |   0.6267 |     20.268 |   0.8872 |     29.113 |     2.0
   33 |   0.6007 |     19.749 |   0.8790 |     28.135 |     2.0
   34 |   0.5761 |     18.755 |   0.8855 |     27.676 |     2.1
   35 |   0.5625 |     18.103 |   0.8946 |     28.226 |     2.1
   36 |   0.5336 |     17.313 |   0.8988 |     27.370 |     2.2
   37 |   0.5109 |     16.606 |   0.8923 |     28.073 |     2.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 732,386

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5425 |     68.191 |   2.0100 |     58.654 |     0.0
    2 |   1.7851 |     50.956 |   1.5586 |     45.994 |     0.1
    3 |   1.5039 |     46.105 |   1.4296 |     45.994 |     0.1
    4 |   1.4280 |     46.061 |   1.3902 |     45.841 |     0.2
    5 |   1.3984 |     45.918 |   1.3705 |     45.627 |     0.2
    6 |   1.3796 |     45.912 |   1.3570 |     45.627 |     0.2
    7 |   1.3643 |     45.873 |   1.3408 |     45.291 |     0.3
    8 |   1.3409 |     45.575 |   1.3266 |     45.933 |     0.3
    9 |   1.3259 |     45.663 |   1.3132 |     45.749 |     0.4
   10 |   1.3115 |     45.315 |   1.2974 |     45.474 |     0.4
   11 |   1.2960 |     44.912 |   1.2856 |     44.893 |     0.4
   12 |   1.2825 |     44.586 |   1.2666 |     44.679 |     0.5
   13 |   1.2670 |     44.260 |   1.2578 |     43.884 |     0.5
   14 |   1.2548 |     44.183 |   1.2485 |     43.884 |     0.6
   15 |   1.2413 |     44.017 |   1.2367 |     43.578 |     0.6
   16 |   1.2321 |     43.702 |   1.2261 |     44.220 |     0.6
   17 |   1.2256 |     43.537 |   1.2175 |     43.792 |     0.7
   18 |   1.2162 |     43.365 |   1.2159 |     43.089 |     0.7
   19 |   1.2076 |     42.979 |   1.2127 |     43.119 |     0.7
   20 |   1.2030 |     43.078 |   1.2059 |     43.303 |     0.8
   21 |   1.1969 |     42.846 |   1.2025 |     43.425 |     0.8
   22 |   1.1921 |     42.940 |   1.2026 |     43.333 |     0.9
   23 |   1.1879 |     42.570 |   1.1957 |     42.599 |     0.9
   24 |   1.1841 |     42.609 |   1.1919 |     43.150 |     0.9
   25 |   1.1794 |     42.354 |   1.1902 |     42.232 |     1.0
   26 |   1.1754 |     42.504 |   1.1856 |     42.416 |     1.0
   27 |   1.1745 |     42.211 |   1.1901 |     42.446 |     1.1
   28 |   1.1696 |     42.266 |   1.1798 |     42.324 |     1.1
   29 |   1.1668 |     42.073 |   1.1759 |     42.049 |     1.1
   30 |   1.1617 |     41.907 |   1.1739 |     42.202 |     1.2
   31 |   1.1591 |     41.708 |   1.1738 |     41.896 |     1.2
   32 |   1.1564 |     41.874 |   1.1688 |     41.651 |     1.3
   33 |   1.1513 |     41.614 |   1.1662 |     41.498 |     1.3
   34 |   1.1500 |     41.609 |   1.1647 |     42.202 |     1.3
   35 |   1.1428 |     41.471 |   1.1636 |     41.590 |     1.4
   36 |   1.1420 |     41.338 |   1.1604 |     41.621 |     1.4
   37 |   1.1425 |     41.244 |   1.1588 |     41.131 |     1.5
   38 |   1.1367 |     41.299 |   1.1585 |     41.284 |     1.5
   39 |   1.1352 |     41.128 |   1.1587 |     41.284 |     1.5
   40 |   1.1321 |     41.139 |   1.1521 |     41.376 |     1.6
   41 |   1.1325 |     41.233 |   1.1561 |     40.673 |     1.6
   42 |   1.1276 |     41.073 |   1.1561 |     41.131 |     1.7
   43 |   1.1272 |     40.979 |   1.1548 |     41.193 |     1.7
   44 |   1.1245 |     40.791 |   1.1544 |     40.612 |     1.7
   45 |   1.1212 |     40.852 |   1.1445 |     40.826 |     1.8
   46 |   1.1203 |     40.868 |   1.1484 |     40.245 |     1.8
   47 |   1.1175 |     40.708 |   1.1485 |     40.581 |     1.9
   48 |   1.1166 |     40.658 |   1.1467 |     40.642 |     1.9
   49 |   1.1158 |     40.565 |   1.1409 |     40.367 |     1.9
   50 |   1.1108 |     40.614 |   1.1475 |     40.917 |     2.0
   51 |   1.1121 |     40.736 |   1.1396 |     40.336 |     2.0
   52 |   1.1081 |     40.520 |   1.1388 |     40.245 |     2.0
   53 |   1.1064 |     40.244 |   1.1373 |     40.245 |     2.1
   54 |   1.1049 |     40.366 |   1.1340 |     39.939 |     2.1
   55 |   1.1041 |     40.404 |   1.1353 |     40.520 |     2.2
   56 |   1.1029 |     40.266 |   1.1363 |     40.306 |     2.2
   57 |   1.1017 |     40.581 |   1.1306 |     40.428 |     2.2
   58 |   1.0980 |     40.007 |   1.1300 |     39.847 |     2.3
   59 |   1.0981 |     40.283 |   1.1305 |     39.908 |     2.3
   60 |   1.0965 |     40.465 |   1.1286 |     40.000 |     2.4
   61 |   1.0974 |     40.327 |   1.1288 |     39.847 |     2.4
   62 |   1.0969 |     40.178 |   1.1250 |     40.336 |     2.4
   63 |   1.0913 |     40.156 |   1.1281 |     40.153 |     2.5
   64 |   1.0926 |     40.415 |   1.1227 |     40.306 |     2.5
   65 |   1.0918 |     40.095 |   1.1239 |     40.153 |     2.6
   66 |   1.0921 |     40.040 |   1.1194 |     40.000 |     2.6
   67 |   1.0884 |     40.106 |   1.1219 |     40.245 |     2.6
   68 |   1.0907 |     40.095 |   1.1211 |     40.245 |     2.7
   69 |   1.0882 |     39.990 |   1.1224 |     39.939 |     2.7
   70 |   1.0857 |     39.896 |   1.1154 |     39.786 |     2.8
   71 |   1.0889 |     39.962 |   1.1164 |     40.092 |     2.8
   72 |   1.0869 |     40.101 |   1.1148 |     40.306 |     2.8
   73 |   1.0828 |     40.023 |   1.1166 |     40.367 |     2.9
   74 |   1.0839 |     39.891 |   1.1089 |     39.908 |     2.9
   75 |   1.0836 |     39.951 |   1.1121 |     40.092 |     3.0
   76 |   1.0831 |     40.045 |   1.1106 |     39.908 |     3.0
   77 |   1.0818 |     39.985 |   1.1110 |     39.786 |     3.0
   78 |   1.0779 |     39.985 |   1.1130 |     39.144 |     3.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,179,938

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3504 |     61.479 |   1.7960 |     48.624 |     0.1
    2 |   1.5981 |     47.205 |   1.4467 |     45.535 |     0.1
    3 |   1.4103 |     45.581 |   1.3616 |     45.596 |     0.2
    4 |   1.3533 |     44.940 |   1.3246 |     45.229 |     0.2
    5 |   1.3205 |     44.327 |   1.3052 |     43.456 |     0.3
    6 |   1.2971 |     44.111 |   1.2848 |     43.976 |     0.3
    7 |   1.2757 |     43.719 |   1.2743 |     43.333 |     0.4
    8 |   1.2574 |     43.410 |   1.2538 |     43.486 |     0.5
    9 |   1.2374 |     42.918 |   1.2390 |     42.905 |     0.5
   10 |   1.2194 |     42.200 |   1.2288 |     42.752 |     0.6
   11 |   1.2018 |     41.819 |   1.2098 |     41.896 |     0.6
   12 |   1.1807 |     41.250 |   1.1992 |     41.957 |     0.7
   13 |   1.1634 |     40.548 |   1.1927 |     41.284 |     0.7
   14 |   1.1482 |     40.084 |   1.1951 |     40.367 |     0.8
   15 |   1.1245 |     39.355 |   1.1658 |     39.908 |     0.9
   16 |   1.1016 |     38.073 |   1.1500 |     39.144 |     0.9
   17 |   1.0708 |     36.963 |   1.1324 |     37.217 |     1.0
   18 |   1.0409 |     35.676 |   1.1150 |     36.391 |     1.0
   19 |   1.0176 |     34.731 |   1.0917 |     36.667 |     1.1
   20 |   0.9833 |     32.842 |   1.0778 |     35.229 |     1.2
   21 |   0.9538 |     32.168 |   1.0859 |     36.024 |     1.2
   22 |   0.9237 |     31.201 |   1.0626 |     34.495 |     1.3
   23 |   0.8799 |     28.842 |   1.0304 |     32.202 |     1.3
   24 |   0.8387 |     26.953 |   1.0239 |     32.477 |     1.4
   25 |   0.8067 |     26.063 |   1.0124 |     31.774 |     1.4
   26 |   0.7692 |     24.240 |   1.0092 |     31.682 |     1.5
   27 |   0.7278 |     23.009 |   1.0097 |     30.856 |     1.6
   28 |   0.6931 |     21.495 |   1.0024 |     30.367 |     1.6
   29 |   0.6494 |     20.108 |   1.0053 |     30.367 |     1.7
   30 |   0.6097 |     18.965 |   1.0107 |     30.061 |     1.7
   31 |   0.5840 |     17.865 |   1.0080 |     30.061 |     1.8
   32 |   0.5474 |     16.766 |   0.9812 |     30.367 |     1.8
   33 |   0.5163 |     15.810 |   1.0050 |     29.847 |     1.9
   34 |   0.4827 |     14.385 |   0.9813 |     29.266 |     2.0
   35 |   0.4540 |     13.485 |   1.0075 |     28.318 |     2.0
   36 |   0.4323 |     12.866 |   0.9924 |     28.257 |     2.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 558,242

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3969 |     62.838 |   1.8464 |     51.346 |     0.0
    2 |   1.6478 |     47.531 |   1.4720 |     45.627 |     0.1
    3 |   1.4285 |     45.779 |   1.3720 |     45.596 |     0.1
    4 |   1.3615 |     45.072 |   1.3334 |     44.801 |     0.1
    5 |   1.3251 |     44.498 |   1.3003 |     44.434 |     0.2
    6 |   1.2983 |     44.183 |   1.2884 |     44.128 |     0.2
    7 |   1.2738 |     43.846 |   1.2652 |     44.067 |     0.3
    8 |   1.2537 |     43.735 |   1.2508 |     43.547 |     0.3
    9 |   1.2308 |     42.846 |   1.2289 |     42.080 |     0.3
   10 |   1.2091 |     42.156 |   1.2157 |     42.110 |     0.4
   11 |   1.1883 |     41.482 |   1.1964 |     41.101 |     0.4
   12 |   1.1668 |     40.299 |   1.1792 |     40.336 |     0.4
   13 |   1.1509 |     39.907 |   1.1654 |     40.214 |     0.5
   14 |   1.1317 |     39.288 |   1.1617 |     39.725 |     0.5
   15 |   1.1131 |     38.449 |   1.1451 |     39.021 |     0.5
   16 |   1.0986 |     38.029 |   1.1350 |     38.318 |     0.6
   17 |   1.0820 |     37.349 |   1.1280 |     38.349 |     0.6
   18 |   1.0674 |     37.106 |   1.1095 |     37.370 |     0.6
   19 |   1.0510 |     36.195 |   1.1114 |     36.911 |     0.7
   20 |   1.0332 |     35.604 |   1.0903 |     36.483 |     0.7
   21 |   1.0178 |     34.919 |   1.0870 |     36.544 |     0.7
   22 |   1.0018 |     34.405 |   1.0615 |     35.657 |     0.8
   23 |   0.9849 |     33.576 |   1.0681 |     36.116 |     0.8
   24 |   0.9668 |     33.007 |   1.0507 |     34.495 |     0.9
   25 |   0.9538 |     32.361 |   1.0482 |     34.465 |     0.9
   26 |   0.9377 |     31.919 |   1.0366 |     34.404 |     0.9
   27 |   0.9181 |     31.002 |   1.0337 |     34.893 |     1.0
   28 |   0.9038 |     30.538 |   1.0400 |     33.884 |     1.0
   29 |   0.8910 |     30.301 |   1.0104 |     33.211 |     1.0
   30 |   0.8707 |     29.301 |   1.0081 |     32.446 |     1.1
   31 |   0.8534 |     28.936 |   1.0009 |     32.202 |     1.1
   32 |   0.8520 |     28.621 |   1.0005 |     33.119 |     1.1
   33 |   0.8290 |     27.931 |   0.9895 |     32.997 |     1.2
   34 |   0.8053 |     26.881 |   0.9766 |     31.804 |     1.2
   35 |   0.7822 |     26.014 |   0.9750 |     31.131 |     1.2
   36 |   0.7699 |     25.428 |   0.9938 |     31.927 |     1.3
   37 |   0.7560 |     25.168 |   0.9706 |     31.284 |     1.3
   38 |   0.7359 |     24.185 |   0.9522 |     30.398 |     1.3
   39 |   0.7158 |     23.688 |   0.9530 |     30.856 |     1.4
   40 |   0.7032 |     23.064 |   0.9414 |     30.367 |     1.4
   41 |   0.6826 |     22.401 |   0.9663 |     30.336 |     1.5
   42 |   0.6652 |     21.677 |   0.9470 |     29.725 |     1.5
   43 |   0.6565 |     21.346 |   0.9499 |     30.061 |     1.5
   44 |   0.6337 |     20.600 |   0.9442 |     30.092 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 406,818

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3308 |     60.866 |   1.7221 |     48.807 |     0.0
    2 |   1.5626 |     46.282 |   1.4370 |     46.361 |     0.0
    3 |   1.4134 |     45.713 |   1.3648 |     45.291 |     0.1
    4 |   1.3533 |     45.183 |   1.3238 |     44.006 |     0.1
    5 |   1.3120 |     44.117 |   1.2956 |     44.343 |     0.1
    6 |   1.2806 |     43.769 |   1.2687 |     43.456 |     0.1
    7 |   1.2552 |     43.327 |   1.2504 |     43.058 |     0.2
    8 |   1.2307 |     42.752 |   1.2360 |     43.180 |     0.2
    9 |   1.2088 |     42.178 |   1.2124 |     41.529 |     0.2
   10 |   1.1810 |     41.233 |   1.1991 |     41.498 |     0.2
   11 |   1.1607 |     40.741 |   1.1828 |     41.193 |     0.3
   12 |   1.1355 |     39.443 |   1.1752 |     39.969 |     0.3
   13 |   1.1122 |     38.244 |   1.1544 |     39.755 |     0.3
   14 |   1.0851 |     37.488 |   1.1374 |     38.991 |     0.3
   15 |   1.0606 |     36.239 |   1.1231 |     38.135 |     0.4
   16 |   1.0325 |     35.471 |   1.1116 |     36.728 |     0.4
   17 |   1.0049 |     33.996 |   1.0910 |     36.269 |     0.4
   18 |   0.9710 |     33.002 |   1.0832 |     36.177 |     0.4
   19 |   0.9477 |     31.632 |   1.0544 |     35.688 |     0.5
   20 |   0.9164 |     30.637 |   1.0473 |     34.404 |     0.5
   21 |   0.8768 |     29.008 |   1.0274 |     34.220 |     0.5
   22 |   0.8506 |     27.754 |   1.0167 |     33.303 |     0.5
   23 |   0.8181 |     26.726 |   1.0147 |     32.538 |     0.6
   24 |   0.7838 |     25.428 |   1.0092 |     32.049 |     0.6
   25 |   0.7541 |     24.086 |   0.9908 |     31.498 |     0.6
   26 |   0.7198 |     22.881 |   0.9998 |     30.795 |     0.6
   27 |   0.6895 |     21.611 |   0.9687 |     30.214 |     0.7
   28 |   0.6589 |     21.031 |   0.9451 |     30.031 |     0.7
   29 |   0.6295 |     19.396 |   0.9594 |     29.939 |     0.7
   30 |   0.6133 |     19.075 |   0.9395 |     29.694 |     0.7
   31 |   0.5818 |     17.932 |   0.9570 |     29.878 |     0.7
   32 |   0.5607 |     17.252 |   0.9540 |     29.327 |     0.8
   33 |   0.5333 |     16.236 |   0.9547 |     29.602 |     0.8
   34 |   0.5082 |     15.247 |   0.9814 |     29.847 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,088,098

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2343 |     60.883 |   1.6188 |     45.994 |     0.1
    2 |   1.4895 |     46.155 |   1.4058 |     45.994 |     0.2
    3 |   1.4076 |     46.127 |   1.3770 |     45.902 |     0.2
    4 |   1.3828 |     46.050 |   1.3580 |     46.024 |     0.3
    5 |   1.3461 |     45.426 |   1.3138 |     45.291 |     0.4
    6 |   1.3179 |     45.017 |   1.2945 |     45.168 |     0.5
    7 |   1.2991 |     44.923 |   1.2781 |     44.281 |     0.6
    8 |   1.2804 |     44.525 |   1.2655 |     44.098 |     0.7
    9 |   1.2644 |     44.161 |   1.2551 |     43.945 |     0.7
   10 |   1.2470 |     43.824 |   1.2542 |     43.792 |     0.8
   11 |   1.2303 |     43.415 |   1.2324 |     42.538 |     0.9
   12 |   1.2152 |     42.785 |   1.2277 |     42.875 |     1.0
   13 |   1.2034 |     42.741 |   1.2213 |     42.630 |     1.1
   14 |   1.1930 |     42.349 |   1.2077 |     42.844 |     1.1
   15 |   1.1821 |     42.150 |   1.1992 |     42.813 |     1.2
   16 |   1.1718 |     42.095 |   1.1992 |     42.263 |     1.3
   17 |   1.1602 |     41.515 |   1.1875 |     41.651 |     1.4
   18 |   1.1527 |     41.515 |   1.1888 |     41.988 |     1.5
   19 |   1.1411 |     41.139 |   1.1822 |     41.621 |     1.5
   20 |   1.1346 |     41.040 |   1.1698 |     41.437 |     1.6
   21 |   1.1297 |     40.896 |   1.1618 |     41.254 |     1.7
   22 |   1.1164 |     40.239 |   1.1491 |     40.459 |     1.8
   23 |   1.1010 |     39.526 |   1.1438 |     40.122 |     1.9
   24 |   1.0963 |     39.382 |   1.1373 |     39.664 |     1.9
   25 |   1.0880 |     39.377 |   1.1308 |     39.052 |     2.0
   26 |   1.0789 |     38.664 |   1.1316 |     39.664 |     2.1
   27 |   1.0698 |     38.543 |   1.1208 |     38.471 |     2.2
   28 |   1.0608 |     38.382 |   1.1171 |     38.073 |     2.3
   29 |   1.0494 |     37.731 |   1.0973 |     38.104 |     2.4
   30 |   1.0402 |     37.372 |   1.0971 |     38.685 |     2.4
   31 |   1.0325 |     37.189 |   1.0991 |     37.890 |     2.5
   32 |   1.0232 |     36.631 |   1.0897 |     37.645 |     2.6
   33 |   1.0104 |     36.134 |   1.0709 |     37.156 |     2.7
   34 |   1.0036 |     36.013 |   1.0634 |     36.820 |     2.8
   35 |   0.9938 |     35.538 |   1.0775 |     37.951 |     2.8
   36 |   0.9872 |     35.422 |   1.0620 |     36.942 |     2.9
   37 |   0.9761 |     34.637 |   1.0442 |     37.064 |     3.0
   38 |   0.9655 |     34.444 |   1.0424 |     35.841 |     3.1
   39 |   0.9575 |     33.996 |   1.0518 |     36.514 |     3.2
   40 |   0.9531 |     33.930 |   1.0586 |     35.902 |     3.3
   41 |   0.9407 |     33.576 |   1.0359 |     35.382 |     3.3
   42 |   0.9365 |     33.046 |   1.0308 |     35.046 |     3.4
   43 |   0.9258 |     32.659 |   1.0386 |     34.985 |     3.5
   44 |   0.9178 |     32.389 |   1.0285 |     34.862 |     3.6
   45 |   0.9085 |     31.886 |   1.0010 |     34.128 |     3.7
   46 |   0.8998 |     31.731 |   1.0157 |     33.700 |     3.7
   47 |   0.8906 |     31.311 |   1.0044 |     34.434 |     3.8
   48 |   0.8849 |     30.919 |   1.0136 |     33.853 |     3.9
   49 |   0.8730 |     30.218 |   0.9928 |     32.875 |     4.0
   50 |   0.8672 |     30.196 |   0.9989 |     33.211 |     4.1
   51 |   0.8551 |     29.604 |   0.9800 |     33.303 |     4.1
   52 |   0.8472 |     29.582 |   0.9858 |     33.333 |     4.2
   53 |   0.8392 |     28.947 |   0.9930 |     32.630 |     4.3
   54 |   0.8297 |     28.522 |   0.9679 |     32.355 |     4.4
   55 |   0.8218 |     28.422 |   0.9641 |     32.477 |     4.5
   56 |   0.8160 |     28.008 |   0.9657 |     32.477 |     4.6
   57 |   0.8083 |     27.953 |   0.9634 |     32.385 |     4.6
   58 |   0.7986 |     27.312 |   0.9616 |     31.835 |     4.7
   59 |   0.7930 |     27.196 |   0.9517 |     32.416 |     4.8
   60 |   0.7868 |     26.798 |   0.9465 |     31.988 |     4.9
   61 |   0.7795 |     26.445 |   0.9514 |     31.254 |     5.0
   62 |   0.7646 |     25.936 |   0.9348 |     32.018 |     5.0
   63 |   0.7604 |     25.975 |   0.9589 |     31.254 |     5.1
   64 |   0.7460 |     25.356 |   0.9584 |     31.315 |     5.2
   65 |   0.7409 |     25.008 |   0.9659 |     31.193 |     5.3
   66 |   0.7362 |     25.030 |   0.9364 |     31.070 |     5.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 885,730

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1803 |     59.634 |   1.5631 |     45.963 |     0.0
    2 |   1.4641 |     46.188 |   1.3845 |     45.963 |     0.1
    3 |   1.3807 |     46.039 |   1.3401 |     45.382 |     0.1
    4 |   1.3461 |     45.597 |   1.3187 |     45.596 |     0.1
    5 |   1.3208 |     45.034 |   1.2999 |     45.046 |     0.2
    6 |   1.2975 |     44.564 |   1.2794 |     44.434 |     0.2
    7 |   1.2777 |     44.089 |   1.2677 |     43.914 |     0.2
    8 |   1.2598 |     44.017 |   1.2510 |     43.823 |     0.3
    9 |   1.2382 |     43.487 |   1.2361 |     43.578 |     0.3
   10 |   1.2181 |     42.857 |   1.2169 |     43.150 |     0.3
   11 |   1.1960 |     42.332 |   1.2033 |     42.355 |     0.4
   12 |   1.1722 |     41.355 |   1.1821 |     41.009 |     0.4
   13 |   1.1553 |     40.631 |   1.1632 |     40.703 |     0.5
   14 |   1.1338 |     40.106 |   1.1439 |     39.939 |     0.5
   15 |   1.1128 |     39.305 |   1.1394 |     38.807 |     0.5
   16 |   1.0975 |     38.725 |   1.1358 |     39.419 |     0.6
   17 |   1.0839 |     37.990 |   1.1184 |     38.073 |     0.6
   18 |   1.0541 |     37.200 |   1.1054 |     36.972 |     0.6
   19 |   1.0315 |     36.322 |   1.1004 |     37.920 |     0.7
   20 |   1.0105 |     35.201 |   1.0650 |     36.300 |     0.7
   21 |   0.9882 |     34.659 |   1.0578 |     36.055 |     0.7
   22 |   0.9654 |     33.411 |   1.0639 |     35.627 |     0.8
   23 |   0.9371 |     32.278 |   1.0446 |     34.832 |     0.8
   24 |   0.9138 |     31.599 |   1.0510 |     34.465 |     0.8
   25 |   0.8875 |     30.361 |   1.0388 |     34.404 |     0.9
   26 |   0.8593 |     29.367 |   1.0334 |     33.639 |     0.9
   27 |   0.8382 |     28.748 |   1.0167 |     33.639 |     0.9
   28 |   0.8021 |     27.268 |   1.0254 |     33.303 |     1.0
   29 |   0.7810 |     26.787 |   1.0238 |     32.905 |     1.0
   30 |   0.7733 |     26.030 |   0.9951 |     31.896 |     1.0
   31 |   0.7400 |     25.130 |   1.0081 |     31.774 |     1.1
   32 |   0.7092 |     23.815 |   1.0022 |     31.560 |     1.1
   33 |   0.6818 |     22.473 |   1.0068 |     31.835 |     1.1
   34 |   0.6613 |     21.705 |   1.0092 |     30.917 |     1.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 772,258

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1925 |     58.866 |   1.5982 |     45.963 |     0.0
    2 |   1.4782 |     46.089 |   1.4025 |     45.963 |     0.1
    3 |   1.3920 |     46.006 |   1.3596 |     45.352 |     0.1
    4 |   1.3512 |     45.592 |   1.3336 |     45.168 |     0.1
    5 |   1.3237 |     45.222 |   1.3060 |     45.138 |     0.1
    6 |   1.3079 |     44.907 |   1.2992 |     44.893 |     0.2
    7 |   1.2872 |     44.288 |   1.2812 |     44.251 |     0.2
    8 |   1.2732 |     44.089 |   1.2752 |     43.823 |     0.2
    9 |   1.2604 |     43.945 |   1.2600 |     43.792 |     0.2
   10 |   1.2492 |     43.487 |   1.2512 |     43.670 |     0.3
   11 |   1.2411 |     43.310 |   1.2430 |     44.373 |     0.3
   12 |   1.2275 |     43.128 |   1.2339 |     42.844 |     0.3
   13 |   1.2117 |     42.426 |   1.2227 |     42.844 |     0.4
   14 |   1.1929 |     42.040 |   1.2095 |     42.324 |     0.4
   15 |   1.1705 |     41.205 |   1.2058 |     41.560 |     0.4
   16 |   1.1544 |     40.786 |   1.1883 |     41.407 |     0.4
   17 |   1.1289 |     39.918 |   1.1817 |     41.223 |     0.5
   18 |   1.1067 |     39.283 |   1.1590 |     40.275 |     0.5
   19 |   1.0826 |     38.349 |   1.1446 |     39.694 |     0.5
   20 |   1.0564 |     37.084 |   1.1419 |     39.021 |     0.5
   21 |   1.0226 |     35.626 |   1.1287 |     37.492 |     0.6
   22 |   0.9936 |     34.195 |   1.1169 |     37.859 |     0.6
   23 |   0.9629 |     33.129 |   1.0951 |     36.636 |     0.6
   24 |   0.9218 |     30.693 |   1.0828 |     35.627 |     0.7
   25 |   0.8874 |     29.566 |   1.0669 |     34.526 |     0.7
   26 |   0.8484 |     27.793 |   1.0650 |     34.220 |     0.7
   27 |   0.8121 |     26.572 |   1.0490 |     33.119 |     0.7
   28 |   0.7805 |     24.920 |   1.0646 |     34.190 |     0.8
   29 |   0.7403 |     23.533 |   1.0416 |     33.333 |     0.8
   30 |   0.7071 |     22.473 |   1.0498 |     33.089 |     0.8
   31 |   0.6586 |     20.953 |   1.0664 |     32.997 |     0.8
   32 |   0.6262 |     19.815 |   1.0656 |     32.294 |     0.9
   33 |   0.5911 |     18.512 |   1.0524 |     31.774 |     0.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 519,394

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5512 |     69.639 |   1.9701 |     55.566 |     0.0
    2 |   1.7472 |     49.133 |   1.5440 |     45.963 |     0.1
    3 |   1.4927 |     46.188 |   1.4337 |     45.963 |     0.1
    4 |   1.4298 |     46.089 |   1.3926 |     45.963 |     0.1
    5 |   1.3992 |     46.166 |   1.3737 |     45.963 |     0.2
    6 |   1.3774 |     45.630 |   1.3515 |     45.382 |     0.2
    7 |   1.3619 |     45.233 |   1.3429 |     45.291 |     0.2
    8 |   1.3501 |     45.183 |   1.3290 |     45.627 |     0.2
    9 |   1.3298 |     45.161 |   1.3175 |     44.740 |     0.3
   10 |   1.3137 |     44.885 |   1.3035 |     44.343 |     0.3
   11 |   1.3002 |     44.658 |   1.2907 |     44.526 |     0.3
   12 |   1.2851 |     44.310 |   1.2822 |     44.343 |     0.4
   13 |   1.2692 |     44.028 |   1.2561 |     43.700 |     0.4
   14 |   1.2544 |     43.929 |   1.2499 |     43.578 |     0.4
   15 |   1.2404 |     43.338 |   1.2353 |     43.242 |     0.5
   16 |   1.2345 |     43.084 |   1.2285 |     43.547 |     0.5
   17 |   1.2239 |     43.299 |   1.2229 |     42.997 |     0.5
   18 |   1.2171 |     42.901 |   1.2230 |     42.783 |     0.6
   19 |   1.2100 |     42.647 |   1.2131 |     42.446 |     0.6
   20 |   1.2032 |     42.575 |   1.2088 |     42.355 |     0.6
   21 |   1.1959 |     42.465 |   1.2079 |     42.691 |     0.6
   22 |   1.1880 |     42.283 |   1.1958 |     42.141 |     0.7
   23 |   1.1785 |     42.023 |   1.1937 |     41.682 |     0.7
   24 |   1.1741 |     41.697 |   1.1888 |     41.437 |     0.7
   25 |   1.1672 |     41.482 |   1.1849 |     41.070 |     0.8
   26 |   1.1608 |     41.393 |   1.1793 |     42.294 |     0.8
   27 |   1.1547 |     41.382 |   1.1801 |     41.468 |     0.8
   28 |   1.1501 |     41.139 |   1.1707 |     40.887 |     0.9
   29 |   1.1447 |     40.891 |   1.1721 |     40.917 |     0.9
   30 |   1.1420 |     41.189 |   1.1651 |     41.223 |     0.9
   31 |   1.1351 |     40.924 |   1.1601 |     40.734 |     0.9
   32 |   1.1323 |     40.747 |   1.1555 |     40.673 |     1.0
   33 |   1.1272 |     40.747 |   1.1543 |     41.070 |     1.0
   34 |   1.1232 |     40.775 |   1.1474 |     40.826 |     1.0
   35 |   1.1216 |     40.846 |   1.1468 |     40.917 |     1.1
   36 |   1.1190 |     40.576 |   1.1439 |     41.651 |     1.1
   37 |   1.1171 |     40.565 |   1.1463 |     40.826 |     1.1
   38 |   1.1146 |     40.703 |   1.1438 |     40.673 |     1.2
   39 |   1.1090 |     40.609 |   1.1308 |     40.581 |     1.2
   40 |   1.1080 |     40.432 |   1.1323 |     40.306 |     1.2
   41 |   1.1048 |     40.233 |   1.1362 |     40.642 |     1.3
   42 |   1.1010 |     40.073 |   1.1273 |     40.734 |     1.3
   43 |   1.0994 |     40.056 |   1.1256 |     40.673 |     1.3
   44 |   1.0987 |     40.233 |   1.1214 |     40.122 |     1.3
   45 |   1.0961 |     40.106 |   1.1252 |     40.031 |     1.4
   46 |   1.0920 |     40.051 |   1.1235 |     40.459 |     1.4
   47 |   1.0897 |     40.062 |   1.1162 |     40.428 |     1.4
   48 |   1.0883 |     39.708 |   1.1160 |     40.000 |     1.5
   49 |   1.0866 |     39.642 |   1.1133 |     39.694 |     1.5
   50 |   1.0842 |     39.670 |   1.1154 |     39.908 |     1.5
   51 |   1.0824 |     39.316 |   1.1091 |     40.122 |     1.6
   52 |   1.0784 |     39.543 |   1.1093 |     39.541 |     1.6
   53 |   1.0787 |     39.388 |   1.1113 |     39.205 |     1.6
   54 |   1.0745 |     39.189 |   1.1063 |     39.327 |     1.6
   55 |   1.0731 |     38.985 |   1.1130 |     39.786 |     1.7
   56 |   1.0697 |     39.189 |   1.0996 |     39.450 |     1.7
   57 |   1.0680 |     39.167 |   1.0995 |     39.511 |     1.7
   58 |   1.0648 |     38.758 |   1.1077 |     39.480 |     1.8
   59 |   1.0602 |     38.824 |   1.0993 |     39.664 |     1.8
   60 |   1.0598 |     38.653 |   1.0990 |     39.266 |     1.8
   61 |   1.0583 |     38.648 |   1.0965 |     39.113 |     1.9
   62 |   1.0549 |     38.438 |   1.0960 |     38.960 |     1.9
   63 |   1.0504 |     38.117 |   1.0864 |     38.869 |     1.9
   64 |   1.0491 |     38.062 |   1.0862 |     38.624 |     1.9
   65 |   1.0483 |     38.222 |   1.0774 |     39.144 |     2.0
   66 |   1.0439 |     38.062 |   1.0833 |     38.379 |     2.0
   67 |   1.0405 |     37.885 |   1.0760 |     38.654 |     2.0
   68 |   1.0372 |     37.769 |   1.0885 |     38.960 |     2.1
   69 |   1.0361 |     37.631 |   1.0760 |     38.624 |     2.1
   70 |   1.0324 |     37.510 |   1.0788 |     38.410 |     2.1
   71 |   1.0320 |     37.559 |   1.0704 |     38.502 |     2.2
   72 |   1.0284 |     37.245 |   1.0704 |     38.532 |     2.2
   73 |   1.0269 |     37.416 |   1.0683 |     38.196 |     2.2
   74 |   1.0235 |     36.880 |   1.0692 |     37.798 |     2.3
   75 |   1.0202 |     36.803 |   1.0651 |     38.196 |     2.3
   76 |   1.0169 |     37.040 |   1.0659 |     37.768 |     2.3
   77 |   1.0140 |     36.609 |   1.0576 |     37.829 |     2.3
   78 |   1.0135 |     36.598 |   1.0571 |     37.982 |     2.4
   79 |   1.0077 |     36.692 |   1.0469 |     38.318 |     2.4
   80 |   1.0060 |     36.443 |   1.0558 |     37.951 |     2.4
   81 |   1.0028 |     36.405 |   1.0628 |     38.104 |     2.5
   82 |   1.0002 |     35.996 |   1.0466 |     37.462 |     2.5
   83 |   0.9961 |     35.935 |   1.0464 |     37.645 |     2.5
   84 |   0.9925 |     35.593 |   1.0468 |     36.972 |     2.6
   85 |   0.9887 |     35.410 |   1.0405 |     36.636 |     2.6
   86 |   0.9834 |     35.278 |   1.0319 |     36.972 |     2.6
   87 |   0.9815 |     35.090 |   1.0326 |     36.789 |     2.6
   88 |   0.9786 |     35.018 |   1.0319 |     36.086 |     2.7
   89 |   0.9759 |     34.847 |   1.0348 |     36.391 |     2.7
   90 |   0.9744 |     34.902 |   1.0286 |     36.606 |     2.7
   91 |   0.9683 |     34.736 |   1.0278 |     35.963 |     2.8
   92 |   0.9664 |     34.775 |   1.0281 |     36.391 |     2.8
   93 |   0.9631 |     34.278 |   1.0309 |     36.300 |     2.8
   94 |   0.9590 |     34.471 |   1.0226 |     35.994 |     2.9
   95 |   0.9580 |     34.289 |   1.0304 |     35.719 |     2.9
   96 |   0.9550 |     34.228 |   1.0085 |     35.535 |     2.9
   97 |   0.9442 |     33.731 |   1.0187 |     35.352 |     3.0
   98 |   0.9403 |     33.433 |   1.0190 |     36.453 |     3.0
   99 |   0.9362 |     33.239 |   1.0073 |     35.382 |     3.0
  100 |   0.9299 |     32.582 |   1.0111 |     35.076 |     3.0
  101 |   0.9234 |     32.466 |   1.0043 |     35.810 |     3.1
  102 |   0.9212 |     32.731 |   1.0058 |     35.596 |     3.1
  103 |   0.9173 |     32.140 |   0.9934 |     35.627 |     3.1
  104 |   0.9119 |     32.096 |   0.9954 |     35.474 |     3.2
  105 |   0.9070 |     32.074 |   0.9943 |     35.382 |     3.2
  106 |   0.9066 |     32.030 |   0.9952 |     34.587 |     3.2
  107 |   0.8988 |     31.521 |   0.9905 |     34.924 |     3.3
  108 |   0.8977 |     31.626 |   0.9930 |     34.251 |     3.3
  109 |   0.8908 |     31.240 |   1.0032 |     34.862 |     3.3
  110 |   0.8887 |     31.190 |   0.9899 |     35.046 |     3.3
  111 |   0.8808 |     30.947 |   1.0092 |     34.985 |     3.4
  112 |   0.8777 |     30.754 |   1.0056 |     34.434 |     3.4
  113 |   0.8710 |     30.405 |   0.9953 |     34.373 |     3.4
  114 |   0.8674 |     30.041 |   1.0101 |     34.312 |     3.5
  115 |   0.8642 |     30.207 |   0.9865 |     33.945 |     3.5
  116 |   0.8543 |     29.593 |   0.9933 |     33.914 |     3.5
  117 |   0.8507 |     29.544 |   0.9837 |     33.670 |     3.6
  118 |   0.8455 |     29.323 |   0.9790 |     34.006 |     3.6
  119 |   0.8450 |     29.245 |   0.9788 |     34.281 |     3.6
  120 |   0.8474 |     29.964 |   1.0011 |     35.076 |     3.7
  121 |   0.8345 |     28.798 |   0.9805 |     33.792 |     3.7
  122 |   0.8339 |     28.798 |   0.9834 |     33.333 |     3.7
  123 |   0.8267 |     28.455 |   0.9864 |     33.333 |     3.7
  124 |   0.8151 |     27.925 |   0.9612 |     32.997 |     3.8
  125 |   0.8195 |     28.373 |   0.9882 |     33.119 |     3.8
  126 |   0.8077 |     27.715 |   0.9732 |     32.966 |     3.8
  127 |   0.8002 |     27.621 |   0.9902 |     32.844 |     3.9
  128 |   0.7981 |     27.362 |   0.9602 |     33.089 |     3.9
  129 |   0.7948 |     27.317 |   0.9741 |     33.150 |     3.9
  130 |   0.7880 |     26.942 |   0.9839 |     33.150 |     4.0
  131 |   0.7849 |     26.815 |   0.9734 |     33.976 |     4.0
  132 |   0.7829 |     27.014 |   0.9759 |     32.813 |     4.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 736,546

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2479 |     61.347 |   1.6213 |     45.963 |     0.0
    2 |   1.4901 |     46.122 |   1.4075 |     45.963 |     0.1
    3 |   1.4090 |     46.144 |   1.3703 |     45.963 |     0.1
    4 |   1.3825 |     45.907 |   1.3582 |     45.260 |     0.1
    5 |   1.3499 |     45.393 |   1.3175 |     45.168 |     0.1
    6 |   1.3190 |     45.072 |   1.2997 |     44.709 |     0.2
    7 |   1.3028 |     44.990 |   1.2877 |     44.465 |     0.2
    8 |   1.2801 |     44.503 |   1.2701 |     44.190 |     0.2
    9 |   1.2593 |     44.155 |   1.2547 |     44.220 |     0.2
   10 |   1.2421 |     43.863 |   1.2308 |     44.006 |     0.3
   11 |   1.2256 |     43.503 |   1.2318 |     43.242 |     0.3
   12 |   1.2131 |     43.376 |   1.2137 |     43.884 |     0.3
   13 |   1.2079 |     43.393 |   1.2122 |     42.966 |     0.3
   14 |   1.1944 |     42.785 |   1.2052 |     42.752 |     0.4
   15 |   1.1892 |     42.642 |   1.2024 |     42.599 |     0.4
   16 |   1.1809 |     42.437 |   1.1922 |     42.080 |     0.4
   17 |   1.1711 |     42.388 |   1.1923 |     42.018 |     0.4
   18 |   1.1646 |     41.736 |   1.1918 |     41.529 |     0.5
   19 |   1.1589 |     41.570 |   1.1911 |     41.835 |     0.5
   20 |   1.1513 |     41.161 |   1.1792 |     41.223 |     0.5
   21 |   1.1436 |     41.123 |   1.1742 |     41.346 |     0.5
   22 |   1.1387 |     40.879 |   1.1609 |     40.489 |     0.6
   23 |   1.1319 |     40.438 |   1.1594 |     40.581 |     0.6
   24 |   1.1239 |     40.576 |   1.1470 |     40.489 |     0.6
   25 |   1.1185 |     40.117 |   1.1459 |     40.398 |     0.6
   26 |   1.1092 |     39.913 |   1.1412 |     39.755 |     0.7
   27 |   1.1031 |     39.708 |   1.1342 |     39.450 |     0.7
   28 |   1.0976 |     39.316 |   1.1251 |     39.817 |     0.7
   29 |   1.0926 |     39.322 |   1.1227 |     39.388 |     0.7
   30 |   1.0831 |     39.112 |   1.1248 |     39.633 |     0.8
   31 |   1.0723 |     38.863 |   1.1084 |     38.563 |     0.8
   32 |   1.0694 |     38.946 |   1.0910 |     38.471 |     0.8
   33 |   1.0607 |     38.162 |   1.1001 |     38.410 |     0.9
   34 |   1.0510 |     37.880 |   1.0920 |     38.624 |     0.9
   35 |   1.0464 |     37.565 |   1.0943 |     37.859 |     0.9
   36 |   1.0410 |     37.482 |   1.0933 |     39.144 |     0.9
   37 |   1.0353 |     37.211 |   1.0775 |     38.410 |     1.0
   38 |   1.0290 |     37.012 |   1.0904 |     37.339 |     1.0
   39 |   1.0251 |     36.863 |   1.0814 |     37.554 |     1.0
   40 |   1.0171 |     36.195 |   1.0821 |     37.462 |     1.0
   41 |   1.0145 |     36.300 |   1.0716 |     37.554 |     1.1
   42 |   1.0047 |     36.084 |   1.0753 |     37.737 |     1.1
   43 |   1.0003 |     35.902 |   1.0707 |     37.737 |     1.1
   44 |   0.9906 |     35.311 |   1.0659 |     37.248 |     1.1
   45 |   0.9930 |     35.228 |   1.0565 |     36.758 |     1.2
   46 |   0.9854 |     35.090 |   1.0623 |     36.116 |     1.2
   47 |   0.9772 |     34.875 |   1.0499 |     36.575 |     1.2
   48 |   0.9735 |     34.477 |   1.0510 |     36.636 |     1.2
   49 |   0.9660 |     33.908 |   1.0422 |     36.789 |     1.3
   50 |   0.9631 |     34.405 |   1.0565 |     37.156 |     1.3
   51 |   0.9544 |     33.919 |   1.0391 |     35.994 |     1.3
   52 |   0.9468 |     33.803 |   1.0323 |     35.107 |     1.3
   53 |   0.9427 |     33.405 |   1.0263 |     36.147 |     1.4
   54 |   0.9317 |     32.775 |   1.0160 |     35.015 |     1.4
   55 |   0.9271 |     33.052 |   1.0170 |     34.893 |     1.4
   56 |   0.9223 |     32.571 |   1.0165 |     36.514 |     1.4
   57 |   0.9119 |     32.052 |   1.0213 |     35.505 |     1.5
   58 |   0.9065 |     31.659 |   1.0170 |     35.229 |     1.5
   59 |   0.8907 |     31.267 |   1.0021 |     34.709 |     1.5
   60 |   0.8927 |     31.394 |   1.0213 |     34.648 |     1.5
   61 |   0.8891 |     31.251 |   1.0089 |     34.985 |     1.6
   62 |   0.8802 |     30.963 |   1.0213 |     34.434 |     1.6
   63 |   0.8759 |     30.544 |   0.9839 |     33.639 |     1.6
   64 |   0.8642 |     30.328 |   1.0160 |     33.547 |     1.7
   65 |   0.8577 |     29.964 |   1.0057 |     34.037 |     1.7
   66 |   0.8513 |     29.638 |   0.9905 |     32.905 |     1.7
   67 |   0.8443 |     29.080 |   0.9741 |     33.578 |     1.7
   68 |   0.8364 |     29.019 |   0.9760 |     33.364 |     1.8
   69 |   0.8267 |     28.892 |   0.9739 |     33.547 |     1.8
   70 |   0.8186 |     28.223 |   0.9631 |     33.303 |     1.8
   71 |   0.8143 |     28.676 |   0.9641 |     32.141 |     1.8
   72 |   0.8079 |     27.881 |   0.9726 |     32.232 |     1.9
   73 |   0.8123 |     28.328 |   0.9699 |     32.813 |     1.9
   74 |   0.7994 |     27.942 |   0.9684 |     31.896 |     1.9
   75 |   0.7924 |     27.461 |   0.9569 |     31.957 |     1.9
   76 |   0.7867 |     27.422 |   0.9789 |     32.263 |     2.0
   77 |   0.7774 |     26.804 |   0.9729 |     31.743 |     2.0
   78 |   0.7669 |     26.362 |   0.9621 |     32.691 |     2.0
   79 |   0.7604 |     26.086 |   0.9671 |     31.988 |     2.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 885,730

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2037 |     60.060 |   1.5826 |     45.963 |     0.0
    2 |   1.4707 |     46.271 |   1.3837 |     45.963 |     0.1
    3 |   1.3807 |     45.962 |   1.3460 |     45.841 |     0.1
    4 |   1.3376 |     45.735 |   1.3067 |     45.872 |     0.1
    5 |   1.3127 |     45.271 |   1.2927 |     45.168 |     0.2
    6 |   1.2949 |     45.056 |   1.2731 |     44.404 |     0.2
    7 |   1.2725 |     44.575 |   1.2616 |     43.945 |     0.2
    8 |   1.2501 |     44.222 |   1.2250 |     44.098 |     0.3
    9 |   1.2281 |     43.526 |   1.2177 |     42.905 |     0.3
   10 |   1.2104 |     43.249 |   1.1968 |     42.538 |     0.3
   11 |   1.1963 |     42.675 |   1.1957 |     42.783 |     0.4
   12 |   1.1845 |     42.255 |   1.1909 |     42.110 |     0.4
   13 |   1.1731 |     41.780 |   1.1776 |     42.171 |     0.4
   14 |   1.1628 |     41.857 |   1.1705 |     41.376 |     0.5
   15 |   1.1529 |     41.266 |   1.1644 |     41.376 |     0.5
   16 |   1.1468 |     40.885 |   1.1647 |     40.398 |     0.5
   17 |   1.1377 |     40.857 |   1.1564 |     41.346 |     0.6
   18 |   1.1315 |     40.819 |   1.1363 |     40.979 |     0.6
   19 |   1.1217 |     40.504 |   1.1379 |     40.122 |     0.6
   20 |   1.1165 |     40.327 |   1.1311 |     40.367 |     0.7
   21 |   1.1072 |     40.161 |   1.1345 |     40.459 |     0.7
   22 |   1.1024 |     39.592 |   1.1228 |     39.817 |     0.7
   23 |   1.0942 |     39.592 |   1.1247 |     39.541 |     0.8
   24 |   1.0895 |     39.200 |   1.1235 |     39.511 |     0.8
   25 |   1.0828 |     39.189 |   1.1110 |     40.183 |     0.8
   26 |   1.0754 |     38.819 |   1.1073 |     39.021 |     0.9
   27 |   1.0682 |     38.769 |   1.0861 |     38.716 |     0.9
   28 |   1.0651 |     38.482 |   1.0919 |     38.257 |     0.9
   29 |   1.0587 |     38.322 |   1.0834 |     38.440 |     1.0
   30 |   1.0500 |     37.736 |   1.0887 |     38.349 |     1.0
   31 |   1.0526 |     38.034 |   1.0756 |     38.440 |     1.0
   32 |   1.0446 |     37.885 |   1.0795 |     37.645 |     1.1
   33 |   1.0434 |     37.990 |   1.0744 |     38.685 |     1.1
   34 |   1.0408 |     37.648 |   1.0669 |     37.676 |     1.1
   35 |   1.0361 |     37.493 |   1.0673 |     38.318 |     1.2
   36 |   1.0335 |     37.814 |   1.0705 |     37.706 |     1.2
   37 |   1.0268 |     37.449 |   1.0645 |     37.431 |     1.2
   38 |   1.0248 |     37.245 |   1.0551 |     37.798 |     1.3
   39 |   1.0171 |     36.742 |   1.0577 |     37.431 |     1.3
   40 |   1.0163 |     36.477 |   1.0558 |     37.462 |     1.3
   41 |   1.0139 |     36.869 |   1.0521 |     37.492 |     1.4
   42 |   1.0148 |     37.018 |   1.0448 |     37.339 |     1.4
   43 |   1.0018 |     36.305 |   1.0389 |     36.422 |     1.4
   44 |   0.9989 |     36.107 |   1.0356 |     37.095 |     1.5
   45 |   0.9903 |     35.770 |   1.0329 |     36.942 |     1.5
   46 |   0.9870 |     35.709 |   1.0259 |     36.300 |     1.5
   47 |   0.9828 |     35.753 |   1.0319 |     37.217 |     1.6
   48 |   0.9777 |     35.444 |   1.0286 |     36.330 |     1.6
   49 |   0.9744 |     35.107 |   1.0104 |     36.453 |     1.7
   50 |   0.9684 |     34.742 |   1.0229 |     36.269 |     1.7
   51 |   0.9615 |     34.698 |   1.0030 |     36.147 |     1.7
   52 |   0.9570 |     34.372 |   1.0040 |     36.911 |     1.8
   53 |   0.9479 |     34.239 |   1.0124 |     35.841 |     1.8
   54 |   0.9455 |     34.167 |   1.0069 |     35.382 |     1.8
   55 |   0.9403 |     33.654 |   1.0281 |     36.208 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,461,666

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0309 |     56.546 |   1.4676 |     45.627 |     0.2
    2 |   1.3919 |     45.271 |   1.3310 |     45.076 |     0.3
    3 |   1.3134 |     44.376 |   1.2894 |     44.159 |     0.5
    4 |   1.2769 |     44.376 |   1.2699 |     43.517 |     0.6
    5 |   1.2437 |     43.492 |   1.2433 |     43.089 |     0.8
    6 |   1.2158 |     42.863 |   1.2294 |     42.263 |     0.9
    7 |   1.1898 |     42.034 |   1.2139 |     42.263 |     1.1
    8 |   1.1589 |     41.029 |   1.1780 |     40.367 |     1.2
    9 |   1.1297 |     39.653 |   1.1568 |     39.725 |     1.4
   10 |   1.1020 |     38.487 |   1.1435 |     39.113 |     1.5
   11 |   1.0685 |     37.383 |   1.1097 |     37.248 |     1.7
   12 |   1.0367 |     35.830 |   1.0794 |     37.248 |     1.8
   13 |   1.0028 |     34.643 |   1.0589 |     36.758 |     2.0
   14 |   0.9666 |     33.101 |   1.0305 |     34.648 |     2.1
   15 |   0.9290 |     31.737 |   1.0047 |     34.006 |     2.3
   16 |   0.8862 |     29.925 |   0.9816 |     33.058 |     2.4
   17 |   0.8458 |     28.129 |   0.9840 |     32.141 |     2.6
   18 |   0.7971 |     26.362 |   0.9401 |     31.437 |     2.7
   19 |   0.7530 |     24.820 |   0.9355 |     30.979 |     2.9
   20 |   0.7154 |     23.456 |   0.9333 |     29.664 |     3.0
   21 |   0.6793 |     22.103 |   0.9155 |     29.174 |     3.2
   22 |   0.6458 |     20.876 |   0.9381 |     29.664 |     3.3
   23 |   0.5960 |     19.252 |   0.9020 |     28.869 |     3.5
   24 |   0.5523 |     17.545 |   0.8993 |     27.829 |     3.6
   25 |   0.5177 |     16.424 |   0.8852 |     27.095 |     3.8
   26 |   0.4888 |     15.545 |   0.9375 |     29.021 |     3.9
   27 |   0.4528 |     14.137 |   0.8950 |     27.217 |     4.1
   28 |   0.4149 |     13.004 |   0.8977 |     27.370 |     4.2
   29 |   0.3869 |     12.120 |   0.9213 |     26.575 |     4.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 675,106

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9947 |     55.469 |   1.4301 |     45.994 |     0.0
    2 |   1.3859 |     45.763 |   1.3224 |     44.557 |     0.1
    3 |   1.3059 |     44.487 |   1.2759 |     44.281 |     0.1
    4 |   1.2617 |     44.183 |   1.2500 |     43.058 |     0.1
    5 |   1.2277 |     43.050 |   1.2245 |     42.813 |     0.1
    6 |   1.2033 |     42.537 |   1.2048 |     41.988 |     0.2
    7 |   1.1716 |     41.294 |   1.1817 |     41.101 |     0.2
    8 |   1.1382 |     40.139 |   1.1615 |     39.878 |     0.2
    9 |   1.1095 |     39.371 |   1.1357 |     38.104 |     0.3
   10 |   1.0727 |     37.869 |   1.1137 |     38.257 |     0.3
   11 |   1.0382 |     35.996 |   1.0858 |     36.361 |     0.3
   12 |   1.0039 |     34.736 |   1.0565 |     36.024 |     0.3
   13 |   0.9684 |     33.355 |   1.0516 |     35.138 |     0.4
   14 |   0.9293 |     31.621 |   1.0147 |     34.190 |     0.4
   15 |   0.8891 |     30.091 |   0.9873 |     33.394 |     0.4
   16 |   0.8566 |     28.814 |   0.9660 |     32.630 |     0.4
   17 |   0.8230 |     27.649 |   0.9493 |     31.804 |     0.5
   18 |   0.7827 |     25.815 |   0.9289 |     31.315 |     0.5
   19 |   0.7499 |     25.091 |   0.9293 |     30.459 |     0.5
   20 |   0.7170 |     23.749 |   0.9210 |     30.306 |     0.6
   21 |   0.6819 |     22.097 |   0.9124 |     29.664 |     0.6
   22 |   0.6531 |     21.390 |   0.8942 |     29.205 |     0.6
   23 |   0.6267 |     20.384 |   0.8902 |     28.716 |     0.6
   24 |   0.5930 |     19.285 |   0.8952 |     28.991 |     0.7
   25 |   0.5605 |     18.086 |   0.9026 |     28.226 |     0.7
   26 |   0.5409 |     17.744 |   0.8794 |     27.584 |     0.7
   27 |   0.5205 |     16.689 |   0.8976 |     28.349 |     0.8
   28 |   0.4985 |     16.009 |   0.8931 |     28.104 |     0.8
   29 |   0.4848 |     15.645 |   0.9079 |     27.829 |     0.8
   30 |   0.4582 |     14.860 |   0.9102 |     27.554 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,888,226

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0111 |     54.961 |   1.4597 |     45.841 |     0.1
    2 |   1.4024 |     45.846 |   1.3391 |     44.557 |     0.2
    3 |   1.3270 |     44.769 |   1.3000 |     44.128 |     0.2
    4 |   1.2860 |     44.376 |   1.2667 |     43.456 |     0.3
    5 |   1.2517 |     43.829 |   1.2479 |     43.761 |     0.4
    6 |   1.2221 |     43.056 |   1.2187 |     42.355 |     0.5
    7 |   1.1947 |     42.410 |   1.1989 |     42.080 |     0.6
    8 |   1.1721 |     41.255 |   1.1883 |     41.713 |     0.6
    9 |   1.1503 |     40.670 |   1.1580 |     40.520 |     0.7
   10 |   1.1299 |     40.150 |   1.1457 |     40.245 |     0.8
   11 |   1.1180 |     39.648 |   1.1454 |     39.297 |     0.9
   12 |   1.1010 |     39.056 |   1.1335 |     39.664 |     1.0
   13 |   1.0905 |     38.808 |   1.1297 |     38.807 |     1.0
   14 |   1.0819 |     38.742 |   1.1156 |     39.327 |     1.1
   15 |   1.0670 |     37.902 |   1.1075 |     37.554 |     1.2
   16 |   1.0530 |     37.532 |   1.0929 |     38.165 |     1.3
   17 |   1.0423 |     37.344 |   1.0906 |     38.257 |     1.4
   18 |   1.0329 |     37.040 |   1.0793 |     37.187 |     1.4
   19 |   1.0278 |     36.637 |   1.0780 |     37.095 |     1.5
   20 |   1.0119 |     35.935 |   1.0652 |     37.523 |     1.6
   21 |   1.0046 |     35.974 |   1.0554 |     37.187 |     1.7
   22 |   0.9919 |     35.328 |   1.0546 |     36.911 |     1.8
   23 |   0.9833 |     35.184 |   1.0465 |     36.300 |     1.8
   24 |   0.9754 |     34.819 |   1.0343 |     35.994 |     1.9
   25 |   0.9631 |     34.267 |   1.0183 |     35.780 |     2.0
   26 |   0.9513 |     33.808 |   1.0391 |     35.810 |     2.1
   27 |   0.9426 |     33.300 |   1.0090 |     34.771 |     2.2
   28 |   0.9271 |     32.731 |   0.9965 |     34.098 |     2.2
   29 |   0.9268 |     32.610 |   0.9984 |     34.220 |     2.3
   30 |   0.9025 |     31.814 |   0.9879 |     34.006 |     2.4
   31 |   0.8897 |     31.223 |   0.9697 |     32.997 |     2.5
   32 |   0.8699 |     30.345 |   0.9804 |     33.670 |     2.6
   33 |   0.8603 |     29.930 |   0.9451 |     32.569 |     2.6
   34 |   0.8389 |     29.146 |   0.9376 |     31.682 |     2.7
   35 |   0.8199 |     28.295 |   0.9217 |     30.765 |     2.8
   36 |   0.8012 |     27.494 |   0.9531 |     32.263 |     2.9
   37 |   0.7941 |     27.295 |   0.9317 |     31.713 |     3.0
   38 |   0.7746 |     26.356 |   0.9249 |     30.520 |     3.0
   39 |   0.7576 |     25.942 |   0.9270 |     30.703 |     3.1
   40 |   0.7425 |     25.080 |   0.9081 |     29.694 |     3.2
   41 |   0.7253 |     24.605 |   0.9098 |     29.266 |     3.3
   42 |   0.7114 |     24.356 |   0.9057 |     30.031 |     3.4
   43 |   0.6963 |     23.395 |   0.8971 |     29.725 |     3.4
   44 |   0.6792 |     22.754 |   0.8785 |     28.991 |     3.5
   45 |   0.6602 |     22.097 |   0.9012 |     28.349 |     3.6
   46 |   0.6486 |     21.727 |   0.8980 |     29.847 |     3.7
   47 |   0.6443 |     21.694 |   0.8984 |     29.388 |     3.8
   48 |   0.6251 |     21.130 |   0.8845 |     28.257 |     3.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 254,690

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3751 |     61.938 |   1.7737 |     49.297 |     0.0
    2 |   1.5962 |     47.243 |   1.4461 |     45.627 |     0.0
    3 |   1.4308 |     45.895 |   1.3799 |     45.566 |     0.1
    4 |   1.3826 |     45.719 |   1.3496 |     45.688 |     0.1
    5 |   1.3472 |     45.299 |   1.3224 |     44.709 |     0.1
    6 |   1.3193 |     44.625 |   1.3013 |     44.373 |     0.1
    7 |   1.2999 |     44.343 |   1.2845 |     44.404 |     0.1
    8 |   1.2793 |     44.023 |   1.2708 |     43.333 |     0.1
    9 |   1.2610 |     43.934 |   1.2515 |     43.119 |     0.2
   10 |   1.2438 |     43.465 |   1.2388 |     43.456 |     0.2
   11 |   1.2274 |     43.316 |   1.2222 |     42.508 |     0.2
   12 |   1.2101 |     42.371 |   1.2096 |     41.498 |     0.2
   13 |   1.1964 |     41.946 |   1.2031 |     41.560 |     0.2
   14 |   1.1830 |     41.603 |   1.1920 |     41.529 |     0.2
   15 |   1.1730 |     41.167 |   1.1790 |     41.162 |     0.3
   16 |   1.1605 |     40.924 |   1.1689 |     41.101 |     0.3
   17 |   1.1509 |     40.708 |   1.1689 |     40.948 |     0.3
   18 |   1.1410 |     40.327 |   1.1571 |     40.214 |     0.3
   19 |   1.1341 |     40.134 |   1.1535 |     39.878 |     0.3
   20 |   1.1240 |     39.891 |   1.1433 |     39.755 |     0.3
   21 |   1.1127 |     39.195 |   1.1449 |     39.235 |     0.4
   22 |   1.1035 |     38.852 |   1.1245 |     39.419 |     0.4
   23 |   1.0902 |     38.449 |   1.1181 |     37.982 |     0.4
   24 |   1.0813 |     37.869 |   1.1112 |     37.645 |     0.4
   25 |   1.0708 |     37.405 |   1.1158 |     37.554 |     0.4
   26 |   1.0778 |     37.907 |   1.1135 |     37.920 |     0.4
   27 |   1.0579 |     36.836 |   1.0998 |     36.972 |     0.5
   28 |   1.0476 |     36.664 |   1.0913 |     36.453 |     0.5
   29 |   1.0411 |     36.388 |   1.0908 |     36.972 |     0.5
   30 |   1.0296 |     35.946 |   1.0751 |     36.728 |     0.5
   31 |   1.0232 |     35.792 |   1.0913 |     36.575 |     0.5
   32 |   1.0097 |     35.289 |   1.0725 |     35.688 |     0.5
   33 |   1.0034 |     35.156 |   1.0607 |     35.535 |     0.6
   34 |   0.9905 |     34.543 |   1.0514 |     35.321 |     0.6
   35 |   0.9835 |     34.267 |   1.0551 |     35.413 |     0.6
   36 |   0.9807 |     34.300 |   1.0334 |     35.413 |     0.6
   37 |   0.9666 |     33.654 |   1.0586 |     35.260 |     0.6
   38 |   0.9589 |     33.151 |   1.0400 |     35.168 |     0.7
   39 |   0.9526 |     32.930 |   1.0395 |     34.954 |     0.7
   40 |   0.9365 |     32.565 |   1.0211 |     35.413 |     0.7
   41 |   0.9338 |     32.588 |   1.0096 |     34.281 |     0.7
   42 |   0.9249 |     32.234 |   1.0113 |     34.098 |     0.7
   43 |   0.9129 |     31.593 |   0.9991 |     34.159 |     0.7
   44 |   0.9049 |     31.113 |   1.0032 |     33.456 |     0.8
   45 |   0.9011 |     31.090 |   1.0084 |     34.190 |     0.8
   46 |   0.8945 |     30.908 |   1.0041 |     34.312 |     0.8
   47 |   0.8810 |     30.096 |   0.9924 |     33.364 |     0.8
   48 |   0.8753 |     30.185 |   0.9909 |     33.119 |     0.8
   49 |   0.8570 |     29.367 |   0.9689 |     32.813 |     0.8
   50 |   0.8478 |     28.770 |   0.9836 |     32.294 |     0.9
   51 |   0.8374 |     28.389 |   0.9794 |     32.508 |     0.9
   52 |   0.8939 |     30.825 |   1.0038 |     33.945 |     0.9
   53 |   0.8558 |     29.163 |   0.9646 |     32.569 |     0.9
   54 |   0.8332 |     28.207 |   0.9897 |     32.202 |     0.9
   55 |   0.8210 |     28.008 |   0.9643 |     31.743 |     0.9
   56 |   0.8099 |     27.096 |   0.9642 |     31.131 |     1.0
   57 |   0.7971 |     27.036 |   0.9632 |     31.315 |     1.0
   58 |   0.7869 |     26.539 |   0.9476 |     30.856 |     1.0
   59 |   0.7723 |     25.887 |   0.9473 |     31.651 |     1.0
   60 |   0.7646 |     25.787 |   0.9517 |     31.040 |     1.0
   61 |   0.7773 |     26.185 |   0.9355 |     30.336 |     1.0
   62 |   0.7488 |     25.052 |   0.9626 |     30.765 |     1.1
   63 |   0.7858 |     26.456 |   0.9594 |     32.355 |     1.1
   64 |   0.7531 |     25.395 |   0.9320 |     30.428 |     1.1
   65 |   0.7304 |     24.390 |   0.9271 |     29.664 |     1.1
   66 |   0.7167 |     23.826 |   0.9275 |     29.480 |     1.1
   67 |   0.7030 |     23.158 |   0.9356 |     29.755 |     1.2
   68 |   0.6966 |     22.981 |   0.9401 |     30.367 |     1.2
   69 |   0.6864 |     22.810 |   0.9222 |     29.725 |     1.2
   70 |   0.6829 |     22.517 |   0.9133 |     29.144 |     1.2
   71 |   0.6663 |     22.185 |   0.9296 |     29.327 |     1.2
   72 |   0.6605 |     21.567 |   0.9212 |     30.031 |     1.2
   73 |   0.6543 |     21.522 |   0.9219 |     29.297 |     1.3
   74 |   0.6418 |     21.290 |   0.9194 |     29.327 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 771,490

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1983 |     59.728 |   1.5633 |     45.963 |     0.0
    2 |   1.4661 |     46.194 |   1.3909 |     45.963 |     0.1
    3 |   1.3819 |     45.757 |   1.3512 |     45.688 |     0.1
    4 |   1.3452 |     45.409 |   1.3110 |     45.657 |     0.1
    5 |   1.3051 |     44.813 |   1.2851 |     44.373 |     0.1
    6 |   1.2787 |     44.111 |   1.2698 |     44.709 |     0.2
    7 |   1.2602 |     44.150 |   1.2597 |     44.190 |     0.2
    8 |   1.2415 |     43.608 |   1.2416 |     43.150 |     0.2
    9 |   1.2253 |     43.045 |   1.2265 |     42.569 |     0.2
   10 |   1.2048 |     42.625 |   1.2058 |     42.691 |     0.3
   11 |   1.1899 |     42.222 |   1.1904 |     41.621 |     0.3
   12 |   1.1716 |     41.708 |   1.1804 |     42.049 |     0.3
   13 |   1.1575 |     41.205 |   1.1699 |     40.673 |     0.3
   14 |   1.1467 |     40.642 |   1.1658 |     40.826 |     0.4
   15 |   1.1337 |     40.299 |   1.1521 |     41.193 |     0.4
   16 |   1.1254 |     40.399 |   1.1554 |     40.673 |     0.4
   17 |   1.1162 |     39.968 |   1.1353 |     40.673 |     0.4
   18 |   1.1030 |     39.581 |   1.1259 |     40.826 |     0.5
   19 |   1.0915 |     39.056 |   1.1262 |     39.694 |     0.5
   20 |   1.0848 |     38.957 |   1.1143 |     38.869 |     0.5
   21 |   1.0749 |     38.703 |   1.1080 |     38.746 |     0.5
   22 |   1.0663 |     38.046 |   1.1046 |     38.563 |     0.6
   23 |   1.0583 |     38.139 |   1.0974 |     39.144 |     0.6
   24 |   1.0496 |     37.720 |   1.0861 |     38.226 |     0.6
   25 |   1.0410 |     37.162 |   1.0918 |     38.226 |     0.6
   26 |   1.0292 |     36.803 |   1.0771 |     37.554 |     0.7
   27 |   1.0231 |     35.990 |   1.0665 |     36.972 |     0.7
   28 |   1.0094 |     35.366 |   1.0561 |     36.911 |     0.7
   29 |   1.0059 |     35.582 |   1.0528 |     36.697 |     0.7
   30 |   0.9946 |     35.085 |   1.0471 |     36.636 |     0.8
   31 |   0.9859 |     34.582 |   1.0383 |     35.443 |     0.8
   32 |   0.9793 |     34.538 |   1.0357 |     36.116 |     0.8
   33 |   0.9705 |     34.085 |   1.0382 |     35.933 |     0.9
   34 |   0.9659 |     33.969 |   1.0300 |     36.422 |     0.9
   35 |   0.9515 |     33.300 |   1.0213 |     35.566 |     0.9
   36 |   0.9508 |     33.162 |   1.0172 |     35.505 |     0.9
   37 |   0.9355 |     32.693 |   1.0157 |     34.771 |     1.0
   38 |   0.9261 |     32.411 |   1.0038 |     34.618 |     1.0
   39 |   0.9208 |     32.311 |   1.0137 |     35.352 |     1.0
   40 |   0.9076 |     31.687 |   0.9930 |     34.373 |     1.0
   41 |   0.9034 |     31.543 |   0.9917 |     34.037 |     1.1
   42 |   0.8922 |     30.991 |   1.0061 |     34.587 |     1.1
   43 |   0.8842 |     31.002 |   0.9897 |     34.067 |     1.1
   44 |   0.8772 |     30.383 |   0.9823 |     33.945 |     1.1
   45 |   0.8699 |     30.284 |   0.9689 |     32.936 |     1.2
   46 |   0.8587 |     29.798 |   0.9711 |     33.547 |     1.2
   47 |   0.8458 |     29.477 |   0.9675 |     33.486 |     1.2
   48 |   0.8357 |     29.140 |   0.9553 |     32.783 |     1.2
   49 |   0.8235 |     28.422 |   0.9825 |     32.936 |     1.3
   50 |   0.8251 |     28.505 |   0.9538 |     32.110 |     1.3
   51 |   0.8084 |     27.886 |   0.9355 |     31.865 |     1.3
   52 |   0.7927 |     27.229 |   0.9415 |     31.835 |     1.3
   53 |   0.7833 |     26.942 |   0.9291 |     31.131 |     1.4
   54 |   0.7693 |     26.152 |   0.9133 |     30.948 |     1.4
   55 |   0.7583 |     25.743 |   0.9205 |     30.367 |     1.4
   56 |   0.7515 |     25.876 |   0.9259 |     30.917 |     1.4
   57 |   0.7503 |     25.831 |   0.9208 |     30.734 |     1.5
   58 |   0.7360 |     25.152 |   0.9265 |     31.529 |     1.5
   59 |   0.7201 |     24.367 |   0.9078 |     30.550 |     1.5
   60 |   0.7086 |     24.069 |   0.9084 |     30.336 |     1.5
   61 |   0.7029 |     24.086 |   0.9238 |     29.939 |     1.6
   62 |   0.6883 |     23.395 |   0.9110 |     30.245 |     1.6
   63 |   0.6810 |     23.368 |   0.9062 |     29.755 |     1.6
   64 |   0.6739 |     22.771 |   0.8999 |     29.511 |     1.7
   65 |   0.6616 |     22.567 |   0.9062 |     29.847 |     1.7
   66 |   0.6556 |     22.379 |   0.9160 |     29.633 |     1.7
   67 |   0.6478 |     21.915 |   0.9010 |     29.419 |     1.7
   68 |   0.6412 |     21.843 |   0.9304 |     28.899 |     1.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 868,642

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1926 |     59.607 |   1.5925 |     45.963 |     0.0
    2 |   1.4754 |     46.177 |   1.4073 |     45.963 |     0.1
    3 |   1.4064 |     46.194 |   1.3774 |     45.963 |     0.1
    4 |   1.3734 |     45.923 |   1.3526 |     45.382 |     0.1
    5 |   1.3468 |     45.746 |   1.3461 |     46.483 |     0.2
    6 |   1.3250 |     45.796 |   1.3050 |     46.330 |     0.2
    7 |   1.3062 |     45.547 |   1.2972 |     45.046 |     0.2
    8 |   1.2923 |     45.454 |   1.3032 |     45.994 |     0.3
    9 |   1.2729 |     44.713 |   1.2647 |     44.771 |     0.3
   10 |   1.2551 |     44.443 |   1.2549 |     44.281 |     0.3
   11 |   1.2393 |     43.907 |   1.2339 |     43.028 |     0.3
   12 |   1.2220 |     43.139 |   1.2138 |     43.364 |     0.4
   13 |   1.2089 |     42.570 |   1.2218 |     42.446 |     0.4
   14 |   1.1955 |     42.454 |   1.2048 |     42.630 |     0.4
   15 |   1.1881 |     42.194 |   1.1991 |     42.569 |     0.5
   16 |   1.1789 |     42.564 |   1.1858 |     42.599 |     0.5
   17 |   1.1674 |     42.029 |   1.1728 |     41.743 |     0.5
   18 |   1.1540 |     41.288 |   1.1935 |     42.294 |     0.6
   19 |   1.1473 |     41.123 |   1.1571 |     40.979 |     0.6
   20 |   1.1362 |     40.548 |   1.1582 |     40.581 |     0.6
   21 |   1.1282 |     40.471 |   1.1569 |     40.489 |     0.7
   22 |   1.1198 |     40.156 |   1.1438 |     40.092 |     0.7
   23 |   1.1085 |     39.730 |   1.1500 |     40.612 |     0.7
   24 |   1.1058 |     39.869 |   1.1378 |     40.275 |     0.8
   25 |   1.0933 |     39.399 |   1.1276 |     39.664 |     0.8
   26 |   1.0856 |     39.123 |   1.1185 |     39.052 |     0.8
   27 |   1.0760 |     38.692 |   1.1098 |     38.777 |     0.8
   28 |   1.0662 |     38.526 |   1.1057 |     38.532 |     0.9
   29 |   1.0572 |     38.250 |   1.1014 |     38.349 |     0.9
   30 |   1.0492 |     37.791 |   1.0946 |     37.706 |     0.9
   31 |   1.0343 |     37.145 |   1.0950 |     37.859 |     1.0
   32 |   1.0293 |     36.510 |   1.0893 |     38.073 |     1.0
   33 |   1.0208 |     36.466 |   1.0727 |     37.064 |     1.0
   34 |   1.0101 |     36.040 |   1.0698 |     38.165 |     1.1
   35 |   1.0030 |     35.847 |   1.0666 |     36.391 |     1.1
   36 |   0.9870 |     35.123 |   1.0570 |     36.728 |     1.1
   37 |   0.9746 |     34.725 |   1.0621 |     37.248 |     1.2
   38 |   0.9692 |     33.952 |   1.0590 |     36.269 |     1.2
   39 |   0.9603 |     34.173 |   1.0405 |     36.820 |     1.2
   40 |   0.9499 |     33.372 |   1.0312 |     35.810 |     1.2
   41 |   0.9400 |     33.157 |   1.0343 |     36.239 |     1.3
   42 |   0.9274 |     32.560 |   1.0286 |     35.596 |     1.3
   43 |   0.9175 |     32.024 |   1.0259 |     35.596 |     1.3
   44 |   0.9037 |     31.560 |   1.0201 |     35.841 |     1.4
   45 |   0.8935 |     31.179 |   0.9944 |     34.587 |     1.4
   46 |   0.8810 |     30.660 |   0.9956 |     34.098 |     1.4
   47 |   0.8708 |     30.284 |   1.0070 |     35.015 |     1.5
   48 |   0.8603 |     30.278 |   0.9803 |     33.333 |     1.5
   49 |   0.8447 |     29.505 |   0.9855 |     33.425 |     1.5
   50 |   0.8298 |     28.748 |   0.9699 |     33.180 |     1.6
   51 |   0.8214 |     28.301 |   0.9692 |     33.792 |     1.6
   52 |   0.8120 |     27.975 |   0.9700 |     33.150 |     1.6
   53 |   0.7955 |     27.422 |   0.9724 |     32.569 |     1.7
   54 |   0.7813 |     26.787 |   0.9539 |     32.080 |     1.7
   55 |   0.7630 |     25.865 |   0.9615 |     32.416 |     1.7
   56 |   0.7556 |     25.793 |   0.9612 |     32.997 |     1.7
   57 |   0.7478 |     25.345 |   0.9515 |     31.498 |     1.8
   58 |   0.7266 |     24.644 |   0.9457 |     31.040 |     1.8
   59 |   0.7177 |     24.296 |   0.9483 |     31.376 |     1.8
   60 |   0.7000 |     23.500 |   0.9497 |     31.621 |     1.9
   61 |   0.6889 |     23.152 |   0.9436 |     31.101 |     1.9
   62 |   0.6780 |     22.666 |   0.9600 |     31.284 |     1.9
   63 |   0.7049 |     23.694 |   0.9494 |     31.835 |     2.0
   64 |   0.6681 |     22.307 |   0.9760 |     31.131 |     2.0
   65 |   0.6488 |     21.545 |   0.9574 |     31.254 |     2.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,853,282

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0705 |     57.054 |   1.5053 |     45.841 |     0.1
    2 |   1.4332 |     45.918 |   1.3730 |     45.963 |     0.2
    3 |   1.3616 |     45.697 |   1.3280 |     45.474 |     0.2
    4 |   1.3171 |     45.128 |   1.2977 |     45.138 |     0.3
    5 |   1.2879 |     44.619 |   1.2794 |     44.312 |     0.4
    6 |   1.2634 |     44.188 |   1.2495 |     43.853 |     0.5
    7 |   1.2428 |     43.669 |   1.2311 |     43.945 |     0.6
    8 |   1.2198 |     43.012 |   1.2133 |     42.783 |     0.6
    9 |   1.1992 |     42.222 |   1.1959 |     42.783 |     0.7
   10 |   1.1847 |     42.067 |   1.1975 |     42.294 |     0.8
   11 |   1.1674 |     41.266 |   1.1800 |     41.927 |     0.9
   12 |   1.1551 |     40.741 |   1.1578 |     39.878 |     1.0
   13 |   1.1312 |     39.692 |   1.1545 |     40.428 |     1.0
   14 |   1.1460 |     41.266 |   1.1571 |     40.459 |     1.1
   15 |   1.1166 |     39.637 |   1.1324 |     39.388 |     1.2
   16 |   1.1020 |     39.222 |   1.1291 |     38.135 |     1.3
   17 |   1.0863 |     38.576 |   1.1145 |     38.257 |     1.4
   18 |   1.0729 |     38.222 |   1.1144 |     37.951 |     1.4
   19 |   1.0610 |     37.736 |   1.0973 |     38.257 |     1.5
   20 |   1.0473 |     37.305 |   1.0905 |     37.034 |     1.6
   21 |   1.0353 |     36.615 |   1.0748 |     37.309 |     1.7
   22 |   1.0229 |     36.129 |   1.0732 |     36.728 |     1.8
   23 |   1.0093 |     35.538 |   1.0794 |     36.697 |     1.9
   24 |   1.0039 |     35.620 |   1.0616 |     36.820 |     1.9
   25 |   0.9855 |     34.549 |   1.0339 |     34.862 |     2.0
   26 |   0.9742 |     34.140 |   1.0432 |     35.168 |     2.1
   27 |   0.9608 |     33.643 |   1.0308 |     35.872 |     2.2
   28 |   0.9498 |     33.438 |   1.0319 |     35.229 |     2.3
   29 |   0.9343 |     32.643 |   1.0205 |     34.557 |     2.3
   30 |   0.9219 |     31.936 |   1.0056 |     34.159 |     2.4
   31 |   0.9082 |     31.709 |   1.0152 |     33.670 |     2.5
   32 |   0.8922 |     30.615 |   1.0045 |     34.037 |     2.6
   33 |   0.8838 |     30.715 |   0.9852 |     32.630 |     2.7
   34 |   0.8670 |     29.610 |   0.9891 |     32.875 |     2.7
   35 |   0.8551 |     29.411 |   0.9725 |     32.141 |     2.8
   36 |   0.8380 |     28.715 |   0.9591 |     32.508 |     2.9
   37 |   0.8233 |     28.047 |   0.9564 |     31.376 |     3.0
   38 |   0.8099 |     27.351 |   0.9577 |     31.713 |     3.1
   39 |   0.7991 |     26.914 |   0.9436 |     31.040 |     3.1
   40 |   0.7827 |     26.610 |   0.9433 |     31.713 |     3.2
   41 |   0.7644 |     25.721 |   0.9394 |     31.101 |     3.3
   42 |   0.7587 |     25.677 |   0.9277 |     30.183 |     3.4
   43 |   0.7390 |     24.655 |   0.9292 |     31.101 |     3.5
   44 |   0.7261 |     24.599 |   0.9271 |     30.612 |     3.6
   45 |   0.7158 |     24.119 |   0.9263 |     30.306 |     3.6
   46 |   0.7010 |     23.660 |   0.9290 |     30.061 |     3.7
   47 |   0.6834 |     23.097 |   0.9282 |     30.336 |     3.8
   48 |   0.6702 |     22.578 |   0.9091 |     30.061 |     3.9
   49 |   0.6539 |     21.666 |   0.9177 |     29.908 |     4.0
   50 |   0.6389 |     21.390 |   0.9175 |     29.878 |     4.0
   51 |   0.6253 |     20.970 |   0.9221 |     29.664 |     4.1
   52 |   0.6115 |     20.235 |   0.9227 |     29.235 |     4.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 507,298

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0088 |     55.033 |   1.4574 |     45.627 |     0.0
    2 |   1.3826 |     45.547 |   1.3295 |     44.801 |     0.0
    3 |   1.3076 |     44.200 |   1.2839 |     44.190 |     0.1
    4 |   1.2685 |     43.802 |   1.2605 |     44.373 |     0.1
    5 |   1.2371 |     43.454 |   1.2413 |     43.517 |     0.1
    6 |   1.2139 |     42.708 |   1.2190 |     42.232 |     0.1
    7 |   1.1860 |     41.664 |   1.1981 |     41.529 |     0.1
    8 |   1.1646 |     41.100 |   1.1908 |     41.437 |     0.1
    9 |   1.1443 |     40.531 |   1.1694 |     41.162 |     0.2
   10 |   1.1228 |     39.891 |   1.1507 |     39.694 |     0.2
   11 |   1.1001 |     38.957 |   1.1319 |     39.358 |     0.2
   12 |   1.0820 |     38.338 |   1.1179 |     38.379 |     0.2
   13 |   1.0632 |     37.924 |   1.1060 |     37.615 |     0.2
   14 |   1.0411 |     36.582 |   1.1038 |     37.278 |     0.2
   15 |   1.0238 |     36.107 |   1.0728 |     37.187 |     0.3
   16 |   1.0016 |     35.317 |   1.0673 |     36.728 |     0.3
   17 |   0.9788 |     34.256 |   1.0456 |     35.902 |     0.3
   18 |   0.9562 |     33.217 |   1.0197 |     35.566 |     0.3
   19 |   0.9394 |     32.825 |   1.0095 |     35.107 |     0.3
   20 |   0.9206 |     32.008 |   1.0145 |     34.740 |     0.3
   21 |   0.8961 |     30.941 |   0.9833 |     34.098 |     0.4
   22 |   0.8833 |     30.637 |   0.9854 |     34.893 |     0.4
   23 |   0.8584 |     29.980 |   0.9716 |     33.119 |     0.4
   24 |   0.8377 |     28.776 |   0.9736 |     33.303 |     0.4
   25 |   0.8180 |     27.897 |   0.9519 |     32.569 |     0.4
   26 |   0.7938 |     27.146 |   0.9545 |     31.957 |     0.4
   27 |   0.7704 |     26.047 |   0.9564 |     31.498 |     0.5
   28 |   0.7474 |     25.417 |   0.9478 |     31.835 |     0.5
   29 |   0.7276 |     24.412 |   0.9731 |     32.263 |     0.5
   30 |   0.7066 |     23.544 |   0.9362 |     31.040 |     0.5
   31 |   0.6864 |     22.683 |   0.9485 |     30.520 |     0.5
   32 |   0.6600 |     21.666 |   0.9345 |     29.939 |     0.5
   33 |   0.6380 |     21.263 |   0.9547 |     31.223 |     0.6
   34 |   0.6444 |     21.219 |   0.9378 |     29.939 |     0.6
   35 |   0.6101 |     20.020 |   0.9358 |     30.398 |     0.6
   36 |   0.5847 |     18.992 |   0.9236 |     29.174 |     0.6
   37 |   0.5560 |     17.678 |   0.9411 |     29.602 |     0.6
   38 |   0.5432 |     17.644 |   0.9393 |     29.205 |     0.6
   39 |   0.5198 |     16.639 |   0.9557 |     29.113 |     0.7
   40 |   0.5037 |     16.241 |   0.9682 |     28.930 |     0.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 591,522

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5442 |     68.744 |   1.9672 |     58.624 |     0.0
    2 |   1.7819 |     50.652 |   1.5828 |     45.963 |     0.1
    3 |   1.5177 |     46.232 |   1.4349 |     45.841 |     0.1
    4 |   1.4252 |     46.011 |   1.3822 |     45.963 |     0.1
    5 |   1.3835 |     45.929 |   1.3488 |     45.963 |     0.2
    6 |   1.3511 |     45.741 |   1.3214 |     45.107 |     0.2
    7 |   1.3191 |     44.956 |   1.2963 |     44.618 |     0.3
    8 |   1.2987 |     44.680 |   1.2785 |     44.404 |     0.3
    9 |   1.2789 |     44.072 |   1.2675 |     43.700 |     0.3
   10 |   1.2652 |     44.017 |   1.2570 |     44.251 |     0.4
   11 |   1.2532 |     43.807 |   1.2415 |     43.670 |     0.4
   12 |   1.2387 |     43.802 |   1.2377 |     43.211 |     0.4
   13 |   1.2267 |     43.360 |   1.2173 |     43.486 |     0.5
   14 |   1.2149 |     42.951 |   1.2164 |     42.630 |     0.5
   15 |   1.2030 |     42.603 |   1.2066 |     42.355 |     0.5
   16 |   1.1921 |     42.305 |   1.1994 |     42.080 |     0.6
   17 |   1.1851 |     41.791 |   1.1848 |     41.284 |     0.6
   18 |   1.1730 |     41.542 |   1.1821 |     40.703 |     0.6
   19 |   1.1626 |     41.189 |   1.1779 |     41.346 |     0.7
   20 |   1.1535 |     40.780 |   1.1697 |     40.642 |     0.7
   21 |   1.1496 |     40.791 |   1.1683 |     40.856 |     0.8
   22 |   1.1399 |     40.299 |   1.1612 |     41.040 |     0.8
   23 |   1.1324 |     40.112 |   1.1523 |     40.520 |     0.8
   24 |   1.1253 |     40.023 |   1.1488 |     40.214 |     0.9
   25 |   1.1165 |     39.388 |   1.1476 |     40.122 |     0.9
   26 |   1.1102 |     39.393 |   1.1405 |     39.694 |     0.9
   27 |   1.1037 |     39.222 |   1.1433 |     39.878 |     1.0
   28 |   1.0951 |     38.747 |   1.1383 |     40.153 |     1.0
   29 |   1.0951 |     38.581 |   1.1232 |     39.419 |     1.0
   30 |   1.0844 |     38.626 |   1.1228 |     39.419 |     1.1
   31 |   1.0875 |     38.714 |   1.1323 |     38.899 |     1.1
   32 |   1.0757 |     38.449 |   1.1231 |     40.092 |     1.1
   33 |   1.0709 |     38.355 |   1.1167 |     38.899 |     1.2
   34 |   1.0617 |     37.891 |   1.1116 |     38.869 |     1.2
   35 |   1.0558 |     37.416 |   1.1099 |     38.440 |     1.2
   36 |   1.0507 |     37.311 |   1.1166 |     38.349 |     1.3
   37 |   1.0421 |     36.731 |   1.1034 |     38.318 |     1.3
   38 |   1.0401 |     36.803 |   1.1087 |     38.196 |     1.4
   39 |   1.0370 |     36.758 |   1.0941 |     37.859 |     1.4
   40 |   1.0327 |     36.410 |   1.0990 |     38.104 |     1.4
   41 |   1.0237 |     36.245 |   1.0981 |     38.502 |     1.5
   42 |   1.0169 |     36.123 |   1.0891 |     38.226 |     1.5
   43 |   1.0086 |     35.571 |   1.0828 |     37.309 |     1.5
   44 |   1.0025 |     35.455 |   1.0856 |     37.615 |     1.6
   45 |   1.0024 |     35.140 |   1.0762 |     36.728 |     1.6
   46 |   0.9993 |     35.654 |   1.0745 |     36.575 |     1.6
   47 |   0.9956 |     34.996 |   1.0790 |     37.003 |     1.7
   48 |   0.9848 |     34.632 |   1.0645 |     36.483 |     1.7
   49 |   0.9787 |     34.554 |   1.0652 |     35.780 |     1.7
   50 |   0.9720 |     34.228 |   1.0619 |     36.269 |     1.8
   51 |   0.9647 |     33.753 |   1.0483 |     35.076 |     1.8
   52 |   0.9572 |     33.146 |   1.0584 |     35.382 |     1.9
   53 |   0.9525 |     33.278 |   1.0518 |     36.606 |     1.9
   54 |   0.9424 |     32.764 |   1.0540 |     35.474 |     1.9
   55 |   0.9387 |     33.090 |   1.0463 |     35.688 |     2.0
   56 |   0.9293 |     32.610 |   1.0478 |     34.862 |     2.0
   57 |   0.9215 |     32.350 |   1.0142 |     36.055 |     2.0
   58 |   0.9172 |     31.908 |   1.0129 |     35.199 |     2.1
   59 |   0.9112 |     31.499 |   1.0054 |     35.474 |     2.1
   60 |   0.8991 |     30.941 |   1.0194 |     34.740 |     2.1
   61 |   0.8999 |     30.952 |   1.0394 |     35.076 |     2.2
   62 |   0.8844 |     30.439 |   1.0051 |     35.107 |     2.2
   63 |   0.8876 |     30.698 |   0.9986 |     34.801 |     2.2
   64 |   0.8885 |     30.737 |   1.0166 |     34.312 |     2.3
   65 |   0.8728 |     29.941 |   1.0132 |     33.700 |     2.3
   66 |   0.8633 |     29.571 |   1.0039 |     33.792 |     2.4
   67 |   0.8580 |     29.466 |   1.0065 |     34.037 |     2.4
   68 |   0.8394 |     28.654 |   0.9880 |     33.089 |     2.4
   69 |   0.8352 |     28.533 |   0.9729 |     33.456 |     2.5
   70 |   0.8314 |     28.312 |   0.9939 |     33.303 |     2.5
   71 |   0.8261 |     28.174 |   1.0006 |     32.324 |     2.5
   72 |   0.8061 |     27.030 |   0.9914 |     33.303 |     2.6
   73 |   0.8081 |     27.179 |   0.9744 |     32.630 |     2.6
   74 |   0.8023 |     27.312 |   0.9706 |     32.813 |     2.6
   75 |   0.8265 |     28.008 |   1.0390 |     34.373 |     2.7
   76 |   0.8507 |     28.991 |   0.9772 |     32.691 |     2.7
   77 |   0.8053 |     27.229 |   0.9935 |     31.590 |     2.8
   78 |   0.7819 |     26.373 |   0.9683 |     31.927 |     2.8
   79 |   0.7747 |     25.986 |   0.9769 |     32.385 |     2.8
   80 |   0.7720 |     25.870 |   0.9598 |     31.835 |     2.9
   81 |   0.7526 |     24.964 |   0.9725 |     31.040 |     2.9
   82 |   0.7490 |     25.036 |   0.9571 |     31.896 |     2.9
   83 |   0.7376 |     24.754 |   0.9626 |     31.376 |     3.0
   84 |   0.7287 |     24.174 |   0.9600 |     31.774 |     3.0
   85 |   0.7163 |     23.810 |   0.9888 |     31.376 |     3.0
   86 |   0.7110 |     23.738 |   0.9772 |     30.581 |     3.1
   87 |   0.7078 |     23.390 |   0.9563 |     30.153 |     3.1
   88 |   0.7250 |     24.174 |   0.9791 |     31.162 |     3.1
   89 |   0.7054 |     23.406 |   0.9588 |     30.275 |     3.2
   90 |   0.6850 |     22.743 |   0.9615 |     30.703 |     3.2
   91 |   0.6743 |     22.174 |   0.9619 |     30.428 |     3.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,662,242

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1516 |     59.579 |   1.5837 |     45.963 |     0.1
    2 |   1.4808 |     46.183 |   1.4046 |     46.024 |     0.1
    3 |   1.3960 |     46.039 |   1.3566 |     45.382 |     0.2
    4 |   1.3535 |     45.824 |   1.3199 |     45.841 |     0.2
    5 |   1.3183 |     45.349 |   1.2946 |     45.413 |     0.3
    6 |   1.2931 |     44.680 |   1.2730 |     44.709 |     0.4
    7 |   1.2684 |     44.548 |   1.2565 |     43.547 |     0.4
    8 |   1.2496 |     43.813 |   1.2308 |     43.609 |     0.5
    9 |   1.2323 |     43.608 |   1.2266 |     43.517 |     0.6
   10 |   1.2153 |     42.940 |   1.2163 |     43.028 |     0.6
   11 |   1.2033 |     42.481 |   1.2005 |     41.988 |     0.7
   12 |   1.1934 |     41.830 |   1.1962 |     42.080 |     0.7
   13 |   1.1792 |     41.669 |   1.1855 |     41.376 |     0.8
   14 |   1.1676 |     41.482 |   1.1771 |     41.498 |     0.9
   15 |   1.1563 |     41.095 |   1.1679 |     41.009 |     0.9
   16 |   1.1484 |     40.763 |   1.1614 |     40.306 |     1.0
   17 |   1.1372 |     40.438 |   1.1607 |     40.581 |     1.0
   18 |   1.1286 |     40.244 |   1.1408 |     39.939 |     1.1
   19 |   1.1173 |     40.194 |   1.1434 |     39.725 |     1.2
   20 |   1.1083 |     39.526 |   1.1348 |     39.511 |     1.2
   21 |   1.0995 |     39.554 |   1.1269 |     39.021 |     1.3
   22 |   1.0886 |     39.145 |   1.1308 |     39.908 |     1.4
   23 |   1.0819 |     38.902 |   1.1161 |     38.654 |     1.4
   24 |   1.0701 |     38.576 |   1.1176 |     38.318 |     1.5
   25 |   1.0626 |     38.206 |   1.0905 |     38.043 |     1.5
   26 |   1.0515 |     37.581 |   1.0943 |     37.951 |     1.6
   27 |   1.0436 |     37.184 |   1.1049 |     38.502 |     1.7
   28 |   1.0346 |     36.990 |   1.0833 |     37.125 |     1.7
   29 |   1.0276 |     36.637 |   1.0715 |     37.554 |     1.8
   30 |   1.0174 |     36.073 |   1.0619 |     36.942 |     1.8
   31 |   1.0093 |     36.035 |   1.0619 |     36.300 |     1.9
   32 |   0.9998 |     35.582 |   1.0645 |     36.575 |     2.0
   33 |   0.9935 |     35.538 |   1.0654 |     36.483 |     2.0
   34 |   0.9837 |     34.985 |   1.0592 |     37.217 |     2.1
   35 |   0.9749 |     34.670 |   1.0401 |     37.095 |     2.1
   36 |   0.9677 |     34.576 |   1.0552 |     37.370 |     2.2
   37 |   0.9591 |     33.648 |   1.0513 |     36.239 |     2.3
   38 |   0.9462 |     33.853 |   1.0174 |     35.963 |     2.3
   39 |   0.9410 |     33.118 |   1.0275 |     35.627 |     2.4
   40 |   0.9283 |     32.825 |   1.0090 |     35.321 |     2.5
   41 |   0.9204 |     32.687 |   1.0037 |     34.954 |     2.5
   42 |   0.9160 |     32.389 |   0.9993 |     34.526 |     2.6
   43 |   0.9091 |     32.118 |   0.9968 |     33.853 |     2.6
   44 |   0.8941 |     31.389 |   1.0058 |     34.312 |     2.7
   45 |   0.8794 |     30.555 |   0.9903 |     34.006 |     2.8
   46 |   0.8719 |     30.350 |   0.9888 |     34.709 |     2.8
   47 |   0.8671 |     30.234 |   0.9801 |     33.639 |     2.9
   48 |   0.8587 |     29.654 |   0.9742 |     33.119 |     3.0
   49 |   0.8480 |     29.488 |   0.9541 |     32.722 |     3.0
   50 |   0.8423 |     29.632 |   0.9783 |     32.844 |     3.1
   51 |   0.8329 |     29.047 |   0.9766 |     33.150 |     3.1
   52 |   0.8190 |     28.698 |   0.9636 |     32.232 |     3.2
   53 |   0.8100 |     28.063 |   0.9476 |     32.263 |     3.3
   54 |   0.8026 |     27.903 |   0.9568 |     32.263 |     3.3
   55 |   0.7937 |     27.317 |   0.9392 |     31.774 |     3.4
   56 |   0.7854 |     27.008 |   0.9543 |     31.529 |     3.4
   57 |   0.7747 |     26.864 |   0.9488 |     32.232 |     3.5
   58 |   0.7684 |     26.351 |   0.9490 |     32.232 |     3.6
   59 |   0.7581 |     26.157 |   0.9303 |     31.284 |     3.6
   60 |   0.7465 |     25.605 |   0.9406 |     31.070 |     3.7
   61 |   0.7358 |     25.163 |   0.9421 |     30.673 |     3.8
   62 |   0.7337 |     25.185 |   0.9221 |     30.612 |     3.8
   63 |   0.7246 |     24.682 |   0.9384 |     31.498 |     3.9
   64 |   0.7157 |     24.235 |   0.9293 |     31.223 |     3.9
   65 |   0.7064 |     24.108 |   0.9198 |     30.367 |     4.0
   66 |   0.6970 |     23.473 |   0.9172 |     30.520 |     4.1
   67 |   0.6928 |     23.566 |   0.9193 |     30.367 |     4.1
   68 |   0.6802 |     23.125 |   0.9226 |     30.092 |     4.2
   69 |   0.6728 |     22.937 |   0.9197 |     29.266 |     4.2
   70 |   0.6619 |     22.423 |   0.9236 |     30.183 |     4.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,132,962

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1769 |     59.866 |   1.5802 |     45.963 |     0.0
    2 |   1.4745 |     46.116 |   1.3908 |     45.963 |     0.1
    3 |   1.3945 |     46.078 |   1.3499 |     45.963 |     0.1
    4 |   1.3583 |     45.442 |   1.3321 |     45.474 |     0.2
    5 |   1.3295 |     45.199 |   1.3051 |     45.138 |     0.2
    6 |   1.3033 |     45.089 |   1.2825 |     44.893 |     0.2
    7 |   1.2827 |     44.531 |   1.2730 |     45.107 |     0.3
    8 |   1.2667 |     44.310 |   1.2622 |     44.893 |     0.3
    9 |   1.2491 |     44.227 |   1.2529 |     44.373 |     0.4
   10 |   1.2378 |     44.039 |   1.2323 |     43.119 |     0.4
   11 |   1.2188 |     43.161 |   1.2098 |     42.630 |     0.5
   12 |   1.1993 |     42.631 |   1.2060 |     42.385 |     0.5
   13 |   1.1808 |     42.161 |   1.1860 |     41.927 |     0.5
   14 |   1.1602 |     41.100 |   1.1651 |     41.070 |     0.6
   15 |   1.1446 |     40.642 |   1.1617 |     40.367 |     0.6
   16 |   1.1288 |     39.924 |   1.1434 |     39.908 |     0.7
   17 |   1.1074 |     39.427 |   1.1332 |     39.358 |     0.7
   18 |   1.0886 |     38.670 |   1.1112 |     38.593 |     0.7
   19 |   1.0720 |     38.090 |   1.0995 |     38.930 |     0.8
   20 |   1.0539 |     37.421 |   1.1062 |     37.920 |     0.8
   21 |   1.0352 |     36.537 |   1.0801 |     37.554 |     0.9
   22 |   1.0110 |     35.670 |   1.0646 |     37.003 |     0.9
   23 |   0.9883 |     34.488 |   1.0549 |     35.872 |     0.9
   24 |   0.9659 |     33.753 |   1.0445 |     35.260 |     1.0
   25 |   0.9440 |     32.505 |   1.0147 |     34.067 |     1.0
   26 |   0.9211 |     31.521 |   1.0244 |     33.639 |     1.1
   27 |   0.8991 |     30.604 |   1.0099 |     33.914 |     1.1
   28 |   0.8693 |     29.367 |   1.0058 |     33.333 |     1.1
   29 |   0.8471 |     28.991 |   1.0013 |     32.661 |     1.2
   30 |   0.8249 |     27.699 |   1.0103 |     33.272 |     1.2
   31 |   0.8034 |     27.146 |   0.9956 |     31.927 |     1.3
   32 |   0.7804 |     26.185 |   0.9893 |     30.979 |     1.3
   33 |   0.7598 |     25.583 |   0.9832 |     31.682 |     1.4
   34 |   0.7410 |     24.743 |   0.9762 |     31.162 |     1.4
   35 |   0.7233 |     24.246 |   0.9656 |     31.101 |     1.4
   36 |   0.6993 |     23.202 |   0.9650 |     30.031 |     1.5
   37 |   0.6863 |     22.716 |   0.9817 |     30.612 |     1.5
   38 |   0.6711 |     22.257 |   0.9765 |     30.122 |     1.6
   39 |   0.6450 |     21.197 |   0.9516 |     29.664 |     1.6
   40 |   0.6274 |     20.451 |   0.9496 |     29.694 |     1.6
   41 |   0.6149 |     19.998 |   0.9635 |     29.205 |     1.7
   42 |   0.6027 |     19.694 |   0.9686 |     28.746 |     1.7
   43 |   0.5732 |     18.794 |   0.9386 |     28.502 |     1.8
   44 |   0.5557 |     18.186 |   0.9816 |     29.817 |     1.8
   45 |   0.5426 |     17.534 |   0.9652 |     28.379 |     1.9
   46 |   0.5337 |     17.307 |   0.9841 |     28.379 |     1.9
   47 |   0.5201 |     16.722 |   0.9730 |     28.257 |     1.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 586,594

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0888 |     57.115 |   1.4996 |     45.963 |     0.0
    2 |   1.4143 |     45.873 |   1.3481 |     44.557 |     0.0
    3 |   1.3313 |     44.962 |   1.3180 |     44.037 |     0.1
    4 |   1.2959 |     44.161 |   1.2904 |     44.067 |     0.1
    5 |   1.2689 |     44.006 |   1.2598 |     43.303 |     0.1
    6 |   1.2423 |     43.520 |   1.2432 |     43.792 |     0.1
    7 |   1.2228 |     42.857 |   1.2303 |     42.661 |     0.2
    8 |   1.1939 |     42.001 |   1.2086 |     41.498 |     0.2
    9 |   1.1703 |     40.984 |   1.1829 |     41.040 |     0.2
   10 |   1.1430 |     39.670 |   1.1624 |     39.602 |     0.2
   11 |   1.1161 |     39.034 |   1.1476 |     39.480 |     0.3
   12 |   1.0881 |     38.217 |   1.1281 |     38.838 |     0.3
   13 |   1.0651 |     37.057 |   1.1069 |     37.492 |     0.3
   14 |   1.0261 |     35.582 |   1.0904 |     36.728 |     0.3
   15 |   1.0016 |     34.709 |   1.0673 |     37.095 |     0.4
   16 |   0.9686 |     33.372 |   1.0519 |     34.893 |     0.4
   17 |   0.9309 |     31.809 |   1.0260 |     34.159 |     0.4
   18 |   0.9031 |     30.952 |   1.0404 |     35.413 |     0.4
   19 |   0.8843 |     30.080 |   1.0077 |     32.661 |     0.5
   20 |   0.8436 |     28.489 |   0.9864 |     31.560 |     0.5
   21 |   0.8082 |     26.975 |   0.9653 |     31.529 |     0.5
   22 |   0.7754 |     25.942 |   0.9678 |     30.917 |     0.5
   23 |   0.7401 |     24.240 |   0.9395 |     31.162 |     0.6
   24 |   0.7080 |     23.318 |   0.9371 |     30.336 |     0.6
   25 |   0.6814 |     22.567 |   0.9477 |     30.061 |     0.6
   26 |   0.6425 |     20.898 |   0.9196 |     29.144 |     0.6
   27 |   0.6055 |     19.567 |   0.9194 |     29.847 |     0.7
   28 |   0.5784 |     18.782 |   0.9081 |     29.327 |     0.7
   29 |   0.5495 |     17.523 |   0.9140 |     29.052 |     0.7
   30 |   0.5238 |     16.716 |   0.9325 |     28.440 |     0.7
   31 |   0.4978 |     15.926 |   0.9351 |     28.960 |     0.8
   32 |   0.4699 |     14.960 |   0.9352 |     28.563 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 586,594

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2155 |     60.358 |   1.6017 |     45.963 |     0.0
    2 |   1.4799 |     46.177 |   1.4045 |     45.963 |     0.0
    3 |   1.3890 |     45.741 |   1.3587 |     45.688 |     0.1
    4 |   1.3644 |     45.763 |   1.3398 |     45.688 |     0.1
    5 |   1.3358 |     45.768 |   1.3129 |     46.239 |     0.1
    6 |   1.3145 |     45.332 |   1.3021 |     45.352 |     0.1
    7 |   1.3033 |     45.006 |   1.2928 |     44.740 |     0.1
    8 |   1.2895 |     44.664 |   1.2759 |     44.281 |     0.1
    9 |   1.2720 |     44.763 |   1.2644 |     44.404 |     0.2
   10 |   1.2580 |     44.525 |   1.2550 |     44.128 |     0.2
   11 |   1.2456 |     44.078 |   1.2452 |     44.373 |     0.2
   12 |   1.2322 |     43.631 |   1.2383 |     43.823 |     0.2
   13 |   1.2171 |     43.023 |   1.2265 |     43.272 |     0.2
   14 |   1.2038 |     42.305 |   1.2120 |     42.294 |     0.3
   15 |   1.1883 |     41.857 |   1.1967 |     42.355 |     0.3
   16 |   1.1708 |     41.548 |   1.1846 |     41.896 |     0.3
   17 |   1.1572 |     41.172 |   1.1738 |     40.917 |     0.3
   18 |   1.1408 |     40.830 |   1.1575 |     40.550 |     0.3
   19 |   1.1228 |     39.962 |   1.1463 |     40.703 |     0.3
   20 |   1.1037 |     39.200 |   1.1392 |     39.511 |     0.4
   21 |   1.0869 |     38.869 |   1.1207 |     38.502 |     0.4
   22 |   1.0669 |     37.615 |   1.1149 |     38.257 |     0.4
   23 |   1.0546 |     37.189 |   1.1088 |     38.196 |     0.4
   24 |   1.0346 |     36.758 |   1.0832 |     37.615 |     0.4
   25 |   1.0206 |     36.112 |   1.0720 |     37.034 |     0.4
   26 |   1.0028 |     35.444 |   1.0547 |     36.422 |     0.5
   27 |   0.9851 |     34.908 |   1.0518 |     36.636 |     0.5
   28 |   0.9637 |     34.085 |   1.0374 |     36.667 |     0.5
   29 |   0.9433 |     33.046 |   1.0220 |     35.474 |     0.5
   30 |   0.9332 |     32.891 |   1.0188 |     35.688 |     0.5
   31 |   0.9081 |     31.571 |   0.9999 |     35.596 |     0.6
   32 |   0.8889 |     30.820 |   0.9943 |     34.281 |     0.6
   33 |   0.8691 |     30.024 |   0.9840 |     33.394 |     0.6
   34 |   0.8510 |     29.207 |   0.9916 |     34.220 |     0.6
   35 |   0.8295 |     28.157 |   0.9851 |     33.517 |     0.6
   36 |   0.8106 |     27.583 |   0.9680 |     32.202 |     0.6
   37 |   0.7904 |     26.776 |   0.9514 |     32.049 |     0.7
   38 |   0.7723 |     26.086 |   0.9622 |     31.957 |     0.7
   39 |   0.7500 |     25.008 |   0.9414 |     32.324 |     0.7
   40 |   0.7342 |     24.771 |   0.9368 |     31.529 |     0.7
   41 |   0.7132 |     23.705 |   0.9307 |     31.009 |     0.7
   42 |   0.6967 |     23.224 |   0.9219 |     30.061 |     0.8
   43 |   0.6872 |     23.185 |   0.9545 |     30.489 |     0.8
   44 |   0.6611 |     21.920 |   0.9401 |     30.550 |     0.8
   45 |   0.6367 |     20.998 |   0.9278 |     29.327 |     0.8
   46 |   0.6254 |     20.528 |   0.9429 |     29.572 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,132,962

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1874 |     60.170 |   1.5696 |     45.963 |     0.0
    2 |   1.4679 |     46.282 |   1.4004 |     47.523 |     0.1
    3 |   1.3848 |     45.879 |   1.3512 |     45.474 |     0.1
    4 |   1.3451 |     45.619 |   1.3241 |     45.474 |     0.2
    5 |   1.3203 |     45.398 |   1.3018 |     45.657 |     0.2
    6 |   1.3002 |     45.100 |   1.2854 |     45.168 |     0.2
    7 |   1.2833 |     44.691 |   1.2734 |     44.006 |     0.3
    8 |   1.2672 |     44.277 |   1.2743 |     44.404 |     0.3
    9 |   1.2528 |     44.028 |   1.2465 |     43.823 |     0.4
   10 |   1.2349 |     43.139 |   1.2412 |     43.058 |     0.4
   11 |   1.2189 |     43.111 |   1.2202 |     43.456 |     0.4
   12 |   1.1972 |     42.476 |   1.1976 |     41.590 |     0.5
   13 |   1.1754 |     41.559 |   1.1812 |     41.865 |     0.5
   14 |   1.1515 |     41.117 |   1.1607 |     40.581 |     0.6
   15 |   1.1339 |     40.045 |   1.1607 |     40.520 |     0.6
   16 |   1.1052 |     38.946 |   1.1357 |     39.755 |     0.6
   17 |   1.0825 |     38.438 |   1.1161 |     38.563 |     0.7
   18 |   1.0580 |     37.095 |   1.1013 |     38.012 |     0.7
   19 |   1.0305 |     36.283 |   1.0812 |     37.187 |     0.8
   20 |   1.0054 |     35.162 |   1.0804 |     37.125 |     0.8
   21 |   0.9844 |     33.935 |   1.0673 |     36.361 |     0.8
   22 |   0.9478 |     32.792 |   1.0533 |     35.902 |     0.9
   23 |   0.9152 |     31.566 |   1.0310 |     34.893 |     0.9
   24 |   0.8840 |     30.212 |   1.0318 |     34.190 |     0.9
   25 |   0.8511 |     29.013 |   1.0081 |     33.609 |     1.0
   26 |   0.8109 |     27.003 |   1.0114 |     33.945 |     1.0
   27 |   0.7869 |     26.638 |   0.9926 |     33.089 |     1.1
   28 |   0.7494 |     25.075 |   0.9916 |     32.691 |     1.1
   29 |   0.7111 |     23.727 |   0.9839 |     31.651 |     1.1
   30 |   0.6750 |     22.241 |   0.9594 |     31.254 |     1.2
   31 |   0.6328 |     20.826 |   0.9605 |     30.612 |     1.2
   32 |   0.5967 |     19.246 |   0.9896 |     30.581 |     1.3
   33 |   0.5665 |     18.219 |   0.9816 |     30.061 |     1.3
   34 |   0.5324 |     17.048 |   0.9833 |     29.388 |     1.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,281,698

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4675 |     65.689 |   1.8988 |     52.752 |     0.1
    2 |   1.7166 |     49.254 |   1.5300 |     45.963 |     0.1
    3 |   1.4923 |     46.089 |   1.4272 |     45.963 |     0.2
    4 |   1.4285 |     46.172 |   1.3942 |     45.963 |     0.2
    5 |   1.3975 |     46.083 |   1.3701 |     45.963 |     0.3
    6 |   1.3793 |     45.802 |   1.3519 |     45.413 |     0.4
    7 |   1.3652 |     45.553 |   1.3434 |     45.994 |     0.4
    8 |   1.3556 |     45.663 |   1.3295 |     45.749 |     0.5
    9 |   1.3447 |     45.570 |   1.3199 |     45.107 |     0.6
   10 |   1.3317 |     45.244 |   1.3103 |     45.627 |     0.6
   11 |   1.3213 |     45.061 |   1.3001 |     44.862 |     0.7
   12 |   1.3100 |     44.653 |   1.2902 |     44.465 |     0.7
   13 |   1.2962 |     44.630 |   1.2758 |     43.976 |     0.8
   14 |   1.2789 |     44.321 |   1.2629 |     43.853 |     0.9
   15 |   1.2634 |     43.918 |   1.2527 |     44.067 |     0.9
   16 |   1.2515 |     43.719 |   1.2402 |     43.364 |     1.0
   17 |   1.2379 |     43.349 |   1.2279 |     43.058 |     1.1
   18 |   1.2272 |     42.979 |   1.2213 |     42.813 |     1.1
   19 |   1.2148 |     42.620 |   1.2103 |     42.661 |     1.2
   20 |   1.2035 |     42.382 |   1.2006 |     42.202 |     1.2
   21 |   1.1935 |     41.946 |   1.1956 |     41.804 |     1.3
   22 |   1.1793 |     41.642 |   1.1908 |     41.590 |     1.4
   23 |   1.1644 |     41.084 |   1.1798 |     40.306 |     1.4
   24 |   1.1533 |     40.857 |   1.1692 |     40.367 |     1.5
   25 |   1.1424 |     40.603 |   1.1610 |     39.572 |     1.6
   26 |   1.1298 |     39.841 |   1.1542 |     39.450 |     1.6
   27 |   1.1136 |     39.316 |   1.1452 |     38.991 |     1.7
   28 |   1.1024 |     38.570 |   1.1388 |     38.869 |     1.8
   29 |   1.0875 |     38.156 |   1.1331 |     38.807 |     1.8
   30 |   1.0762 |     37.581 |   1.1153 |     38.012 |     1.9
   31 |   1.0577 |     37.012 |   1.1156 |     37.920 |     1.9
   32 |   1.0464 |     36.383 |   1.1039 |     37.737 |     2.0
   33 |   1.0287 |     35.963 |   1.0947 |     36.850 |     2.1
   34 |   1.0144 |     35.250 |   1.0972 |     37.248 |     2.1
   35 |   1.0017 |     34.687 |   1.0871 |     36.483 |     2.2
   36 |   0.9854 |     34.118 |   1.0865 |     37.003 |     2.3
   37 |   0.9705 |     33.217 |   1.0727 |     35.321 |     2.3
   38 |   0.9503 |     32.687 |   1.0747 |     35.443 |     2.4
   39 |   0.9371 |     32.024 |   1.0663 |     35.505 |     2.4
   40 |   0.9204 |     31.256 |   1.0604 |     34.343 |     2.5
   41 |   0.9149 |     31.057 |   1.0734 |     35.291 |     2.6
   42 |   0.8884 |     30.052 |   1.0654 |     33.700 |     2.6
   43 |   0.8637 |     29.168 |   1.0651 |     33.731 |     2.7
   44 |   0.8466 |     28.240 |   1.0543 |     32.936 |     2.8
   45 |   0.8230 |     27.638 |   1.0573 |     33.364 |     2.8
   46 |   0.8070 |     26.726 |   1.0582 |     32.783 |     2.9
   47 |   0.7830 |     26.207 |   1.0693 |     32.661 |     2.9
   48 |   0.7727 |     25.787 |   1.0471 |     32.110 |     3.0
   49 |   0.7526 |     25.157 |   1.0427 |     32.049 |     3.1
   50 |   0.7381 |     24.671 |   1.0334 |     32.141 |     3.1
   51 |   0.7137 |     23.572 |   1.0689 |     31.468 |     3.2
   52 |   0.6916 |     22.699 |   1.0443 |     31.651 |     3.3
   53 |   0.6847 |     22.870 |   1.0902 |     31.498 |     3.3
   54 |   0.6620 |     21.738 |   1.0414 |     31.070 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 306,530

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5039 |     67.214 |   1.9116 |     50.917 |     0.0
    2 |   1.7117 |     48.094 |   1.5239 |     45.963 |     0.0
    3 |   1.4778 |     46.089 |   1.4246 |     45.963 |     0.0
    4 |   1.4192 |     46.089 |   1.3844 |     45.963 |     0.1
    5 |   1.3858 |     46.139 |   1.3591 |     45.963 |     0.1
    6 |   1.3579 |     45.868 |   1.3404 |     46.086 |     0.1
    7 |   1.3384 |     45.675 |   1.3187 |     46.116 |     0.1
    8 |   1.3184 |     45.233 |   1.3042 |     45.413 |     0.1
    9 |   1.3029 |     44.945 |   1.3031 |     45.321 |     0.1
   10 |   1.2887 |     44.222 |   1.2886 |     44.709 |     0.2
   11 |   1.2736 |     43.680 |   1.2701 |     43.333 |     0.2
   12 |   1.2602 |     43.299 |   1.2658 |     44.067 |     0.2
   13 |   1.2455 |     43.039 |   1.2484 |     42.813 |     0.2
   14 |   1.2308 |     42.598 |   1.2409 |     43.150 |     0.2
   15 |   1.2135 |     42.520 |   1.2276 |     42.508 |     0.2
   16 |   1.1994 |     41.885 |   1.2086 |     41.988 |     0.3
   17 |   1.1848 |     41.708 |   1.2005 |     41.927 |     0.3
   18 |   1.1655 |     41.128 |   1.1880 |     41.498 |     0.3
   19 |   1.1498 |     40.316 |   1.1781 |     41.162 |     0.3
   20 |   1.1322 |     39.758 |   1.1656 |     40.398 |     0.3
   21 |   1.1163 |     39.211 |   1.1538 |     39.939 |     0.3
   22 |   1.0949 |     38.316 |   1.1490 |     38.930 |     0.4
   23 |   1.0789 |     37.637 |   1.1359 |     38.318 |     0.4
   24 |   1.0575 |     36.499 |   1.1408 |     38.502 |     0.4
   25 |   1.0386 |     35.968 |   1.1134 |     37.645 |     0.4
   26 |   1.0209 |     35.355 |   1.1261 |     37.523 |     0.4
   27 |   1.0021 |     34.560 |   1.1055 |     36.575 |     0.4
   28 |   0.9820 |     33.604 |   1.1005 |     36.606 |     0.5
   29 |   0.9578 |     32.483 |   1.0897 |     35.719 |     0.5
   30 |   0.9364 |     32.240 |   1.0782 |     35.046 |     0.5
   31 |   0.9149 |     31.184 |   1.0639 |     34.985 |     0.5
   32 |   0.8942 |     30.157 |   1.0653 |     34.098 |     0.5
   33 |   0.8719 |     29.074 |   1.0561 |     34.067 |     0.5
   34 |   0.8457 |     28.384 |   1.0258 |     33.180 |     0.6
   35 |   0.8275 |     27.643 |   1.0216 |     33.547 |     0.6
   36 |   0.8104 |     26.765 |   1.0111 |     33.242 |     0.6
   37 |   0.7867 |     25.986 |   1.0213 |     32.905 |     0.6
   38 |   0.7684 |     25.401 |   1.0238 |     33.761 |     0.6
   39 |   0.7589 |     25.058 |   1.0464 |     33.028 |     0.6
   40 |   0.7344 |     24.124 |   1.0094 |     32.477 |     0.7
   41 |   0.7215 |     23.826 |   1.0184 |     32.080 |     0.7
   42 |   0.6923 |     22.633 |   0.9973 |     31.437 |     0.7
   43 |   0.6747 |     21.948 |   1.0115 |     31.376 |     0.7
   44 |   0.6530 |     20.987 |   1.0083 |     32.202 |     0.7
   45 |   0.6399 |     20.517 |   1.0479 |     31.682 |     0.7
   46 |   0.6219 |     20.003 |   0.9958 |     31.560 |     0.8
   47 |   0.6037 |     19.219 |   0.9954 |     30.673 |     0.8
   48 |   0.5972 |     18.766 |   1.0252 |     31.346 |     0.8
   49 |   0.5777 |     18.252 |   1.0046 |     30.826 |     0.8
   50 |   0.5634 |     17.633 |   1.0279 |     31.743 |     0.8
   51 |   0.5462 |     17.169 |   1.0068 |     30.765 |     0.8
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,000,866

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0066 |     54.640 |   1.4616 |     45.841 |     0.0
    2 |   1.3903 |     45.520 |   1.3319 |     44.220 |     0.1
    3 |   1.3050 |     44.001 |   1.2820 |     43.670 |     0.1
    4 |   1.2604 |     43.343 |   1.2494 |     43.150 |     0.2
    5 |   1.2340 |     42.885 |   1.2290 |     42.936 |     0.2
    6 |   1.1975 |     42.067 |   1.2007 |     40.673 |     0.2
    7 |   1.1679 |     40.769 |   1.1823 |     41.437 |     0.3
    8 |   1.1400 |     40.106 |   1.1580 |     40.214 |     0.3
    9 |   1.1081 |     38.847 |   1.1462 |     38.104 |     0.4
   10 |   1.0734 |     37.338 |   1.1147 |     37.645 |     0.4
   11 |   1.0380 |     35.996 |   1.0955 |     37.339 |     0.4
   12 |   1.0051 |     34.676 |   1.0769 |     36.728 |     0.5
   13 |   0.9646 |     33.306 |   1.0298 |     34.067 |     0.5
   14 |   0.9311 |     31.671 |   1.0379 |     34.679 |     0.6
   15 |   0.8928 |     30.201 |   0.9819 |     33.028 |     0.6
   16 |   0.8539 |     29.245 |   0.9928 |     32.416 |     0.6
   17 |   0.8104 |     27.096 |   0.9635 |     31.743 |     0.7
   18 |   0.7665 |     25.428 |   0.9644 |     32.538 |     0.7
   19 |   0.7295 |     24.367 |   0.9281 |     30.856 |     0.8
   20 |   0.6882 |     22.727 |   0.9280 |     30.917 |     0.8
   21 |   0.6571 |     21.589 |   0.9194 |     30.031 |     0.9
   22 |   0.6179 |     20.147 |   0.9078 |     29.052 |     0.9
   23 |   0.5827 |     18.882 |   0.9118 |     29.297 |     0.9
   24 |   0.5515 |     17.810 |   0.9252 |     29.113 |     1.0
   25 |   0.5081 |     16.070 |   0.9227 |     29.174 |     1.0
   26 |   0.4815 |     15.291 |   0.9191 |     29.327 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 440,098

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5661 |     69.092 |   2.0249 |     58.654 |     0.0
    2 |   1.8049 |     51.088 |   1.5929 |     45.963 |     0.0
    3 |   1.5227 |     46.089 |   1.4496 |     45.963 |     0.1
    4 |   1.4353 |     46.139 |   1.3974 |     45.872 |     0.1
    5 |   1.3886 |     45.918 |   1.3517 |     45.566 |     0.1
    6 |   1.3443 |     45.161 |   1.3229 |     44.954 |     0.1
    7 |   1.3092 |     44.791 |   1.2875 |     44.190 |     0.2
    8 |   1.2821 |     43.879 |   1.2718 |     43.456 |     0.2
    9 |   1.2632 |     43.675 |   1.2527 |     43.486 |     0.2
   10 |   1.2456 |     43.222 |   1.2361 |     42.997 |     0.2
   11 |   1.2306 |     42.979 |   1.2251 |     42.813 |     0.3
   12 |   1.2117 |     42.377 |   1.2061 |     43.028 |     0.3
   13 |   1.1964 |     42.161 |   1.2016 |     41.743 |     0.3
   14 |   1.1813 |     41.631 |   1.1924 |     41.040 |     0.3
   15 |   1.1689 |     41.283 |   1.1819 |     40.887 |     0.4
   16 |   1.1566 |     40.984 |   1.1737 |     40.979 |     0.4
   17 |   1.1455 |     40.614 |   1.1619 |     40.306 |     0.4
   18 |   1.1331 |     39.957 |   1.1594 |     40.031 |     0.4
   19 |   1.1265 |     39.780 |   1.1485 |     39.602 |     0.4
   20 |   1.1147 |     39.443 |   1.1456 |     39.755 |     0.5
   21 |   1.1051 |     39.228 |   1.1460 |     39.450 |     0.5
   22 |   1.0968 |     38.570 |   1.1370 |     39.419 |     0.5
   23 |   1.0861 |     38.261 |   1.1242 |     39.266 |     0.5
   24 |   1.0786 |     37.996 |   1.1301 |     38.624 |     0.6
   25 |   1.0715 |     37.394 |   1.1289 |     39.113 |     0.6
   26 |   1.0606 |     37.189 |   1.1155 |     38.807 |     0.6
   27 |   1.0521 |     37.024 |   1.1152 |     38.685 |     0.6
   28 |   1.0421 |     36.377 |   1.1193 |     38.716 |     0.7
   29 |   1.0355 |     36.073 |   1.1023 |     37.523 |     0.7
   30 |   1.0254 |     35.897 |   1.1004 |     38.073 |     0.7
   31 |   1.0193 |     35.747 |   1.1053 |     37.768 |     0.7
   32 |   1.0065 |     35.167 |   1.0910 |     37.554 |     0.7
   33 |   1.0034 |     35.145 |   1.0941 |     37.829 |     0.8
   34 |   0.9929 |     34.748 |   1.0794 |     37.584 |     0.8
   35 |   0.9859 |     34.593 |   1.0930 |     37.401 |     0.8
   36 |   0.9752 |     33.958 |   1.0689 |     36.850 |     0.8
   37 |   0.9648 |     33.532 |   1.0684 |     36.850 |     0.9
   38 |   0.9536 |     33.079 |   1.0756 |     36.361 |     0.9
   39 |   0.9457 |     32.919 |   1.0635 |     36.208 |     0.9
   40 |   0.9337 |     32.378 |   1.0550 |     35.810 |     0.9
   41 |   0.9242 |     31.731 |   1.0535 |     35.688 |     1.0
   42 |   0.9140 |     31.549 |   1.0455 |     35.443 |     1.0
   43 |   0.9047 |     31.350 |   1.0512 |     35.535 |     1.0
   44 |   0.8932 |     30.925 |   1.0464 |     34.954 |     1.0
   45 |   0.8800 |     30.157 |   1.0342 |     35.352 |     1.1
   46 |   0.8705 |     29.986 |   1.0252 |     34.893 |     1.1
   47 |   0.8608 |     29.500 |   1.0430 |     35.443 |     1.1
   48 |   0.8518 |     29.494 |   1.0422 |     34.618 |     1.1
   49 |   0.8396 |     28.566 |   1.0215 |     34.373 |     1.1
   50 |   0.8232 |     28.008 |   1.0437 |     34.343 |     1.2
   51 |   0.8115 |     27.378 |   1.0253 |     33.639 |     1.2
   52 |   0.8005 |     27.063 |   1.0267 |     34.037 |     1.2
   53 |   0.7859 |     26.533 |   1.0297 |     34.159 |     1.2
   54 |   0.7738 |     25.820 |   1.0180 |     33.425 |     1.3
   55 |   0.7688 |     25.749 |   1.0308 |     33.547 |     1.3
   56 |   0.7500 |     25.080 |   1.0296 |     32.416 |     1.3
   57 |   0.7354 |     24.539 |   1.0355 |     32.661 |     1.3
   58 |   0.7203 |     23.760 |   1.0150 |     32.324 |     1.4
   59 |   0.7064 |     23.113 |   1.0193 |     32.538 |     1.4
   60 |   0.6916 |     22.854 |   1.0314 |     32.416 |     1.4
   61 |   0.6818 |     22.423 |   1.0380 |     32.110 |     1.4
   62 |   0.6650 |     21.848 |   1.0221 |     31.560 |     1.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,887,522

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1524 |     58.927 |   1.5772 |     45.994 |     0.1
    2 |   1.4662 |     46.199 |   1.3859 |     45.994 |     0.2
    3 |   1.3741 |     45.929 |   1.3343 |     44.709 |     0.2
    4 |   1.3291 |     44.840 |   1.3040 |     44.801 |     0.3
    5 |   1.2979 |     44.719 |   1.2779 |     44.404 |     0.4
    6 |   1.2681 |     44.089 |   1.2636 |     43.884 |     0.5
    7 |   1.2483 |     43.835 |   1.2420 |     43.547 |     0.6
    8 |   1.2278 |     43.393 |   1.2265 |     42.691 |     0.6
    9 |   1.2081 |     42.785 |   1.2098 |     42.232 |     0.7
   10 |   1.1924 |     42.178 |   1.2017 |     41.468 |     0.8
   11 |   1.1739 |     41.819 |   1.1932 |     41.131 |     0.9
   12 |   1.1561 |     41.222 |   1.1737 |     41.009 |     1.0
   13 |   1.1433 |     40.670 |   1.1711 |     39.480 |     1.0
   14 |   1.1237 |     39.924 |   1.1599 |     39.725 |     1.1
   15 |   1.1090 |     39.134 |   1.1465 |     39.083 |     1.2
   16 |   1.0921 |     38.565 |   1.1352 |     38.685 |     1.3
   17 |   1.0763 |     38.007 |   1.1259 |     38.257 |     1.4
   18 |   1.0718 |     37.725 |   1.1182 |     38.379 |     1.4
   19 |   1.0457 |     36.913 |   1.1015 |     37.798 |     1.5
   20 |   1.0335 |     36.377 |   1.0869 |     37.615 |     1.6
   21 |   1.0148 |     35.637 |   1.0663 |     36.942 |     1.7
   22 |   0.9904 |     34.510 |   1.0531 |     36.269 |     1.8
   23 |   0.9813 |     34.123 |   1.0530 |     35.780 |     1.9
   24 |   0.9577 |     33.333 |   1.0278 |     35.566 |     1.9
   25 |   0.9422 |     32.681 |   1.0307 |     35.566 |     2.0
   26 |   0.9292 |     32.604 |   1.0286 |     35.352 |     2.1
   27 |   0.9081 |     31.582 |   1.0045 |     34.067 |     2.2
   28 |   0.8926 |     30.914 |   0.9959 |     34.526 |     2.3
   29 |   0.8678 |     29.975 |   0.9744 |     33.517 |     2.3
   30 |   0.8559 |     29.577 |   0.9546 |     31.865 |     2.4
   31 |   0.8336 |     28.428 |   0.9704 |     33.028 |     2.5
   32 |   0.8154 |     27.754 |   0.9518 |     32.477 |     2.6
   33 |   0.7992 |     27.174 |   0.9559 |     31.896 |     2.7
   34 |   0.7857 |     26.771 |   0.9428 |     31.743 |     2.7
   35 |   0.7613 |     25.610 |   0.9598 |     31.651 |     2.8
   36 |   0.7450 |     25.058 |   0.9244 |     30.612 |     2.9
   37 |   0.7274 |     24.522 |   0.9066 |     30.489 |     3.0
   38 |   0.7087 |     23.611 |   0.9091 |     30.336 |     3.1
   39 |   0.6902 |     23.020 |   0.8978 |     29.725 |     3.1
   40 |   0.6740 |     22.567 |   0.8957 |     29.205 |     3.2
   41 |   0.6513 |     21.583 |   0.9076 |     29.786 |     3.3
   42 |   0.6377 |     21.097 |   0.8881 |     28.777 |     3.4
   43 |   0.6181 |     20.451 |   0.8718 |     28.318 |     3.5
   44 |   0.6062 |     20.208 |   0.8971 |     29.694 |     3.5
   45 |   0.5854 |     19.412 |   0.8878 |     28.716 |     3.6
   46 |   0.5697 |     18.655 |   0.8711 |     28.440 |     3.7
   47 |   0.5617 |     18.307 |   0.8763 |     28.532 |     3.8
   48 |   0.5395 |     17.744 |   0.8710 |     28.257 |     3.9
   49 |   0.5216 |     16.783 |   0.8743 |     27.920 |     3.9
   50 |   0.5088 |     16.716 |   0.9065 |     27.951 |     4.0
   51 |   0.4937 |     16.004 |   0.8895 |     28.502 |     4.1
   52 |   0.4799 |     15.728 |   0.8822 |     27.859 |     4.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 537,250

Training started
X_train.shape: torch.Size([3017, 702])
Y_train.shape: torch.Size([3017, 7])
X_dev.shape: torch.Size([545, 323])
Y_dev.shape: torch.Size([545, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5706 |     68.528 |   1.9905 |     55.474 |     0.0
    2 |   1.7685 |     50.254 |   1.5526 |     45.963 |     0.1
    3 |   1.5059 |     46.100 |   1.4388 |     45.963 |     0.1
    4 |   1.4354 |     46.172 |   1.4080 |     45.994 |     0.1
    5 |   1.4075 |     46.172 |   1.3821 |     45.994 |     0.2
    6 |   1.3878 |     46.028 |   1.3631 |     45.994 |     0.2
    7 |   1.3679 |     45.829 |   1.3434 |     45.229 |     0.2
    8 |   1.3401 |     45.199 |   1.3170 |     45.719 |     0.3
    9 |   1.3203 |     45.233 |   1.3016 |     44.924 |     0.3
   10 |   1.2980 |     44.862 |   1.2840 |     45.107 |     0.3
   11 |   1.2845 |     44.818 |   1.2769 |     44.801 |     0.4
   12 |   1.2715 |     44.608 |   1.2652 |     43.731 |     0.4
   13 |   1.2607 |     43.890 |   1.2539 |     43.547 |     0.4
   14 |   1.2485 |     43.537 |   1.2437 |     42.813 |     0.5
   15 |   1.2382 |     43.277 |   1.2385 |     42.936 |     0.5
   16 |   1.2300 |     43.222 |   1.2319 |     42.446 |     0.5
   17 |   1.2209 |     42.824 |   1.2269 |     42.722 |     0.6
   18 |   1.2137 |     42.879 |   1.2249 |     42.783 |     0.6
   19 |   1.2044 |     42.647 |   1.2138 |     42.783 |     0.6
   20 |   1.1993 |     42.647 |   1.2108 |     42.844 |     0.7
   21 |   1.1915 |     42.454 |   1.2082 |     41.927 |     0.7
   22 |   1.1893 |     42.310 |   1.2022 |     42.263 |     0.7
   23 |   1.1811 |     42.089 |   1.1973 |     42.385 |     0.8
   24 |   1.1761 |     42.073 |   1.1993 |     42.141 |     0.8
   25 |   1.1690 |     41.824 |   1.1885 |     42.599 |     0.8
   26 |   1.1640 |     41.675 |   1.1926 |     42.232 |     0.9
   27 |   1.1595 |     41.758 |   1.1831 |     41.621 |     0.9
   28 |   1.1536 |     41.377 |   1.1794 |     41.437 |     0.9
   29 |   1.1479 |     40.946 |   1.1772 |     41.131 |     1.0
   30 |   1.1453 |     41.051 |   1.1708 |     41.193 |     1.0
   31 |   1.1399 |     40.857 |   1.1689 |     40.795 |     1.0
   32 |   1.1332 |     40.620 |   1.1611 |     40.275 |     1.1
   33 |   1.1316 |     40.576 |   1.1582 |     40.917 |     1.1
   34 |   1.1254 |     40.377 |   1.1627 |     40.489 |     1.1
   35 |   1.1220 |     40.432 |   1.1586 |     40.183 |     1.2
   36 |   1.1175 |     40.056 |   1.1451 |     40.245 |     1.2
   37 |   1.1157 |     39.880 |   1.1505 |     40.153 |     1.2
   38 |   1.1094 |     39.985 |   1.1457 |     40.765 |     1.3
   39 |   1.1060 |     39.813 |   1.1450 |     39.725 |     1.3
   40 |   1.1051 |     39.797 |   1.1382 |     39.755 |     1.3
   41 |   1.0978 |     39.272 |   1.1350 |     39.480 |     1.4
   42 |   1.0948 |     39.427 |   1.1392 |     39.174 |     1.4
   43 |   1.0903 |     39.156 |   1.1367 |     39.847 |     1.4
   44 |   1.0881 |     39.123 |   1.1238 |     39.297 |     1.5
   45 |   1.0845 |     39.161 |   1.1274 |     39.113 |     1.5
   46 |   1.0803 |     39.007 |   1.1282 |     39.572 |     1.5
   47 |   1.0803 |     38.891 |   1.1174 |     38.624 |     1.6
   48 |   1.0744 |     38.747 |   1.1188 |     38.716 |     1.6
   49 |   1.0678 |     38.653 |   1.1141 |     39.083 |     1.6
   50 |   1.0686 |     38.399 |   1.1115 |     38.440 |     1.7
   51 |   1.0649 |     38.609 |   1.1044 |     38.440 |     1.7
   52 |   1.0619 |     38.244 |   1.1008 |     38.104 |     1.7
   53 |   1.0555 |     38.173 |   1.1072 |     38.257 |     1.8
   54 |   1.0532 |     37.963 |   1.0929 |     38.104 |     1.8
   55 |   1.0477 |     37.482 |   1.0936 |     37.737 |     1.8
   56 |   1.0450 |     37.825 |   1.1008 |     38.349 |     1.9
   57 |   1.0397 |     37.465 |   1.0921 |     37.982 |     1.9
   58 |   1.0395 |     37.499 |   1.0971 |     38.043 |     1.9
   59 |   1.0354 |     37.366 |   1.0849 |     37.615 |     2.0
   60 |   1.0312 |     37.140 |   1.0920 |     38.287 |     2.0
   61 |   1.0277 |     37.046 |   1.0833 |     37.768 |     2.0
   62 |   1.0237 |     36.670 |   1.0723 |     36.850 |     2.1
   63 |   1.0239 |     36.908 |   1.0789 |     36.758 |     2.1
   64 |   1.0150 |     36.405 |   1.0710 |     37.217 |     2.1
   65 |   1.0128 |     36.223 |   1.0653 |     37.248 |     2.2
   66 |   1.0080 |     36.107 |   1.0761 |     36.575 |     2.2
   67 |   1.0055 |     36.118 |   1.0562 |     36.208 |     2.2
   68 |   1.0019 |     35.615 |   1.0713 |     36.514 |     2.3
   69 |   0.9988 |     35.852 |   1.0535 |     36.300 |     2.3
   70 |   0.9933 |     35.543 |   1.0507 |     35.933 |     2.3
   71 |   0.9886 |     35.234 |   1.0536 |     36.086 |     2.4
   72 |   0.9831 |     34.830 |   1.0486 |     35.810 |     2.4
   73 |   0.9808 |     35.029 |   1.0478 |     35.963 |     2.4
   74 |   0.9798 |     34.908 |   1.0425 |     35.443 |     2.5
   75 |   0.9694 |     34.405 |   1.0435 |     35.780 |     2.5
   76 |   0.9671 |     34.046 |   1.0403 |     35.015 |     2.5
   77 |   0.9634 |     34.063 |   1.0298 |     34.434 |     2.6
   78 |   0.9558 |     33.764 |   1.0263 |     34.557 |     2.6
   79 |   0.9523 |     33.427 |   1.0283 |     34.924 |     2.6
   80 |   0.9502 |     33.206 |   1.0267 |     34.190 |     2.7
   81 |   0.9421 |     33.002 |   1.0280 |     34.954 |     2.7
   82 |   0.9383 |     32.853 |   1.0229 |     35.015 |     2.7
   83 |   0.9343 |     32.405 |   1.0068 |     34.159 |     2.8
   84 |   0.9297 |     32.284 |   0.9995 |     34.067 |     2.8
   85 |   0.9276 |     32.052 |   1.0048 |     34.067 |     2.8
   86 |   0.9163 |     31.715 |   0.9982 |     33.670 |     2.9
   87 |   0.9096 |     31.378 |   0.9918 |     33.517 |     2.9
   88 |   0.9071 |     31.659 |   1.0005 |     34.220 |     2.9
   89 |   0.9017 |     31.173 |   1.0067 |     32.966 |     3.0
   90 |   0.8973 |     31.179 |   0.9796 |     33.578 |     3.0
   91 |   0.8908 |     30.881 |   0.9915 |     33.089 |     3.0
   92 |   0.8879 |     30.577 |   0.9914 |     32.997 |     3.1
   93 |   0.8885 |     30.742 |   1.0117 |     33.700 |     3.1
   94 |   0.9055 |     31.461 |   0.9741 |     33.303 |     3.1
   95 |   0.8786 |     30.301 |   0.9883 |     33.731 |     3.2
   96 |   0.8738 |     30.013 |   0.9740 |     32.508 |     3.2
   97 |   0.8593 |     29.632 |   0.9724 |     32.630 |     3.2
   98 |   0.8616 |     29.864 |   0.9713 |     32.477 |     3.3
   99 |   0.8575 |     29.345 |   0.9676 |     32.630 |     3.3
  100 |   0.8454 |     28.947 |   0.9737 |     32.661 |     3.3
  101 |   0.8474 |     28.991 |   1.0053 |     32.966 |     3.4
  102 |   0.8461 |     28.953 |   0.9743 |     32.783 |     3.4
  103 |   0.8309 |     28.345 |   0.9639 |     32.294 |     3.4
  104 |   0.8268 |     28.025 |   0.9687 |     32.141 |     3.5
  105 |   0.8238 |     28.102 |   0.9742 |     32.110 |     3.5
  106 |   0.8182 |     28.036 |   0.9668 |     32.385 |     3.5
  107 |   0.8179 |     27.936 |   0.9663 |     32.294 |     3.6
  108 |   0.8114 |     27.577 |   0.9634 |     31.927 |     3.6
  109 |   0.8042 |     27.422 |   0.9741 |     32.294 |     3.6
  110 |   0.7959 |     26.887 |   0.9647 |     32.049 |     3.7
  111 |   0.7988 |     26.809 |   0.9669 |     32.110 |     3.7
  112 |   0.7869 |     26.566 |   0.9737 |     32.355 |     3.7
  113 |   0.7836 |     26.555 |   0.9574 |     31.957 |     3.8
  114 |   0.7792 |     26.202 |   0.9776 |     31.651 |     3.8
  115 |   0.7748 |     26.345 |   0.9747 |     31.468 |     3.8
  116 |   0.7706 |     26.097 |   0.9525 |     32.110 |     3.9
  117 |   0.7669 |     26.003 |   0.9838 |     32.110 |     3.9
  118 |   0.7634 |     25.511 |   0.9590 |     32.110 |     3.9
  119 |   0.7581 |     25.671 |   0.9617 |     31.162 |     4.0
  120 |   0.7463 |     25.334 |   0.9522 |     31.070 |     4.0
  121 |   0.7401 |     24.920 |   0.9677 |     31.346 |     4.0
  122 |   0.7520 |     25.395 |   0.9554 |     31.774 |     4.1
  123 |   0.7357 |     24.782 |   0.9460 |     31.774 |     4.1
  124 |   0.7352 |     24.379 |   0.9526 |     31.468 |     4.1
  125 |   0.7257 |     24.362 |   0.9498 |     31.040 |     4.2
  126 |   0.7336 |     24.522 |   0.9490 |     31.407 |     4.2
  127 |   0.7234 |     24.345 |   0.9651 |     31.101 |     4.2
  128 |   0.7141 |     24.008 |   0.9443 |     31.131 |     4.3
  129 |   0.7057 |     23.456 |   0.9379 |     30.520 |     4.3
  130 |   0.7028 |     23.246 |   0.9318 |     30.581 |     4.3
  131 |   0.6931 |     23.080 |   0.9324 |     30.214 |     4.4
  132 |   0.7107 |     23.500 |   0.9624 |     30.887 |     4.4
  133 |   0.6885 |     23.080 |   0.9759 |     30.306 |     4.4
  134 |   0.6846 |     22.975 |   0.9456 |     30.612 |     4.5
Early stopping

