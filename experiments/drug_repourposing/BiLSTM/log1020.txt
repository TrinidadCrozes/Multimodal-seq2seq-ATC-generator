Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 766,497

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5013 |     67.372 |   1.9269 |     55.431 |     0.0
    2 |   1.6963 |     47.802 |   1.5329 |     47.083 |     0.1
    3 |   1.4666 |     45.840 |   1.4401 |     47.083 |     0.1
    4 |   1.4104 |     45.846 |   1.4006 |     47.083 |     0.1
    5 |   1.3790 |     45.818 |   1.3862 |     47.207 |     0.2
    6 |   1.3580 |     45.383 |   1.3695 |     46.834 |     0.2
    7 |   1.3400 |     45.543 |   1.3418 |     47.300 |     0.3
    8 |   1.3224 |     45.124 |   1.3213 |     46.524 |     0.3
    9 |   1.3122 |     45.152 |   1.3205 |     46.586 |     0.3
   10 |   1.3021 |     44.865 |   1.3064 |     46.493 |     0.4
   11 |   1.2924 |     44.375 |   1.2999 |     46.027 |     0.4
   12 |   1.2843 |     44.270 |   1.2844 |     45.903 |     0.4
   13 |   1.2711 |     43.912 |   1.2709 |     45.407 |     0.5
   14 |   1.2604 |     43.928 |   1.2695 |     45.313 |     0.5
   15 |   1.2501 |     43.614 |   1.2494 |     44.910 |     0.6
   16 |   1.2381 |     43.262 |   1.2382 |     44.848 |     0.6
   17 |   1.2300 |     42.915 |   1.2270 |     44.413 |     0.6
   18 |   1.2229 |     42.727 |   1.2285 |     43.637 |     0.7
   19 |   1.2180 |     42.661 |   1.2116 |     43.762 |     0.7
   20 |   1.2073 |     42.501 |   1.2089 |     43.855 |     0.7
   21 |   1.2006 |     42.485 |   1.2035 |     43.700 |     0.8
   22 |   1.1924 |     42.545 |   1.2005 |     42.955 |     0.8
   23 |   1.1850 |     41.840 |   1.1935 |     42.551 |     0.9
   24 |   1.1760 |     41.499 |   1.1806 |     42.800 |     0.9
   25 |   1.1703 |     41.377 |   1.1819 |     42.489 |     0.9
   26 |   1.1622 |     41.036 |   1.1771 |     42.179 |     1.0
   27 |   1.1564 |     40.700 |   1.1704 |     42.024 |     1.0
   28 |   1.1489 |     40.606 |   1.1603 |     41.372 |     1.1
   29 |   1.1438 |     40.672 |   1.1530 |     41.372 |     1.1
   30 |   1.1326 |     40.160 |   1.1489 |     40.968 |     1.1
   31 |   1.1276 |     39.934 |   1.1440 |     40.658 |     1.2
   32 |   1.1261 |     39.967 |   1.1398 |     40.503 |     1.2
   33 |   1.1137 |     39.603 |   1.1336 |     41.248 |     1.2
   34 |   1.1103 |     39.603 |   1.1365 |     40.720 |     1.3
   35 |   1.1040 |     39.444 |   1.1293 |     40.317 |     1.3
   36 |   1.0990 |     39.322 |   1.1245 |     40.844 |     1.4
   37 |   1.0915 |     38.964 |   1.1198 |     40.223 |     1.4
   38 |   1.0868 |     38.760 |   1.1123 |     40.161 |     1.4
   39 |   1.0818 |     38.534 |   1.1124 |     40.286 |     1.5
   40 |   1.0764 |     38.540 |   1.1054 |     39.634 |     1.5
   41 |   1.0713 |     38.121 |   1.1028 |     39.603 |     1.6
   42 |   1.0639 |     38.094 |   1.1053 |     39.448 |     1.6
   43 |   1.0579 |     37.736 |   1.1047 |     40.223 |     1.6
   44 |   1.0551 |     37.785 |   1.0969 |     39.572 |     1.7
   45 |   1.0480 |     37.548 |   1.1006 |     39.758 |     1.7
   46 |   1.0458 |     37.322 |   1.0917 |     39.789 |     1.7
   47 |   1.0354 |     36.672 |   1.0879 |     39.665 |     1.8
   48 |   1.0306 |     36.661 |   1.0857 |     38.299 |     1.8
   49 |   1.0220 |     36.292 |   1.0723 |     38.579 |     1.9
   50 |   1.0211 |     36.264 |   1.0773 |     38.765 |     1.9
   51 |   1.0190 |     36.099 |   1.0801 |     38.361 |     1.9
   52 |   1.0121 |     35.802 |   1.0846 |     38.827 |     2.0
   53 |   1.0067 |     35.669 |   1.0657 |     38.144 |     2.0
   54 |   1.0003 |     35.488 |   1.0642 |     37.554 |     2.1
   55 |   0.9913 |     35.091 |   1.0606 |     37.834 |     2.1
   56 |   0.9861 |     35.091 |   1.0575 |     37.089 |     2.1
   57 |   0.9804 |     34.523 |   1.0624 |     38.020 |     2.2
   58 |   0.9814 |     35.025 |   1.0520 |     37.337 |     2.2
   59 |   0.9712 |     34.259 |   1.0587 |     37.865 |     2.3
   60 |   0.9681 |     34.050 |   1.0715 |     38.082 |     2.3
   61 |   0.9656 |     34.138 |   1.0531 |     36.623 |     2.3
   62 |   0.9550 |     33.697 |   1.0479 |     36.778 |     2.4
   63 |   0.9522 |     33.521 |   1.0481 |     37.306 |     2.4
   64 |   0.9462 |     33.107 |   1.0426 |     36.623 |     2.4
   65 |   0.9335 |     32.661 |   1.0343 |     36.220 |     2.5
   66 |   0.9305 |     32.529 |   1.0395 |     36.996 |     2.5
   67 |   0.9289 |     32.518 |   1.0433 |     35.878 |     2.6
   68 |   0.9234 |     32.402 |   1.0402 |     35.785 |     2.6
   69 |   0.9160 |     32.160 |   1.0235 |     35.723 |     2.6
   70 |   0.9040 |     31.449 |   1.0304 |     35.599 |     2.7
   71 |   0.8988 |     31.229 |   1.0241 |     35.568 |     2.7
   72 |   0.8910 |     31.047 |   1.0354 |     35.537 |     2.8
   73 |   0.8843 |     30.766 |   1.0315 |     35.661 |     2.8
   74 |   0.8768 |     30.149 |   1.0232 |     34.885 |     2.8
   75 |   0.8685 |     29.791 |   1.0186 |     34.730 |     2.9
   76 |   0.8602 |     29.609 |   1.0207 |     34.606 |     2.9
   77 |   0.8546 |     29.510 |   1.0244 |     35.320 |     3.0
   78 |   0.8405 |     28.540 |   1.0143 |     34.047 |     3.0
   79 |   0.8294 |     28.138 |   1.0172 |     33.364 |     3.0
   80 |   0.8164 |     27.322 |   1.0154 |     34.327 |     3.1
   81 |   0.8092 |     27.311 |   1.0049 |     33.302 |     3.1
   82 |   0.8013 |     26.909 |   0.9976 |     33.116 |     3.1
   83 |   0.7824 |     25.983 |   0.9952 |     32.651 |     3.2
   84 |   0.7722 |     25.879 |   1.0129 |     34.047 |     3.2
   85 |   0.7659 |     25.377 |   1.0079 |     33.178 |     3.3
   86 |   0.7522 |     24.997 |   0.9931 |     32.806 |     3.3
   87 |   0.7462 |     24.926 |   0.9885 |     32.030 |     3.3
   88 |   0.7283 |     23.769 |   0.9981 |     32.464 |     3.4
   89 |   0.7151 |     23.763 |   0.9898 |     31.906 |     3.4
   90 |   0.7013 |     22.937 |   0.9947 |     31.999 |     3.5
   91 |   0.6926 |     22.672 |   1.0033 |     31.595 |     3.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 591,329

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4152 |     64.358 |   1.8260 |     49.441 |     0.0
    2 |   1.6438 |     47.934 |   1.5101 |     47.083 |     0.1
    3 |   1.4541 |     45.862 |   1.4204 |     47.083 |     0.1
    4 |   1.3919 |     45.846 |   1.3773 |     46.958 |     0.1
    5 |   1.3578 |     45.713 |   1.3459 |     47.052 |     0.2
    6 |   1.3331 |     45.636 |   1.3314 |     46.927 |     0.2
    7 |   1.3144 |     45.096 |   1.3041 |     46.493 |     0.2
    8 |   1.2933 |     44.380 |   1.2882 |     45.624 |     0.3
    9 |   1.2718 |     43.879 |   1.2721 |     45.065 |     0.3
   10 |   1.2537 |     43.851 |   1.2455 |     44.724 |     0.4
   11 |   1.2362 |     43.333 |   1.2314 |     44.755 |     0.4
   12 |   1.2195 |     43.003 |   1.2169 |     44.196 |     0.4
   13 |   1.2075 |     42.782 |   1.2108 |     43.917 |     0.5
   14 |   1.1979 |     42.545 |   1.1978 |     43.731 |     0.5
   15 |   1.1872 |     41.945 |   1.1980 |     43.420 |     0.5
   16 |   1.1793 |     41.631 |   1.1795 |     41.620 |     0.6
   17 |   1.1712 |     41.482 |   1.1754 |     42.148 |     0.6
   18 |   1.1621 |     41.074 |   1.1671 |     41.527 |     0.6
   19 |   1.1547 |     40.848 |   1.1608 |     41.962 |     0.7
   20 |   1.1456 |     40.281 |   1.1547 |     41.279 |     0.7
   21 |   1.1388 |     40.220 |   1.1511 |     41.217 |     0.7
   22 |   1.1346 |     40.138 |   1.1543 |     41.155 |     0.8
   23 |   1.1290 |     40.154 |   1.1415 |     41.155 |     0.8
   24 |   1.1191 |     39.763 |   1.1355 |     40.968 |     0.9
   25 |   1.1133 |     39.510 |   1.1236 |     40.099 |     0.9
   26 |   1.1077 |     39.278 |   1.1251 |     40.441 |     0.9
   27 |   1.1006 |     39.019 |   1.1168 |     40.286 |     1.0
   28 |   1.0950 |     38.926 |   1.1229 |     40.223 |     1.0
   29 |   1.0854 |     38.579 |   1.1139 |     39.882 |     1.0
   30 |   1.0832 |     38.551 |   1.1020 |     39.448 |     1.1
   31 |   1.0755 |     38.160 |   1.1078 |     39.820 |     1.1
   32 |   1.0728 |     38.088 |   1.1011 |     39.448 |     1.1
   33 |   1.0667 |     37.846 |   1.0903 |     38.734 |     1.2
   34 |   1.0591 |     37.570 |   1.0931 |     39.044 |     1.2
   35 |   1.0544 |     37.565 |   1.0896 |     38.392 |     1.2
   36 |   1.0533 |     37.229 |   1.0874 |     38.299 |     1.3
   37 |   1.0478 |     37.433 |   1.0861 |     38.361 |     1.3
   38 |   1.0408 |     37.152 |   1.0825 |     38.206 |     1.4
   39 |   1.0323 |     36.259 |   1.0785 |     37.616 |     1.4
   40 |   1.0298 |     36.050 |   1.0787 |     38.175 |     1.4
   41 |   1.0241 |     36.215 |   1.0655 |     37.865 |     1.5
   42 |   1.0192 |     36.022 |   1.0676 |     37.834 |     1.5
   43 |   1.0128 |     35.554 |   1.0622 |     37.368 |     1.5
   44 |   1.0041 |     35.614 |   1.0629 |     37.027 |     1.6
   45 |   1.0037 |     35.477 |   1.0641 |     37.120 |     1.6
   46 |   0.9973 |     35.488 |   1.0635 |     37.834 |     1.6
   47 |   0.9930 |     35.140 |   1.0510 |     36.903 |     1.7
   48 |   0.9913 |     34.904 |   1.0513 |     36.592 |     1.7
   49 |   0.9786 |     34.490 |   1.0452 |     36.437 |     1.7
   50 |   0.9798 |     34.490 |   1.0480 |     36.530 |     1.8
   51 |   0.9688 |     34.033 |   1.0414 |     36.437 |     1.8
   52 |   0.9646 |     33.890 |   1.0486 |     36.282 |     1.9
   53 |   0.9593 |     33.311 |   1.0413 |     36.189 |     1.9
   54 |   0.9581 |     33.515 |   1.0338 |     35.940 |     1.9
   55 |   0.9504 |     33.118 |   1.0342 |     35.040 |     2.0
   56 |   0.9435 |     32.777 |   1.0280 |     35.320 |     2.0
   57 |   0.9377 |     32.694 |   1.0354 |     35.506 |     2.0
   58 |   0.9335 |     32.270 |   1.0227 |     34.823 |     2.1
   59 |   0.9251 |     32.094 |   1.0301 |     35.878 |     2.1
   60 |   0.9273 |     32.110 |   1.0244 |     35.258 |     2.1
   61 |   0.9097 |     31.449 |   1.0265 |     35.009 |     2.2
   62 |   0.9045 |     31.080 |   1.0283 |     34.823 |     2.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 558,049

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2913 |     58.259 |   1.7129 |     49.348 |     0.0
    2 |   1.5421 |     46.287 |   1.4458 |     46.958 |     0.1
    3 |   1.3919 |     45.163 |   1.3626 |     45.996 |     0.1
    4 |   1.3327 |     44.408 |   1.3133 |     45.220 |     0.1
    5 |   1.2955 |     43.658 |   1.2855 |     44.786 |     0.2
    6 |   1.2668 |     43.311 |   1.2609 |     44.693 |     0.2
    7 |   1.2399 |     42.678 |   1.2332 |     43.731 |     0.2
    8 |   1.2155 |     41.972 |   1.2082 |     42.613 |     0.3
    9 |   1.1879 |     41.410 |   1.1902 |     41.868 |     0.3
   10 |   1.1606 |     40.242 |   1.1662 |     40.844 |     0.3
   11 |   1.1346 |     39.085 |   1.1450 |     40.441 |     0.4
   12 |   1.1073 |     38.419 |   1.1221 |     38.610 |     0.4
   13 |   1.0747 |     36.904 |   1.1084 |     38.392 |     0.4
   14 |   1.0412 |     35.758 |   1.0861 |     37.058 |     0.5
   15 |   1.0139 |     34.595 |   1.0613 |     35.444 |     0.5
   16 |   0.9793 |     33.107 |   1.0410 |     34.854 |     0.6
   17 |   0.9442 |     31.923 |   1.0274 |     34.389 |     0.6
   18 |   0.9114 |     30.077 |   1.0094 |     33.054 |     0.6
   19 |   0.8782 |     28.738 |   0.9919 |     32.651 |     0.7
   20 |   0.8422 |     27.405 |   0.9815 |     32.092 |     0.7
   21 |   0.8080 |     25.846 |   0.9732 |     30.726 |     0.7
   22 |   0.7747 |     24.562 |   0.9638 |     30.912 |     0.8
   23 |   0.7430 |     23.416 |   0.9598 |     30.788 |     0.8
   24 |   0.7099 |     22.088 |   0.9432 |     30.106 |     0.8
   25 |   0.6761 |     20.970 |   0.9410 |     29.981 |     0.9
   26 |   0.6451 |     19.972 |   0.9519 |     29.888 |     0.9
   27 |   0.6186 |     19.185 |   0.9521 |     29.547 |     0.9
   28 |   0.5855 |     18.055 |   0.9383 |     29.019 |     1.0
   29 |   0.5543 |     16.848 |   0.9550 |     28.305 |     1.0
   30 |   0.5268 |     15.840 |   0.9616 |     28.212 |     1.0
   31 |   0.5069 |     15.614 |   0.9636 |     28.957 |     1.1
   32 |   0.4836 |     14.722 |   0.9628 |     28.430 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 537,761

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4518 |     65.328 |   1.8937 |     49.690 |     0.0
    2 |   1.6843 |     48.099 |   1.5352 |     47.083 |     0.1
    3 |   1.4707 |     45.846 |   1.4341 |     47.083 |     0.1
    4 |   1.4100 |     45.906 |   1.3998 |     47.020 |     0.1
    5 |   1.3729 |     45.802 |   1.3725 |     47.207 |     0.2
    6 |   1.3452 |     44.749 |   1.3447 |     46.369 |     0.2
    7 |   1.3258 |     44.358 |   1.3263 |     46.307 |     0.2
    8 |   1.3105 |     44.485 |   1.3126 |     45.965 |     0.3
    9 |   1.2968 |     44.413 |   1.2935 |     45.624 |     0.3
   10 |   1.2811 |     44.055 |   1.2839 |     45.189 |     0.3
   11 |   1.2668 |     43.713 |   1.2659 |     44.910 |     0.3
   12 |   1.2476 |     43.300 |   1.2528 |     44.538 |     0.4
   13 |   1.2282 |     43.063 |   1.2322 |     44.475 |     0.4
   14 |   1.2089 |     42.650 |   1.2125 |     43.048 |     0.4
   15 |   1.1891 |     41.532 |   1.1977 |     42.737 |     0.5
   16 |   1.1700 |     40.959 |   1.1860 |     42.086 |     0.5
   17 |   1.1536 |     40.386 |   1.1795 |     42.024 |     0.5
   18 |   1.1369 |     40.033 |   1.1588 |     41.372 |     0.6
   19 |   1.1249 |     39.019 |   1.1448 |     40.999 |     0.6
   20 |   1.1046 |     38.419 |   1.1335 |     40.565 |     0.6
   21 |   1.0911 |     37.895 |   1.1148 |     39.479 |     0.7
   22 |   1.0738 |     37.267 |   1.1137 |     39.417 |     0.7
   23 |   1.0581 |     36.579 |   1.1062 |     38.610 |     0.7
   24 |   1.0428 |     36.110 |   1.0969 |     38.485 |     0.8
   25 |   1.0245 |     35.421 |   1.0849 |     37.709 |     0.8
   26 |   1.0103 |     34.556 |   1.0736 |     37.058 |     0.8
   27 |   0.9925 |     34.193 |   1.0779 |     36.965 |     0.9
   28 |   0.9717 |     33.041 |   1.0716 |     36.654 |     0.9
   29 |   0.9539 |     32.187 |   1.0510 |     35.785 |     0.9
   30 |   0.9326 |     31.433 |   1.0522 |     35.909 |     0.9
   31 |   0.9155 |     31.025 |   1.0451 |     35.133 |     1.0
   32 |   0.8913 |     29.901 |   1.0292 |     33.799 |     1.0
   33 |   0.8632 |     28.832 |   1.0236 |     33.768 |     1.0
   34 |   0.8386 |     27.625 |   1.0229 |     33.520 |     1.1
   35 |   0.8107 |     26.452 |   1.0093 |     32.495 |     1.1
   36 |   0.7938 |     25.928 |   1.0062 |     32.557 |     1.1
   37 |   0.7670 |     25.096 |   1.0091 |     32.092 |     1.2
   38 |   0.7432 |     24.154 |   1.0087 |     31.533 |     1.2
   39 |   0.7246 |     23.791 |   1.0195 |     31.719 |     1.2
   40 |   0.6968 |     22.518 |   1.0047 |     31.192 |     1.3
   41 |   0.6805 |     21.928 |   1.0086 |     30.602 |     1.3
   42 |   0.6509 |     21.058 |   1.0090 |     30.074 |     1.3
   43 |   0.6319 |     20.187 |   1.0174 |     30.726 |     1.4
   44 |   0.6049 |     19.433 |   1.0310 |     30.633 |     1.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,263,009

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5688 |     68.672 |   2.0393 |     59.404 |     0.1
    2 |   1.7986 |     50.865 |   1.6094 |     47.083 |     0.1
    3 |   1.5120 |     45.868 |   1.4670 |     47.083 |     0.2
    4 |   1.4350 |     45.978 |   1.4261 |     47.114 |     0.2
    5 |   1.4055 |     45.857 |   1.3968 |     47.114 |     0.3
    6 |   1.3793 |     45.813 |   1.3680 |     47.052 |     0.4
    7 |   1.3480 |     44.981 |   1.3437 |     46.120 |     0.4
    8 |   1.3275 |     44.898 |   1.3282 |     46.214 |     0.5
    9 |   1.3079 |     44.424 |   1.3070 |     45.345 |     0.5
   10 |   1.2936 |     44.143 |   1.2988 |     45.562 |     0.6
   11 |   1.2778 |     43.906 |   1.2800 |     46.089 |     0.7
   12 |   1.2658 |     43.741 |   1.2674 |     44.972 |     0.7
   13 |   1.2534 |     43.807 |   1.2534 |     45.562 |     0.8
   14 |   1.2421 |     44.000 |   1.2491 |     44.941 |     0.8
   15 |   1.2318 |     43.697 |   1.2390 |     44.972 |     0.9
   16 |   1.2244 |     43.752 |   1.2303 |     44.786 |     1.0
   17 |   1.2153 |     43.543 |   1.2200 |     44.351 |     1.0
   18 |   1.2061 |     43.311 |   1.2119 |     44.227 |     1.1
   19 |   1.1998 |     42.931 |   1.2083 |     44.227 |     1.2
   20 |   1.1928 |     42.661 |   1.2101 |     43.420 |     1.2
   21 |   1.1872 |     42.479 |   1.1966 |     43.048 |     1.3
   22 |   1.1794 |     42.143 |   1.1887 |     43.172 |     1.3
   23 |   1.1737 |     41.989 |   1.1842 |     42.893 |     1.4
   24 |   1.1689 |     41.620 |   1.1803 |     43.203 |     1.5
   25 |   1.1621 |     41.317 |   1.1756 |     42.737 |     1.5
   26 |   1.1553 |     41.124 |   1.1685 |     42.179 |     1.6
   27 |   1.1508 |     41.168 |   1.1581 |     41.124 |     1.6
   28 |   1.1458 |     40.843 |   1.1643 |     42.551 |     1.7
   29 |   1.1411 |     40.920 |   1.1568 |     41.713 |     1.8
   30 |   1.1348 |     40.584 |   1.1443 |     40.906 |     1.8
   31 |   1.1306 |     40.336 |   1.1427 |     41.092 |     1.9
   32 |   1.1224 |     40.132 |   1.1413 |     41.092 |     1.9
   33 |   1.1206 |     40.022 |   1.1311 |     40.627 |     2.0
   34 |   1.1140 |     39.967 |   1.1343 |     40.937 |     2.1
   35 |   1.1099 |     39.807 |   1.1265 |     40.161 |     2.1
   36 |   1.1071 |     39.488 |   1.1213 |     40.472 |     2.2
   37 |   1.1007 |     39.328 |   1.1224 |     40.565 |     2.2
   38 |   1.0958 |     39.504 |   1.1181 |     40.472 |     2.3
   39 |   1.0920 |     39.526 |   1.1108 |     39.882 |     2.4
   40 |   1.0856 |     39.052 |   1.1094 |     40.192 |     2.4
   41 |   1.0798 |     39.107 |   1.1039 |     39.572 |     2.5
   42 |   1.0768 |     38.898 |   1.1021 |     39.944 |     2.5
   43 |   1.0710 |     38.534 |   1.0987 |     39.820 |     2.6
   44 |   1.0679 |     38.551 |   1.0914 |     39.323 |     2.7
   45 |   1.0642 |     38.298 |   1.0888 |     38.982 |     2.7
   46 |   1.0572 |     38.072 |   1.0887 |     39.075 |     2.8
   47 |   1.0530 |     37.835 |   1.0800 |     38.827 |     2.8
   48 |   1.0490 |     37.774 |   1.0810 |     38.206 |     2.9
   49 |   1.0446 |     37.835 |   1.0793 |     38.516 |     3.0
   50 |   1.0423 |     37.366 |   1.0748 |     38.889 |     3.0
   51 |   1.0340 |     37.174 |   1.0678 |     38.454 |     3.1
   52 |   1.0293 |     37.025 |   1.0598 |     37.523 |     3.1
   53 |   1.0250 |     36.705 |   1.0602 |     37.896 |     3.2
   54 |   1.0340 |     37.085 |   1.0628 |     38.299 |     3.3
   55 |   1.0201 |     36.722 |   1.0525 |     37.834 |     3.3
   56 |   1.0138 |     36.446 |   1.0670 |     37.865 |     3.4
   57 |   1.0143 |     36.584 |   1.0553 |     38.237 |     3.5
   58 |   1.0073 |     36.061 |   1.0505 |     37.616 |     3.5
   59 |   1.0065 |     36.061 |   1.0505 |     37.120 |     3.6
   60 |   1.0030 |     36.452 |   1.0520 |     37.027 |     3.6
   61 |   0.9951 |     35.532 |   1.0570 |     36.934 |     3.7
   62 |   0.9923 |     35.774 |   1.0447 |     37.399 |     3.8
   63 |   0.9884 |     35.444 |   1.0473 |     36.778 |     3.8
   64 |   0.9862 |     35.251 |   1.0405 |     36.903 |     3.9
   65 |   0.9796 |     34.942 |   1.0388 |     36.561 |     3.9
   66 |   0.9733 |     34.689 |   1.0369 |     36.716 |     4.0
   67 |   0.9711 |     34.843 |   1.0439 |     36.778 |     4.1
   68 |   0.9717 |     34.893 |   1.0356 |     35.754 |     4.1
   69 |   0.9638 |     34.606 |   1.0347 |     36.437 |     4.2
   70 |   0.9584 |     33.983 |   1.0405 |     36.561 |     4.2
   71 |   0.9578 |     34.364 |   1.0243 |     36.220 |     4.3
   72 |   0.9496 |     33.669 |   1.0239 |     36.437 |     4.4
   73 |   0.9468 |     33.835 |   1.0159 |     35.940 |     4.4
   74 |   0.9427 |     33.466 |   1.0156 |     36.251 |     4.5
   75 |   0.9373 |     33.355 |   1.0190 |     35.475 |     4.5
   76 |   0.9341 |     33.388 |   1.0196 |     35.258 |     4.6
   77 |   0.9292 |     32.948 |   1.0161 |     35.351 |     4.7
   78 |   0.9220 |     32.501 |   1.0099 |     34.885 |     4.7
   79 |   0.9192 |     32.518 |   1.0210 |     36.065 |     4.8
   80 |   0.9246 |     32.854 |   1.0108 |     34.544 |     4.9
   81 |   0.9124 |     32.336 |   0.9923 |     34.171 |     4.9
   82 |   0.9048 |     31.609 |   1.0019 |     33.923 |     5.0
   83 |   0.9002 |     31.890 |   0.9992 |     33.861 |     5.0
   84 |   0.8984 |     31.813 |   0.9923 |     33.675 |     5.1
   85 |   0.8922 |     31.421 |   0.9964 |     33.395 |     5.2
   86 |   0.8868 |     31.377 |   0.9966 |     33.768 |     5.2
   87 |   0.8848 |     30.975 |   0.9971 |     33.892 |     5.3
   88 |   0.8797 |     31.152 |   0.9862 |     33.768 |     5.4
   89 |   0.8745 |     30.507 |   0.9892 |     33.768 |     5.4
   90 |   0.8631 |     30.187 |   0.9883 |     33.426 |     5.5
   91 |   0.8669 |     30.358 |   0.9884 |     33.985 |     5.5
   92 |   0.8572 |     29.857 |   0.9825 |     33.457 |     5.6
   93 |   0.8524 |     29.609 |   0.9888 |     33.861 |     5.7
   94 |   0.8459 |     29.234 |   0.9759 |     32.495 |     5.7
   95 |   0.8408 |     28.997 |   0.9743 |     33.675 |     5.8
   96 |   0.8373 |     29.212 |   0.9855 |     33.302 |     5.8
   97 |   0.8324 |     29.085 |   0.9778 |     32.682 |     5.9
   98 |   0.8250 |     29.003 |   0.9825 |     32.899 |     6.0
   99 |   0.8198 |     28.336 |   0.9911 |     33.023 |     6.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,248,225

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3779 |     63.570 |   1.8056 |     49.441 |     0.1
    2 |   1.5900 |     46.959 |   1.4459 |     47.083 |     0.1
    3 |   1.4079 |     45.664 |   1.3732 |     46.772 |     0.2
    4 |   1.3531 |     45.477 |   1.3253 |     46.058 |     0.2
    5 |   1.3146 |     44.645 |   1.2994 |     45.593 |     0.3
    6 |   1.2906 |     44.149 |   1.2808 |     45.717 |     0.4
    7 |   1.2717 |     43.581 |   1.2577 |     44.724 |     0.4
    8 |   1.2508 |     43.372 |   1.2418 |     44.507 |     0.5
    9 |   1.2329 |     43.069 |   1.2196 |     44.475 |     0.5
   10 |   1.2164 |     42.738 |   1.2072 |     44.196 |     0.6
   11 |   1.2025 |     42.386 |   1.2015 |     43.451 |     0.7
   12 |   1.1851 |     41.791 |   1.1791 |     41.682 |     0.7
   13 |   1.1722 |     41.174 |   1.1667 |     41.868 |     0.8
   14 |   1.1566 |     40.419 |   1.1544 |     40.751 |     0.9
   15 |   1.1422 |     40.138 |   1.1446 |     41.092 |     0.9
   16 |   1.1293 |     39.410 |   1.1339 |     40.379 |     1.0
   17 |   1.1193 |     39.052 |   1.1260 |     40.068 |     1.0
   18 |   1.1050 |     38.788 |   1.1091 |     39.541 |     1.1
   19 |   1.0907 |     38.496 |   1.1033 |     39.261 |     1.2
   20 |   1.0825 |     37.950 |   1.0987 |     39.044 |     1.2
   21 |   1.0710 |     37.598 |   1.0879 |     38.361 |     1.3
   22 |   1.0586 |     37.041 |   1.0790 |     37.709 |     1.3
   23 |   1.0549 |     36.749 |   1.0722 |     37.337 |     1.4
   24 |   1.0419 |     36.325 |   1.0607 |     36.840 |     1.5
   25 |   1.0263 |     35.824 |   1.0516 |     36.654 |     1.5
   26 |   1.0160 |     35.471 |   1.0470 |     36.251 |     1.6
   27 |   1.0026 |     34.562 |   1.0340 |     36.468 |     1.6
   28 |   0.9948 |     34.766 |   1.0388 |     36.158 |     1.7
   29 |   0.9809 |     33.983 |   1.0301 |     35.599 |     1.8
   30 |   0.9697 |     33.691 |   1.0140 |     34.792 |     1.8
   31 |   0.9577 |     33.328 |   1.0171 |     34.916 |     1.9
   32 |   0.9462 |     32.485 |   0.9999 |     34.854 |     1.9
   33 |   0.9296 |     32.072 |   0.9990 |     34.264 |     2.0
   34 |   0.9171 |     31.835 |   0.9964 |     33.706 |     2.1
   35 |   0.9096 |     31.168 |   0.9833 |     33.489 |     2.1
   36 |   0.8933 |     30.253 |   0.9818 |     32.775 |     2.2
   37 |   0.8755 |     29.719 |   0.9665 |     32.961 |     2.3
   38 |   0.8581 |     29.102 |   0.9668 |     32.744 |     2.3
   39 |   0.8495 |     28.650 |   0.9546 |     32.092 |     2.4
   40 |   0.8312 |     28.242 |   0.9389 |     31.440 |     2.4
   41 |   0.8163 |     27.504 |   0.9403 |     31.533 |     2.5
   42 |   0.7989 |     27.096 |   0.9389 |     31.750 |     2.6
   43 |   0.7842 |     26.275 |   0.9390 |     31.192 |     2.6
   44 |   0.7751 |     25.829 |   0.9229 |     30.540 |     2.7
   45 |   0.7595 |     25.388 |   0.9132 |     29.640 |     2.7
   46 |   0.7402 |     24.507 |   0.9201 |     30.261 |     2.8
   47 |   0.7261 |     23.967 |   0.9087 |     29.423 |     2.9
   48 |   0.7032 |     23.063 |   0.9140 |     29.578 |     2.9
   49 |   0.6922 |     22.518 |   0.9016 |     29.361 |     3.0
   50 |   0.6666 |     21.994 |   0.9210 |     28.771 |     3.0
   51 |   0.6607 |     21.636 |   0.8958 |     28.088 |     3.1
   52 |   0.6533 |     21.405 |   0.9137 |     28.709 |     3.2
   53 |   0.6208 |     20.259 |   0.9042 |     28.336 |     3.2
   54 |   0.6228 |     20.353 |   0.9023 |     27.933 |     3.3
   55 |   0.6007 |     19.322 |   0.9112 |     27.995 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,691,041

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5086 |     66.342 |   1.9814 |     55.680 |     0.1
    2 |   1.7471 |     49.041 |   1.5657 |     47.114 |     0.2
    3 |   1.4912 |     45.846 |   1.4491 |     47.114 |     0.2
    4 |   1.4189 |     45.879 |   1.4049 |     46.989 |     0.3
    5 |   1.3873 |     45.565 |   1.3838 |     46.927 |     0.4
    6 |   1.3648 |     45.212 |   1.3589 |     46.338 |     0.5
    7 |   1.3402 |     44.424 |   1.3382 |     45.779 |     0.5
    8 |   1.3181 |     44.270 |   1.3188 |     45.407 |     0.6
    9 |   1.3007 |     43.846 |   1.3075 |     45.748 |     0.7
   10 |   1.2832 |     43.736 |   1.2875 |     46.027 |     0.8
   11 |   1.2680 |     43.686 |   1.2667 |     45.158 |     0.8
   12 |   1.2522 |     43.581 |   1.2536 |     45.407 |     0.9
   13 |   1.2391 |     43.168 |   1.2417 |     45.003 |     1.0
   14 |   1.2270 |     42.843 |   1.2362 |     44.165 |     1.1
   15 |   1.2163 |     42.435 |   1.2224 |     43.110 |     1.1
   16 |   1.2017 |     42.116 |   1.2166 |     43.234 |     1.2
   17 |   1.1919 |     41.769 |   1.2042 |     42.737 |     1.3
   18 |   1.1799 |     41.421 |   1.1915 |     42.365 |     1.4
   19 |   1.1700 |     41.218 |   1.1822 |     42.117 |     1.4
   20 |   1.1588 |     41.107 |   1.1765 |     41.806 |     1.5
   21 |   1.1499 |     40.452 |   1.1712 |     41.682 |     1.6
   22 |   1.1412 |     40.215 |   1.1648 |     41.465 |     1.7
   23 |   1.1320 |     39.994 |   1.1579 |     41.092 |     1.7
   24 |   1.1238 |     39.565 |   1.1494 |     40.006 |     1.8
   25 |   1.1162 |     39.218 |   1.1531 |     40.317 |     1.9
   26 |   1.1076 |     38.953 |   1.1400 |     40.317 |     2.0
   27 |   1.0966 |     38.590 |   1.1353 |     39.913 |     2.0
   28 |   1.0914 |     38.215 |   1.1258 |     39.758 |     2.1
   29 |   1.0822 |     38.061 |   1.1278 |     39.758 |     2.2
   30 |   1.0743 |     37.934 |   1.1262 |     40.223 |     2.3
   31 |   1.0641 |     37.361 |   1.1205 |     39.075 |     2.3
   32 |   1.0568 |     37.267 |   1.1172 |     38.516 |     2.4
   33 |   1.0489 |     36.937 |   1.1117 |     38.920 |     2.5
   34 |   1.0399 |     36.683 |   1.1031 |     38.113 |     2.6
   35 |   1.0339 |     36.507 |   1.0981 |     38.330 |     2.6
   36 |   1.0259 |     36.055 |   1.0858 |     37.834 |     2.7
   37 |   1.0156 |     35.592 |   1.0821 |     36.778 |     2.8
   38 |   1.0062 |     35.146 |   1.0903 |     37.896 |     2.9
   39 |   0.9957 |     34.672 |   1.0761 |     36.375 |     2.9
   40 |   0.9916 |     34.314 |   1.0696 |     36.561 |     3.0
   41 |   0.9777 |     33.978 |   1.0660 |     36.220 |     3.1
   42 |   0.9644 |     33.405 |   1.0608 |     36.623 |     3.2
   43 |   0.9554 |     32.920 |   1.0554 |     35.630 |     3.3
   44 |   0.9423 |     32.623 |   1.0519 |     35.413 |     3.3
   45 |   0.9324 |     31.917 |   1.0481 |     35.227 |     3.4
   46 |   0.9176 |     31.113 |   1.0319 |     34.792 |     3.5
   47 |   0.9094 |     30.606 |   1.0356 |     34.420 |     3.6
   48 |   0.8971 |     30.452 |   1.0334 |     34.047 |     3.6
   49 |   0.8847 |     30.220 |   1.0252 |     34.109 |     3.7
   50 |   0.8725 |     29.499 |   1.0255 |     32.992 |     3.8
   51 |   0.8568 |     28.821 |   1.0132 |     33.457 |     3.9
   52 |   0.8482 |     28.353 |   1.0189 |     33.364 |     3.9
   53 |   0.8351 |     27.879 |   1.0253 |     33.644 |     4.0
   54 |   0.8272 |     27.620 |   1.0179 |     33.457 |     4.1
   55 |   0.8123 |     27.284 |   1.0168 |     33.178 |     4.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 421,409

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5681 |     69.840 |   2.0092 |     55.121 |     0.2
    2 |   1.7678 |     49.873 |   1.5595 |     47.083 |     0.3
    3 |   1.4851 |     45.901 |   1.4468 |     47.083 |     0.5
    4 |   1.4147 |     45.791 |   1.4062 |     47.083 |     0.7
    5 |   1.3890 |     45.702 |   1.3815 |     46.989 |     0.8
    6 |   1.3701 |     45.653 |   1.3652 |     46.989 |     1.0
    7 |   1.3490 |     45.466 |   1.3408 |     46.493 |     1.2
    8 |   1.3273 |     45.168 |   1.3176 |     46.493 |     1.3
    9 |   1.3114 |     44.848 |   1.3040 |     46.214 |     1.5
   10 |   1.2972 |     44.755 |   1.2866 |     45.189 |     1.7
   11 |   1.2823 |     44.143 |   1.2764 |     45.096 |     1.8
   12 |   1.2688 |     43.906 |   1.2616 |     44.662 |     2.0
   13 |   1.2569 |     43.510 |   1.2533 |     44.444 |     2.1
   14 |   1.2461 |     43.207 |   1.2435 |     44.320 |     2.3
   15 |   1.2360 |     42.948 |   1.2423 |     44.227 |     2.5
   16 |   1.2238 |     42.639 |   1.2261 |     43.389 |     2.6
   17 |   1.2109 |     41.890 |   1.2131 |     43.110 |     2.8
   18 |   1.1983 |     41.449 |   1.2054 |     42.924 |     3.0
   19 |   1.1860 |     41.295 |   1.1920 |     42.365 |     3.1
   20 |   1.1709 |     40.904 |   1.1881 |     42.303 |     3.3
   21 |   1.1590 |     40.656 |   1.1652 |     42.024 |     3.5
   22 |   1.1426 |     40.044 |   1.1604 |     41.155 |     3.6
   23 |   1.1403 |     40.094 |   1.1499 |     40.689 |     3.8
   24 |   1.1222 |     39.851 |   1.1364 |     41.030 |     4.0
   25 |   1.1132 |     39.223 |   1.1301 |     40.596 |     4.1
   26 |   1.1318 |     40.028 |   1.1290 |     40.223 |     4.3
   27 |   1.1060 |     38.843 |   1.1181 |     39.882 |     4.4
   28 |   1.0867 |     38.320 |   1.1066 |     39.385 |     4.6
   29 |   1.0751 |     37.868 |   1.0988 |     38.951 |     4.8
   30 |   1.0634 |     37.223 |   1.0846 |     38.827 |     4.9
   31 |   1.0483 |     36.645 |   1.0754 |     38.082 |     5.1
   32 |   1.0383 |     36.281 |   1.0742 |     37.927 |     5.3
   33 |   1.0238 |     35.912 |   1.0715 |     37.834 |     5.4
   34 |   1.0140 |     35.548 |   1.0669 |     36.903 |     5.6
   35 |   1.0030 |     34.590 |   1.0528 |     36.654 |     5.8
   36 |   0.9898 |     34.490 |   1.0586 |     36.530 |     5.9
   37 |   0.9777 |     33.956 |   1.0562 |     36.778 |     6.1
   38 |   0.9640 |     33.526 |   1.0347 |     36.220 |     6.3
   39 |   0.9550 |     33.003 |   1.0341 |     35.785 |     6.4
   40 |   0.9387 |     32.226 |   1.0366 |     35.661 |     6.6
   41 |   0.9275 |     32.088 |   1.0186 |     35.009 |     6.7
   42 |   0.9127 |     31.328 |   1.0153 |     34.792 |     6.9
   43 |   0.8999 |     30.882 |   1.0066 |     34.327 |     7.1
   44 |   0.8912 |     30.408 |   0.9956 |     33.240 |     7.2
   45 |   0.8772 |     30.055 |   0.9933 |     33.209 |     7.4
   46 |   0.8639 |     29.339 |   0.9847 |     33.178 |     7.6
   47 |   0.8493 |     28.860 |   0.9937 |     33.147 |     7.7
   48 |   0.8377 |     28.419 |   0.9777 |     31.875 |     7.9
   49 |   0.8258 |     27.592 |   0.9821 |     32.713 |     8.1
   50 |   0.8169 |     27.063 |   0.9689 |     31.657 |     8.2
   51 |   0.8016 |     26.975 |   0.9693 |     31.719 |     8.4
   52 |   0.7856 |     26.220 |   0.9625 |     31.223 |     8.6
   53 |   0.7826 |     26.336 |   0.9795 |     31.937 |     8.7
   54 |   0.7653 |     25.576 |   0.9641 |     31.471 |     8.9
   55 |   0.7560 |     25.212 |   0.9704 |     31.782 |     9.1
   56 |   0.7434 |     24.402 |   0.9595 |     30.944 |     9.2
   57 |   0.7345 |     24.617 |   0.9467 |     30.602 |     9.4
   58 |   0.7202 |     23.989 |   0.9614 |     30.664 |     9.5
   59 |   0.7102 |     23.636 |   0.9760 |     30.478 |     9.7
   60 |   0.7014 |     23.107 |   0.9584 |     30.261 |     9.9
   61 |   0.6898 |     22.871 |   0.9569 |     29.888 |    10.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,034,209

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0583 |     56.788 |   1.5263 |     47.052 |     0.0
    2 |   1.4295 |     45.647 |   1.3815 |     46.741 |     0.1
    3 |   1.3513 |     45.350 |   1.3350 |     46.462 |     0.1
    4 |   1.3111 |     44.617 |   1.2946 |     46.058 |     0.2
    5 |   1.2835 |     44.342 |   1.2713 |     45.345 |     0.2
    6 |   1.2597 |     43.901 |   1.2531 |     44.972 |     0.2
    7 |   1.2398 |     43.691 |   1.2379 |     43.606 |     0.3
    8 |   1.2153 |     42.727 |   1.2121 |     44.693 |     0.3
    9 |   1.1896 |     41.967 |   1.1843 |     42.862 |     0.4
   10 |   1.1609 |     40.821 |   1.1564 |     41.186 |     0.4
   11 |   1.1285 |     39.537 |   1.1303 |     39.230 |     0.5
   12 |   1.0959 |     37.807 |   1.0966 |     37.492 |     0.5
   13 |   1.0516 |     36.187 |   1.0657 |     36.530 |     0.5
   14 |   1.0095 |     34.485 |   1.0384 |     34.327 |     0.6
   15 |   0.9660 |     32.832 |   1.0089 |     34.016 |     0.6
   16 |   0.9274 |     31.350 |   0.9842 |     33.085 |     0.7
   17 |   0.8786 |     29.091 |   0.9558 |     31.968 |     0.7
   18 |   0.8238 |     27.201 |   0.9282 |     30.819 |     0.7
   19 |   0.7870 |     25.884 |   0.9326 |     30.478 |     0.8
   20 |   0.7401 |     24.039 |   0.9223 |     29.919 |     0.8
   21 |   0.6986 |     22.760 |   0.9038 |     28.585 |     0.9
   22 |   0.6551 |     21.284 |   0.8951 |     27.778 |     0.9
   23 |   0.6117 |     19.780 |   0.8858 |     27.467 |     0.9
   24 |   0.5730 |     18.353 |   0.8881 |     27.343 |     1.0
   25 |   0.5461 |     17.488 |   0.9040 |     26.723 |     1.0
   26 |   0.4980 |     15.802 |   0.9001 |     27.219 |     1.1
   27 |   0.4651 |     14.590 |   0.9152 |     26.257 |     1.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 326,625

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3367 |     60.182 |   1.7619 |     49.100 |     0.1
    2 |   1.5675 |     46.926 |   1.4482 |     47.176 |     0.3
    3 |   1.4001 |     45.333 |   1.3605 |     46.245 |     0.4
    4 |   1.3386 |     44.242 |   1.3179 |     45.810 |     0.5
    5 |   1.3051 |     43.950 |   1.2943 |     45.189 |     0.6
    6 |   1.2811 |     43.620 |   1.2738 |     44.910 |     0.8
    7 |   1.2622 |     43.223 |   1.2551 |     44.289 |     0.9
    8 |   1.2416 |     42.793 |   1.2338 |     43.544 |     1.0
    9 |   1.2216 |     42.325 |   1.2184 |     43.172 |     1.1
   10 |   1.2018 |     41.780 |   1.2003 |     42.768 |     1.3
   11 |   1.1789 |     41.096 |   1.1850 |     41.527 |     1.4
   12 |   1.1647 |     40.562 |   1.1685 |     41.837 |     1.5
   13 |   1.1470 |     39.978 |   1.1521 |     40.720 |     1.6
   14 |   1.1202 |     38.843 |   1.1355 |     41.217 |     1.8
   15 |   1.1034 |     38.457 |   1.1185 |     39.634 |     1.9
   16 |   1.0820 |     37.603 |   1.0977 |     38.485 |     2.0
   17 |   1.0615 |     37.003 |   1.0903 |     38.579 |     2.1
   18 |   1.0448 |     36.198 |   1.0708 |     37.865 |     2.3
   19 |   1.0207 |     35.008 |   1.0557 |     36.965 |     2.4
   20 |   1.0034 |     34.782 |   1.0455 |     36.220 |     2.5
   21 |   0.9783 |     33.185 |   1.0247 |     35.351 |     2.7
   22 |   0.9578 |     32.419 |   1.0202 |     34.730 |     2.8
   23 |   0.9384 |     31.361 |   1.0053 |     33.985 |     2.9
   24 |   0.9165 |     30.716 |   1.0027 |     34.420 |     3.0
   25 |   0.8930 |     29.835 |   0.9893 |     33.489 |     3.2
   26 |   0.8736 |     29.102 |   0.9789 |     32.588 |     3.3
   27 |   0.8569 |     28.479 |   0.9712 |     32.775 |     3.4
   28 |   0.8321 |     27.708 |   0.9577 |     31.657 |     3.5
   29 |   0.8156 |     27.074 |   0.9562 |     32.123 |     3.7
   30 |   0.7942 |     26.165 |   0.9471 |     31.161 |     3.8
   31 |   0.7804 |     25.835 |   0.9622 |     31.285 |     3.9
   32 |   0.7589 |     24.997 |   0.9535 |     31.192 |     4.1
   33 |   0.7453 |     24.628 |   0.9543 |     31.223 |     4.2
   34 |   0.7275 |     24.006 |   0.9564 |     31.223 |     4.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 772,065

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2628 |     62.215 |   1.6374 |     47.083 |     0.2
    2 |   1.4806 |     45.961 |   1.4176 |     47.083 |     0.4
    3 |   1.3914 |     45.983 |   1.3662 |     46.772 |     0.6
    4 |   1.3477 |     45.322 |   1.3267 |     46.648 |     0.8
    5 |   1.3134 |     45.207 |   1.3033 |     46.089 |     1.0
    6 |   1.2979 |     44.722 |   1.2926 |     46.027 |     1.2
    7 |   1.2830 |     44.369 |   1.2713 |     45.717 |     1.4
    8 |   1.2643 |     44.314 |   1.2563 |     45.500 |     1.6
    9 |   1.2524 |     44.033 |   1.2396 |     45.158 |     1.8
   10 |   1.2366 |     43.730 |   1.2264 |     44.569 |     2.0
   11 |   1.2234 |     43.510 |   1.2187 |     44.600 |     2.2
   12 |   1.2102 |     42.915 |   1.2016 |     43.265 |     2.4
   13 |   1.1956 |     42.193 |   1.1997 |     43.513 |     2.6
   14 |   1.1866 |     42.138 |   1.1993 |     43.110 |     2.8
   15 |   1.1766 |     41.829 |   1.1708 |     42.334 |     3.0
   16 |   1.1656 |     41.493 |   1.1574 |     41.930 |     3.2
   17 |   1.1580 |     40.959 |   1.1538 |     41.930 |     3.4
   18 |   1.1469 |     40.744 |   1.1650 |     41.589 |     3.6
   19 |   1.1389 |     40.518 |   1.1431 |     41.217 |     3.8
   20 |   1.1260 |     40.259 |   1.1340 |     41.155 |     4.0
   21 |   1.1146 |     39.691 |   1.1258 |     40.161 |     4.2
   22 |   1.1124 |     39.840 |   1.1271 |     40.286 |     4.4
   23 |   1.0992 |     39.135 |   1.1070 |     40.006 |     4.6
   24 |   1.0839 |     38.430 |   1.1043 |     39.758 |     4.8
   25 |   1.0728 |     38.259 |   1.1037 |     39.385 |     5.0
   26 |   1.0648 |     37.713 |   1.0871 |     38.827 |     5.2
   27 |   1.0524 |     37.405 |   1.0693 |     38.144 |     5.4
   28 |   1.0420 |     37.019 |   1.0651 |     37.430 |     5.6
   29 |   1.0295 |     36.408 |   1.0549 |     37.741 |     5.8
   30 |   1.0186 |     36.138 |   1.0548 |     37.337 |     6.0
   31 |   1.0078 |     35.796 |   1.0464 |     36.840 |     6.2
   32 |   0.9959 |     35.295 |   1.0439 |     36.965 |     6.4
   33 |   0.9855 |     34.782 |   1.0282 |     36.127 |     6.6
   34 |   0.9734 |     34.237 |   1.0281 |     36.220 |     6.8
   35 |   0.9621 |     33.884 |   1.0155 |     35.816 |     7.0
   36 |   0.9517 |     33.410 |   1.0067 |     34.854 |     7.2
   37 |   0.9439 |     33.377 |   1.0187 |     36.220 |     7.4
   38 |   0.9342 |     32.915 |   0.9962 |     34.513 |     7.6
   39 |   0.9207 |     32.854 |   0.9981 |     35.040 |     7.8
   40 |   0.9090 |     32.055 |   0.9934 |     34.699 |     8.0
   41 |   0.9085 |     32.094 |   0.9845 |     34.544 |     8.2
   42 |   0.8961 |     31.515 |   0.9763 |     34.109 |     8.4
   43 |   0.8839 |     31.328 |   0.9817 |     34.078 |     8.6
   44 |   0.8800 |     30.815 |   0.9769 |     33.768 |     8.7
   45 |   0.8655 |     30.353 |   0.9779 |     33.457 |     8.9
   46 |   0.8568 |     29.967 |   0.9731 |     33.240 |     9.1
   47 |   0.8469 |     29.620 |   0.9533 |     32.744 |     9.3
   48 |   0.8350 |     29.333 |   0.9574 |     32.340 |     9.5
   49 |   0.8301 |     28.964 |   0.9508 |     32.123 |     9.7
   50 |   0.8179 |     28.496 |   0.9504 |     32.278 |     9.9
   51 |   0.8109 |     28.050 |   0.9494 |     32.092 |    10.1
   52 |   0.8015 |     28.182 |   0.9445 |     31.782 |    10.3
   53 |   0.7964 |     27.460 |   0.9362 |     31.347 |    10.5
   54 |   0.7838 |     27.355 |   0.9601 |     31.937 |    10.7
   55 |   0.7788 |     26.975 |   0.9254 |     31.192 |    10.9
   56 |   0.7704 |     26.810 |   0.9563 |     32.123 |    11.1
   57 |   0.7568 |     26.094 |   0.9247 |     30.819 |    11.3
   58 |   0.7469 |     25.438 |   0.9413 |     31.006 |    11.5
   59 |   0.7643 |     26.149 |   0.9359 |     31.254 |    11.7
   60 |   0.7300 |     25.240 |   0.9182 |     30.043 |    11.9
   61 |   0.7218 |     24.573 |   0.9251 |     29.826 |    12.1
   62 |   0.7051 |     23.928 |   0.9336 |     30.509 |    12.3
   63 |   0.7044 |     23.989 |   0.9296 |     30.106 |    12.5
   64 |   0.6940 |     23.846 |   0.9252 |     30.137 |    12.7
   65 |   0.6852 |     23.135 |   0.9172 |     29.640 |    12.9
   66 |   0.6801 |     23.267 |   0.9127 |     29.516 |    13.1
   67 |   0.6675 |     22.584 |   0.9292 |     30.137 |    13.3
   68 |   0.6576 |     22.231 |   0.9197 |     29.826 |    13.5
   69 |   0.6512 |     22.154 |   0.9229 |     29.671 |    13.7
   70 |   0.6424 |     21.691 |   0.9227 |     29.485 |    13.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 507,873

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0792 |     56.061 |   1.5193 |     46.741 |     0.1
    2 |   1.4108 |     45.433 |   1.3642 |     46.120 |     0.3
    3 |   1.3313 |     44.733 |   1.3100 |     46.462 |     0.4
    4 |   1.2966 |     44.419 |   1.2802 |     44.910 |     0.5
    5 |   1.2704 |     43.813 |   1.2664 |     44.879 |     0.6
    6 |   1.2468 |     43.499 |   1.2384 |     44.910 |     0.8
    7 |   1.2229 |     43.124 |   1.2027 |     43.358 |     0.9
    8 |   1.2028 |     42.237 |   1.1962 |     42.241 |     1.0
    9 |   1.1804 |     41.416 |   1.1729 |     41.496 |     1.1
   10 |   1.1565 |     40.804 |   1.1524 |     40.937 |     1.3
   11 |   1.1371 |     40.187 |   1.1358 |     40.161 |     1.4
   12 |   1.1147 |     39.394 |   1.1238 |     40.006 |     1.5
   13 |   1.0997 |     38.584 |   1.1027 |     39.137 |     1.6
   14 |   1.0789 |     37.824 |   1.0898 |     38.610 |     1.8
   15 |   1.0599 |     36.970 |   1.0650 |     37.430 |     1.9
   16 |   1.0404 |     36.358 |   1.0493 |     36.344 |     2.0
   17 |   1.0243 |     35.791 |   1.0450 |     36.840 |     2.1
   18 |   1.0084 |     35.074 |   1.0326 |     36.282 |     2.3
   19 |   0.9872 |     34.336 |   1.0181 |     35.537 |     2.4
   20 |   0.9716 |     33.669 |   1.0014 |     35.133 |     2.5
   21 |   0.9527 |     33.074 |   0.9951 |     34.854 |     2.7
   22 |   0.9353 |     32.347 |   0.9813 |     33.302 |     2.8
   23 |   0.9131 |     31.647 |   0.9781 |     34.233 |     2.9
   24 |   0.9003 |     31.466 |   0.9525 |     33.116 |     3.0
   25 |   0.8746 |     30.171 |   0.9390 |     31.999 |     3.2
   26 |   0.8583 |     29.565 |   0.9379 |     32.092 |     3.3
   27 |   0.8367 |     28.705 |   0.9428 |     32.030 |     3.4
   28 |   0.8168 |     27.928 |   0.9162 |     31.130 |     3.5
   29 |   0.7976 |     27.102 |   0.9048 |     31.161 |     3.7
   30 |   0.7764 |     26.204 |   0.9113 |     31.564 |     3.8
   31 |   0.7588 |     25.763 |   0.9005 |     31.223 |     3.9
   32 |   0.7448 |     25.212 |   0.8925 |     30.230 |     4.0
   33 |   0.7256 |     24.242 |   0.8816 |     29.764 |     4.2
   34 |   0.7018 |     23.460 |   0.8967 |     30.168 |     4.3
   35 |   0.6857 |     22.871 |   0.8857 |     29.019 |     4.4
   36 |   0.6727 |     22.540 |   0.8670 |     28.150 |     4.5
   37 |   0.6529 |     21.813 |   0.8869 |     29.205 |     4.7
   38 |   0.6389 |     20.964 |   0.8672 |     28.305 |     4.8
   39 |   0.6182 |     20.523 |   0.8844 |     28.554 |     4.9
   40 |   0.6070 |     20.523 |   0.8774 |     28.119 |     5.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,559,265

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1778 |     59.559 |   1.5773 |     47.083 |     0.1
    2 |   1.4593 |     45.923 |   1.4176 |     47.083 |     0.1
    3 |   1.3781 |     45.482 |   1.3681 |     46.741 |     0.2
    4 |   1.3403 |     45.295 |   1.3366 |     46.958 |     0.2
    5 |   1.3128 |     45.003 |   1.3076 |     46.214 |     0.3
    6 |   1.2940 |     44.518 |   1.2845 |     45.220 |     0.4
    7 |   1.2714 |     44.143 |   1.2681 |     45.624 |     0.4
    8 |   1.2497 |     43.686 |   1.2433 |     44.693 |     0.5
    9 |   1.2295 |     43.493 |   1.2247 |     44.693 |     0.5
   10 |   1.2138 |     43.058 |   1.2150 |     43.637 |     0.6
   11 |   1.2012 |     42.760 |   1.1984 |     43.637 |     0.7
   12 |   1.1878 |     42.364 |   1.2022 |     43.575 |     0.7
   13 |   1.1769 |     42.039 |   1.1790 |     42.427 |     0.8
   14 |   1.1691 |     41.906 |   1.1676 |     42.737 |     0.8
   15 |   1.1602 |     41.769 |   1.1684 |     42.396 |     0.9
   16 |   1.1513 |     41.328 |   1.1502 |     42.210 |     0.9
   17 |   1.1440 |     40.986 |   1.1477 |     41.930 |     1.0
   18 |   1.1370 |     41.019 |   1.1435 |     41.496 |     1.1
   19 |   1.1293 |     40.887 |   1.1419 |     41.279 |     1.1
   20 |   1.1261 |     40.898 |   1.1314 |     41.744 |     1.2
   21 |   1.1157 |     40.534 |   1.1239 |     41.310 |     1.2
   22 |   1.1101 |     40.375 |   1.1225 |     40.441 |     1.3
   23 |   1.1066 |     40.083 |   1.1158 |     40.689 |     1.4
   24 |   1.0985 |     39.956 |   1.1115 |     41.217 |     1.4
   25 |   1.0920 |     39.912 |   1.1134 |     40.596 |     1.5
   26 |   1.0913 |     40.072 |   1.1082 |     40.565 |     1.5
   27 |   1.0832 |     39.444 |   1.1007 |     40.161 |     1.6
   28 |   1.0775 |     39.223 |   1.1032 |     40.379 |     1.7
   29 |   1.0755 |     39.504 |   1.1020 |     40.472 |     1.7
   30 |   1.0668 |     38.898 |   1.0943 |     39.292 |     1.8
   31 |   1.0648 |     38.595 |   1.0926 |     39.199 |     1.8
   32 |   1.0590 |     38.468 |   1.0796 |     38.547 |     1.9
   33 |   1.0525 |     38.138 |   1.0790 |     38.734 |     2.0
   34 |   1.0441 |     37.642 |   1.0760 |     39.479 |     2.0
   35 |   1.0352 |     37.449 |   1.0678 |     38.237 |     2.1
   36 |   1.0323 |     37.207 |   1.0576 |     37.616 |     2.2
   37 |   1.0256 |     36.893 |   1.0530 |     38.175 |     2.2
   38 |   1.0227 |     36.986 |   1.0593 |     38.113 |     2.3
   39 |   1.0154 |     36.556 |   1.0552 |     38.765 |     2.3
   40 |   1.0070 |     36.298 |   1.0484 |     37.585 |     2.4
   41 |   0.9989 |     35.818 |   1.0463 |     37.741 |     2.5
   42 |   0.9944 |     35.813 |   1.0443 |     37.461 |     2.5
   43 |   0.9884 |     35.433 |   1.0402 |     37.647 |     2.6
   44 |   0.9834 |     35.267 |   1.0378 |     36.903 |     2.6
   45 |   0.9755 |     34.601 |   1.0275 |     36.313 |     2.7
   46 |   0.9692 |     34.959 |   1.0256 |     36.685 |     2.8
   47 |   0.9627 |     34.534 |   1.0185 |     35.754 |     2.8
   48 |   0.9557 |     33.967 |   1.0236 |     35.971 |     2.9
   49 |   0.9516 |     33.895 |   1.0159 |     36.189 |     2.9
   50 |   0.9429 |     33.763 |   1.0048 |     35.382 |     3.0
   51 |   0.9393 |     33.521 |   1.0091 |     35.909 |     3.1
   52 |   0.9343 |     33.300 |   1.0121 |     35.723 |     3.1
   53 |   0.9280 |     33.135 |   1.0167 |     36.282 |     3.2
   54 |   0.9212 |     32.821 |   1.0052 |     34.978 |     3.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 903,329

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1929 |     59.769 |   1.5877 |     47.083 |     0.1
    2 |   1.4557 |     45.884 |   1.4057 |     46.989 |     0.3
    3 |   1.3683 |     45.625 |   1.3500 |     46.896 |     0.4
    4 |   1.3269 |     44.882 |   1.3195 |     45.903 |     0.5
    5 |   1.3003 |     44.419 |   1.2868 |     45.282 |     0.6
    6 |   1.2747 |     43.989 |   1.2702 |     44.817 |     0.8
    7 |   1.2533 |     43.620 |   1.2453 |     44.662 |     0.9
    8 |   1.2334 |     43.240 |   1.2254 |     43.420 |     1.0
    9 |   1.2112 |     42.435 |   1.2128 |     42.582 |     1.1
   10 |   1.1950 |     42.176 |   1.1918 |     42.737 |     1.3
   11 |   1.1802 |     41.741 |   1.1915 |     41.962 |     1.4
   12 |   1.1664 |     41.444 |   1.1664 |     42.365 |     1.5
   13 |   1.1496 |     40.623 |   1.1554 |     41.279 |     1.7
   14 |   1.1375 |     40.490 |   1.1439 |     41.372 |     1.8
   15 |   1.1240 |     39.961 |   1.1381 |     40.596 |     1.9
   16 |   1.1148 |     39.791 |   1.1277 |     40.099 |     2.0
   17 |   1.0940 |     39.107 |   1.1179 |     40.068 |     2.2
   18 |   1.0856 |     38.678 |   1.1109 |     39.013 |     2.3
   19 |   1.0743 |     38.121 |   1.0959 |     39.292 |     2.4
   20 |   1.0599 |     37.736 |   1.0923 |     38.765 |     2.5
   21 |   1.0461 |     37.102 |   1.0868 |     38.858 |     2.7
   22 |   1.0333 |     36.639 |   1.0891 |     38.547 |     2.8
   23 |   1.0209 |     36.033 |   1.0720 |     38.206 |     2.9
   24 |   1.0102 |     35.669 |   1.0558 |     37.182 |     3.1
   25 |   0.9958 |     35.306 |   1.0509 |     36.375 |     3.2
   26 |   0.9818 |     34.606 |   1.0565 |     37.958 |     3.3
   27 |   0.9695 |     34.479 |   1.0284 |     35.847 |     3.4
   28 |   0.9530 |     33.636 |   1.0040 |     34.699 |     3.6
   29 |   0.9385 |     32.997 |   1.0144 |     35.351 |     3.7
   30 |   0.9258 |     32.606 |   1.0075 |     35.568 |     3.8
   31 |   0.9042 |     31.846 |   1.0026 |     34.513 |     3.9
   32 |   0.8952 |     31.383 |   1.0087 |     34.978 |     4.1
   33 |   0.8807 |     30.948 |   0.9898 |     33.985 |     4.2
   34 |   0.8581 |     30.000 |   0.9897 |     33.426 |     4.3
   35 |   0.8402 |     28.904 |   0.9832 |     33.582 |     4.5
   36 |   0.8300 |     28.579 |   0.9900 |     33.085 |     4.6
   37 |   0.8097 |     27.994 |   0.9638 |     32.651 |     4.7
   38 |   0.7927 |     27.135 |   0.9628 |     32.588 |     4.8
   39 |   0.7730 |     26.529 |   0.9465 |     31.440 |     5.0
   40 |   0.7592 |     25.763 |   0.9674 |     31.719 |     5.1
   41 |   0.7477 |     25.548 |   0.9430 |     31.316 |     5.2
   42 |   0.7204 |     24.264 |   0.9557 |     30.881 |     5.3
   43 |   0.7083 |     23.873 |   0.9530 |     30.912 |     5.5
   44 |   0.6915 |     23.284 |   0.9395 |     30.664 |     5.6
   45 |   0.6768 |     22.645 |   0.9677 |     30.385 |     5.7
   46 |   0.6630 |     22.253 |   0.9514 |     31.719 |     5.9
   47 |   0.6468 |     21.774 |   0.9353 |     30.106 |     6.0
   48 |   0.6293 |     20.975 |   0.9753 |     30.509 |     6.1
   49 |   0.6209 |     20.854 |   0.9440 |     29.081 |     6.2
   50 |   0.6096 |     20.331 |   0.9386 |     29.857 |     6.4
   51 |   0.5937 |     19.884 |   0.9512 |     29.454 |     6.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 806,945

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1078 |     56.667 |   1.5518 |     47.083 |     0.2
    2 |   1.4445 |     45.879 |   1.4037 |     47.083 |     0.4
    3 |   1.3714 |     45.532 |   1.3596 |     46.896 |     0.6
    4 |   1.3391 |     45.344 |   1.3363 |     46.462 |     0.8
    5 |   1.3144 |     44.942 |   1.3089 |     46.307 |     1.0
    6 |   1.2915 |     44.419 |   1.2952 |     45.686 |     1.2
    7 |   1.2738 |     44.298 |   1.2706 |     45.251 |     1.4
    8 |   1.2554 |     43.785 |   1.2618 |     45.345 |     1.6
    9 |   1.2377 |     43.229 |   1.2298 |     44.103 |     1.8
   10 |   1.2137 |     42.380 |   1.2140 |     43.296 |     2.0
   11 |   1.1899 |     42.275 |   1.1925 |     42.675 |     2.2
   12 |   1.1674 |     40.882 |   1.1799 |     42.800 |     2.4
   13 |   1.1460 |     40.490 |   1.1510 |     41.217 |     2.6
   14 |   1.1251 |     39.532 |   1.1338 |     40.503 |     2.8
   15 |   1.0997 |     38.964 |   1.1174 |     39.075 |     3.0
   16 |   1.0833 |     38.193 |   1.0999 |     39.044 |     3.2
   17 |   1.0605 |     37.537 |   1.0868 |     37.772 |     3.4
   18 |   1.0341 |     36.375 |   1.0754 |     37.616 |     3.6
   19 |   1.0098 |     35.085 |   1.0425 |     35.227 |     3.8
   20 |   0.9829 |     34.204 |   1.0391 |     35.878 |     4.0
   21 |   0.9596 |     33.096 |   1.0320 |     34.885 |     4.2
   22 |   0.9378 |     32.143 |   0.9950 |     33.551 |     4.4
   23 |   0.9046 |     30.871 |   0.9886 |     33.395 |     4.6
   24 |   0.8800 |     30.154 |   0.9778 |     32.713 |     4.8
   25 |   0.8478 |     28.689 |   0.9677 |     32.433 |     5.0
   26 |   0.8182 |     27.592 |   0.9676 |     32.061 |     5.2
   27 |   0.7915 |     26.171 |   0.9454 |     30.788 |     5.4
   28 |   0.7661 |     25.482 |   0.9491 |     31.037 |     5.6
   29 |   0.7364 |     24.072 |   0.9307 |     29.454 |     5.8
   30 |   0.7111 |     23.460 |   0.9242 |     29.299 |     6.0
   31 |   0.6922 |     22.909 |   0.9357 |     28.771 |     6.2
   32 |   0.6675 |     21.824 |   0.9191 |     28.771 |     6.4
   33 |   0.6379 |     20.661 |   0.9255 |     28.647 |     6.6
   34 |   0.6208 |     20.292 |   0.9197 |     27.623 |     6.8
   35 |   0.5973 |     19.466 |   0.9243 |     27.747 |     7.0
   36 |   0.5759 |     18.529 |   0.9368 |     27.995 |     7.2
   37 |   0.5553 |     17.972 |   0.9184 |     27.126 |     7.4
   38 |   0.5339 |     17.251 |   0.9295 |     26.691 |     7.6
   39 |   0.5201 |     16.826 |   0.9288 |     26.723 |     7.8
   40 |   0.5023 |     16.187 |   0.9326 |     26.660 |     8.0
   41 |   0.4775 |     15.322 |   0.9334 |     26.257 |     8.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 542,753

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9857 |     54.733 |   1.4625 |     46.679 |     0.1
    2 |   1.3773 |     45.047 |   1.3292 |     45.531 |     0.3
    3 |   1.3024 |     44.028 |   1.2837 |     45.189 |     0.4
    4 |   1.2650 |     43.802 |   1.2571 |     45.251 |     0.5
    5 |   1.2292 |     42.865 |   1.2171 |     43.482 |     0.6
    6 |   1.1993 |     41.978 |   1.1920 |     42.365 |     0.8
    7 |   1.1720 |     41.190 |   1.1616 |     41.372 |     0.9
    8 |   1.1467 |     40.413 |   1.1366 |     41.217 |     1.0
    9 |   1.1144 |     39.207 |   1.1082 |     39.106 |     1.1
   10 |   1.0848 |     38.198 |   1.0869 |     37.647 |     1.3
   11 |   1.0485 |     36.507 |   1.0521 |     36.375 |     1.4
   12 |   1.0140 |     35.008 |   1.0353 |     35.196 |     1.5
   13 |   0.9766 |     33.504 |   1.0017 |     33.302 |     1.7
   14 |   0.9459 |     32.722 |   0.9936 |     33.582 |     1.8
   15 |   0.9101 |     31.300 |   0.9646 |     32.744 |     1.9
   16 |   0.8742 |     29.824 |   0.9608 |     32.402 |     2.0
   17 |   0.8455 |     28.722 |   0.9349 |     31.533 |     2.2
   18 |   0.8088 |     27.091 |   0.9134 |     30.416 |     2.3
   19 |   0.7866 |     26.325 |   0.9184 |     30.168 |     2.4
   20 |   0.7442 |     24.661 |   0.9033 |     29.609 |     2.5
   21 |   0.7165 |     23.752 |   0.9052 |     29.640 |     2.7
   22 |   0.6849 |     22.606 |   0.8868 |     28.274 |     2.8
   23 |   0.6621 |     21.708 |   0.8732 |     27.902 |     2.9
   24 |   0.6405 |     20.909 |   0.8738 |     27.281 |     3.1
   25 |   0.6176 |     20.006 |   0.8716 |     27.219 |     3.2
   26 |   0.5923 |     19.592 |   0.8831 |     27.219 |     3.3
   27 |   0.5733 |     18.661 |   0.8907 |     27.126 |     3.4
   28 |   0.5495 |     18.061 |   0.8794 |     27.095 |     3.6
   29 |   0.5313 |     17.174 |   0.8698 |     26.567 |     3.7
   30 |   0.5183 |     16.860 |   0.8923 |     26.909 |     3.8
   31 |   0.4972 |     16.204 |   0.8994 |     27.529 |     3.9
   32 |   0.4795 |     15.730 |   0.8903 |     26.971 |     4.1
   33 |   0.4658 |     15.102 |   0.8923 |     26.443 |     4.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,097,825

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2250 |     60.755 |   1.6213 |     47.083 |     0.1
    2 |   1.4766 |     46.000 |   1.4310 |     47.083 |     0.2
    3 |   1.3979 |     46.033 |   1.3944 |     47.083 |     0.2
    4 |   1.3639 |     45.543 |   1.3696 |     46.369 |     0.3
    5 |   1.3440 |     45.113 |   1.3428 |     46.462 |     0.4
    6 |   1.3218 |     45.074 |   1.3364 |     45.996 |     0.5
    7 |   1.3066 |     44.799 |   1.3044 |     45.531 |     0.5
    8 |   1.2906 |     44.424 |   1.2944 |     46.245 |     0.6
    9 |   1.2787 |     44.567 |   1.2803 |     45.376 |     0.7
   10 |   1.2649 |     44.099 |   1.2749 |     45.407 |     0.8
   11 |   1.2551 |     43.741 |   1.2661 |     45.438 |     0.8
   12 |   1.2433 |     43.499 |   1.2498 |     45.345 |     0.9
   13 |   1.2336 |     43.306 |   1.2384 |     44.538 |     1.0
   14 |   1.2176 |     42.766 |   1.2224 |     43.917 |     1.1
   15 |   1.2017 |     42.424 |   1.2014 |     43.637 |     1.1
   16 |   1.1854 |     41.796 |   1.1973 |     42.768 |     1.2
   17 |   1.1674 |     41.124 |   1.1701 |     42.396 |     1.3
   18 |   1.1467 |     40.391 |   1.1559 |     40.968 |     1.4
   19 |   1.1216 |     39.163 |   1.1368 |     40.006 |     1.4
   20 |   1.0961 |     38.116 |   1.1145 |     40.192 |     1.5
   21 |   1.0722 |     37.532 |   1.1070 |     39.354 |     1.6
   22 |   1.0483 |     36.683 |   1.0785 |     37.772 |     1.7
   23 |   1.0234 |     35.664 |   1.0516 |     36.716 |     1.7
   24 |   0.9934 |     34.804 |   1.0350 |     35.816 |     1.8
   25 |   0.9674 |     34.000 |   1.0396 |     35.971 |     1.9
   26 |   0.9380 |     32.634 |   1.0191 |     34.668 |     2.0
   27 |   0.9111 |     31.598 |   1.0050 |     33.923 |     2.0
   28 |   0.8790 |     30.033 |   0.9760 |     33.706 |     2.1
   29 |   0.8445 |     28.837 |   0.9706 |     32.216 |     2.2
   30 |   0.8104 |     27.251 |   0.9655 |     31.285 |     2.3
   31 |   0.7742 |     25.344 |   0.9549 |     31.037 |     2.4
   32 |   0.7443 |     24.402 |   0.9527 |     31.254 |     2.4
   33 |   0.7035 |     22.882 |   0.9674 |     31.502 |     2.5
   34 |   0.6702 |     21.543 |   0.9602 |     30.261 |     2.6
   35 |   0.6386 |     20.705 |   0.9303 |     29.268 |     2.7
   36 |   0.5959 |     18.898 |   0.9345 |     28.833 |     2.7
   37 |   0.5738 |     18.154 |   0.9526 |     29.236 |     2.8
   38 |   0.5353 |     16.678 |   0.9438 |     27.716 |     2.9
   39 |   0.5025 |     15.625 |   0.9571 |     27.964 |     3.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 519,265

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5456 |     68.171 |   2.0063 |     55.369 |     0.1
    2 |   1.7968 |     50.683 |   1.6095 |     47.083 |     0.3
    3 |   1.5247 |     45.906 |   1.4717 |     47.083 |     0.4
    4 |   1.4417 |     45.917 |   1.4285 |     47.083 |     0.5
    5 |   1.4091 |     45.851 |   1.4163 |     47.083 |     0.7
    6 |   1.3850 |     45.796 |   1.3848 |     46.400 |     0.8
    7 |   1.3620 |     45.339 |   1.3686 |     46.803 |     0.9
    8 |   1.3455 |     45.174 |   1.3462 |     46.648 |     1.1
    9 |   1.3286 |     45.102 |   1.3306 |     46.741 |     1.2
   10 |   1.3129 |     44.788 |   1.3143 |     46.431 |     1.3
   11 |   1.2969 |     44.507 |   1.2978 |     46.120 |     1.5
   12 |   1.2812 |     43.950 |   1.2833 |     45.996 |     1.6
   13 |   1.2655 |     43.873 |   1.2664 |     45.158 |     1.7
   14 |   1.2492 |     43.647 |   1.2512 |     44.910 |     1.9
   15 |   1.2340 |     43.328 |   1.2376 |     44.351 |     2.0
   16 |   1.2186 |     42.711 |   1.2191 |     43.513 |     2.1
   17 |   1.2053 |     42.105 |   1.2081 |     43.017 |     2.2
   18 |   1.1903 |     41.631 |   1.1954 |     42.613 |     2.4
   19 |   1.1793 |     41.438 |   1.1894 |     43.048 |     2.5
   20 |   1.1659 |     40.904 |   1.1760 |     42.489 |     2.6
   21 |   1.1546 |     40.545 |   1.1671 |     41.899 |     2.8
   22 |   1.1381 |     39.675 |   1.1581 |     40.534 |     2.9
   23 |   1.1310 |     39.620 |   1.1513 |     40.565 |     3.0
   24 |   1.1203 |     39.140 |   1.1384 |     40.410 |     3.2
   25 |   1.1100 |     38.821 |   1.1339 |     40.286 |     3.3
   26 |   1.1000 |     38.457 |   1.1251 |     39.603 |     3.4
   27 |   1.0879 |     38.110 |   1.1213 |     39.789 |     3.6
   28 |   1.0797 |     37.603 |   1.1140 |     38.206 |     3.7
   29 |   1.0692 |     37.163 |   1.1002 |     38.361 |     3.8
   30 |   1.0544 |     36.793 |   1.0986 |     38.547 |     4.0
   31 |   1.0428 |     36.507 |   1.0869 |     38.299 |     4.1
   32 |   1.0295 |     35.708 |   1.0936 |     38.175 |     4.2
   33 |   1.0205 |     35.912 |   1.0797 |     37.492 |     4.4
   34 |   1.0075 |     34.920 |   1.0691 |     36.996 |     4.5
   35 |   0.9988 |     34.810 |   1.0654 |     37.151 |     4.6
   36 |   0.9847 |     34.253 |   1.0631 |     36.437 |     4.8
   37 |   0.9706 |     33.813 |   1.0568 |     36.747 |     4.9
   38 |   0.9553 |     33.212 |   1.0479 |     35.971 |     5.0
   39 |   0.9408 |     32.391 |   1.0541 |     36.127 |     5.2
   40 |   0.9343 |     32.099 |   1.0353 |     34.730 |     5.3
   41 |   0.9151 |     31.212 |   1.0341 |     34.947 |     5.4
   42 |   0.9045 |     31.014 |   1.0146 |     33.675 |     5.6
   43 |   0.8839 |     30.099 |   1.0200 |     34.171 |     5.7
   44 |   0.8761 |     30.105 |   1.0225 |     34.389 |     5.8
   45 |   0.8647 |     29.427 |   1.0284 |     34.109 |     5.9
   46 |   0.8446 |     28.556 |   1.0141 |     33.520 |     6.1
   47 |   0.8346 |     28.165 |   1.0038 |     33.520 |     6.2
   48 |   0.8235 |     27.813 |   1.0167 |     33.582 |     6.3
   49 |   0.8099 |     27.019 |   1.0146 |     32.713 |     6.5
   50 |   0.7938 |     26.623 |   0.9944 |     32.278 |     6.6
   51 |   0.7788 |     25.868 |   0.9998 |     31.875 |     6.7
   52 |   0.7657 |     25.488 |   1.0033 |     32.030 |     6.9
   53 |   0.7592 |     25.256 |   0.9994 |     32.464 |     7.0
   54 |   0.7431 |     24.782 |   0.9890 |     31.564 |     7.1
   55 |   0.7387 |     24.661 |   1.0038 |     31.564 |     7.3
   56 |   0.7324 |     24.545 |   1.0064 |     31.099 |     7.4
   57 |   0.7105 |     23.758 |   0.9884 |     31.316 |     7.5
   58 |   0.7016 |     23.333 |   1.0047 |     31.006 |     7.7
   59 |   0.6959 |     23.190 |   0.9925 |     30.012 |     7.8
   60 |   0.6870 |     22.887 |   1.0031 |     31.130 |     7.9
   61 |   0.6694 |     21.978 |   1.0031 |     30.819 |     8.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,573,537

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3601 |     63.041 |   1.8120 |     49.286 |     0.1
    2 |   1.6059 |     47.565 |   1.4751 |     46.989 |     0.2
    3 |   1.4289 |     45.664 |   1.3933 |     46.462 |     0.2
    4 |   1.3751 |     45.493 |   1.3584 |     46.276 |     0.3
    5 |   1.3430 |     45.273 |   1.3303 |     46.058 |     0.4
    6 |   1.3182 |     44.942 |   1.3069 |     46.214 |     0.5
    7 |   1.3015 |     44.518 |   1.2899 |     45.438 |     0.6
    8 |   1.2838 |     44.215 |   1.2779 |     45.469 |     0.6
    9 |   1.2691 |     43.989 |   1.2578 |     44.972 |     0.7
   10 |   1.2510 |     43.493 |   1.2429 |     44.382 |     0.8
   11 |   1.2308 |     42.689 |   1.2291 |     43.327 |     0.9
   12 |   1.2141 |     42.347 |   1.2060 |     42.613 |     1.0
   13 |   1.1955 |     41.989 |   1.1934 |     41.806 |     1.0
   14 |   1.1780 |     41.416 |   1.1754 |     41.372 |     1.1
   15 |   1.1586 |     40.375 |   1.1656 |     40.565 |     1.2
   16 |   1.1411 |     39.526 |   1.1549 |     40.658 |     1.3
   17 |   1.1254 |     39.102 |   1.1397 |     39.510 |     1.4
   18 |   1.1033 |     38.242 |   1.1225 |     39.696 |     1.5
   19 |   1.0815 |     37.300 |   1.1046 |     38.485 |     1.5
   20 |   1.0560 |     35.978 |   1.0865 |     36.685 |     1.6
   21 |   1.0321 |     35.201 |   1.0644 |     36.189 |     1.7
   22 |   1.0051 |     34.154 |   1.0505 |     35.630 |     1.8
   23 |   0.9856 |     33.471 |   1.0303 |     34.761 |     1.9
   24 |   0.9616 |     32.435 |   1.0245 |     34.295 |     1.9
   25 |   0.9270 |     31.036 |   1.0118 |     33.985 |     2.0
   26 |   0.8963 |     29.664 |   0.9949 |     33.799 |     2.1
   27 |   0.8690 |     28.733 |   0.9764 |     32.899 |     2.2
   28 |   0.8312 |     27.085 |   0.9713 |     32.216 |     2.3
   29 |   0.8027 |     26.171 |   0.9438 |     31.409 |     2.4
   30 |   0.7661 |     24.854 |   0.9368 |     31.223 |     2.4
   31 |   0.7338 |     23.565 |   0.9301 |     30.509 |     2.5
   32 |   0.7055 |     22.567 |   0.9172 |     29.702 |     2.6
   33 |   0.6745 |     21.394 |   0.9096 |     28.554 |     2.7
   34 |   0.6373 |     20.094 |   0.9128 |     28.616 |     2.8
   35 |   0.6079 |     19.168 |   0.8969 |     27.592 |     2.9
   36 |   0.5820 |     18.590 |   0.9090 |     27.281 |     2.9
   37 |   0.5554 |     17.394 |   0.8987 |     27.467 |     3.0
   38 |   0.5231 |     15.747 |   0.9216 |     27.312 |     3.1
   39 |   0.5188 |     16.220 |   0.9277 |     27.250 |     3.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,098,465

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1687 |     59.096 |   1.6002 |     47.083 |     0.1
    2 |   1.4659 |     45.862 |   1.4150 |     47.083 |     0.3
    3 |   1.3871 |     45.730 |   1.3788 |     47.114 |     0.4
    4 |   1.3511 |     45.124 |   1.3554 |     46.462 |     0.5
    5 |   1.3245 |     44.821 |   1.3168 |     46.089 |     0.6
    6 |   1.3011 |     44.386 |   1.2897 |     45.934 |     0.8
    7 |   1.2785 |     44.298 |   1.2756 |     45.624 |     0.9
    8 |   1.2613 |     43.917 |   1.2499 |     45.593 |     1.0
    9 |   1.2399 |     43.576 |   1.2462 |     44.631 |     1.1
   10 |   1.2251 |     43.537 |   1.2248 |     44.569 |     1.3
   11 |   1.2025 |     42.915 |   1.2014 |     43.669 |     1.4
   12 |   1.1829 |     42.138 |   1.1807 |     41.403 |     1.5
   13 |   1.1646 |     41.388 |   1.1668 |     41.403 |     1.6
   14 |   1.1463 |     40.909 |   1.1579 |     41.527 |     1.8
   15 |   1.1259 |     39.928 |   1.1350 |     40.937 |     1.9
   16 |   1.1091 |     39.421 |   1.1309 |     40.068 |     2.0
   17 |   1.0959 |     38.810 |   1.1174 |     39.975 |     2.2
   18 |   1.0755 |     37.592 |   1.0954 |     38.734 |     2.3
   19 |   1.0602 |     36.937 |   1.0963 |     38.423 |     2.4
   20 |   1.0510 |     36.793 |   1.0737 |     37.213 |     2.5
   21 |   1.0285 |     35.763 |   1.0678 |     37.709 |     2.7
   22 |   1.0106 |     35.223 |   1.0559 |     37.182 |     2.8
   23 |   0.9890 |     34.441 |   1.0474 |     36.840 |     2.9
   24 |   0.9706 |     33.923 |   1.0302 |     35.940 |     3.0
   25 |   0.9500 |     33.157 |   1.0246 |     35.102 |     3.2
   26 |   0.9312 |     32.320 |   1.0303 |     34.978 |     3.3
   27 |   0.9075 |     31.096 |   1.0123 |     34.575 |     3.4
   28 |   0.8833 |     30.364 |   0.9994 |     34.047 |     3.5
   29 |   0.8527 |     29.163 |   0.9849 |     33.023 |     3.7
   30 |   0.8340 |     28.353 |   0.9827 |     32.899 |     3.8
   31 |   0.8068 |     27.322 |   0.9837 |     32.899 |     3.9
   32 |   0.7980 |     27.289 |   0.9566 |     32.775 |     4.1
   33 |   0.7596 |     25.510 |   0.9567 |     31.906 |     4.2
   34 |   0.7327 |     24.457 |   0.9629 |     31.688 |     4.3
   35 |   0.7075 |     23.592 |   0.9553 |     30.881 |     4.4
   36 |   0.6892 |     23.085 |   0.9643 |     30.881 |     4.6
   37 |   0.6661 |     22.099 |   0.9522 |     30.043 |     4.7
   38 |   0.6380 |     21.168 |   0.9441 |     29.330 |     4.8
   39 |   0.6130 |     20.314 |   0.9506 |     29.268 |     4.9
   40 |   0.5846 |     19.091 |   0.9561 |     29.299 |     5.1
   41 |   0.5750 |     19.014 |   0.9595 |     29.268 |     5.2
   42 |   0.5562 |     18.617 |   0.9663 |     28.926 |     5.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 555,617

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4549 |     64.579 |   1.8713 |     49.348 |     0.1
    2 |   1.6816 |     48.303 |   1.5307 |     47.083 |     0.3
    3 |   1.4686 |     45.851 |   1.4374 |     47.083 |     0.4
    4 |   1.4093 |     45.818 |   1.3983 |     47.083 |     0.5
    5 |   1.3740 |     45.581 |   1.3649 |     46.741 |     0.7
    6 |   1.3413 |     44.848 |   1.3324 |     46.462 |     0.8
    7 |   1.3127 |     44.270 |   1.3052 |     45.996 |     0.9
    8 |   1.2902 |     43.939 |   1.2770 |     45.220 |     1.0
    9 |   1.2690 |     43.769 |   1.2619 |     45.096 |     1.2
   10 |   1.2506 |     43.791 |   1.2423 |     44.786 |     1.3
   11 |   1.2316 |     43.074 |   1.2254 |     44.165 |     1.4
   12 |   1.2179 |     42.700 |   1.2255 |     43.451 |     1.6
   13 |   1.2062 |     42.419 |   1.2039 |     43.606 |     1.7
   14 |   1.1963 |     42.275 |   1.1972 |     43.606 |     1.8
   15 |   1.1899 |     42.088 |   1.1890 |     42.582 |     2.0
   16 |   1.1804 |     41.653 |   1.1824 |     42.241 |     2.1
   17 |   1.1722 |     41.587 |   1.1755 |     41.589 |     2.2
   18 |   1.1655 |     40.948 |   1.1684 |     41.775 |     2.3
   19 |   1.1561 |     41.003 |   1.1574 |     41.434 |     2.5
   20 |   1.1523 |     40.821 |   1.1596 |     41.930 |     2.6
   21 |   1.1467 |     40.672 |   1.1572 |     40.689 |     2.7
   22 |   1.1399 |     40.391 |   1.1408 |     41.248 |     2.9
   23 |   1.1322 |     40.270 |   1.1359 |     41.217 |     3.0
   24 |   1.1267 |     40.011 |   1.1325 |     40.813 |     3.1
   25 |   1.1204 |     39.862 |   1.1311 |     40.875 |     3.2
   26 |   1.1165 |     39.774 |   1.1241 |     40.534 |     3.4
   27 |   1.1110 |     39.488 |   1.1199 |     40.627 |     3.5
   28 |   1.1055 |     39.245 |   1.1258 |     40.937 |     3.6
   29 |   1.0996 |     39.361 |   1.1100 |     39.913 |     3.8
   30 |   1.0952 |     39.096 |   1.1103 |     39.944 |     3.9
   31 |   1.0913 |     38.788 |   1.1095 |     40.317 |     4.0
   32 |   1.0880 |     38.683 |   1.1061 |     39.634 |     4.2
   33 |   1.0814 |     38.424 |   1.1040 |     40.255 |     4.3
   34 |   1.0807 |     38.314 |   1.1047 |     39.696 |     4.4
   35 |   1.0735 |     38.264 |   1.0948 |     39.572 |     4.5
   36 |   1.0693 |     38.055 |   1.0955 |     39.479 |     4.7
   37 |   1.0661 |     38.072 |   1.0852 |     38.889 |     4.8
   38 |   1.0586 |     37.983 |   1.0833 |     39.044 |     4.9
   39 |   1.0597 |     37.923 |   1.0903 |     39.479 |     5.1
   40 |   1.0542 |     37.455 |   1.0857 |     39.106 |     5.2
   41 |   1.0487 |     37.377 |   1.0838 |     39.075 |     5.3
   42 |   1.0421 |     37.355 |   1.0771 |     38.703 |     5.5
   43 |   1.0392 |     36.771 |   1.0769 |     38.982 |     5.6
   44 |   1.0347 |     36.981 |   1.0723 |     38.734 |     5.7
   45 |   1.0281 |     36.534 |   1.0656 |     38.579 |     5.8
   46 |   1.0228 |     36.661 |   1.0663 |     37.834 |     6.0
   47 |   1.0202 |     36.408 |   1.0689 |     38.020 |     6.1
   48 |   1.0141 |     36.088 |   1.0674 |     38.299 |     6.2
   49 |   1.0104 |     35.824 |   1.0595 |     38.454 |     6.4
   50 |   1.0107 |     35.846 |   1.0558 |     38.330 |     6.5
   51 |   1.0018 |     35.769 |   1.0604 |     38.268 |     6.6
   52 |   1.0009 |     35.813 |   1.0636 |     38.082 |     6.7
   53 |   0.9950 |     35.466 |   1.0517 |     37.585 |     6.9
   54 |   0.9879 |     35.223 |   1.0537 |     37.678 |     7.0
   55 |   0.9825 |     35.157 |   1.0470 |     37.430 |     7.1
   56 |   0.9873 |     35.118 |   1.0469 |     37.461 |     7.3
   57 |   0.9779 |     34.744 |   1.0379 |     37.027 |     7.4
   58 |   0.9705 |     34.386 |   1.0323 |     37.027 |     7.5
   59 |   0.9702 |     34.623 |   1.0329 |     37.058 |     7.7
   60 |   0.9663 |     34.545 |   1.0307 |     37.120 |     7.8
   61 |   0.9617 |     34.226 |   1.0398 |     36.654 |     7.9
   62 |   0.9574 |     34.320 |   1.0347 |     37.275 |     8.0
   63 |   0.9530 |     33.961 |   1.0224 |     36.406 |     8.2
   64 |   0.9441 |     33.758 |   1.0320 |     36.685 |     8.3
   65 |   0.9460 |     33.785 |   1.0238 |     35.754 |     8.4
   66 |   0.9428 |     33.780 |   1.0256 |     36.437 |     8.6
   67 |   0.9336 |     33.168 |   1.0217 |     36.592 |     8.7
   68 |   0.9300 |     33.085 |   1.0250 |     35.940 |     8.8
   69 |   0.9250 |     33.030 |   1.0082 |     35.351 |     9.0
   70 |   0.9225 |     32.920 |   1.0102 |     35.909 |     9.1
   71 |   0.9218 |     33.113 |   1.0065 |     36.065 |     9.2
   72 |   0.9092 |     32.248 |   1.0093 |     35.785 |     9.3
   73 |   0.9010 |     31.989 |   1.0079 |     35.723 |     9.5
   74 |   0.8969 |     31.939 |   1.0079 |     35.351 |     9.6
   75 |   0.8972 |     31.857 |   0.9979 |     35.071 |     9.7
   76 |   0.8888 |     31.477 |   0.9943 |     35.258 |     9.9
   77 |   0.8834 |     31.399 |   0.9920 |     35.071 |    10.0
   78 |   0.8769 |     30.848 |   0.9916 |     34.358 |    10.1
   79 |   0.8672 |     30.639 |   0.9917 |     34.016 |    10.2
   80 |   0.8649 |     30.391 |   0.9863 |     33.644 |    10.4
   81 |   0.8554 |     29.994 |   0.9823 |     34.420 |    10.5
   82 |   0.8480 |     29.912 |   0.9808 |     33.551 |    10.6
   83 |   0.8425 |     29.559 |   0.9814 |     33.582 |    10.8
   84 |   0.8392 |     29.273 |   0.9801 |     33.954 |    10.9
   85 |   0.8274 |     28.893 |   0.9715 |     33.457 |    11.0
   86 |   0.8219 |     28.567 |   0.9743 |     33.023 |    11.2
   87 |   0.8126 |     28.523 |   0.9620 |     33.116 |    11.3
   88 |   0.8068 |     28.099 |   0.9593 |     32.651 |    11.4
   89 |   0.8043 |     28.270 |   0.9726 |     32.806 |    11.5
   90 |   0.7914 |     26.942 |   0.9743 |     32.557 |    11.7
   91 |   0.7941 |     27.702 |   0.9640 |     32.713 |    11.8
   92 |   0.7875 |     27.212 |   0.9706 |     32.464 |    11.9
   93 |   0.7796 |     26.942 |   0.9542 |     32.278 |    12.1
   94 |   0.7692 |     26.617 |   0.9550 |     32.030 |    12.2
   95 |   0.7628 |     26.353 |   0.9597 |     32.185 |    12.3
   96 |   0.7614 |     26.121 |   0.9624 |     31.564 |    12.5
   97 |   0.7531 |     25.906 |   0.9640 |     32.030 |    12.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 326,625

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2885 |     60.171 |   1.6762 |     49.193 |     0.0
    2 |   1.5160 |     46.303 |   1.4320 |     46.741 |     0.0
    3 |   1.3890 |     45.152 |   1.3608 |     46.369 |     0.1
    4 |   1.3314 |     44.028 |   1.3225 |     45.748 |     0.1
    5 |   1.2983 |     43.906 |   1.2945 |     44.817 |     0.1
    6 |   1.2739 |     43.433 |   1.2700 |     44.786 |     0.1
    7 |   1.2547 |     43.240 |   1.2494 |     44.600 |     0.2
    8 |   1.2326 |     42.650 |   1.2313 |     44.413 |     0.2
    9 |   1.2155 |     42.303 |   1.2160 |     43.917 |     0.2
   10 |   1.1970 |     41.725 |   1.2009 |     42.303 |     0.2
   11 |   1.1794 |     41.085 |   1.1858 |     42.210 |     0.3
   12 |   1.1599 |     40.579 |   1.1700 |     41.589 |     0.3
   13 |   1.1427 |     40.253 |   1.1558 |     41.186 |     0.3
   14 |   1.1262 |     39.537 |   1.1351 |     40.379 |     0.3
   15 |   1.1076 |     38.760 |   1.1241 |     39.913 |     0.3
   16 |   1.0909 |     38.215 |   1.1042 |     39.323 |     0.4
   17 |   1.0694 |     37.388 |   1.0992 |     38.920 |     0.4
   18 |   1.0545 |     36.705 |   1.0815 |     37.523 |     0.4
   19 |   1.0352 |     35.741 |   1.0680 |     37.958 |     0.4
   20 |   1.0148 |     34.744 |   1.0473 |     36.375 |     0.5
   21 |   0.9928 |     33.653 |   1.0380 |     36.499 |     0.5
   22 |   0.9749 |     33.256 |   1.0281 |     34.823 |     0.5
   23 |   0.9554 |     32.551 |   1.0231 |     34.575 |     0.5
   24 |   0.9315 |     31.317 |   1.0031 |     34.171 |     0.5
   25 |   0.9065 |     30.281 |   0.9885 |     33.302 |     0.6
   26 |   0.8809 |     29.421 |   0.9824 |     33.582 |     0.6
   27 |   0.8593 |     28.562 |   0.9673 |     32.588 |     0.6
   28 |   0.8339 |     27.537 |   0.9674 |     32.775 |     0.6
   29 |   0.8191 |     26.959 |   0.9683 |     32.402 |     0.7
   30 |   0.7953 |     26.154 |   0.9467 |     31.750 |     0.7
   31 |   0.7662 |     24.689 |   0.9321 |     31.130 |     0.7
   32 |   0.7474 |     24.275 |   0.9294 |     30.944 |     0.7
   33 |   0.7229 |     23.251 |   0.9299 |     31.037 |     0.8
   34 |   0.6982 |     22.474 |   0.9177 |     30.695 |     0.8
   35 |   0.6799 |     21.675 |   0.9327 |     30.726 |     0.8
   36 |   0.6700 |     21.168 |   0.9314 |     30.385 |     0.8
   37 |   0.6444 |     20.287 |   0.9157 |     29.050 |     0.9
   38 |   0.6169 |     19.570 |   0.9164 |     28.616 |     0.9
   39 |   0.6028 |     18.860 |   0.9185 |     29.143 |     0.9
   40 |   0.5835 |     18.782 |   0.9224 |     28.585 |     0.9
   41 |   0.5632 |     17.697 |   0.9093 |     27.374 |     0.9
   42 |   0.5408 |     16.926 |   0.9294 |     28.150 |     1.0
   43 |   0.5277 |     16.413 |   0.9290 |     27.529 |     1.0
   44 |   0.5102 |     15.818 |   0.9322 |     27.964 |     1.0
   45 |   0.4950 |     15.118 |   0.9310 |     27.064 |     1.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 732,257

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4935 |     67.394 |   1.9355 |     54.935 |     0.1
    2 |   1.7487 |     49.884 |   1.5785 |     47.083 |     0.2
    3 |   1.4977 |     45.862 |   1.4488 |     47.083 |     0.4
    4 |   1.4246 |     45.939 |   1.4114 |     47.083 |     0.5
    5 |   1.3972 |     45.901 |   1.3955 |     47.083 |     0.6
    6 |   1.3725 |     45.713 |   1.3762 |     46.617 |     0.8
    7 |   1.3572 |     45.311 |   1.3581 |     46.772 |     0.9
    8 |   1.3433 |     45.289 |   1.3442 |     46.710 |     1.0
    9 |   1.3327 |     45.212 |   1.3329 |     47.207 |     1.1
   10 |   1.3196 |     45.135 |   1.3168 |     46.400 |     1.3
   11 |   1.3062 |     44.683 |   1.2992 |     46.120 |     1.4
   12 |   1.2897 |     44.430 |   1.2881 |     45.717 |     1.5
   13 |   1.2757 |     44.281 |   1.2721 |     45.748 |     1.6
   14 |   1.2633 |     44.028 |   1.2638 |     45.903 |     1.8
   15 |   1.2533 |     43.857 |   1.2501 |     45.345 |     1.9
   16 |   1.2455 |     43.614 |   1.2419 |     45.158 |     2.0
   17 |   1.2364 |     43.543 |   1.2441 |     45.034 |     2.1
   18 |   1.2280 |     43.273 |   1.2234 |     44.972 |     2.3
   19 |   1.2207 |     43.168 |   1.2198 |     44.289 |     2.4
   20 |   1.2138 |     42.871 |   1.2085 |     43.606 |     2.5
   21 |   1.2045 |     42.711 |   1.2063 |     43.451 |     2.6
   22 |   1.1976 |     42.369 |   1.2039 |     43.669 |     2.8
   23 |   1.1911 |     42.391 |   1.1960 |     43.110 |     2.9
   24 |   1.1842 |     42.237 |   1.1929 |     43.389 |     3.0
   25 |   1.1771 |     42.242 |   1.1849 |     43.700 |     3.1
   26 |   1.1704 |     41.972 |   1.1812 |     42.675 |     3.3
   27 |   1.1665 |     41.532 |   1.1770 |     43.172 |     3.4
   28 |   1.1564 |     41.460 |   1.1660 |     42.303 |     3.5
   29 |   1.1508 |     41.030 |   1.1589 |     41.930 |     3.6
   30 |   1.1439 |     41.036 |   1.1539 |     41.930 |     3.8
   31 |   1.1339 |     40.391 |   1.1501 |     41.651 |     3.9
   32 |   1.1280 |     40.402 |   1.1451 |     41.217 |     4.0
   33 |   1.1196 |     39.912 |   1.1434 |     41.217 |     4.1
   34 |   1.1122 |     39.664 |   1.1303 |     40.968 |     4.3
   35 |   1.1027 |     39.438 |   1.1288 |     40.875 |     4.4
   36 |   1.0964 |     39.212 |   1.1196 |     40.689 |     4.5
   37 |   1.0859 |     38.738 |   1.1148 |     40.006 |     4.6
   38 |   1.0808 |     38.623 |   1.1141 |     40.068 |     4.8
   39 |   1.0715 |     37.956 |   1.1028 |     39.448 |     4.9
   40 |   1.0646 |     37.923 |   1.1011 |     39.354 |     5.0
   41 |   1.0542 |     37.273 |   1.0986 |     38.734 |     5.1
   42 |   1.0445 |     36.893 |   1.0912 |     38.610 |     5.3
   43 |   1.0390 |     36.579 |   1.0830 |     38.672 |     5.4
   44 |   1.0301 |     36.556 |   1.0843 |     38.547 |     5.5
   45 |   1.0237 |     36.380 |   1.0725 |     37.709 |     5.7
   46 |   1.0114 |     35.598 |   1.0702 |     37.772 |     5.8
   47 |   1.0040 |     35.598 |   1.0600 |     36.996 |     5.9
   48 |   0.9926 |     34.700 |   1.0671 |     37.337 |     6.0
   49 |   0.9854 |     34.948 |   1.0547 |     36.996 |     6.2
   50 |   0.9759 |     34.264 |   1.0535 |     36.406 |     6.3
   51 |   0.9668 |     34.116 |   1.0431 |     35.940 |     6.4
   52 |   0.9494 |     33.240 |   1.0382 |     35.289 |     6.5
   53 |   0.9379 |     32.733 |   1.0267 |     35.102 |     6.7
   54 |   0.9239 |     31.939 |   1.0119 |     34.358 |     6.8
   55 |   0.9156 |     32.006 |   1.0120 |     34.513 |     6.9
   56 |   0.9014 |     31.019 |   1.0065 |     34.295 |     7.0
   57 |   0.8863 |     30.507 |   1.0197 |     34.358 |     7.2
   58 |   0.8767 |     30.314 |   0.9971 |     33.271 |     7.3
   59 |   0.8656 |     29.884 |   1.0003 |     33.489 |     7.4
   60 |   0.8509 |     29.295 |   0.9973 |     33.551 |     7.5
   61 |   0.8321 |     28.386 |   0.9901 |     32.868 |     7.7
   62 |   0.8300 |     28.419 |   0.9983 |     32.992 |     7.8
   63 |   0.8231 |     28.165 |   0.9836 |     32.526 |     7.9
   64 |   0.8054 |     27.366 |   0.9783 |     32.092 |     8.0
   65 |   0.7895 |     26.749 |   0.9858 |     32.371 |     8.2
   66 |   0.7784 |     26.248 |   0.9810 |     31.626 |     8.3
   67 |   0.7652 |     25.537 |   0.9808 |     31.564 |     8.4
   68 |   0.7505 |     25.190 |   0.9726 |     31.223 |     8.5
   69 |   0.7432 |     24.832 |   0.9708 |     31.285 |     8.7
   70 |   0.7312 |     24.556 |   0.9690 |     31.037 |     8.8
   71 |   0.7170 |     23.994 |   0.9715 |     30.385 |     8.9
   72 |   0.7068 |     23.466 |   0.9663 |     30.850 |     9.0
   73 |   0.6925 |     23.003 |   0.9696 |     30.912 |     9.2
   74 |   0.6762 |     22.309 |   0.9736 |     30.199 |     9.3
   75 |   0.6677 |     22.138 |   0.9785 |     30.385 |     9.4
   76 |   0.6533 |     21.614 |   0.9827 |     30.354 |     9.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,559,265

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2284 |     60.981 |   1.6599 |     47.083 |     0.1
    2 |   1.4992 |     46.011 |   1.4276 |     47.083 |     0.1
    3 |   1.4004 |     45.873 |   1.3939 |     47.020 |     0.2
    4 |   1.3671 |     45.256 |   1.3598 |     46.803 |     0.2
    5 |   1.3443 |     45.179 |   1.3396 |     47.020 |     0.3
    6 |   1.3246 |     45.278 |   1.3138 |     46.214 |     0.4
    7 |   1.3039 |     44.893 |   1.2960 |     46.710 |     0.4
    8 |   1.2868 |     44.540 |   1.2832 |     45.903 |     0.5
    9 |   1.2668 |     44.435 |   1.2631 |     45.500 |     0.5
   10 |   1.2495 |     43.906 |   1.2472 |     45.189 |     0.6
   11 |   1.2350 |     43.543 |   1.2272 |     44.196 |     0.7
   12 |   1.2206 |     43.118 |   1.2153 |     43.886 |     0.7
   13 |   1.2060 |     42.755 |   1.2039 |     43.948 |     0.8
   14 |   1.1888 |     42.678 |   1.1916 |     43.700 |     0.9
   15 |   1.1739 |     42.270 |   1.1830 |     42.986 |     0.9
   16 |   1.1637 |     41.658 |   1.1659 |     43.017 |     1.0
   17 |   1.1511 |     41.449 |   1.1496 |     41.496 |     1.0
   18 |   1.1390 |     40.865 |   1.1471 |     41.837 |     1.1
   19 |   1.1276 |     40.601 |   1.1394 |     41.899 |     1.2
   20 |   1.1155 |     40.143 |   1.1325 |     40.875 |     1.2
   21 |   1.1057 |     39.813 |   1.1232 |     40.223 |     1.3
   22 |   1.0943 |     39.229 |   1.1120 |     40.565 |     1.3
   23 |   1.0828 |     38.716 |   1.1011 |     39.417 |     1.4
   24 |   1.0745 |     38.386 |   1.0919 |     39.199 |     1.5
   25 |   1.0584 |     37.653 |   1.0831 |     37.927 |     1.5
   26 |   1.0468 |     37.504 |   1.0751 |     38.268 |     1.6
   27 |   1.0372 |     37.030 |   1.0682 |     37.461 |     1.6
   28 |   1.0217 |     36.435 |   1.0590 |     37.430 |     1.7
   29 |   1.0112 |     35.961 |   1.0494 |     36.903 |     1.8
   30 |   1.0019 |     35.736 |   1.0562 |     37.368 |     1.8
   31 |   0.9871 |     35.229 |   1.0420 |     35.816 |     1.9
   32 |   0.9771 |     34.876 |   1.0368 |     36.996 |     1.9
   33 |   0.9649 |     34.612 |   1.0261 |     35.661 |     2.0
   34 |   0.9527 |     34.193 |   1.0197 |     35.785 |     2.1
   35 |   0.9412 |     33.565 |   1.0133 |     35.568 |     2.1
   36 |   0.9242 |     32.937 |   1.0023 |     34.823 |     2.2
   37 |   0.9107 |     32.540 |   1.0055 |     34.078 |     2.2
   38 |   0.8962 |     31.741 |   0.9961 |     34.171 |     2.3
   39 |   0.8926 |     31.664 |   1.0031 |     34.761 |     2.4
   40 |   0.8758 |     30.871 |   0.9862 |     33.768 |     2.4
   41 |   0.8529 |     29.840 |   0.9851 |     33.551 |     2.5
   42 |   0.8355 |     29.240 |   0.9806 |     33.147 |     2.6
   43 |   0.8202 |     28.700 |   0.9617 |     32.278 |     2.6
   44 |   0.8030 |     28.077 |   0.9687 |     32.588 |     2.7
   45 |   0.7916 |     27.554 |   0.9538 |     32.340 |     2.7
   46 |   0.7623 |     26.331 |   0.9508 |     31.192 |     2.8
   47 |   0.7440 |     25.857 |   0.9484 |     31.099 |     2.9
   48 |   0.7298 |     25.091 |   0.9523 |     31.099 |     2.9
   49 |   0.7153 |     24.231 |   0.9411 |     30.602 |     3.0
   50 |   0.6949 |     23.923 |   0.9277 |     29.764 |     3.0
   51 |   0.6815 |     23.289 |   0.9354 |     30.571 |     3.1
   52 |   0.6582 |     22.193 |   0.9369 |     30.106 |     3.2
   53 |   0.6397 |     21.647 |   0.9421 |     29.919 |     3.2
   54 |   0.6205 |     20.848 |   0.9458 |     29.112 |     3.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,588,321

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5219 |     67.344 |   1.9722 |     56.269 |     0.1
    2 |   1.7714 |     49.939 |   1.5865 |     47.083 |     0.2
    3 |   1.5009 |     46.000 |   1.4495 |     47.083 |     0.2
    4 |   1.4240 |     45.862 |   1.4063 |     47.083 |     0.3
    5 |   1.3868 |     45.725 |   1.3770 |     47.083 |     0.4
    6 |   1.3550 |     45.289 |   1.3469 |     46.493 |     0.5
    7 |   1.3317 |     45.074 |   1.3243 |     46.710 |     0.6
    8 |   1.3154 |     44.799 |   1.3112 |     46.120 |     0.6
    9 |   1.2999 |     44.545 |   1.2962 |     46.400 |     0.7
   10 |   1.2843 |     44.391 |   1.2757 |     45.313 |     0.8
   11 |   1.2683 |     43.895 |   1.2645 |     45.127 |     0.9
   12 |   1.2574 |     43.532 |   1.2547 |     44.538 |     1.0
   13 |   1.2470 |     43.647 |   1.2432 |     45.189 |     1.0
   14 |   1.2390 |     43.333 |   1.2313 |     44.879 |     1.1
   15 |   1.2279 |     43.366 |   1.2207 |     44.475 |     1.2
   16 |   1.2213 |     43.455 |   1.2186 |     44.507 |     1.3
   17 |   1.2158 |     43.229 |   1.2108 |     44.227 |     1.4
   18 |   1.2105 |     43.118 |   1.2014 |     43.731 |     1.4
   19 |   1.2063 |     43.047 |   1.1982 |     43.855 |     1.5
   20 |   1.1986 |     42.920 |   1.1927 |     43.513 |     1.6
   21 |   1.1948 |     42.738 |   1.1934 |     43.731 |     1.7
   22 |   1.1912 |     42.579 |   1.1891 |     43.327 |     1.8
   23 |   1.1858 |     42.369 |   1.1776 |     43.265 |     1.8
   24 |   1.1827 |     42.325 |   1.1767 |     42.706 |     1.9
   25 |   1.1770 |     42.292 |   1.1733 |     42.396 |     2.0
   26 |   1.1735 |     42.182 |   1.1691 |     42.458 |     2.1
   27 |   1.1663 |     41.890 |   1.1706 |     43.048 |     2.2
   28 |   1.1637 |     41.967 |   1.1593 |     41.899 |     2.2
   29 |   1.1608 |     41.741 |   1.1631 |     42.365 |     2.3
   30 |   1.1560 |     41.449 |   1.1515 |     41.434 |     2.4
   31 |   1.1520 |     41.477 |   1.1430 |     41.868 |     2.5
   32 |   1.1438 |     41.212 |   1.1440 |     41.403 |     2.6
   33 |   1.1428 |     40.777 |   1.1471 |     41.930 |     2.6
   34 |   1.1396 |     41.041 |   1.1328 |     41.527 |     2.7
   35 |   1.1359 |     40.986 |   1.1325 |     41.248 |     2.8
   36 |   1.1305 |     40.738 |   1.1299 |     41.030 |     2.9
   37 |   1.1265 |     40.452 |   1.1276 |     41.527 |     3.0
   38 |   1.1240 |     40.391 |   1.1285 |     40.689 |     3.0
   39 |   1.1202 |     40.331 |   1.1200 |     40.565 |     3.1
   40 |   1.1157 |     40.039 |   1.1171 |     40.161 |     3.2
   41 |   1.1112 |     39.912 |   1.1105 |     40.379 |     3.3
   42 |   1.1099 |     39.802 |   1.1129 |     40.317 |     3.4
   43 |   1.1024 |     39.614 |   1.1065 |     40.006 |     3.4
   44 |   1.1018 |     39.416 |   1.1065 |     40.223 |     3.5
   45 |   1.0979 |     39.399 |   1.1011 |     39.261 |     3.6
   46 |   1.0916 |     38.915 |   1.1079 |     40.317 |     3.7
   47 |   1.0878 |     38.882 |   1.0969 |     39.448 |     3.8
   48 |   1.0863 |     38.837 |   1.0913 |     39.137 |     3.8
   49 |   1.0820 |     38.556 |   1.0901 |     39.541 |     3.9
   50 |   1.0820 |     38.474 |   1.0898 |     39.230 |     4.0
   51 |   1.0794 |     38.579 |   1.0847 |     38.516 |     4.1
   52 |   1.0729 |     38.281 |   1.0848 |     38.547 |     4.2
   53 |   1.0668 |     37.972 |   1.0806 |     38.672 |     4.2
   54 |   1.0694 |     38.248 |   1.0894 |     39.199 |     4.3
   55 |   1.0659 |     38.033 |   1.0794 |     38.330 |     4.4
   56 |   1.0599 |     37.620 |   1.0720 |     38.920 |     4.5
   57 |   1.0553 |     37.752 |   1.0725 |     38.703 |     4.6
   58 |   1.0521 |     37.730 |   1.0656 |     38.268 |     4.6
   59 |   1.0484 |     37.322 |   1.0638 |     38.579 |     4.7
   60 |   1.0438 |     37.003 |   1.0634 |     38.703 |     4.8
   61 |   1.0417 |     36.992 |   1.0607 |     38.423 |     4.9
   62 |   1.0354 |     37.058 |   1.0678 |     38.734 |     5.0
   63 |   1.0347 |     36.810 |   1.0506 |     37.958 |     5.0
   64 |   1.0281 |     36.573 |   1.0642 |     38.330 |     5.1
   65 |   1.0241 |     36.424 |   1.0522 |     37.368 |     5.2
   66 |   1.0230 |     36.413 |   1.0540 |     37.803 |     5.3
   67 |   1.0204 |     36.039 |   1.0538 |     37.616 |     5.4
   68 |   1.0173 |     36.242 |   1.0475 |     37.616 |     5.4
   69 |   1.0108 |     36.039 |   1.0536 |     37.182 |     5.5
   70 |   1.0139 |     36.066 |   1.0412 |     37.337 |     5.6
   71 |   1.0056 |     35.879 |   1.0426 |     36.747 |     5.7
   72 |   1.0030 |     35.554 |   1.0461 |     37.709 |     5.8
   73 |   1.0024 |     35.554 |   1.0445 |     37.275 |     5.9
   74 |   0.9993 |     35.576 |   1.0434 |     37.430 |     5.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 555,617

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4777 |     66.744 |   1.8997 |     49.814 |     0.1
    2 |   1.6904 |     48.193 |   1.5367 |     47.083 |     0.3
    3 |   1.4700 |     45.813 |   1.4357 |     47.083 |     0.4
    4 |   1.4047 |     45.719 |   1.4082 |     46.772 |     0.5
    5 |   1.3718 |     45.223 |   1.3644 |     46.803 |     0.7
    6 |   1.3438 |     44.915 |   1.3448 |     46.710 |     0.8
    7 |   1.3234 |     44.810 |   1.3111 |     46.214 |     0.9
    8 |   1.2991 |     44.193 |   1.2917 |     45.282 |     1.0
    9 |   1.2755 |     43.950 |   1.2799 |     45.034 |     1.2
   10 |   1.2582 |     43.410 |   1.2580 |     44.475 |     1.3
   11 |   1.2454 |     43.328 |   1.2489 |     44.351 |     1.4
   12 |   1.2367 |     43.366 |   1.2384 |     44.600 |     1.6
   13 |   1.2257 |     42.887 |   1.2258 |     43.979 |     1.7
   14 |   1.2152 |     42.766 |   1.2208 |     43.917 |     1.8
   15 |   1.2041 |     42.667 |   1.2147 |     43.389 |     2.0
   16 |   1.1976 |     42.320 |   1.1979 |     43.172 |     2.1
   17 |   1.1852 |     42.083 |   1.1927 |     43.141 |     2.2
   18 |   1.1816 |     41.989 |   1.1852 |     42.365 |     2.3
   19 |   1.1708 |     41.438 |   1.1757 |     42.396 |     2.5
   20 |   1.1633 |     41.080 |   1.1691 |     42.179 |     2.6
   21 |   1.1567 |     40.953 |   1.1639 |     41.930 |     2.7
   22 |   1.1488 |     40.601 |   1.1602 |     41.310 |     2.9
   23 |   1.1410 |     40.342 |   1.1466 |     40.937 |     3.0
   24 |   1.1324 |     40.220 |   1.1421 |     41.310 |     3.1
   25 |   1.1250 |     39.763 |   1.1452 |     41.713 |     3.3
   26 |   1.1180 |     39.625 |   1.1330 |     41.403 |     3.4
   27 |   1.1112 |     39.554 |   1.1201 |     40.596 |     3.5
   28 |   1.1021 |     39.124 |   1.1226 |     40.161 |     3.6
   29 |   1.0986 |     39.063 |   1.1124 |     39.944 |     3.8
   30 |   1.0885 |     38.887 |   1.1032 |     39.913 |     3.9
   31 |   1.0809 |     38.634 |   1.0945 |     39.479 |     4.0
   32 |   1.0783 |     38.479 |   1.1018 |     39.913 |     4.2
   33 |   1.0707 |     38.187 |   1.0846 |     38.827 |     4.3
   34 |   1.0633 |     37.603 |   1.0857 |     39.199 |     4.4
   35 |   1.0554 |     37.477 |   1.0815 |     38.796 |     4.6
   36 |   1.0490 |     37.510 |   1.0725 |     38.454 |     4.7
   37 |   1.0460 |     37.262 |   1.0722 |     38.672 |     4.8
   38 |   1.0367 |     36.882 |   1.0669 |     38.237 |     4.9
   39 |   1.0300 |     36.860 |   1.0638 |     37.803 |     5.1
   40 |   1.0262 |     36.314 |   1.0647 |     37.585 |     5.2
   41 |   1.0239 |     36.182 |   1.0570 |     37.337 |     5.3
   42 |   1.0139 |     36.000 |   1.0523 |     37.213 |     5.5
   43 |   1.0080 |     35.840 |   1.0483 |     37.803 |     5.6
   44 |   1.0028 |     35.504 |   1.0454 |     37.027 |     5.7
   45 |   0.9993 |     35.598 |   1.0460 |     37.213 |     5.9
   46 |   0.9951 |     35.344 |   1.0432 |     37.337 |     6.0
   47 |   0.9894 |     35.152 |   1.0372 |     37.523 |     6.1
   48 |   0.9834 |     34.959 |   1.0391 |     36.934 |     6.2
   49 |   0.9797 |     34.733 |   1.0297 |     37.120 |     6.4
   50 |   0.9766 |     34.667 |   1.0333 |     36.778 |     6.5
   51 |   0.9701 |     34.457 |   1.0276 |     36.685 |     6.6
   52 |   0.9624 |     34.050 |   1.0342 |     37.089 |     6.8
   53 |   0.9605 |     34.066 |   1.0236 |     36.375 |     6.9
   54 |   0.9557 |     33.736 |   1.0219 |     36.468 |     7.0
   55 |   0.9481 |     33.664 |   1.0259 |     36.375 |     7.2
   56 |   0.9391 |     33.008 |   1.0206 |     36.872 |     7.3
   57 |   0.9365 |     32.887 |   1.0104 |     35.785 |     7.4
   58 |   0.9297 |     32.705 |   1.0169 |     36.034 |     7.5
   59 |   0.9256 |     32.667 |   1.0143 |     35.102 |     7.7
   60 |   0.9212 |     32.182 |   1.0098 |     35.164 |     7.8
   61 |   0.9186 |     32.314 |   0.9980 |     34.916 |     7.9
   62 |   0.9071 |     31.989 |   1.0005 |     34.854 |     8.1
   63 |   0.8996 |     31.835 |   1.0072 |     34.482 |     8.2
   64 |   0.8999 |     31.658 |   0.9869 |     33.613 |     8.3
   65 |   0.8940 |     31.427 |   0.9902 |     33.861 |     8.5
   66 |   0.8860 |     30.997 |   0.9885 |     33.675 |     8.6
   67 |   0.8822 |     31.019 |   0.9782 |     33.457 |     8.7
   68 |   0.8710 |     30.435 |   0.9837 |     32.961 |     8.8
   69 |   0.8682 |     30.413 |   0.9822 |     33.271 |     9.0
   70 |   0.8628 |     30.088 |   0.9796 |     33.395 |     9.1
   71 |   0.8544 |     29.587 |   0.9804 |     33.054 |     9.2
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,593,505

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1744 |     58.948 |   1.5886 |     47.083 |     0.1
    2 |   1.4746 |     45.994 |   1.4233 |     47.083 |     0.1
    3 |   1.3934 |     45.736 |   1.3893 |     46.648 |     0.2
    4 |   1.3583 |     45.251 |   1.3540 |     46.772 |     0.2
    5 |   1.3311 |     44.986 |   1.3337 |     46.214 |     0.3
    6 |   1.3143 |     44.612 |   1.3177 |     45.779 |     0.4
    7 |   1.2992 |     44.309 |   1.2998 |     45.655 |     0.4
    8 |   1.2860 |     44.479 |   1.2811 |     45.717 |     0.5
    9 |   1.2688 |     44.121 |   1.2693 |     45.407 |     0.5
   10 |   1.2566 |     43.840 |   1.2683 |     44.941 |     0.6
   11 |   1.2441 |     43.510 |   1.2451 |     44.972 |     0.7
   12 |   1.2261 |     43.135 |   1.2230 |     44.196 |     0.7
   13 |   1.2066 |     42.485 |   1.2016 |     43.669 |     0.8
   14 |   1.1879 |     42.088 |   1.1889 |     42.117 |     0.9
   15 |   1.1686 |     41.003 |   1.1795 |     41.899 |     0.9
   16 |   1.1524 |     40.562 |   1.1587 |     41.155 |     1.0
   17 |   1.1344 |     39.598 |   1.1446 |     40.565 |     1.0
   18 |   1.1182 |     39.118 |   1.1286 |     40.161 |     1.1
   19 |   1.1020 |     38.788 |   1.1208 |     39.882 |     1.2
   20 |   1.0860 |     38.039 |   1.1102 |     40.037 |     1.2
   21 |   1.0708 |     37.554 |   1.1009 |     38.920 |     1.3
   22 |   1.0580 |     37.267 |   1.0868 |     38.858 |     1.3
   23 |   1.0417 |     36.325 |   1.0741 |     38.268 |     1.4
   24 |   1.0228 |     36.138 |   1.0678 |     37.896 |     1.5
   25 |   1.0045 |     35.477 |   1.0632 |     37.647 |     1.5
   26 |   0.9844 |     34.683 |   1.0541 |     36.437 |     1.6
   27 |   0.9643 |     33.835 |   1.0339 |     35.785 |     1.6
   28 |   0.9414 |     32.595 |   1.0296 |     35.785 |     1.7
   29 |   0.9139 |     31.763 |   1.0186 |     34.389 |     1.8
   30 |   0.8916 |     31.019 |   1.0140 |     34.482 |     1.8
   31 |   0.8693 |     29.686 |   1.0046 |     34.202 |     1.9
   32 |   0.8436 |     29.074 |   0.9975 |     33.302 |     2.0
   33 |   0.8132 |     27.554 |   0.9839 |     32.309 |     2.0
   34 |   0.7905 |     26.926 |   0.9793 |     32.682 |     2.1
   35 |   0.7546 |     25.460 |   0.9783 |     31.750 |     2.1
   36 |   0.7307 |     24.534 |   0.9690 |     31.378 |     2.2
   37 |   0.7053 |     23.526 |   0.9650 |     30.043 |     2.3
   38 |   0.6776 |     22.590 |   0.9608 |     30.106 |     2.3
   39 |   0.6558 |     21.598 |   0.9628 |     29.888 |     2.4
   40 |   0.6245 |     20.402 |   0.9673 |     29.423 |     2.4
   41 |   0.5995 |     19.631 |   0.9704 |     29.361 |     2.5
   42 |   0.5753 |     18.810 |   0.9757 |     28.616 |     2.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 423,841

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3375 |     62.088 |   1.7977 |     49.876 |     0.2
    2 |   1.5952 |     46.837 |   1.4683 |     46.834 |     0.4
    3 |   1.4152 |     45.510 |   1.3789 |     46.151 |     0.6
    4 |   1.3528 |     44.788 |   1.3407 |     46.120 |     0.8
    5 |   1.3126 |     44.298 |   1.2967 |     45.624 |     1.0
    6 |   1.2866 |     43.763 |   1.2773 |     45.158 |     1.2
    7 |   1.2627 |     43.427 |   1.2521 |     44.320 |     1.4
    8 |   1.2394 |     42.937 |   1.2322 |     43.762 |     1.6
    9 |   1.2175 |     42.446 |   1.2093 |     42.706 |     1.8
   10 |   1.1979 |     41.736 |   1.1851 |     41.899 |     2.0
   11 |   1.1714 |     40.711 |   1.1708 |     41.217 |     2.2
   12 |   1.1517 |     39.956 |   1.1581 |     40.906 |     2.4
   13 |   1.1263 |     38.766 |   1.1359 |     39.727 |     2.6
   14 |   1.1048 |     38.116 |   1.1134 |     38.547 |     2.8
   15 |   1.0754 |     36.771 |   1.1020 |     38.765 |     3.0
   16 |   1.0490 |     36.028 |   1.0815 |     36.872 |     3.2
   17 |   1.0217 |     34.601 |   1.0671 |     36.127 |     3.4
   18 |   0.9946 |     33.499 |   1.0435 |     34.699 |     3.6
   19 |   0.9622 |     31.785 |   1.0203 |     33.489 |     3.8
   20 |   0.9279 |     30.915 |   1.0130 |     33.923 |     4.0
   21 |   0.9006 |     29.493 |   0.9935 |     32.619 |     4.2
   22 |   0.8696 |     28.545 |   0.9771 |     32.061 |     4.4
   23 |   0.8487 |     27.708 |   0.9703 |     31.750 |     4.5
   24 |   0.8114 |     26.424 |   0.9531 |     30.602 |     4.7
   25 |   0.7791 |     24.865 |   0.9582 |     31.192 |     4.9
   26 |   0.7505 |     24.237 |   0.9307 |     30.043 |     5.1
   27 |   0.7221 |     23.014 |   0.9232 |     29.361 |     5.3
   28 |   0.6895 |     21.802 |   0.9227 |     29.361 |     5.5
   29 |   0.6689 |     21.350 |   0.9174 |     28.709 |     5.7
   30 |   0.6393 |     19.758 |   0.9317 |     29.019 |     5.9
   31 |   0.6156 |     19.377 |   0.9163 |     28.150 |     6.1
   32 |   0.5915 |     18.584 |   0.9133 |     27.902 |     6.3
   33 |   0.5661 |     17.433 |   0.9155 |     27.623 |     6.5
   34 |   0.5370 |     16.606 |   0.9229 |     27.374 |     6.7
   35 |   0.5130 |     15.719 |   0.9115 |     26.878 |     6.9
   36 |   0.4971 |     15.300 |   0.9510 |     28.026 |     7.1
   37 |   0.4800 |     14.981 |   0.9454 |     27.095 |     7.3
   38 |   0.4590 |     14.077 |   0.9381 |     27.126 |     7.5
   39 |   0.4402 |     13.548 |   0.9467 |     26.971 |     7.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,495,009

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1551 |     58.628 |   1.6150 |     47.083 |     0.1
    2 |   1.4732 |     45.824 |   1.4016 |     46.989 |     0.1
    3 |   1.3703 |     45.339 |   1.3455 |     46.648 |     0.2
    4 |   1.3215 |     44.490 |   1.3100 |     45.376 |     0.2
    5 |   1.2945 |     44.259 |   1.2822 |     45.872 |     0.3
    6 |   1.2722 |     43.978 |   1.2690 |     46.151 |     0.4
    7 |   1.2535 |     43.758 |   1.2485 |     45.003 |     0.4
    8 |   1.2313 |     42.948 |   1.2297 |     43.451 |     0.5
    9 |   1.2087 |     42.413 |   1.2026 |     42.831 |     0.6
   10 |   1.1875 |     41.620 |   1.1892 |     42.706 |     0.6
   11 |   1.1643 |     40.832 |   1.1600 |     40.689 |     0.7
   12 |   1.1374 |     39.675 |   1.1465 |     40.192 |     0.7
   13 |   1.1093 |     38.821 |   1.1102 |     38.268 |     0.8
   14 |   1.0712 |     36.926 |   1.0903 |     38.144 |     0.9
   15 |   1.0378 |     35.421 |   1.0671 |     36.685 |     0.9
   16 |   1.0030 |     33.879 |   1.0205 |     34.916 |     1.0
   17 |   0.9606 |     32.645 |   0.9953 |     33.333 |     1.1
   18 |   0.9164 |     30.369 |   0.9777 |     32.868 |     1.1
   19 |   0.8831 |     29.433 |   0.9636 |     31.657 |     1.2
   20 |   0.8376 |     27.669 |   0.9433 |     30.137 |     1.2
   21 |   0.7918 |     26.105 |   0.9242 |     30.012 |     1.3
   22 |   0.7494 |     24.275 |   0.9182 |     29.826 |     1.4
   23 |   0.7123 |     23.185 |   0.9009 |     29.516 |     1.4
   24 |   0.6658 |     21.455 |   0.8936 |     29.081 |     1.5
   25 |   0.6453 |     20.865 |   0.8949 |     28.399 |     1.6
   26 |   0.5924 |     18.953 |   0.9003 |     28.150 |     1.6
   27 |   0.5507 |     17.322 |   0.9039 |     27.654 |     1.7
   28 |   0.5127 |     16.397 |   0.9041 |     27.809 |     1.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,985,185

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1985 |     59.179 |   1.6156 |     47.083 |     0.1
    2 |   1.4798 |     45.917 |   1.4274 |     47.083 |     0.2
    3 |   1.4053 |     45.884 |   1.4064 |     47.083 |     0.2
    4 |   1.3904 |     45.934 |   1.3876 |     47.083 |     0.3
    5 |   1.3713 |     45.730 |   1.3925 |     47.145 |     0.4
    6 |   1.3430 |     45.311 |   1.3301 |     46.524 |     0.5
    7 |   1.3119 |     45.008 |   1.3102 |     46.927 |     0.6
    8 |   1.2928 |     44.771 |   1.2871 |     46.307 |     0.7
    9 |   1.2749 |     44.590 |   1.2711 |     45.345 |     0.7
   10 |   1.2546 |     44.072 |   1.2494 |     45.779 |     0.8
   11 |   1.2391 |     43.813 |   1.2392 |     45.127 |     0.9
   12 |   1.2220 |     43.322 |   1.2156 |     44.910 |     1.0
   13 |   1.2085 |     43.190 |   1.2065 |     43.855 |     1.1
   14 |   1.1885 |     42.347 |   1.1886 |     43.513 |     1.1
   15 |   1.1756 |     41.747 |   1.1860 |     43.234 |     1.2
   16 |   1.1651 |     41.840 |   1.1696 |     42.024 |     1.3
   17 |   1.1486 |     40.898 |   1.1615 |     41.465 |     1.4
   18 |   1.1391 |     40.584 |   1.1436 |     41.061 |     1.5
   19 |   1.1232 |     40.000 |   1.1349 |     40.503 |     1.5
   20 |   1.1105 |     39.570 |   1.1262 |     40.596 |     1.6
   21 |   1.0969 |     39.185 |   1.1170 |     40.410 |     1.7
   22 |   1.0827 |     38.782 |   1.1114 |     40.596 |     1.8
   23 |   1.0754 |     38.347 |   1.0946 |     39.603 |     1.9
   24 |   1.0540 |     37.444 |   1.0916 |     39.385 |     1.9
   25 |   1.0406 |     36.970 |   1.0819 |     38.268 |     2.0
   26 |   1.0262 |     36.854 |   1.0753 |     38.144 |     2.1
   27 |   1.0103 |     36.253 |   1.0676 |     37.430 |     2.2
   28 |   0.9973 |     35.421 |   1.0591 |     37.803 |     2.3
   29 |   0.9786 |     35.091 |   1.0446 |     36.840 |     2.3
   30 |   0.9660 |     34.446 |   1.0436 |     36.965 |     2.4
   31 |   0.9498 |     33.719 |   1.0326 |     36.034 |     2.5
   32 |   0.9290 |     32.672 |   1.0160 |     35.133 |     2.6
   33 |   0.9148 |     31.983 |   1.0112 |     34.854 |     2.7
   34 |   0.8966 |     31.410 |   0.9967 |     33.520 |     2.7
   35 |   0.8752 |     30.722 |   0.9932 |     34.016 |     2.8
   36 |   0.8597 |     30.061 |   0.9926 |     33.923 |     2.9
   37 |   0.8404 |     29.559 |   0.9879 |     32.992 |     3.0
   38 |   0.8234 |     28.755 |   0.9837 |     32.744 |     3.1
   39 |   0.8053 |     28.077 |   0.9852 |     32.744 |     3.2
   40 |   0.7865 |     27.455 |   0.9777 |     32.433 |     3.2
   41 |   0.7684 |     26.843 |   0.9655 |     31.875 |     3.3
   42 |   0.7430 |     25.499 |   0.9635 |     30.850 |     3.4
   43 |   0.7214 |     24.683 |   0.9606 |     31.068 |     3.5
   44 |   0.6981 |     23.565 |   0.9531 |     30.385 |     3.6
   45 |   0.6837 |     22.882 |   0.9515 |     30.323 |     3.6
   46 |   0.6785 |     22.986 |   0.9517 |     30.416 |     3.7
   47 |   0.6386 |     21.383 |   0.9367 |     28.833 |     3.8
   48 |   0.6109 |     20.391 |   0.9488 |     29.205 |     3.9
   49 |   0.5907 |     19.537 |   0.9373 |     28.926 |     4.0
   50 |   0.5697 |     19.118 |   0.9546 |     28.647 |     4.1
   51 |   0.5508 |     18.039 |   0.9458 |     28.119 |     4.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,691,041

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5587 |     67.917 |   2.0390 |     59.404 |     0.1
    2 |   1.8209 |     51.802 |   1.6166 |     47.114 |     0.2
    3 |   1.5222 |     45.884 |   1.4703 |     47.114 |     0.2
    4 |   1.4360 |     45.840 |   1.4176 |     47.083 |     0.3
    5 |   1.3953 |     45.807 |   1.3795 |     47.083 |     0.4
    6 |   1.3620 |     45.581 |   1.3557 |     46.710 |     0.5
    7 |   1.3395 |     45.355 |   1.3298 |     45.996 |     0.6
    8 |   1.3198 |     44.799 |   1.3135 |     46.958 |     0.6
    9 |   1.3029 |     44.749 |   1.2982 |     46.151 |     0.7
   10 |   1.2858 |     44.567 |   1.2795 |     45.841 |     0.8
   11 |   1.2703 |     44.039 |   1.2661 |     45.562 |     0.9
   12 |   1.2567 |     43.912 |   1.2473 |     44.724 |     1.0
   13 |   1.2448 |     43.471 |   1.2408 |     44.662 |     1.1
   14 |   1.2345 |     42.953 |   1.2320 |     43.762 |     1.1
   15 |   1.2248 |     42.843 |   1.2223 |     43.265 |     1.2
   16 |   1.2162 |     42.375 |   1.2186 |     43.762 |     1.3
   17 |   1.2122 |     42.562 |   1.2102 |     43.234 |     1.4
   18 |   1.2053 |     42.264 |   1.2069 |     43.296 |     1.5
   19 |   1.1987 |     42.105 |   1.1999 |     43.265 |     1.5
   20 |   1.1938 |     42.099 |   1.1949 |     42.862 |     1.6
   21 |   1.1892 |     42.105 |   1.1900 |     43.048 |     1.7
   22 |   1.1821 |     41.747 |   1.1824 |     42.675 |     1.8
   23 |   1.1792 |     41.631 |   1.1759 |     42.986 |     1.9
   24 |   1.1730 |     41.609 |   1.1800 |     42.396 |     1.9
   25 |   1.1657 |     41.427 |   1.1667 |     42.055 |     2.0
   26 |   1.1619 |     41.273 |   1.1639 |     42.024 |     2.1
   27 |   1.1574 |     41.273 |   1.1535 |     42.334 |     2.2
   28 |   1.1528 |     41.218 |   1.1496 |     41.620 |     2.3
   29 |   1.1454 |     40.716 |   1.1473 |     41.837 |     2.4
   30 |   1.1392 |     40.562 |   1.1472 |     41.775 |     2.4
   31 |   1.1361 |     40.479 |   1.1438 |     41.806 |     2.5
   32 |   1.1315 |     40.182 |   1.1349 |     40.844 |     2.6
   33 |   1.1234 |     39.901 |   1.1286 |     40.906 |     2.7
   34 |   1.1198 |     39.978 |   1.1305 |     41.248 |     2.8
   35 |   1.1158 |     39.824 |   1.1185 |     40.627 |     2.8
   36 |   1.1115 |     39.477 |   1.1263 |     41.589 |     2.9
   37 |   1.1061 |     39.460 |   1.1226 |     41.030 |     3.0
   38 |   1.1041 |     39.449 |   1.1189 |     41.092 |     3.1
   39 |   1.0964 |     39.196 |   1.1107 |     40.565 |     3.2
   40 |   1.0933 |     39.267 |   1.1175 |     40.627 |     3.2
   41 |   1.0868 |     38.876 |   1.1084 |     40.503 |     3.3
   42 |   1.0842 |     38.733 |   1.1020 |     39.975 |     3.4
   43 |   1.0803 |     38.529 |   1.1021 |     40.472 |     3.5
   44 |   1.0778 |     38.529 |   1.1014 |     40.286 |     3.6
   45 |   1.0723 |     38.435 |   1.0992 |     40.441 |     3.7
   46 |   1.0674 |     38.209 |   1.1006 |     39.944 |     3.7
   47 |   1.0643 |     37.901 |   1.0956 |     39.510 |     3.8
   48 |   1.0572 |     37.708 |   1.0884 |     39.261 |     3.9
   49 |   1.0573 |     37.741 |   1.0851 |     39.417 |     4.0
   50 |   1.0524 |     37.625 |   1.0856 |     39.106 |     4.1
   51 |   1.0458 |     37.394 |   1.0795 |     39.044 |     4.1
   52 |   1.0425 |     37.107 |   1.0801 |     39.230 |     4.2
   53 |   1.0373 |     37.267 |   1.0752 |     39.013 |     4.3
   54 |   1.0339 |     36.959 |   1.0844 |     39.075 |     4.4
   55 |   1.0307 |     37.129 |   1.0681 |     38.579 |     4.5
   56 |   1.0235 |     36.518 |   1.0641 |     38.113 |     4.6
   57 |   1.0170 |     36.298 |   1.0638 |     38.175 |     4.6
   58 |   1.0145 |     36.072 |   1.0598 |     37.741 |     4.7
   59 |   1.0081 |     35.890 |   1.0524 |     37.678 |     4.8
   60 |   1.0062 |     35.989 |   1.0536 |     37.399 |     4.9
   61 |   1.0025 |     35.675 |   1.0468 |     36.778 |     5.0
   62 |   0.9965 |     35.201 |   1.0477 |     37.337 |     5.0
   63 |   0.9865 |     35.416 |   1.0412 |     36.623 |     5.1
   64 |   0.9875 |     35.113 |   1.0345 |     36.623 |     5.2
   65 |   0.9808 |     34.766 |   1.0373 |     37.027 |     5.3
   66 |   0.9765 |     34.678 |   1.0376 |     37.182 |     5.4
   67 |   0.9755 |     34.573 |   1.0308 |     36.499 |     5.5
   68 |   0.9687 |     34.529 |   1.0294 |     35.568 |     5.5
   69 |   0.9656 |     34.369 |   1.0266 |     35.382 |     5.6
   70 |   0.9592 |     33.912 |   1.0196 |     35.878 |     5.7
   71 |   0.9523 |     33.846 |   1.0182 |     35.506 |     5.8
   72 |   0.9518 |     33.455 |   1.0212 |     35.754 |     5.9
   73 |   0.9402 |     33.168 |   1.0164 |     35.320 |     5.9
   74 |   0.9405 |     33.328 |   1.0106 |     34.978 |     6.0
   75 |   0.9355 |     33.036 |   1.0065 |     35.164 |     6.1
   76 |   0.9255 |     32.667 |   1.0131 |     34.730 |     6.2
   77 |   0.9258 |     32.623 |   0.9961 |     34.978 |     6.3
   78 |   0.9223 |     32.496 |   1.0015 |     34.699 |     6.4
   79 |   0.9187 |     32.314 |   0.9965 |     33.892 |     6.4
   80 |   0.9144 |     32.529 |   1.0037 |     34.451 |     6.5
   81 |   0.9125 |     32.160 |   1.0043 |     34.947 |     6.6
   82 |   0.9028 |     31.719 |   0.9863 |     34.637 |     6.7
   83 |   0.9013 |     31.697 |   0.9861 |     33.985 |     6.8
   84 |   0.8937 |     31.223 |   0.9948 |     34.451 |     6.8
   85 |   0.8894 |     31.532 |   0.9851 |     34.327 |     6.9
   86 |   0.8881 |     31.399 |   0.9928 |     33.985 |     7.0
   87 |   0.8739 |     30.490 |   0.9817 |     33.830 |     7.1
   88 |   0.8737 |     30.771 |   0.9746 |     33.333 |     7.2
   89 |   0.8625 |     30.116 |   0.9799 |     33.333 |     7.3
   90 |   0.8607 |     30.298 |   0.9784 |     33.333 |     7.3
   91 |   0.8557 |     29.829 |   0.9731 |     33.333 |     7.4
   92 |   0.8533 |     29.917 |   0.9773 |     33.613 |     7.5
   93 |   1.1286 |     40.408 |   1.1130 |     41.341 |     7.6
   94 |   1.0859 |     40.033 |   1.1109 |     40.875 |     7.7
   95 |   1.0836 |     39.824 |   1.0790 |     38.827 |     7.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,098,465

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1362 |     58.121 |   1.5482 |     47.083 |     0.1
    2 |   1.4344 |     45.917 |   1.3968 |     47.083 |     0.1
    3 |   1.3609 |     45.096 |   1.3542 |     46.276 |     0.2
    4 |   1.3308 |     45.146 |   1.3341 |     47.176 |     0.3
    5 |   1.3070 |     44.479 |   1.3061 |     46.089 |     0.4
    6 |   1.2825 |     44.292 |   1.2833 |     46.338 |     0.4
    7 |   1.2655 |     44.022 |   1.2623 |     45.500 |     0.5
    8 |   1.2497 |     43.548 |   1.2543 |     45.065 |     0.6
    9 |   1.2335 |     43.074 |   1.2285 |     43.700 |     0.7
   10 |   1.2135 |     42.590 |   1.2051 |     43.544 |     0.7
   11 |   1.1907 |     41.526 |   1.1892 |     42.551 |     0.8
   12 |   1.1709 |     41.223 |   1.1778 |     42.210 |     0.9
   13 |   1.1477 |     40.628 |   1.1597 |     41.744 |     1.0
   14 |   1.1252 |     39.609 |   1.1419 |     40.410 |     1.0
   15 |   1.0954 |     38.402 |   1.1121 |     39.634 |     1.1
   16 |   1.0681 |     37.102 |   1.0950 |     38.268 |     1.2
   17 |   1.0385 |     35.857 |   1.0794 |     37.120 |     1.3
   18 |   1.0091 |     34.573 |   1.0556 |     35.692 |     1.3
   19 |   0.9804 |     33.725 |   1.0319 |     35.258 |     1.4
   20 |   0.9532 |     32.612 |   1.0386 |     34.916 |     1.5
   21 |   0.9266 |     31.482 |   1.0061 |     34.575 |     1.6
   22 |   0.8923 |     30.242 |   1.0070 |     33.675 |     1.6
   23 |   0.8637 |     29.410 |   0.9880 |     33.830 |     1.7
   24 |   0.8254 |     27.691 |   0.9883 |     33.271 |     1.8
   25 |   0.7921 |     26.612 |   0.9743 |     32.371 |     1.8
   26 |   0.7636 |     25.311 |   0.9794 |     32.216 |     1.9
   27 |   0.7334 |     24.441 |   0.9885 |     32.216 |     2.0
   28 |   0.7001 |     23.146 |   0.9508 |     30.447 |     2.1
   29 |   0.6662 |     21.669 |   0.9713 |     30.571 |     2.1
   30 |   0.6332 |     20.479 |   0.9689 |     29.857 |     2.2
   31 |   0.6019 |     19.416 |   0.9772 |     29.764 |     2.3
   32 |   0.5775 |     18.760 |   0.9853 |     30.168 |     2.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 472,161

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0763 |     56.402 |   1.5067 |     46.741 |     0.0
    2 |   1.4186 |     45.482 |   1.3703 |     46.741 |     0.0
    3 |   1.3338 |     44.843 |   1.3042 |     45.624 |     0.1
    4 |   1.2869 |     44.127 |   1.2765 |     45.189 |     0.1
    5 |   1.2593 |     43.829 |   1.2479 |     45.686 |     0.1
    6 |   1.2360 |     43.223 |   1.2296 |     44.569 |     0.1
    7 |   1.2127 |     42.667 |   1.2053 |     43.079 |     0.1
    8 |   1.1840 |     41.427 |   1.1790 |     42.241 |     0.1
    9 |   1.1525 |     40.237 |   1.1425 |     40.068 |     0.2
   10 |   1.1191 |     38.722 |   1.1170 |     38.423 |     0.2
   11 |   1.0833 |     37.675 |   1.0819 |     37.461 |     0.2
   12 |   1.0499 |     36.298 |   1.0640 |     36.778 |     0.2
   13 |   1.0154 |     35.140 |   1.0410 |     35.351 |     0.2
   14 |   0.9849 |     33.802 |   1.0213 |     34.947 |     0.2
   15 |   0.9511 |     32.380 |   1.0089 |     34.327 |     0.3
   16 |   0.9169 |     30.964 |   0.9757 |     33.271 |     0.3
   17 |   0.8778 |     29.433 |   0.9618 |     31.937 |     0.3
   18 |   0.8456 |     28.331 |   0.9438 |     31.533 |     0.3
   19 |   0.8065 |     26.882 |   0.9317 |     30.664 |     0.3
   20 |   0.7772 |     25.421 |   0.9133 |     30.137 |     0.4
   21 |   0.7386 |     24.309 |   0.9087 |     29.950 |     0.4
   22 |   0.7057 |     23.317 |   0.9104 |     30.137 |     0.4
   23 |   0.6774 |     22.165 |   0.8980 |     28.336 |     0.4
   24 |   0.6515 |     21.212 |   0.9218 |     29.174 |     0.4
   25 |   0.6190 |     20.215 |   0.8739 |     26.878 |     0.4
   26 |   0.5886 |     18.837 |   0.9009 |     27.840 |     0.5
   27 |   0.5599 |     17.934 |   0.8775 |     26.505 |     0.5
   28 |   0.5338 |     16.909 |   0.9086 |     26.971 |     0.5
   29 |   0.5117 |     16.540 |   0.9000 |     26.474 |     0.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 904,161

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2134 |     61.322 |   1.5927 |     47.083 |     0.2
    2 |   1.4651 |     45.928 |   1.4113 |     47.083 |     0.4
    3 |   1.3833 |     45.774 |   1.3793 |     46.617 |     0.5
    4 |   1.3486 |     45.416 |   1.3407 |     46.803 |     0.7
    5 |   1.3230 |     45.074 |   1.3235 |     46.524 |     0.9
    6 |   1.3076 |     44.656 |   1.3101 |     46.679 |     1.1
    7 |   1.2912 |     44.512 |   1.2925 |     45.345 |     1.2
    8 |   1.2765 |     44.402 |   1.2836 |     46.245 |     1.4
    9 |   1.2645 |     44.099 |   1.2680 |     45.096 |     1.6
   10 |   1.2501 |     43.912 |   1.2523 |     44.910 |     1.8
   11 |   1.2333 |     43.229 |   1.2314 |     44.413 |     1.9
   12 |   1.2144 |     42.452 |   1.2148 |     43.793 |     2.1
   13 |   1.1936 |     41.901 |   1.2005 |     43.203 |     2.3
   14 |   1.1717 |     41.168 |   1.1812 |     42.303 |     2.5
   15 |   1.1488 |     40.402 |   1.1696 |     42.148 |     2.6
   16 |   1.1228 |     39.521 |   1.1481 |     40.937 |     2.8
   17 |   1.0944 |     38.248 |   1.1234 |     40.006 |     3.0
   18 |   1.0683 |     37.135 |   1.1034 |     39.106 |     3.2
   19 |   1.0385 |     36.050 |   1.0985 |     38.268 |     3.3
   20 |   1.0098 |     34.755 |   1.0674 |     37.430 |     3.5
   21 |   0.9802 |     33.719 |   1.0496 |     36.034 |     3.7
   22 |   0.9451 |     32.496 |   1.0360 |     35.878 |     3.9
   23 |   0.9161 |     31.135 |   1.0140 |     35.009 |     4.0
   24 |   0.8812 |     29.813 |   1.0155 |     34.451 |     4.2
   25 |   0.8527 |     28.430 |   0.9928 |     32.744 |     4.4
   26 |   0.8223 |     27.311 |   0.9987 |     32.619 |     4.6
   27 |   0.7904 |     26.099 |   0.9749 |     31.719 |     4.7
   28 |   0.7595 |     25.234 |   0.9807 |     32.464 |     4.9
   29 |   0.7305 |     24.110 |   0.9748 |     30.788 |     5.1
   30 |   0.7048 |     23.008 |   0.9689 |     30.385 |     5.3
   31 |   0.6768 |     22.336 |   0.9679 |     30.664 |     5.4
   32 |   0.6479 |     21.245 |   0.9821 |     29.826 |     5.6
   33 |   0.6226 |     20.358 |   0.9791 |     30.168 |     5.8
   34 |   0.6030 |     19.664 |   0.9935 |     29.857 |     6.0
   35 |   0.5831 |     18.804 |   0.9912 |     29.205 |     6.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,281,505

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5100 |     68.253 |   1.9847 |     58.721 |     0.1
    2 |   1.7449 |     48.667 |   1.5669 |     47.114 |     0.1
    3 |   1.4808 |     45.939 |   1.4423 |     47.083 |     0.2
    4 |   1.4027 |     45.846 |   1.3860 |     47.331 |     0.2
    5 |   1.3614 |     45.631 |   1.3571 |     47.020 |     0.3
    6 |   1.3308 |     44.986 |   1.3236 |     46.182 |     0.3
    7 |   1.3040 |     44.601 |   1.2936 |     45.717 |     0.4
    8 |   1.2794 |     43.884 |   1.2713 |     45.313 |     0.5
    9 |   1.2564 |     43.300 |   1.2556 |     44.475 |     0.5
   10 |   1.2393 |     42.887 |   1.2383 |     44.010 |     0.6
   11 |   1.2271 |     42.727 |   1.2328 |     44.134 |     0.6
   12 |   1.2145 |     42.413 |   1.2246 |     43.979 |     0.7
   13 |   1.2045 |     42.402 |   1.2058 |     43.358 |     0.7
   14 |   1.1945 |     42.033 |   1.1974 |     43.575 |     0.8
   15 |   1.1861 |     41.939 |   1.1897 |     43.203 |     0.9
   16 |   1.1745 |     41.543 |   1.1806 |     41.962 |     0.9
   17 |   1.1662 |     41.333 |   1.1780 |     42.365 |     1.0
   18 |   1.1570 |     40.661 |   1.1759 |     42.241 |     1.0
   19 |   1.1498 |     40.650 |   1.1560 |     40.751 |     1.1
   20 |   1.1382 |     40.165 |   1.1503 |     40.999 |     1.1
   21 |   1.1309 |     40.022 |   1.1527 |     40.565 |     1.2
   22 |   1.1249 |     39.642 |   1.1524 |     41.248 |     1.3
   23 |   1.1142 |     39.163 |   1.1357 |     40.317 |     1.3
   24 |   1.1085 |     39.256 |   1.1280 |     39.758 |     1.4
   25 |   1.1009 |     38.970 |   1.1205 |     39.882 |     1.4
   26 |   1.0932 |     38.634 |   1.1213 |     40.255 |     1.5
   27 |   1.0867 |     38.485 |   1.1070 |     38.920 |     1.5
   28 |   1.0777 |     37.945 |   1.1100 |     40.037 |     1.6
   29 |   1.0701 |     37.697 |   1.1096 |     40.223 |     1.7
   30 |   1.0628 |     37.488 |   1.0997 |     39.448 |     1.7
   31 |   1.0547 |     36.893 |   1.0913 |     38.206 |     1.8
   32 |   1.0448 |     36.771 |   1.0866 |     38.641 |     1.8
   33 |   1.0374 |     36.402 |   1.0814 |     38.175 |     1.9
   34 |   1.0295 |     36.099 |   1.0778 |     37.896 |     1.9
   35 |   1.0222 |     35.802 |   1.0698 |     38.113 |     2.0
   36 |   1.0121 |     35.355 |   1.0621 |     38.082 |     2.1
   37 |   1.0021 |     34.826 |   1.0563 |     36.872 |     2.1
   38 |   0.9909 |     34.281 |   1.0492 |     36.499 |     2.2
   39 |   0.9821 |     34.044 |   1.0528 |     36.934 |     2.2
   40 |   0.9743 |     33.658 |   1.0426 |     36.468 |     2.3
   41 |   0.9622 |     33.273 |   1.0313 |     35.816 |     2.3
   42 |   0.9538 |     32.848 |   1.0369 |     35.971 |     2.4
   43 |   0.9435 |     32.799 |   1.0312 |     36.375 |     2.5
   44 |   0.9341 |     32.154 |   1.0310 |     35.878 |     2.5
   45 |   0.9239 |     31.895 |   1.0287 |     36.189 |     2.6
   46 |   0.9128 |     31.642 |   1.0225 |     35.227 |     2.6
   47 |   0.9070 |     31.416 |   1.0192 |     35.258 |     2.7
   48 |   0.8942 |     30.760 |   1.0269 |     35.196 |     2.8
   49 |   0.8882 |     30.523 |   1.0090 |     34.823 |     2.8
   50 |   0.8763 |     30.066 |   1.0162 |     34.978 |     2.9
   51 |   0.8607 |     29.372 |   1.0160 |     35.040 |     2.9
   52 |   0.8586 |     29.311 |   1.0132 |     34.978 |     3.0
   53 |   0.8416 |     28.540 |   1.0014 |     33.830 |     3.0
   54 |   0.8308 |     28.061 |   1.0003 |     33.737 |     3.1
   55 |   0.8167 |     27.488 |   1.0045 |     34.109 |     3.2
   56 |   0.8010 |     26.959 |   0.9945 |     32.806 |     3.2
   57 |   0.7849 |     26.099 |   0.9964 |     33.364 |     3.3
   58 |   0.7761 |     25.978 |   1.0110 |     33.520 |     3.3
   59 |   0.7614 |     25.124 |   0.9877 |     31.937 |     3.4
   60 |   0.7506 |     24.804 |   0.9953 |     32.340 |     3.5
   61 |   0.7355 |     24.110 |   1.0003 |     32.433 |     3.5
   62 |   0.7237 |     23.730 |   1.0029 |     32.185 |     3.6
   63 |   0.7058 |     23.245 |   0.9880 |     31.968 |     3.6
   64 |   0.6918 |     22.468 |   0.9863 |     31.347 |     3.7
   65 |   0.6775 |     21.912 |   0.9928 |     31.099 |     3.7
   66 |   0.6691 |     21.537 |   0.9830 |     30.416 |     3.8
   67 |   0.6535 |     21.069 |   1.0008 |     30.788 |     3.9
   68 |   0.6411 |     20.733 |   0.9948 |     31.068 |     3.9
   69 |   0.6344 |     20.617 |   0.9965 |     29.795 |     4.0
   70 |   0.6096 |     19.449 |   1.0139 |     30.633 |     4.0
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 305,697

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4871 |     66.474 |   1.8869 |     52.297 |     0.0
    2 |   1.6688 |     48.264 |   1.5081 |     47.083 |     0.0
    3 |   1.4581 |     45.879 |   1.4307 |     47.083 |     0.1
    4 |   1.4094 |     45.851 |   1.3983 |     47.083 |     0.1
    5 |   1.3824 |     45.857 |   1.3772 |     47.083 |     0.1
    6 |   1.3583 |     45.438 |   1.3551 |     46.834 |     0.1
    7 |   1.3358 |     45.603 |   1.3289 |     46.741 |     0.1
    8 |   1.3175 |     45.036 |   1.3141 |     46.741 |     0.1
    9 |   1.3004 |     44.722 |   1.2929 |     45.717 |     0.2
   10 |   1.2834 |     44.490 |   1.2744 |     45.593 |     0.2
   11 |   1.2661 |     43.802 |   1.2619 |     45.220 |     0.2
   12 |   1.2539 |     43.598 |   1.2520 |     44.662 |     0.2
   13 |   1.2441 |     43.477 |   1.2458 |     44.693 |     0.2
   14 |   1.2337 |     43.063 |   1.2333 |     44.382 |     0.2
   15 |   1.2249 |     42.826 |   1.2276 |     44.165 |     0.3
   16 |   1.2159 |     42.716 |   1.2148 |     43.451 |     0.3
   17 |   1.2074 |     42.198 |   1.2148 |     43.637 |     0.3
   18 |   1.2020 |     42.402 |   1.2040 |     42.831 |     0.3
   19 |   1.1928 |     41.879 |   1.1940 |     42.831 |     0.3
   20 |   1.1850 |     41.686 |   1.1959 |     42.675 |     0.3
   21 |   1.1793 |     41.603 |   1.1816 |     42.396 |     0.4
   22 |   1.1713 |     41.080 |   1.1833 |     42.800 |     0.4
   23 |   1.1642 |     41.234 |   1.1731 |     42.427 |     0.4
   24 |   1.1565 |     40.948 |   1.1717 |     42.365 |     0.4
   25 |   1.1470 |     40.325 |   1.1671 |     42.024 |     0.4
   26 |   1.1418 |     40.160 |   1.1593 |     41.713 |     0.4
   27 |   1.1345 |     40.088 |   1.1588 |     41.682 |     0.5
   28 |   1.1290 |     40.083 |   1.1516 |     41.682 |     0.5
   29 |   1.1232 |     39.939 |   1.1447 |     41.465 |     0.5
   30 |   1.1200 |     39.603 |   1.1416 |     41.248 |     0.5
   31 |   1.1153 |     39.603 |   1.1306 |     41.061 |     0.5
   32 |   1.1068 |     39.416 |   1.1267 |     40.689 |     0.6
   33 |   1.1035 |     39.278 |   1.1292 |     40.472 |     0.6
   34 |   1.0987 |     38.981 |   1.1293 |     40.286 |     0.6
   35 |   1.0931 |     39.052 |   1.1179 |     40.130 |     0.6
   36 |   1.0877 |     38.683 |   1.1175 |     40.286 |     0.6
   37 |   1.0854 |     38.815 |   1.1228 |     40.068 |     0.6
   38 |   1.0820 |     38.562 |   1.1150 |     40.099 |     0.7
   39 |   1.0784 |     38.584 |   1.1130 |     39.820 |     0.7
   40 |   1.0730 |     38.656 |   1.1033 |     39.665 |     0.7
   41 |   1.0683 |     38.331 |   1.1050 |     39.417 |     0.7
   42 |   1.0629 |     37.983 |   1.0949 |     39.044 |     0.7
   43 |   1.0566 |     37.691 |   1.0952 |     39.665 |     0.7
   44 |   1.0549 |     37.515 |   1.0932 |     39.572 |     0.8
   45 |   1.0513 |     37.846 |   1.0960 |     39.789 |     0.8
   46 |   1.0511 |     37.879 |   1.0934 |     38.579 |     0.8
   47 |   1.0461 |     37.284 |   1.0900 |     39.323 |     0.8
   48 |   1.0401 |     37.344 |   1.0840 |     38.392 |     0.8
   49 |   1.0359 |     36.948 |   1.0749 |     38.361 |     0.9
   50 |   1.0306 |     37.036 |   1.0813 |     38.796 |     0.9
   51 |   1.0299 |     36.650 |   1.0721 |     38.051 |     0.9
   52 |   1.0252 |     36.815 |   1.0746 |     37.554 |     0.9
   53 |   1.0178 |     36.523 |   1.0656 |     37.865 |     0.9
   54 |   1.0147 |     36.353 |   1.0684 |     37.709 |     0.9
   55 |   1.0118 |     35.928 |   1.0695 |     37.306 |     1.0
   56 |   1.0111 |     36.116 |   1.0629 |     36.623 |     1.0
   57 |   1.0084 |     36.050 |   1.0620 |     37.772 |     1.0
   58 |   1.0014 |     35.482 |   1.0615 |     37.430 |     1.0
   59 |   0.9977 |     35.680 |   1.0665 |     37.213 |     1.0
   60 |   0.9958 |     35.741 |   1.0615 |     37.585 |     1.0
   61 |   0.9915 |     35.278 |   1.0559 |     36.468 |     1.1
   62 |   0.9905 |     35.548 |   1.0458 |     36.996 |     1.1
   63 |   0.9863 |     35.058 |   1.0524 |     37.492 |     1.1
   64 |   0.9839 |     35.118 |   1.0518 |     36.561 |     1.1
   65 |   0.9768 |     34.766 |   1.0449 |     36.530 |     1.1
   66 |   0.9764 |     34.876 |   1.0451 |     36.623 |     1.2
   67 |   0.9701 |     34.457 |   1.0462 |     36.530 |     1.2
   68 |   0.9645 |     34.358 |   1.0422 |     35.971 |     1.2
   69 |   0.9617 |     33.851 |   1.0358 |     36.158 |     1.2
   70 |   0.9585 |     33.802 |   1.0424 |     36.034 |     1.2
   71 |   0.9570 |     33.796 |   1.0318 |     35.320 |     1.2
   72 |   0.9539 |     33.614 |   1.0430 |     36.561 |     1.3
   73 |   0.9507 |     33.399 |   1.0333 |     35.102 |     1.3
   74 |   0.9428 |     33.289 |   1.0298 |     35.599 |     1.3
   75 |   0.9407 |     33.179 |   1.0327 |     36.220 |     1.3
   76 |   0.9351 |     32.876 |   1.0259 |     35.320 |     1.3
   77 |   0.9362 |     32.782 |   1.0257 |     35.630 |     1.3
   78 |   0.9303 |     32.430 |   1.0328 |     35.723 |     1.4
   79 |   0.9270 |     32.314 |   1.0270 |     35.351 |     1.4
   80 |   0.9158 |     32.121 |   1.0236 |     35.196 |     1.4
   81 |   0.9171 |     32.154 |   1.0209 |     35.164 |     1.4
   82 |   0.9184 |     32.149 |   1.0316 |     35.630 |     1.4
   83 |   0.9133 |     31.950 |   1.0136 |     34.730 |     1.5
   84 |   0.9028 |     31.438 |   1.0184 |     34.978 |     1.5
   85 |   0.9008 |     30.981 |   1.0090 |     34.761 |     1.5
   86 |   0.8946 |     31.322 |   1.0034 |     34.482 |     1.5
   87 |   0.8930 |     30.909 |   1.0092 |     34.575 |     1.5
   88 |   0.8880 |     30.975 |   1.0081 |     34.202 |     1.5
   89 |   0.8794 |     30.733 |   1.0055 |     34.544 |     1.6
   90 |   0.8793 |     30.584 |   1.0098 |     34.513 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,887,969

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0311 |     55.234 |   1.4923 |     47.020 |     0.1
    2 |   1.3969 |     45.300 |   1.3548 |     46.245 |     0.2
    3 |   1.3127 |     44.231 |   1.3002 |     45.407 |     0.2
    4 |   1.2708 |     43.879 |   1.2599 |     44.972 |     0.3
    5 |   1.2420 |     43.405 |   1.2264 |     43.482 |     0.4
    6 |   1.2163 |     42.821 |   1.2076 |     43.544 |     0.5
    7 |   1.1940 |     41.967 |   1.1855 |     41.558 |     0.5
    8 |   1.1727 |     41.355 |   1.1682 |     41.806 |     0.6
    9 |   1.1514 |     40.876 |   1.1420 |     41.030 |     0.7
   10 |   1.1322 |     40.099 |   1.1247 |     39.944 |     0.8
   11 |   1.1082 |     39.096 |   1.1171 |     40.192 |     0.8
   12 |   1.0882 |     38.220 |   1.1035 |     40.161 |     0.9
   13 |   1.0670 |     37.251 |   1.0856 |     38.454 |     1.0
   14 |   1.0500 |     37.036 |   1.0826 |     38.579 |     1.1
   15 |   1.0366 |     36.281 |   1.0688 |     37.678 |     1.1
   16 |   1.0194 |     35.917 |   1.0524 |     37.709 |     1.2
   17 |   1.0019 |     35.058 |   1.0419 |     37.306 |     1.3
   18 |   0.9839 |     34.364 |   1.0200 |     36.282 |     1.4
   19 |   0.9643 |     34.011 |   1.0096 |     35.475 |     1.4
   20 |   0.9472 |     33.052 |   0.9936 |     35.382 |     1.5
   21 |   0.9315 |     32.645 |   0.9874 |     34.482 |     1.6
   22 |   0.9171 |     32.127 |   0.9897 |     34.264 |     1.7
   23 |   0.9000 |     31.339 |   0.9638 |     33.147 |     1.7
   24 |   0.8783 |     30.226 |   0.9539 |     32.371 |     1.8
   25 |   0.8577 |     29.444 |   0.9634 |     33.209 |     1.9
   26 |   0.8369 |     28.645 |   0.9454 |     32.247 |     2.0
   27 |   0.8143 |     27.736 |   0.9362 |     32.092 |     2.1
   28 |   0.7964 |     27.190 |   0.9158 |     30.354 |     2.1
   29 |   0.7730 |     26.061 |   0.9111 |     30.447 |     2.2
   30 |   0.7496 |     25.179 |   0.8988 |     30.540 |     2.3
   31 |   0.7341 |     24.639 |   0.9155 |     30.292 |     2.4
   32 |   0.7116 |     23.846 |   0.8987 |     29.733 |     2.4
   33 |   0.6836 |     22.529 |   0.8870 |     29.019 |     2.5
   34 |   0.6613 |     21.802 |   0.8980 |     29.050 |     2.6
   35 |   0.6428 |     21.339 |   0.9114 |     29.050 |     2.7
   36 |   0.6275 |     20.408 |   0.8900 |     28.243 |     2.7
   37 |   0.6027 |     19.741 |   0.8846 |     27.592 |     2.8
   38 |   0.5916 |     19.366 |   0.9019 |     28.243 |     2.9
   39 |   0.5690 |     18.606 |   0.9096 |     28.057 |     3.0
   40 |   0.5487 |     17.945 |   0.8932 |     28.305 |     3.0
   41 |   0.5247 |     17.102 |   0.9150 |     27.592 |     3.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,607,777

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3002 |     60.623 |   1.7288 |     49.131 |     0.1
    2 |   1.5524 |     47.003 |   1.4489 |     47.083 |     0.2
    3 |   1.4074 |     45.664 |   1.3800 |     46.741 |     0.2
    4 |   1.3547 |     45.399 |   1.3351 |     45.841 |     0.3
    5 |   1.3204 |     44.562 |   1.3064 |     45.810 |     0.4
    6 |   1.2925 |     44.039 |   1.2834 |     45.189 |     0.5
    7 |   1.2696 |     43.636 |   1.2652 |     45.282 |     0.6
    8 |   1.2488 |     43.747 |   1.2443 |     44.724 |     0.6
    9 |   1.2306 |     43.003 |   1.2226 |     43.979 |     0.7
   10 |   1.2121 |     42.793 |   1.2109 |     42.955 |     0.8
   11 |   1.1969 |     41.906 |   1.1865 |     42.241 |     0.9
   12 |   1.1799 |     41.267 |   1.1694 |     41.434 |     1.0
   13 |   1.1637 |     40.871 |   1.1629 |     41.558 |     1.1
   14 |   1.1500 |     40.331 |   1.1517 |     40.937 |     1.1
   15 |   1.1347 |     39.818 |   1.1385 |     40.844 |     1.2
   16 |   1.1210 |     39.543 |   1.1280 |     40.037 |     1.3
   17 |   1.1109 |     39.113 |   1.1223 |     39.975 |     1.4
   18 |   1.0972 |     38.590 |   1.1124 |     39.199 |     1.5
   19 |   1.0869 |     38.347 |   1.1039 |     38.485 |     1.5
   20 |   1.0727 |     37.510 |   1.0936 |     38.454 |     1.6
   21 |   1.0613 |     37.074 |   1.0892 |     37.741 |     1.7
   22 |   1.0523 |     36.612 |   1.0742 |     37.151 |     1.8
   23 |   1.0428 |     36.452 |   1.0716 |     37.120 |     1.9
   24 |   1.0294 |     35.890 |   1.0556 |     36.934 |     1.9
   25 |   1.0159 |     35.207 |   1.0533 |     37.430 |     2.0
   26 |   1.0017 |     34.672 |   1.0491 |     36.530 |     2.1
   27 |   0.9923 |     34.386 |   1.0361 |     35.568 |     2.2
   28 |   0.9804 |     34.220 |   1.0246 |     34.792 |     2.3
   29 |   0.9668 |     33.328 |   1.0201 |     35.071 |     2.3
   30 |   0.9558 |     32.926 |   1.0141 |     34.047 |     2.4
   31 |   0.9416 |     32.358 |   1.0059 |     34.047 |     2.5
   32 |   0.9270 |     31.686 |   0.9831 |     33.209 |     2.6
   33 |   0.9110 |     31.179 |   0.9871 |     33.209 |     2.7
   34 |   0.8973 |     30.606 |   0.9754 |     33.085 |     2.7
   35 |   0.8790 |     29.769 |   0.9779 |     32.495 |     2.8
   36 |   0.8616 |     29.107 |   0.9617 |     32.185 |     2.9
   37 |   0.8468 |     28.612 |   0.9530 |     32.123 |     3.0
   38 |   0.8270 |     28.017 |   0.9516 |     31.595 |     3.1
   39 |   0.8096 |     27.410 |   0.9471 |     31.657 |     3.2
   40 |   0.7868 |     26.270 |   0.9371 |     30.850 |     3.2
   41 |   0.7705 |     25.840 |   0.9395 |     30.137 |     3.3
   42 |   0.7525 |     25.074 |   0.9371 |     30.137 |     3.4
   43 |   0.7336 |     24.523 |   0.9373 |     29.981 |     3.5
   44 |   0.7098 |     23.366 |   0.9416 |     29.516 |     3.6
   45 |   0.6971 |     23.041 |   0.9326 |     29.454 |     3.6
   46 |   0.6769 |     22.154 |   0.9251 |     28.926 |     3.7
   47 |   0.6615 |     21.713 |   0.9174 |     28.678 |     3.8
   48 |   0.6477 |     21.135 |   0.9239 |     28.864 |     3.9
   49 |   0.6235 |     20.050 |   0.9368 |     28.678 |     4.0
   50 |   0.6066 |     19.493 |   0.9322 |     28.647 |     4.0
   51 |   0.5920 |     19.212 |   0.9398 |     28.057 |     4.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 674,849

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0020 |     54.358 |   1.4624 |     46.741 |     0.2
    2 |   1.3755 |     45.190 |   1.3215 |     45.500 |     0.4
    3 |   1.2942 |     43.906 |   1.2750 |     45.313 |     0.6
    4 |   1.2565 |     43.504 |   1.2459 |     44.631 |     0.8
    5 |   1.2267 |     42.612 |   1.2181 |     43.141 |     1.0
    6 |   1.1968 |     41.851 |   1.1877 |     42.520 |     1.2
    7 |   1.1639 |     40.871 |   1.1492 |     41.341 |     1.4
    8 |   1.1284 |     39.769 |   1.1242 |     39.944 |     1.6
    9 |   1.0931 |     38.672 |   1.0970 |     38.765 |     1.8
   10 |   1.0526 |     36.700 |   1.0536 |     37.213 |     2.0
   11 |   1.0092 |     34.920 |   1.0350 |     35.258 |     2.2
   12 |   0.9684 |     33.339 |   0.9876 |     33.799 |     2.4
   13 |   0.9262 |     31.460 |   0.9708 |     32.682 |     2.6
   14 |   0.8847 |     29.807 |   0.9520 |     31.906 |     2.8
   15 |   0.8391 |     27.956 |   0.9355 |     31.254 |     3.0
   16 |   0.7932 |     26.237 |   0.9146 |     29.702 |     3.2
   17 |   0.7557 |     24.799 |   0.9077 |     29.671 |     3.4
   18 |   0.7151 |     23.212 |   0.8983 |     28.957 |     3.6
   19 |   0.6806 |     22.061 |   0.8809 |     28.150 |     3.8
   20 |   0.6426 |     20.997 |   0.8981 |     28.988 |     4.0
   21 |   0.6005 |     19.427 |   0.8888 |     27.405 |     4.2
   22 |   0.5524 |     17.449 |   0.8828 |     27.840 |     4.4
   23 |   0.5265 |     16.678 |   0.8761 |     27.529 |     4.6
   24 |   0.4915 |     15.427 |   0.8770 |     26.660 |     4.7
   25 |   0.4683 |     14.832 |   0.9002 |     26.940 |     4.9
   26 |   0.4302 |     13.394 |   0.8918 |     26.443 |     5.1
   27 |   0.4024 |     12.590 |   0.8940 |     25.947 |     5.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,127,009

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4072 |     64.848 |   1.8612 |     50.155 |     0.1
    2 |   1.6327 |     46.937 |   1.5008 |     47.083 |     0.1
    3 |   1.4363 |     45.763 |   1.4051 |     46.989 |     0.2
    4 |   1.3748 |     45.642 |   1.3613 |     47.145 |     0.2
    5 |   1.3384 |     44.953 |   1.3299 |     45.531 |     0.3
    6 |   1.3155 |     44.270 |   1.3116 |     46.307 |     0.3
    7 |   1.2976 |     43.989 |   1.3000 |     45.500 |     0.4
    8 |   1.2810 |     43.664 |   1.2831 |     44.631 |     0.5
    9 |   1.2654 |     43.306 |   1.2670 |     44.879 |     0.5
   10 |   1.2514 |     43.455 |   1.2440 |     44.413 |     0.6
   11 |   1.2320 |     42.898 |   1.2321 |     44.755 |     0.6
   12 |   1.2185 |     42.595 |   1.2198 |     43.855 |     0.7
   13 |   1.2028 |     42.468 |   1.2029 |     42.520 |     0.7
   14 |   1.1896 |     41.532 |   1.1904 |     43.296 |     0.8
   15 |   1.1756 |     41.388 |   1.1713 |     42.831 |     0.8
   16 |   1.1631 |     41.124 |   1.1651 |     41.775 |     0.9
   17 |   1.1507 |     40.656 |   1.1569 |     41.962 |     1.0
   18 |   1.1381 |     40.000 |   1.1418 |     40.223 |     1.0
   19 |   1.1272 |     39.449 |   1.1393 |     40.565 |     1.1
   20 |   1.1171 |     39.317 |   1.1265 |     39.944 |     1.1
   21 |   1.1073 |     38.760 |   1.1138 |     38.982 |     1.2
   22 |   1.0980 |     38.551 |   1.1047 |     39.137 |     1.2
   23 |   1.0855 |     37.829 |   1.0998 |     38.610 |     1.3
   24 |   1.0750 |     37.471 |   1.0940 |     38.454 |     1.4
   25 |   1.0665 |     37.366 |   1.0887 |     38.392 |     1.4
   26 |   1.0563 |     36.893 |   1.0752 |     37.865 |     1.5
   27 |   1.0450 |     36.490 |   1.0724 |     37.647 |     1.5
   28 |   1.0337 |     35.868 |   1.0664 |     37.647 |     1.6
   29 |   1.0209 |     35.752 |   1.0578 |     37.151 |     1.6
   30 |   1.0124 |     35.085 |   1.0481 |     36.344 |     1.7
   31 |   0.9959 |     34.287 |   1.0402 |     36.002 |     1.8
   32 |   0.9845 |     33.708 |   1.0308 |     35.537 |     1.8
   33 |   0.9731 |     33.383 |   1.0162 |     35.133 |     1.9
   34 |   0.9586 |     32.749 |   1.0153 |     35.351 |     1.9
   35 |   0.9467 |     32.375 |   1.0059 |     34.482 |     2.0
   36 |   0.9308 |     31.829 |   1.0032 |     34.668 |     2.0
   37 |   0.9323 |     31.978 |   1.0059 |     34.885 |     2.1
   38 |   0.9121 |     31.251 |   0.9980 |     34.140 |     2.2
   39 |   0.8948 |     30.529 |   0.9791 |     33.457 |     2.2
   40 |   0.8894 |     30.165 |   0.9963 |     34.885 |     2.3
   41 |   0.8725 |     29.763 |   0.9834 |     33.613 |     2.3
   42 |   0.8538 |     29.003 |   0.9688 |     33.116 |     2.4
   43 |   0.8419 |     28.501 |   0.9791 |     33.333 |     2.5
   44 |   0.8301 |     27.934 |   0.9571 |     32.557 |     2.5
   45 |   0.8090 |     27.124 |   0.9601 |     32.371 |     2.6
   46 |   0.7962 |     26.942 |   0.9505 |     31.595 |     2.6
   47 |   0.7857 |     26.132 |   0.9422 |     31.626 |     2.7
   48 |   0.7718 |     25.702 |   0.9328 |     31.161 |     2.7
   49 |   0.7430 |     24.865 |   0.9279 |     30.695 |     2.8
   50 |   0.7298 |     24.121 |   0.9338 |     30.944 |     2.9
   51 |   0.7082 |     23.223 |   0.9410 |     30.199 |     2.9
   52 |   0.7079 |     23.229 |   0.9387 |     30.664 |     3.0
   53 |   0.7043 |     23.273 |   0.9279 |     30.106 |     3.0
   54 |   0.6781 |     22.353 |   0.9321 |     29.857 |     3.1
   55 |   0.6511 |     21.190 |   0.9241 |     29.702 |     3.1
   56 |   0.6261 |     19.994 |   0.9257 |     28.709 |     3.2
   57 |   0.6122 |     19.697 |   0.9425 |     29.330 |     3.3
   58 |   0.5910 |     18.953 |   0.9359 |     29.081 |     3.3
   59 |   0.5712 |     18.143 |   0.9485 |     29.764 |     3.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,529,889

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0627 |     56.154 |   1.5295 |     47.238 |     0.1
    2 |   1.4237 |     45.559 |   1.3762 |     46.400 |     0.1
    3 |   1.3331 |     44.711 |   1.3225 |     45.996 |     0.2
    4 |   1.2943 |     44.138 |   1.2867 |     45.593 |     0.2
    5 |   1.2618 |     43.642 |   1.2504 |     44.600 |     0.3
    6 |   1.2308 |     42.766 |   1.2157 |     42.924 |     0.4
    7 |   1.2017 |     41.840 |   1.1884 |     42.396 |     0.4
    8 |   1.1691 |     40.893 |   1.1669 |     41.092 |     0.5
    9 |   1.1372 |     39.807 |   1.1380 |     41.030 |     0.5
   10 |   1.1020 |     38.815 |   1.1088 |     38.703 |     0.6
   11 |   1.0634 |     37.394 |   1.0778 |     36.375 |     0.7
   12 |   1.0214 |     34.959 |   1.0503 |     36.065 |     0.7
   13 |   0.9692 |     32.562 |   1.0262 |     34.233 |     0.8
   14 |   0.9175 |     30.815 |   0.9977 |     33.551 |     0.8
   15 |   0.8641 |     28.826 |   0.9616 |     31.875 |     0.9
   16 |   0.8110 |     26.419 |   0.9457 |     31.006 |     1.0
   17 |   0.7573 |     24.397 |   0.9199 |     29.361 |     1.0
   18 |   0.6982 |     22.347 |   0.9025 |     29.640 |     1.1
   19 |   0.6442 |     20.364 |   0.9021 |     28.461 |     1.1
   20 |   0.5949 |     18.887 |   0.9169 |     28.988 |     1.2
   21 |   0.5468 |     17.124 |   0.8906 |     28.119 |     1.3
   22 |   0.4918 |     15.157 |   0.8977 |     27.343 |     1.3
   23 |   0.4490 |     13.829 |   0.8845 |     26.847 |     1.4
   24 |   0.4092 |     12.579 |   0.9024 |     25.760 |     1.4
   25 |   0.3655 |     11.289 |   0.9211 |     25.885 |     1.5
   26 |   0.3279 |      9.934 |   0.9256 |     25.605 |     1.6
   27 |   0.2970 |      8.915 |   0.9540 |     26.195 |     1.6
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,588,321

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4786 |     65.917 |   1.9289 |     53.631 |     0.1
    2 |   1.7319 |     48.766 |   1.5708 |     47.114 |     0.2
    3 |   1.4959 |     45.851 |   1.4518 |     47.114 |     0.2
    4 |   1.4253 |     45.851 |   1.4201 |     47.083 |     0.3
    5 |   1.3926 |     45.862 |   1.3785 |     47.083 |     0.4
    6 |   1.3559 |     45.587 |   1.3469 |     46.834 |     0.5
    7 |   1.3307 |     45.129 |   1.3318 |     47.393 |     0.6
    8 |   1.3092 |     44.744 |   1.2985 |     45.686 |     0.6
    9 |   1.2878 |     44.149 |   1.2798 |     45.593 |     0.7
   10 |   1.2694 |     43.752 |   1.2618 |     44.662 |     0.8
   11 |   1.2543 |     43.658 |   1.2581 |     44.879 |     0.9
   12 |   1.2435 |     43.201 |   1.2378 |     43.979 |     1.0
   13 |   1.2312 |     43.234 |   1.2313 |     43.389 |     1.0
   14 |   1.2210 |     42.716 |   1.2143 |     43.824 |     1.1
   15 |   1.2136 |     42.485 |   1.2093 |     43.917 |     1.2
   16 |   1.2051 |     42.562 |   1.2009 |     43.482 |     1.3
   17 |   1.1966 |     42.044 |   1.1935 |     43.110 |     1.4
   18 |   1.1920 |     42.072 |   1.1883 |     43.327 |     1.4
   19 |   1.1858 |     42.011 |   1.1872 |     43.172 |     1.5
   20 |   1.1800 |     41.983 |   1.1767 |     42.893 |     1.6
   21 |   1.1738 |     41.736 |   1.1758 |     42.737 |     1.7
   22 |   1.1704 |     41.631 |   1.1690 |     42.117 |     1.8
   23 |   1.1649 |     41.702 |   1.1659 |     42.334 |     1.8
   24 |   1.1576 |     41.267 |   1.1599 |     41.868 |     1.9
   25 |   1.1515 |     41.223 |   1.1559 |     41.558 |     2.0
   26 |   1.1440 |     41.140 |   1.1512 |     41.465 |     2.1
   27 |   1.1417 |     40.656 |   1.1479 |     41.589 |     2.2
   28 |   1.1357 |     40.705 |   1.1434 |     41.434 |     2.3
   29 |   1.1313 |     40.639 |   1.1435 |     41.620 |     2.3
   30 |   1.1270 |     40.287 |   1.1334 |     41.155 |     2.4
   31 |   1.1207 |     39.901 |   1.1290 |     40.937 |     2.5
   32 |   1.1151 |     39.758 |   1.1319 |     40.720 |     2.6
   33 |   1.1097 |     39.636 |   1.1266 |     40.596 |     2.7
   34 |   1.1038 |     39.504 |   1.1197 |     40.720 |     2.7
   35 |   1.1032 |     39.758 |   1.1187 |     40.751 |     2.8
   36 |   1.0985 |     39.350 |   1.1191 |     40.534 |     2.9
   37 |   1.0949 |     39.069 |   1.1149 |     40.286 |     3.0
   38 |   1.0885 |     39.074 |   1.1163 |     40.379 |     3.1
   39 |   1.0858 |     38.678 |   1.1064 |     40.472 |     3.1
   40 |   1.0807 |     38.777 |   1.1015 |     40.068 |     3.2
   41 |   1.0780 |     38.490 |   1.0988 |     40.223 |     3.3
   42 |   1.0728 |     38.463 |   1.1008 |     40.348 |     3.4
   43 |   1.0707 |     38.672 |   1.0985 |     39.913 |     3.5
   44 |   1.0677 |     38.132 |   1.0947 |     39.882 |     3.5
   45 |   1.0644 |     38.441 |   1.0900 |     39.882 |     3.6
   46 |   1.0599 |     37.950 |   1.0931 |     40.472 |     3.7
   47 |   1.0531 |     37.873 |   1.0877 |     39.727 |     3.8
   48 |   1.0529 |     37.846 |   1.0831 |     40.130 |     3.9
   49 |   1.0477 |     37.857 |   1.0781 |     39.541 |     3.9
   50 |   1.0433 |     37.410 |   1.0741 |     39.230 |     4.0
   51 |   1.0378 |     37.041 |   1.0720 |     38.951 |     4.1
   52 |   1.0343 |     36.981 |   1.0759 |     39.292 |     4.2
   53 |   1.0294 |     36.970 |   1.0698 |     39.044 |     4.3
   54 |   1.0269 |     36.821 |   1.0671 |     38.299 |     4.3
   55 |   1.0220 |     36.567 |   1.0644 |     38.485 |     4.4
   56 |   1.0184 |     36.408 |   1.0528 |     37.337 |     4.5
   57 |   1.0073 |     35.813 |   1.0575 |     37.647 |     4.6
   58 |   1.0073 |     35.675 |   1.0548 |     37.989 |     4.7
   59 |   1.0006 |     35.774 |   1.0475 |     37.834 |     4.7
   60 |   0.9940 |     35.427 |   1.0476 |     37.461 |     4.8
   61 |   0.9912 |     35.372 |   1.0414 |     36.996 |     4.9
   62 |   0.9829 |     35.008 |   1.0391 |     37.244 |     5.0
   63 |   0.9785 |     34.931 |   1.0330 |     37.275 |     5.1
   64 |   0.9685 |     34.270 |   1.0336 |     36.747 |     5.1
   65 |   0.9654 |     34.424 |   1.0219 |     36.220 |     5.2
   66 |   0.9619 |     34.083 |   1.0207 |     36.189 |     5.3
   67 |   0.9532 |     33.713 |   1.0262 |     35.971 |     5.4
   68 |   0.9499 |     33.603 |   1.0176 |     35.971 |     5.5
   69 |   0.9438 |     33.460 |   1.0163 |     35.816 |     5.5
   70 |   0.9343 |     33.240 |   1.0131 |     35.382 |     5.6
   71 |   0.9324 |     32.815 |   1.0196 |     35.909 |     5.7
   72 |   0.9259 |     32.650 |   1.0065 |     35.630 |     5.8
   73 |   0.9209 |     32.281 |   1.0078 |     35.568 |     5.9
   74 |   0.9153 |     32.529 |   1.0056 |     35.444 |     6.0
   75 |   0.9110 |     32.143 |   0.9913 |     35.351 |     6.0
   76 |   0.8994 |     31.488 |   0.9927 |     34.513 |     6.1
   77 |   0.8904 |     31.196 |   0.9904 |     34.668 |     6.2
   78 |   0.8868 |     31.118 |   0.9860 |     34.327 |     6.3
   79 |   0.8781 |     30.876 |   0.9844 |     33.861 |     6.4
   80 |   0.8761 |     30.733 |   0.9895 |     34.171 |     6.4
   81 |   0.8702 |     30.325 |   0.9809 |     33.768 |     6.5
   82 |   0.8653 |     30.270 |   0.9763 |     33.395 |     6.6
   83 |   0.8568 |     29.725 |   0.9773 |     33.147 |     6.7
   84 |   0.8520 |     29.581 |   0.9742 |     33.706 |     6.8
   85 |   0.8406 |     29.146 |   0.9727 |     33.395 |     6.8
   86 |   0.8352 |     28.793 |   0.9674 |     33.085 |     6.9
   87 |   0.8266 |     28.579 |   0.9689 |     32.682 |     7.0
   88 |   0.8227 |     28.275 |   0.9778 |     32.961 |     7.1
   89 |   0.8151 |     27.780 |   0.9750 |     32.744 |     7.2
   90 |   0.8072 |     27.824 |   0.9673 |     31.875 |     7.2
   91 |   0.8011 |     27.344 |   0.9667 |     31.750 |     7.3
   92 |   0.7928 |     27.008 |   0.9699 |     31.906 |     7.4
   93 |   0.7860 |     26.882 |   0.9506 |     31.161 |     7.5
   94 |   0.7809 |     26.788 |   0.9517 |     31.440 |     7.6
   95 |   0.7730 |     25.972 |   0.9751 |     31.533 |     7.6
   96 |   0.7696 |     26.187 |   0.9806 |     32.588 |     7.7
   97 |   0.7609 |     25.879 |   0.9494 |     31.192 |     7.8
   98 |   0.7528 |     25.620 |   0.9573 |     31.316 |     7.9
   99 |   0.7461 |     25.262 |   0.9527 |     30.633 |     8.0
  100 |   0.7413 |     24.904 |   0.9676 |     31.037 |     8.1
  101 |   0.7329 |     24.468 |   0.9744 |     31.316 |     8.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 439,905

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4670 |     66.160 |   1.8976 |     53.352 |     0.2
    2 |   1.6827 |     47.928 |   1.5293 |     47.083 |     0.3
    3 |   1.4635 |     45.846 |   1.4269 |     47.083 |     0.5
    4 |   1.3941 |     45.884 |   1.3789 |     47.083 |     0.6
    5 |   1.3557 |     45.102 |   1.3546 |     46.648 |     0.8
    6 |   1.3304 |     44.755 |   1.3226 |     46.089 |     1.0
    7 |   1.3077 |     44.375 |   1.3008 |     45.345 |     1.1
    8 |   1.2850 |     44.011 |   1.2839 |     44.941 |     1.3
    9 |   1.2645 |     43.824 |   1.2604 |     45.189 |     1.4
   10 |   1.2481 |     43.636 |   1.2478 |     44.507 |     1.6
   11 |   1.2339 |     43.278 |   1.2324 |     45.251 |     1.8
   12 |   1.2191 |     43.107 |   1.2178 |     44.134 |     1.9
   13 |   1.2038 |     42.639 |   1.2061 |     43.265 |     2.1
   14 |   1.1924 |     42.358 |   1.1948 |     42.675 |     2.2
   15 |   1.1824 |     41.978 |   1.1990 |     43.234 |     2.4
   16 |   1.1728 |     41.835 |   1.1815 |     42.706 |     2.6
   17 |   1.1634 |     41.642 |   1.1749 |     42.706 |     2.7
   18 |   1.1543 |     41.416 |   1.1664 |     42.241 |     2.9
   19 |   1.1465 |     41.229 |   1.1564 |     41.713 |     3.0
   20 |   1.1363 |     40.512 |   1.1517 |     42.210 |     3.2
   21 |   1.1301 |     40.397 |   1.1352 |     41.589 |     3.4
   22 |   1.1165 |     39.763 |   1.1313 |     41.372 |     3.5
   23 |   1.1107 |     39.532 |   1.1312 |     40.627 |     3.7
   24 |   1.1039 |     39.251 |   1.1175 |     40.130 |     3.8
   25 |   1.0925 |     38.771 |   1.1150 |     40.068 |     4.0
   26 |   1.0847 |     38.562 |   1.1101 |     39.913 |     4.2
   27 |   1.0779 |     38.116 |   1.1048 |     39.448 |     4.3
   28 |   1.0710 |     37.802 |   1.0972 |     39.417 |     4.5
   29 |   1.0614 |     37.554 |   1.0877 |     39.013 |     4.6
   30 |   1.0561 |     37.063 |   1.0828 |     38.485 |     4.8
   31 |   1.0464 |     36.601 |   1.0842 |     39.199 |     5.0
   32 |   1.0422 |     36.777 |   1.0797 |     38.485 |     5.1
   33 |   1.0310 |     36.364 |   1.0804 |     38.268 |     5.3
   34 |   1.0257 |     35.961 |   1.0658 |     37.678 |     5.4
   35 |   1.0201 |     35.691 |   1.0578 |     36.872 |     5.6
   36 |   1.0080 |     35.410 |   1.0605 |     37.430 |     5.8
   37 |   1.0016 |     35.003 |   1.0695 |     38.051 |     5.9
   38 |   0.9988 |     35.058 |   1.0533 |     37.089 |     6.1
   39 |   0.9898 |     34.678 |   1.0603 |     37.089 |     6.2
   40 |   0.9829 |     34.496 |   1.0510 |     37.120 |     6.4
   41 |   0.9761 |     34.231 |   1.0542 |     36.344 |     6.6
   42 |   0.9666 |     33.956 |   1.0453 |     35.909 |     6.7
   43 |   0.9616 |     33.642 |   1.0400 |     36.282 |     6.9
   44 |   0.9561 |     33.653 |   1.0337 |     36.002 |     7.0
   45 |   0.9488 |     33.080 |   1.0356 |     35.754 |     7.2
   46 |   0.9399 |     32.992 |   1.0306 |     35.692 |     7.4
   47 |   0.9291 |     32.628 |   1.0207 |     34.916 |     7.5
   48 |   0.9239 |     32.264 |   1.0331 |     35.847 |     7.7
   49 |   0.9123 |     31.725 |   1.0227 |     35.661 |     7.8
   50 |   0.8994 |     31.399 |   1.0230 |     35.320 |     8.0
   51 |   0.8937 |     31.372 |   1.0210 |     34.885 |     8.2
   52 |   0.8859 |     30.970 |   1.0111 |     34.854 |     8.3
   53 |   0.8737 |     30.231 |   1.0172 |     34.606 |     8.5
   54 |   0.8640 |     30.094 |   1.0050 |     34.544 |     8.6
   55 |   0.8556 |     29.818 |   1.0071 |     34.544 |     8.8
   56 |   0.8471 |     29.201 |   1.0011 |     33.706 |     9.0
   57 |   0.8335 |     28.876 |   0.9952 |     33.830 |     9.1
   58 |   0.8257 |     28.661 |   1.0066 |     33.768 |     9.3
   59 |   0.8100 |     27.796 |   1.0025 |     33.489 |     9.4
   60 |   0.8037 |     27.455 |   1.0160 |     33.364 |     9.6
   61 |   0.8051 |     27.493 |   1.0027 |     33.085 |     9.8
   62 |   0.7806 |     26.320 |   0.9802 |     32.495 |     9.9
   63 |   0.7675 |     25.956 |   0.9843 |     32.619 |    10.1
   64 |   0.7621 |     25.868 |   0.9852 |     32.061 |    10.2
   65 |   0.7460 |     25.256 |   0.9806 |     32.340 |    10.4
   66 |   0.7371 |     24.821 |   0.9846 |     31.844 |    10.6
   67 |   0.7296 |     24.678 |   0.9694 |     31.502 |    10.7
   68 |   0.7178 |     24.331 |   0.9821 |     31.782 |    10.9
   69 |   0.7133 |     24.187 |   0.9806 |     31.750 |    11.1
   70 |   0.6989 |     23.427 |   1.0190 |     31.906 |    11.2
   71 |   0.6910 |     23.256 |   0.9835 |     30.881 |    11.4
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,622,561

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4918 |     66.689 |   1.9185 |     54.935 |     0.1
    2 |   1.7103 |     49.405 |   1.5380 |     47.083 |     0.2
    3 |   1.4751 |     45.851 |   1.4415 |     47.083 |     0.2
    4 |   1.4204 |     45.923 |   1.4153 |     47.083 |     0.3
    5 |   1.4018 |     45.846 |   1.4073 |     47.083 |     0.4
    6 |   1.3897 |     45.846 |   1.3949 |     47.083 |     0.5
    7 |   1.3717 |     45.559 |   1.3717 |     47.083 |     0.6
    8 |   1.3552 |     45.174 |   1.3539 |     46.772 |     0.6
    9 |   1.3396 |     44.981 |   1.3424 |     46.462 |     0.7
   10 |   1.3255 |     44.909 |   1.3268 |     46.369 |     0.8
   11 |   1.3085 |     44.661 |   1.3078 |     46.338 |     0.9
   12 |   1.2932 |     44.275 |   1.2841 |     45.717 |     1.0
   13 |   1.2718 |     43.846 |   1.2698 |     45.469 |     1.0
   14 |   1.2588 |     43.504 |   1.2545 |     44.848 |     1.1
   15 |   1.2437 |     43.234 |   1.2416 |     44.941 |     1.2
   16 |   1.2337 |     43.014 |   1.2245 |     43.700 |     1.3
   17 |   1.2178 |     42.485 |   1.2155 |     43.234 |     1.4
   18 |   1.2012 |     41.895 |   1.2036 |     42.613 |     1.4
   19 |   1.1915 |     41.565 |   1.1864 |     42.117 |     1.5
   20 |   1.1788 |     41.163 |   1.1786 |     41.962 |     1.6
   21 |   1.1677 |     41.196 |   1.1736 |     41.744 |     1.7
   22 |   1.1545 |     40.424 |   1.1607 |     41.155 |     1.8
   23 |   1.1410 |     39.923 |   1.1594 |     41.403 |     1.8
   24 |   1.1320 |     39.835 |   1.1486 |     40.627 |     1.9
   25 |   1.1166 |     39.052 |   1.1395 |     40.379 |     2.0
   26 |   1.1078 |     38.871 |   1.1347 |     39.820 |     2.1
   27 |   1.0931 |     38.165 |   1.1243 |     39.603 |     2.2
   28 |   1.0841 |     37.901 |   1.1240 |     38.982 |     2.2
   29 |   1.0682 |     37.008 |   1.1137 |     39.168 |     2.3
   30 |   1.0570 |     36.975 |   1.1296 |     39.851 |     2.4
   31 |   1.0920 |     38.253 |   1.1127 |     38.920 |     2.5
   32 |   1.0587 |     37.052 |   1.0975 |     38.392 |     2.6
   33 |   1.0345 |     35.945 |   1.0878 |     38.082 |     2.7
   34 |   1.0207 |     35.372 |   1.0766 |     36.840 |     2.7
   35 |   1.0066 |     35.113 |   1.0721 |     36.778 |     2.8
   36 |   0.9908 |     34.408 |   1.0531 |     36.127 |     2.9
   37 |   0.9722 |     33.482 |   1.0541 |     36.251 |     3.0
   38 |   0.9608 |     33.069 |   1.0441 |     35.878 |     3.1
   39 |   0.9417 |     32.402 |   1.0429 |     36.096 |     3.1
   40 |   0.9249 |     31.636 |   1.0371 |     35.133 |     3.2
   41 |   0.9142 |     31.449 |   1.0323 |     34.513 |     3.3
   42 |   0.9008 |     30.953 |   1.0123 |     34.264 |     3.4
   43 |   0.8852 |     30.072 |   1.0090 |     34.078 |     3.5
   44 |   0.8677 |     29.570 |   1.0070 |     33.861 |     3.5
   45 |   0.8525 |     29.124 |   1.0182 |     32.992 |     3.6
   46 |   0.8378 |     28.132 |   1.0056 |     32.868 |     3.7
   47 |   0.8283 |     27.862 |   1.0055 |     32.278 |     3.8
   48 |   0.8061 |     26.755 |   0.9986 |     32.309 |     3.9
   49 |   0.7854 |     26.022 |   0.9932 |     31.719 |     4.0
   50 |   0.7735 |     25.691 |   0.9913 |     31.906 |     4.0
   51 |   0.7640 |     25.317 |   1.0007 |     31.813 |     4.1
   52 |   0.7501 |     24.904 |   0.9926 |     31.502 |     4.2
   53 |   0.7323 |     24.017 |   0.9882 |     30.850 |     4.3
   54 |   0.7136 |     23.190 |   0.9788 |     31.099 |     4.4
   55 |   0.7083 |     23.234 |   1.0041 |     30.788 |     4.4
   56 |   0.6895 |     22.579 |   0.9913 |     30.757 |     4.5
   57 |   0.6727 |     21.895 |   1.0063 |     30.416 |     4.6
   58 |   0.6606 |     21.686 |   0.9985 |     30.230 |     4.7
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,606,817

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4965 |     67.344 |   1.9650 |     55.711 |     0.1
    2 |   1.7485 |     48.573 |   1.5729 |     47.083 |     0.2
    3 |   1.4913 |     45.846 |   1.4479 |     47.083 |     0.2
    4 |   1.4196 |     45.835 |   1.4090 |     47.083 |     0.3
    5 |   1.3921 |     45.780 |   1.3918 |     47.083 |     0.4
    6 |   1.3739 |     45.658 |   1.3733 |     47.207 |     0.5
    7 |   1.3540 |     45.091 |   1.3499 |     46.245 |     0.5
    8 |   1.3309 |     44.871 |   1.3376 |     46.772 |     0.6
    9 |   1.3162 |     44.843 |   1.3145 |     46.027 |     0.7
   10 |   1.3029 |     44.832 |   1.3066 |     46.245 |     0.8
   11 |   1.2917 |     44.523 |   1.2923 |     45.934 |     0.8
   12 |   1.2797 |     44.336 |   1.2772 |     45.624 |     0.9
   13 |   1.2678 |     43.939 |   1.2661 |     44.817 |     1.0
   14 |   1.2523 |     43.730 |   1.2488 |     45.158 |     1.1
   15 |   1.2388 |     43.521 |   1.2428 |     44.941 |     1.2
   16 |   1.2263 |     43.328 |   1.2286 |     44.817 |     1.2
   17 |   1.2122 |     42.832 |   1.2192 |     43.731 |     1.3
   18 |   1.2006 |     42.595 |   1.2080 |     42.893 |     1.4
   19 |   1.1903 |     42.242 |   1.1977 |     42.955 |     1.5
   20 |   1.1807 |     41.802 |   1.1971 |     43.389 |     1.5
   21 |   1.1733 |     41.950 |   1.1826 |     43.110 |     1.6
   22 |   1.1641 |     41.410 |   1.1698 |     42.458 |     1.7
   23 |   1.1549 |     41.025 |   1.1666 |     41.993 |     1.8
   24 |   1.1470 |     40.782 |   1.1553 |     41.620 |     1.9
   25 |   1.1349 |     40.160 |   1.1445 |     41.341 |     1.9
   26 |   1.1245 |     39.730 |   1.1365 |     40.689 |     2.0
   27 |   1.1145 |     39.377 |   1.1239 |     40.658 |     2.1
   28 |   1.1068 |     39.229 |   1.1205 |     39.696 |     2.2
   29 |   1.0953 |     38.485 |   1.1114 |     40.192 |     2.2
   30 |   1.0857 |     38.408 |   1.1088 |     39.261 |     2.3
   31 |   1.0751 |     37.884 |   1.0941 |     38.982 |     2.4
   32 |   1.0640 |     37.438 |   1.0941 |     38.796 |     2.5
   33 |   1.0553 |     37.251 |   1.0893 |     38.858 |     2.6
   34 |   1.0438 |     36.639 |   1.0838 |     38.392 |     2.6
   35 |   1.0320 |     36.011 |   1.0687 |     37.958 |     2.7
   36 |   1.0206 |     35.300 |   1.0806 |     38.113 |     2.8
   37 |   1.0047 |     34.744 |   1.0586 |     37.182 |     2.9
   38 |   0.9934 |     34.474 |   1.0543 |     37.120 |     2.9
   39 |   0.9810 |     34.000 |   1.0480 |     36.561 |     3.0
   40 |   0.9648 |     33.350 |   1.0421 |     36.065 |     3.1
   41 |   0.9472 |     32.733 |   1.0381 |     35.692 |     3.2
   42 |   0.9326 |     32.110 |   1.0262 |     34.916 |     3.3
   43 |   0.9128 |     31.377 |   1.0297 |     34.916 |     3.3
   44 |   0.8933 |     30.325 |   1.0125 |     34.358 |     3.4
   45 |   0.8810 |     29.829 |   1.0047 |     33.240 |     3.5
   46 |   0.8579 |     28.843 |   0.9853 |     32.682 |     3.6
   47 |   0.8341 |     28.077 |   1.0010 |     33.644 |     3.6
   48 |   0.8086 |     26.782 |   0.9847 |     32.464 |     3.7
   49 |   0.7870 |     25.956 |   0.9964 |     32.185 |     3.8
   50 |   0.7637 |     25.003 |   0.9796 |     32.092 |     3.9
   51 |   0.7358 |     23.901 |   0.9692 |     31.192 |     4.0
   52 |   0.7206 |     23.587 |   0.9883 |     31.316 |     4.0
   53 |   0.6945 |     22.386 |   0.9892 |     31.068 |     4.1
   54 |   0.6632 |     21.394 |   0.9714 |     30.602 |     4.2
   55 |   0.6410 |     20.391 |   0.9872 |     30.633 |     4.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,201,185

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1656 |     59.565 |   1.5850 |     47.083 |     0.0
    2 |   1.4676 |     45.895 |   1.4181 |     47.114 |     0.1
    3 |   1.4003 |     45.912 |   1.3953 |     47.114 |     0.1
    4 |   1.3645 |     45.449 |   1.3578 |     47.083 |     0.2
    5 |   1.3325 |     45.129 |   1.3220 |     46.276 |     0.2
    6 |   1.3067 |     44.788 |   1.3032 |     45.934 |     0.3
    7 |   1.2825 |     44.325 |   1.2765 |     45.469 |     0.3
    8 |   1.2619 |     43.890 |   1.2550 |     44.848 |     0.3
    9 |   1.2437 |     43.427 |   1.2404 |     44.972 |     0.4
   10 |   1.2261 |     43.140 |   1.2162 |     43.824 |     0.4
   11 |   1.2117 |     42.590 |   1.2097 |     43.575 |     0.5
   12 |   1.2015 |     42.391 |   1.2048 |     44.041 |     0.5
   13 |   1.1870 |     41.978 |   1.1805 |     42.893 |     0.5
   14 |   1.1774 |     41.840 |   1.1822 |     42.706 |     0.6
   15 |   1.1687 |     41.444 |   1.1701 |     41.899 |     0.6
   16 |   1.1518 |     40.904 |   1.1598 |     41.744 |     0.7
   17 |   1.1431 |     40.744 |   1.1553 |     42.024 |     0.7
   18 |   1.1339 |     40.364 |   1.1451 |     41.372 |     0.8
   19 |   1.1240 |     40.165 |   1.1324 |     41.527 |     0.8
   20 |   1.1114 |     39.625 |   1.1291 |     40.968 |     0.8
   21 |   1.1049 |     39.614 |   1.1229 |     40.627 |     0.9
   22 |   1.0966 |     39.366 |   1.1180 |     40.503 |     0.9
   23 |   1.0877 |     39.074 |   1.1092 |     40.255 |     1.0
   24 |   1.0811 |     38.716 |   1.1110 |     40.410 |     1.0
   25 |   1.0736 |     38.518 |   1.0963 |     39.789 |     1.1
   26 |   1.0627 |     37.983 |   1.0882 |     39.137 |     1.1
   27 |   1.0561 |     37.961 |   1.0904 |     38.951 |     1.1
   28 |   1.0488 |     37.664 |   1.0849 |     39.168 |     1.2
   29 |   1.0422 |     37.372 |   1.0847 |     38.672 |     1.2
   30 |   1.0358 |     36.821 |   1.0700 |     38.516 |     1.3
   31 |   1.0288 |     36.848 |   1.0644 |     38.051 |     1.3
   32 |   1.0213 |     36.336 |   1.0601 |     37.834 |     1.4
   33 |   1.0137 |     36.204 |   1.0651 |     37.896 |     1.4
   34 |   1.0100 |     36.204 |   1.0515 |     36.934 |     1.4
   35 |   1.0011 |     36.033 |   1.0447 |     37.182 |     1.5
   36 |   0.9948 |     35.493 |   1.0394 |     36.934 |     1.5
   37 |   0.9888 |     35.455 |   1.0309 |     36.468 |     1.6
   38 |   0.9751 |     34.964 |   1.0219 |     35.630 |     1.6
   39 |   0.9654 |     34.281 |   1.0211 |     35.909 |     1.6
   40 |   0.9568 |     34.072 |   1.0047 |     34.358 |     1.7
   41 |   0.9547 |     33.835 |   1.0093 |     35.133 |     1.7
   42 |   0.9429 |     33.730 |   0.9989 |     35.133 |     1.8
   43 |   0.9363 |     33.328 |   0.9968 |     34.699 |     1.8
   44 |   0.9230 |     32.848 |   0.9929 |     34.792 |     1.9
   45 |   0.9143 |     32.303 |   0.9861 |     33.830 |     1.9
   46 |   0.9095 |     32.342 |   0.9836 |     33.395 |     1.9
   47 |   0.9018 |     32.011 |   0.9879 |     34.047 |     2.0
   48 |   0.8989 |     31.884 |   0.9873 |     33.582 |     2.0
   49 |   0.9013 |     32.110 |   0.9969 |     33.799 |     2.1
   50 |   0.8807 |     31.421 |   0.9631 |     33.147 |     2.1
   51 |   0.8706 |     30.926 |   0.9673 |     32.619 |     2.2
   52 |   0.8629 |     30.336 |   0.9584 |     32.837 |     2.2
   53 |   0.8572 |     30.325 |   0.9690 |     32.806 |     2.2
   54 |   0.8511 |     30.017 |   0.9435 |     32.619 |     2.3
   55 |   0.8457 |     29.664 |   0.9495 |     31.875 |     2.3
   56 |   0.8368 |     29.504 |   0.9683 |     32.837 |     2.4
   57 |   0.8274 |     28.898 |   0.9519 |     32.154 |     2.4
   58 |   0.8397 |     29.311 |   0.9664 |     32.775 |     2.5
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,589,281

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4091 |     63.587 |   1.8683 |     49.100 |     0.1
    2 |   1.6429 |     47.366 |   1.5018 |     47.020 |     0.2
    3 |   1.4392 |     45.702 |   1.4130 |     47.145 |     0.2
    4 |   1.3747 |     45.444 |   1.3538 |     46.679 |     0.3
    5 |   1.3348 |     44.645 |   1.3286 |     46.369 |     0.4
    6 |   1.3107 |     44.165 |   1.3060 |     45.655 |     0.5
    7 |   1.2893 |     43.824 |   1.2884 |     45.624 |     0.5
    8 |   1.2751 |     43.824 |   1.2768 |     45.438 |     0.6
    9 |   1.2622 |     43.532 |   1.2647 |     45.127 |     0.7
   10 |   1.2441 |     43.333 |   1.2403 |     44.538 |     0.8
   11 |   1.2287 |     42.975 |   1.2227 |     43.979 |     0.8
   12 |   1.2164 |     42.656 |   1.2165 |     43.948 |     0.9
   13 |   1.2046 |     42.303 |   1.2022 |     42.644 |     1.0
   14 |   1.1918 |     41.587 |   1.1928 |     43.762 |     1.1
   15 |   1.1806 |     41.256 |   1.1831 |     42.241 |     1.1
   16 |   1.1685 |     40.909 |   1.1764 |     41.930 |     1.2
   17 |   1.1596 |     40.303 |   1.1684 |     41.993 |     1.3
   18 |   1.1454 |     40.055 |   1.1542 |     40.565 |     1.4
   19 |   1.1311 |     39.300 |   1.1520 |     40.565 |     1.4
   20 |   1.1197 |     38.975 |   1.1356 |     39.603 |     1.5
   21 |   1.1075 |     38.650 |   1.1274 |     39.851 |     1.6
   22 |   1.0928 |     38.275 |   1.1161 |     39.168 |     1.7
   23 |   1.0802 |     37.939 |   1.1012 |     38.734 |     1.7
   24 |   1.0656 |     37.377 |   1.0881 |     37.678 |     1.8
   25 |   1.0495 |     36.628 |   1.0834 |     37.585 |     1.9
   26 |   1.0359 |     36.105 |   1.0702 |     36.468 |     2.0
   27 |   1.0266 |     35.736 |   1.0719 |     36.437 |     2.1
   28 |   1.0141 |     35.372 |   1.0477 |     35.506 |     2.1
   29 |   0.9992 |     34.452 |   1.0442 |     36.189 |     2.2
   30 |   0.9859 |     34.325 |   1.0375 |     35.289 |     2.3
   31 |   0.9735 |     33.466 |   1.0262 |     34.823 |     2.4
   32 |   0.9507 |     32.579 |   1.0076 |     34.482 |     2.4
   33 |   0.9373 |     32.160 |   1.0181 |     35.071 |     2.5
   34 |   0.9274 |     31.780 |   0.9995 |     34.513 |     2.6
   35 |   0.9066 |     30.975 |   0.9975 |     34.171 |     2.7
   36 |   0.8914 |     30.187 |   0.9785 |     32.899 |     2.7
   37 |   0.8749 |     29.455 |   0.9823 |     33.209 |     2.8
   38 |   0.8538 |     28.694 |   0.9745 |     32.775 |     2.9
   39 |   0.8369 |     28.055 |   0.9670 |     32.154 |     3.0
   40 |   0.8173 |     27.278 |   0.9563 |     31.999 |     3.0
   41 |   0.7995 |     26.634 |   0.9397 |     31.161 |     3.1
   42 |   0.7799 |     25.785 |   0.9567 |     31.223 |     3.2
   43 |   0.7640 |     25.185 |   0.9515 |     31.626 |     3.3
   44 |   0.7405 |     24.044 |   0.9565 |     31.844 |     3.3
   45 |   0.7218 |     23.725 |   0.9455 |     31.161 |     3.4
   46 |   0.6983 |     22.705 |   0.9359 |     30.912 |     3.5
   47 |   0.6840 |     22.176 |   0.9344 |     29.733 |     3.6
   48 |   0.6687 |     21.592 |   0.9359 |     30.199 |     3.7
   49 |   0.6321 |     20.248 |   0.9216 |     29.112 |     3.7
   50 |   0.6112 |     19.421 |   0.9208 |     29.081 |     3.8
   51 |   0.5941 |     18.904 |   0.9261 |     28.585 |     3.9
   52 |   0.5661 |     17.669 |   0.9269 |     28.492 |     4.0
   53 |   0.5436 |     16.821 |   0.9405 |     28.585 |     4.0
   54 |   0.5280 |     16.446 |   0.9304 |     28.057 |     4.1
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 32
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 698,977

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3888 |     65.438 |   1.8388 |     53.104 |     0.1
    2 |   1.6297 |     47.785 |   1.4946 |     46.989 |     0.3
    3 |   1.4413 |     45.708 |   1.4051 |     46.741 |     0.4
    4 |   1.3770 |     45.306 |   1.3539 |     46.182 |     0.5
    5 |   1.3399 |     45.218 |   1.3335 |     46.058 |     0.6
    6 |   1.3172 |     44.479 |   1.3099 |     45.841 |     0.8
    7 |   1.3010 |     44.066 |   1.2936 |     45.003 |     0.9
    8 |   1.2876 |     43.934 |   1.2816 |     45.376 |     1.0
    9 |   1.2714 |     43.554 |   1.2710 |     44.941 |     1.1
   10 |   1.2600 |     43.532 |   1.2592 |     44.848 |     1.3
   11 |   1.2453 |     43.157 |   1.2421 |     44.320 |     1.4
   12 |   1.2295 |     42.744 |   1.2288 |     44.103 |     1.5
   13 |   1.2185 |     42.650 |   1.2214 |     43.234 |     1.6
   14 |   1.2067 |     42.143 |   1.2038 |     42.706 |     1.8
   15 |   1.1890 |     41.537 |   1.1853 |     42.737 |     1.9
   16 |   1.1748 |     40.865 |   1.1785 |     41.527 |     2.0
   17 |   1.1573 |     40.253 |   1.1579 |     41.061 |     2.1
   18 |   1.1388 |     39.240 |   1.1461 |     39.541 |     2.3
   19 |   1.1199 |     38.601 |   1.1358 |     39.044 |     2.4
   20 |   1.1068 |     38.314 |   1.1150 |     39.354 |     2.5
   21 |   1.0883 |     37.152 |   1.1029 |     37.492 |     2.6
   22 |   1.0702 |     36.650 |   1.0921 |     37.275 |     2.8
   23 |   1.0511 |     35.840 |   1.0799 |     36.965 |     2.9
   24 |   1.0324 |     35.311 |   1.0622 |     36.623 |     3.0
   25 |   1.0172 |     34.705 |   1.0523 |     36.592 |     3.1
   26 |   0.9952 |     33.780 |   1.0370 |     35.164 |     3.3
   27 |   0.9714 |     32.898 |   1.0252 |     34.916 |     3.4
   28 |   0.9522 |     32.143 |   1.0171 |     34.792 |     3.5
   29 |   0.9265 |     31.322 |   1.0077 |     34.513 |     3.7
   30 |   0.9097 |     30.468 |   0.9931 |     34.109 |     3.8
   31 |   0.8844 |     29.383 |   0.9736 |     32.899 |     3.9
   32 |   0.8579 |     28.424 |   0.9677 |     32.806 |     4.0
   33 |   0.8337 |     27.576 |   0.9560 |     32.123 |     4.2
   34 |   0.8024 |     26.463 |   0.9531 |     31.502 |     4.3
   35 |   0.7791 |     25.598 |   0.9383 |     30.788 |     4.4
   36 |   0.7532 |     24.435 |   0.9291 |     30.540 |     4.5
   37 |   0.7302 |     23.774 |   0.9359 |     31.068 |     4.7
   38 |   0.7047 |     22.826 |   0.9200 |     30.664 |     4.8
   39 |   0.6689 |     21.780 |   0.9195 |     29.888 |     4.9
   40 |   0.6468 |     20.689 |   0.9178 |     29.547 |     5.0
   41 |   0.6212 |     19.862 |   0.9176 |     29.205 |     5.2
   42 |   0.5903 |     18.760 |   0.9225 |     29.143 |     5.3
   43 |   0.5731 |     18.386 |   0.9164 |     28.057 |     5.4
   44 |   0.5466 |     17.416 |   0.9292 |     28.802 |     5.5
   45 |   0.5227 |     16.446 |   0.9270 |     28.305 |     5.7
   46 |   0.5085 |     16.017 |   0.9304 |     27.933 |     5.8
   47 |   0.4771 |     14.810 |   0.9310 |     26.847 |     5.9
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 128
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,087,905

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1831 |     60.099 |   1.5920 |     47.083 |     0.1
    2 |   1.4693 |     46.022 |   1.4222 |     47.083 |     0.2
    3 |   1.4062 |     45.939 |   1.4055 |     47.083 |     0.2
    4 |   1.3927 |     45.851 |   1.4004 |     47.083 |     0.3
    5 |   1.3733 |     45.614 |   1.3765 |     46.896 |     0.4
    6 |   1.3483 |     45.163 |   1.3422 |     46.276 |     0.5
    7 |   1.3274 |     45.212 |   1.3274 |     46.803 |     0.6
    8 |   1.2975 |     44.771 |   1.2836 |     45.376 |     0.7
    9 |   1.2738 |     44.110 |   1.2689 |     45.282 |     0.7
   10 |   1.2514 |     43.857 |   1.2547 |     45.593 |     0.8
   11 |   1.2339 |     43.653 |   1.2328 |     44.475 |     0.9
   12 |   1.2152 |     42.964 |   1.2207 |     43.917 |     1.0
   13 |   1.1984 |     42.331 |   1.2135 |     43.731 |     1.1
   14 |   1.1813 |     41.840 |   1.1976 |     42.862 |     1.1
   15 |   1.1579 |     40.986 |   1.1693 |     42.179 |     1.2
   16 |   1.1363 |     39.862 |   1.1538 |     41.496 |     1.3
   17 |   1.1187 |     39.455 |   1.1337 |     41.434 |     1.4
   18 |   1.0964 |     39.058 |   1.1242 |     40.844 |     1.5
   19 |   1.0777 |     38.143 |   1.1174 |     40.348 |     1.6
   20 |   1.0593 |     37.653 |   1.0993 |     38.485 |     1.6
   21 |   1.0408 |     36.848 |   1.0888 |     38.610 |     1.7
   22 |   1.0215 |     36.215 |   1.0718 |     37.213 |     1.8
   23 |   0.9972 |     35.361 |   1.0577 |     36.778 |     1.9
   24 |   0.9698 |     33.961 |   1.0593 |     36.623 |     2.0
   25 |   0.9438 |     32.942 |   1.0373 |     35.661 |     2.0
   26 |   0.9162 |     31.697 |   1.0107 |     35.040 |     2.1
   27 |   0.8903 |     31.135 |   1.0038 |     34.513 |     2.2
   28 |   0.8625 |     29.824 |   1.0063 |     33.830 |     2.3
   29 |   0.8405 |     28.854 |   0.9929 |     33.333 |     2.4
   30 |   0.8123 |     27.791 |   0.9825 |     32.526 |     2.5
   31 |   0.7838 |     26.628 |   0.9708 |     31.968 |     2.5
   32 |   0.7508 |     25.174 |   0.9697 |     31.068 |     2.6
   33 |   0.7278 |     24.380 |   0.9433 |     29.919 |     2.7
   34 |   0.7155 |     23.950 |   0.9532 |     30.261 |     2.8
   35 |   0.6754 |     22.215 |   0.9404 |     29.733 |     2.9
   36 |   0.6484 |     21.284 |   0.9285 |     28.833 |     3.0
   37 |   0.6139 |     19.961 |   0.9323 |     28.212 |     3.0
   38 |   0.5882 |     18.942 |   0.9575 |     28.957 |     3.1
   39 |   0.5627 |     18.358 |   0.9558 |     29.516 |     3.2
   40 |   0.5318 |     16.981 |   0.9398 |     27.716 |     3.3
Early stopping

Model: Seq2Seq Bi-LSTM
Source index: <Seq2Seq Index with 46 items>
Target index: <Seq2Seq Index with 33 items>
Encoder embedding dimension: 64
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 388,129

Training started
X_train.shape: torch.Size([3025, 702])
Y_train.shape: torch.Size([3025, 7])
X_dev.shape: torch.Size([537, 337])
Y_dev.shape: torch.Size([537, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3680 |     63.598 |   1.8072 |     49.131 |     0.2
    2 |   1.6092 |     47.174 |   1.4826 |     47.114 |     0.3
    3 |   1.4280 |     45.912 |   1.3982 |     46.493 |     0.5
    4 |   1.3620 |     45.080 |   1.3514 |     46.058 |     0.6
    5 |   1.3263 |     44.044 |   1.3172 |     45.686 |     0.8
    6 |   1.3013 |     43.983 |   1.2966 |     45.469 |     1.0
    7 |   1.2812 |     43.598 |   1.2778 |     45.531 |     1.1
    8 |   1.2645 |     43.785 |   1.2635 |     45.034 |     1.3
    9 |   1.2467 |     43.201 |   1.2464 |     44.879 |     1.4
   10 |   1.2279 |     42.661 |   1.2294 |     43.793 |     1.6
   11 |   1.2094 |     42.264 |   1.2220 |     43.917 |     1.8
   12 |   1.1918 |     41.763 |   1.2022 |     43.110 |     1.9
   13 |   1.1715 |     40.738 |   1.1926 |     42.675 |     2.1
   14 |   1.1537 |     40.336 |   1.1702 |     41.092 |     2.3
   15 |   1.1336 |     39.499 |   1.1582 |     40.503 |     2.4
   16 |   1.1122 |     38.882 |   1.1551 |     40.410 |     2.6
   17 |   1.0890 |     38.298 |   1.1200 |     38.423 |     2.7
   18 |   1.0645 |     36.931 |   1.1178 |     38.827 |     2.9
   19 |   1.0418 |     36.149 |   1.0983 |     37.958 |     3.1
   20 |   1.0080 |     34.711 |   1.0742 |     36.561 |     3.2
   21 |   0.9782 |     33.499 |   1.0676 |     36.592 |     3.4
   22 |   0.9509 |     32.270 |   1.0455 |     34.202 |     3.5
   23 |   0.9153 |     30.545 |   1.0343 |     35.320 |     3.7
   24 |   0.8831 |     29.124 |   1.0247 |     33.520 |     3.9
   25 |   0.8490 |     28.132 |   1.0093 |     33.644 |     4.0
   26 |   0.8197 |     26.755 |   1.0009 |     32.278 |     4.2
   27 |   0.7829 |     25.344 |   0.9893 |     32.526 |     4.3
   28 |   0.7483 |     24.198 |   0.9665 |     31.068 |     4.5
   29 |   0.7178 |     22.926 |   0.9915 |     31.968 |     4.7
   30 |   0.6930 |     22.242 |   0.9573 |     30.199 |     4.8
   31 |   0.6548 |     20.573 |   0.9663 |     29.950 |     5.0
   32 |   0.6317 |     19.857 |   0.9605 |     29.174 |     5.1
   33 |   0.5955 |     18.590 |   0.9580 |     29.454 |     5.3
   34 |   0.5619 |     17.251 |   0.9864 |     29.609 |     5.5
Early stopping

