Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 814,242

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0355 |     52.873 |   1.5217 |     45.506 |     0.1
    2 |   1.4312 |     44.672 |   1.3667 |     43.383 |     0.2
    3 |   1.3321 |     43.087 |   1.2889 |     41.292 |     0.2
    4 |   1.2695 |     41.678 |   1.2500 |     40.762 |     0.3
    5 |   1.2281 |     40.709 |   1.2139 |     40.075 |     0.4
    6 |   1.1907 |     40.021 |   1.1752 |     39.794 |     0.5
    7 |   1.1554 |     39.151 |   1.1494 |     38.390 |     0.6
    8 |   1.1257 |     38.177 |   1.1454 |     37.422 |     0.7
    9 |   1.0984 |     37.269 |   1.1047 |     36.860 |     0.7
   10 |   1.0716 |     36.069 |   1.0920 |     35.237 |     0.8
   11 |   1.0439 |     34.786 |   1.0716 |     34.800 |     0.9
   12 |   1.0121 |     33.928 |   1.0454 |     34.176 |     1.0
   13 |   0.9886 |     32.827 |   1.0281 |     33.708 |     1.1
   14 |   0.9637 |     32.188 |   1.0249 |     34.051 |     1.1
   15 |   0.9370 |     30.812 |   1.0110 |     33.645 |     1.2
   16 |   0.9182 |     30.152 |   1.0001 |     32.740 |     1.3
   17 |   0.8863 |     29.178 |   0.9799 |     32.210 |     1.4
   18 |   0.8640 |     28.385 |   0.9805 |     32.522 |     1.5
   19 |   0.8417 |     27.791 |   0.9755 |     32.397 |     1.6
   20 |   0.8158 |     26.690 |   0.9642 |     31.835 |     1.6
   21 |   0.7909 |     26.062 |   0.9584 |     31.866 |     1.7
   22 |   0.7693 |     25.358 |   0.9662 |     31.648 |     1.8
   23 |   0.7486 |     24.560 |   0.9395 |     30.712 |     1.9
   24 |   0.7273 |     23.778 |   0.9489 |     30.868 |     2.0
   25 |   0.7034 |     23.140 |   0.9417 |     30.337 |     2.1
   26 |   0.6855 |     22.363 |   0.9359 |     30.212 |     2.1
   27 |   0.6617 |     21.670 |   0.9528 |     31.055 |     2.2
   28 |   0.6416 |     20.872 |   0.9500 |     30.431 |     2.3
   29 |   0.6209 |     20.354 |   0.9529 |     30.556 |     2.4
   30 |   0.6008 |     19.314 |   0.9369 |     29.838 |     2.5
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 326,626

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3202 |     56.396 |   1.7418 |     45.006 |     0.1
    2 |   1.6144 |     45.762 |   1.5203 |     44.226 |     0.2
    3 |   1.4729 |     44.430 |   1.4262 |     43.758 |     0.3
    4 |   1.3985 |     43.384 |   1.3691 |     41.667 |     0.4
    5 |   1.3453 |     42.338 |   1.3214 |     40.824 |     0.5
    6 |   1.3055 |     41.898 |   1.2869 |     40.980 |     0.6
    7 |   1.2710 |     41.144 |   1.2572 |     40.356 |     0.7
    8 |   1.2406 |     40.456 |   1.2339 |     39.919 |     0.8
    9 |   1.2138 |     40.092 |   1.2060 |     38.327 |     0.8
   10 |   1.1909 |     39.707 |   1.1764 |     38.421 |     0.9
   11 |   1.1658 |     38.981 |   1.1778 |     38.015 |     1.0
   12 |   1.1457 |     38.436 |   1.1453 |     38.046 |     1.1
   13 |   1.1256 |     37.968 |   1.1393 |     37.953 |     1.2
   14 |   1.1097 |     37.423 |   1.1186 |     37.047 |     1.3
   15 |   1.0932 |     37.038 |   1.1035 |     36.142 |     1.4
   16 |   1.0755 |     36.526 |   1.0933 |     36.923 |     1.5
   17 |   1.0609 |     35.887 |   1.0781 |     35.768 |     1.6
   18 |   1.0452 |     35.392 |   1.0642 |     35.861 |     1.7
   19 |   1.0321 |     34.935 |   1.0677 |     35.612 |     1.8
   20 |   1.0135 |     34.225 |   1.0504 |     35.643 |     1.9
   21 |   1.0018 |     33.587 |   1.0402 |     35.424 |     2.0
   22 |   0.9885 |     33.157 |   1.0348 |     34.582 |     2.1
   23 |   0.9780 |     32.838 |   1.0299 |     34.207 |     2.2
   24 |   0.9626 |     32.590 |   1.0223 |     34.332 |     2.3
   25 |   0.9470 |     31.836 |   1.0201 |     34.457 |     2.4
   26 |   0.9394 |     31.297 |   1.0076 |     33.583 |     2.5
   27 |   0.9243 |     31.060 |   1.0080 |     33.302 |     2.5
   28 |   0.9138 |     30.669 |   1.0011 |     33.552 |     2.6
   29 |   0.8977 |     30.108 |   1.0129 |     33.396 |     2.7
   30 |   0.8860 |     29.563 |   0.9959 |     32.928 |     2.8
   31 |   0.8777 |     29.233 |   0.9982 |     33.084 |     2.9
   32 |   0.8608 |     28.726 |   0.9797 |     31.898 |     3.0
   33 |   0.8469 |     28.121 |   0.9969 |     33.021 |     3.1
   34 |   0.8398 |     27.983 |   0.9875 |     32.772 |     3.2
   35 |   0.8243 |     27.059 |   0.9684 |     31.554 |     3.3
   36 |   0.8154 |     27.064 |   0.9687 |     31.617 |     3.4
   37 |   0.8026 |     26.717 |   0.9734 |     31.898 |     3.5
   38 |   0.7907 |     26.128 |   0.9556 |     31.429 |     3.6
   39 |   0.7820 |     25.875 |   0.9748 |     31.898 |     3.7
   40 |   0.7679 |     25.149 |   0.9663 |     31.211 |     3.8
   41 |   0.7585 |     25.088 |   0.9575 |     31.117 |     3.9
   42 |   0.7451 |     24.180 |   0.9736 |     31.617 |     4.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 64
Encoder layers: 2
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 881,442

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0874 |     54.425 |   1.4972 |     44.881 |     0.1
    2 |   1.4147 |     44.876 |   1.3485 |     43.227 |     0.2
    3 |   1.3113 |     42.911 |   1.2786 |     41.448 |     0.3
    4 |   1.2507 |     41.056 |   1.2287 |     39.326 |     0.4
    5 |   1.2001 |     39.713 |   1.1806 |     38.546 |     0.5
    6 |   1.1490 |     38.254 |   1.1466 |     38.202 |     0.6
    7 |   1.1165 |     37.225 |   1.1157 |     36.767 |     0.8
    8 |   1.0745 |     35.997 |   1.0876 |     36.267 |     0.9
    9 |   1.0422 |     34.996 |   1.0634 |     35.081 |     1.0
   10 |   1.0148 |     34.109 |   1.0558 |     35.268 |     1.1
   11 |   0.9821 |     32.860 |   1.0608 |     34.831 |     1.2
   12 |   0.9533 |     31.935 |   1.0222 |     34.738 |     1.3
   13 |   0.9245 |     30.779 |   0.9962 |     32.428 |     1.4
   14 |   0.8992 |     29.866 |   0.9865 |     32.740 |     1.5
   15 |   0.8719 |     28.825 |   0.9799 |     32.241 |     1.6
   16 |   0.8428 |     27.675 |   0.9620 |     32.116 |     1.7
   17 |   0.8175 |     27.136 |   0.9479 |     30.431 |     1.9
   18 |   0.7979 |     26.332 |   0.9560 |     31.679 |     2.0
   19 |   0.7716 |     25.132 |   0.9598 |     31.929 |     2.1
   20 |   0.7458 |     24.284 |   0.9612 |     31.086 |     2.2
   21 |   0.7195 |     23.591 |   0.9422 |     30.400 |     2.3
   22 |   0.6977 |     22.969 |   0.9668 |     31.617 |     2.4
   23 |   0.6748 |     22.121 |   0.9293 |     29.682 |     2.5
   24 |   0.6529 |     21.136 |   0.9534 |     29.931 |     2.6
   25 |   0.6269 |     20.547 |   0.9402 |     29.931 |     2.7
   26 |   0.6011 |     19.435 |   0.9544 |     29.838 |     2.8
   27 |   0.5840 |     19.138 |   0.9606 |     29.869 |     2.9
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 64
Encoder layers: 2
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 881,442

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6924 |     49.934 |   1.3760 |     43.914 |     0.1
    2 |   1.3543 |     45.233 |   1.2809 |     42.759 |     0.2
    3 |   1.2885 |     43.819 |   1.2285 |     42.603 |     0.3
    4 |   1.2397 |     42.294 |   1.2132 |     42.416 |     0.4
    5 |   1.2033 |     41.171 |   1.1535 |     38.951 |     0.5
    6 |   1.1723 |     40.329 |   1.1689 |     40.637 |     0.6
    7 |   1.1373 |     39.316 |   1.1514 |     38.858 |     0.7
    8 |   1.1141 |     38.260 |   1.1352 |     37.828 |     0.8
    9 |   1.0891 |     37.649 |   1.1219 |     37.266 |     0.9
   10 |   1.0609 |     36.762 |   1.1110 |     37.672 |     0.9
   11 |   1.0353 |     35.436 |   1.1132 |     37.516 |     1.0
   12 |   1.0108 |     34.781 |   1.1299 |     37.203 |     1.1
   13 |   0.9947 |     34.390 |   1.0938 |     35.986 |     1.2
   14 |   0.9614 |     33.328 |   1.0761 |     35.237 |     1.3
   15 |   0.9365 |     32.392 |   1.0950 |     35.861 |     1.4
   16 |   0.9190 |     31.814 |   1.0976 |     35.674 |     1.5
   17 |   0.8975 |     31.005 |   1.0888 |     35.331 |     1.6
   18 |   0.8625 |     29.893 |   1.1434 |     36.049 |     1.7
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 980,130

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3769 |     59.720 |   1.5882 |     45.381 |     0.1
    2 |   1.6250 |     46.775 |   1.4348 |     44.975 |     0.2
    3 |   1.4907 |     46.296 |   1.3839 |     44.195 |     0.3
    4 |   1.4376 |     46.279 |   1.3427 |     44.070 |     0.4
    5 |   1.3919 |     45.773 |   1.3007 |     44.070 |     0.5
    6 |   1.3608 |     45.167 |   1.2839 |     42.416 |     0.6
    7 |   1.3346 |     44.138 |   1.2575 |     41.698 |     0.7
    8 |   1.3087 |     43.489 |   1.2415 |     41.261 |     0.8
    9 |   1.2868 |     43.032 |   1.2187 |     40.387 |     0.9
   10 |   1.2709 |     42.498 |   1.1995 |     39.700 |     1.0
   11 |   1.2518 |     42.201 |   1.1888 |     39.139 |     1.1
   12 |   1.2356 |     41.634 |   1.1837 |     39.732 |     1.2
   13 |   1.2204 |     41.144 |   1.1611 |     38.920 |     1.3
   14 |   1.2102 |     40.830 |   1.1551 |     39.045 |     1.3
   15 |   1.1971 |     40.527 |   1.1588 |     39.076 |     1.4
   16 |   1.1904 |     40.423 |   1.1382 |     38.546 |     1.5
   17 |   1.1771 |     40.087 |   1.1373 |     38.327 |     1.6
   18 |   1.1660 |     39.630 |   1.1313 |     39.170 |     1.7
   19 |   1.1578 |     39.685 |   1.1244 |     37.921 |     1.8
   20 |   1.1485 |     39.729 |   1.1119 |     38.015 |     1.9
   21 |   1.1335 |     38.738 |   1.0988 |     38.140 |     2.0
   22 |   1.1267 |     38.827 |   1.1039 |     37.547 |     2.1
   23 |   1.1131 |     38.050 |   1.1110 |     37.141 |     2.2
   24 |   1.1045 |     38.133 |   1.0914 |     37.328 |     2.3
   25 |   1.0939 |     37.511 |   1.0825 |     36.923 |     2.4
   26 |   1.0813 |     37.076 |   1.1017 |     37.079 |     2.5
   27 |   1.0758 |     37.197 |   1.0903 |     36.923 |     2.6
   28 |   1.0672 |     36.581 |   1.0622 |     36.205 |     2.7
   29 |   1.0614 |     36.234 |   1.0646 |     36.111 |     2.8
   30 |   1.0523 |     36.052 |   1.0669 |     35.799 |     2.9
   31 |   1.0395 |     35.568 |   1.0736 |     35.986 |     3.0
   32 |   1.0345 |     35.766 |   1.0550 |     35.237 |     3.1
   33 |   1.0209 |     34.753 |   1.0613 |     35.830 |     3.2
   34 |   1.0185 |     34.522 |   1.0729 |     35.518 |     3.3
   35 |   1.0083 |     34.467 |   1.0437 |     34.613 |     3.4
   36 |   0.9974 |     33.988 |   1.0588 |     34.800 |     3.5
   37 |   0.9971 |     33.977 |   1.0378 |     34.582 |     3.6
   38 |   0.9874 |     33.394 |   1.0475 |     34.769 |     3.7
   39 |   0.9693 |     32.854 |   1.0570 |     34.769 |     3.8
   40 |   0.9694 |     32.783 |   1.0457 |     35.206 |     3.9
   41 |   0.9614 |     32.843 |   1.0446 |     34.176 |     4.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.1
Trainable parameters: 648,354

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.5618 |     47.941 |   1.3174 |     44.257 |     0.1
    2 |   1.2884 |     44.000 |   1.1983 |     41.698 |     0.2
    3 |   1.2185 |     42.272 |   1.1741 |     40.075 |     0.2
    4 |   1.1676 |     40.428 |   1.1298 |     39.950 |     0.3
    5 |   1.1377 |     39.823 |   1.1249 |     39.263 |     0.4
    6 |   1.0992 |     38.496 |   1.1082 |     38.140 |     0.5
    7 |   1.0808 |     37.880 |   1.0704 |     36.829 |     0.6
    8 |   1.0494 |     36.851 |   1.0652 |     36.049 |     0.6
    9 |   1.0218 |     35.711 |   1.0448 |     35.581 |     0.7
   10 |   1.0082 |     34.935 |   1.0193 |     34.551 |     0.8
   11 |   0.9772 |     33.691 |   1.0634 |     35.424 |     0.9
   12 |   0.9652 |     33.498 |   1.0611 |     35.924 |     0.9
   13 |   0.9332 |     32.585 |   1.0406 |     34.582 |     1.0
   14 |   0.9141 |     32.040 |   1.0416 |     34.644 |     1.1
   15 |   0.8842 |     30.609 |   1.0134 |     33.271 |     1.2
   16 |   0.8731 |     30.389 |   1.0053 |     33.770 |     1.3
   17 |   0.8452 |     29.238 |   1.0091 |     32.928 |     1.3
   18 |   0.8176 |     28.352 |   1.0444 |     34.082 |     1.4
   19 |   0.7981 |     27.780 |   1.0280 |     32.054 |     1.5
   20 |   0.7933 |     27.747 |   1.0020 |     32.834 |     1.6
   21 |   0.7609 |     26.387 |   1.0353 |     32.990 |     1.7
   22 |   0.7317 |     25.429 |   1.0327 |     33.365 |     1.8
   23 |   0.7134 |     24.873 |   1.0603 |     33.677 |     1.8
   24 |   0.7017 |     24.229 |   1.0282 |     32.459 |     1.9
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 1,179,298

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2943 |     58.278 |   1.5353 |     45.256 |     0.2
    2 |   1.5359 |     46.054 |   1.3938 |     44.788 |     0.4
    3 |   1.4264 |     45.432 |   1.3356 |     43.477 |     0.6
    4 |   1.3674 |     44.628 |   1.2976 |     42.291 |     0.8
    5 |   1.3286 |     44.144 |   1.2549 |     41.074 |     1.0
    6 |   1.2931 |     42.894 |   1.2430 |     40.262 |     1.1
    7 |   1.2623 |     42.168 |   1.1999 |     39.669 |     1.3
    8 |   1.2412 |     41.551 |   1.1775 |     39.170 |     1.5
    9 |   1.2147 |     40.841 |   1.1596 |     37.828 |     1.7
   10 |   1.1952 |     40.571 |   1.1358 |     37.672 |     1.9
   11 |   1.1702 |     39.718 |   1.1219 |     36.923 |     2.1
   12 |   1.1524 |     39.058 |   1.1146 |     36.517 |     2.3
   13 |   1.1343 |     38.463 |   1.0929 |     36.330 |     2.5
   14 |   1.1137 |     37.913 |   1.0849 |     35.924 |     2.7
   15 |   1.0972 |     37.131 |   1.0740 |     35.393 |     2.9
   16 |   1.0831 |     36.884 |   1.0679 |     35.456 |     3.1
   17 |   1.0592 |     35.563 |   1.0643 |     35.300 |     3.3
   18 |   1.0499 |     35.607 |   1.0666 |     34.863 |     3.5
   19 |   1.0268 |     34.682 |   1.0598 |     34.925 |     3.7
   20 |   1.0116 |     33.774 |   1.0314 |     34.145 |     3.8
   21 |   1.0021 |     33.642 |   1.0356 |     33.333 |     4.0
   22 |   0.9795 |     32.750 |   1.0295 |     34.114 |     4.2
   23 |   0.9682 |     32.271 |   1.0299 |     33.770 |     4.4
   24 |   0.9541 |     31.897 |   1.0298 |     33.052 |     4.6
   25 |   0.9404 |     31.550 |   1.0134 |     32.803 |     4.8
   26 |   0.9230 |     30.691 |   1.0172 |     33.770 |     5.0
   27 |   0.9125 |     30.367 |   1.0075 |     32.772 |     5.2
   28 |   0.8929 |     29.541 |   0.9960 |     33.021 |     5.4
   29 |   0.8812 |     29.354 |   1.0023 |     32.397 |     5.6
   30 |   0.8698 |     28.787 |   0.9995 |     32.459 |     5.8
   31 |   0.8562 |     28.754 |   0.9976 |     32.522 |     6.0
   32 |   0.8450 |     28.292 |   1.0027 |     32.491 |     6.1
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 292,194

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.5687 |     47.375 |   1.2883 |     42.634 |     0.1
    2 |   1.2567 |     42.823 |   1.1920 |     41.448 |     0.1
    3 |   1.1777 |     41.094 |   1.1428 |     40.886 |     0.2
    4 |   1.1245 |     39.322 |   1.0962 |     37.765 |     0.2
    5 |   1.0712 |     37.241 |   1.1123 |     37.297 |     0.3
    6 |   1.0417 |     36.779 |   1.0569 |     36.548 |     0.4
    7 |   1.0014 |     35.128 |   1.0334 |     34.925 |     0.4
    8 |   0.9663 |     33.807 |   1.0228 |     34.894 |     0.5
    9 |   0.9245 |     32.601 |   1.0247 |     35.487 |     0.5
   10 |   0.8915 |     31.198 |   1.0160 |     35.300 |     0.6
   11 |   0.8551 |     29.838 |   1.0026 |     33.864 |     0.6
   12 |   0.8301 |     29.084 |   0.9883 |     33.052 |     0.7
   13 |   0.7846 |     27.174 |   1.0156 |     33.458 |     0.8
   14 |   0.7553 |     26.398 |   0.9951 |     33.177 |     0.8
   15 |   0.7125 |     24.813 |   1.0153 |     33.365 |     0.9
   16 |   0.6714 |     23.250 |   1.0166 |     32.584 |     0.9
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 814,242

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.5533 |     47.314 |   1.2647 |     42.041 |     0.1
    2 |   1.2546 |     42.212 |   1.1980 |     39.451 |     0.2
    3 |   1.1851 |     40.615 |   1.1312 |     38.483 |     0.3
    4 |   1.1326 |     39.069 |   1.1114 |     38.577 |     0.4
    5 |   1.0979 |     37.830 |   1.0864 |     36.704 |     0.5
    6 |   1.0617 |     36.713 |   1.0991 |     36.985 |     0.7
    7 |   1.0267 |     35.463 |   1.0424 |     35.674 |     0.8
    8 |   0.9877 |     34.010 |   1.0567 |     35.518 |     0.9
    9 |   0.9531 |     32.882 |   1.0411 |     34.207 |     1.0
   10 |   0.9232 |     31.880 |   1.0188 |     33.677 |     1.1
   11 |   0.8860 |     30.565 |   1.0249 |     34.488 |     1.2
   12 |   0.8543 |     29.552 |   1.0351 |     33.208 |     1.3
   13 |   0.8197 |     27.747 |   1.0300 |     33.677 |     1.4
   14 |   0.7919 |     27.108 |   1.0126 |     33.084 |     1.5
   15 |   0.7548 |     25.897 |   1.0401 |     33.208 |     1.6
   16 |   0.7291 |     25.127 |   1.0257 |     32.615 |     1.8
   17 |   0.6903 |     23.740 |   1.0736 |     33.801 |     1.9
   18 |   0.6666 |     22.782 |   1.0960 |     33.739 |     2.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 326,626

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6191 |     47.496 |   1.3151 |     44.351 |     0.1
    2 |   1.2642 |     42.779 |   1.1826 |     39.419 |     0.2
    3 |   1.1702 |     40.164 |   1.1590 |     39.700 |     0.3
    4 |   1.1207 |     38.507 |   1.0918 |     37.016 |     0.4
    5 |   1.0622 |     36.691 |   1.0865 |     37.141 |     0.5
    6 |   1.0256 |     35.436 |   1.0506 |     35.300 |     0.6
    7 |   0.9857 |     33.961 |   1.0321 |     35.019 |     0.7
    8 |   0.9505 |     32.612 |   0.9949 |     33.365 |     0.7
    9 |   0.9060 |     31.088 |   1.0007 |     33.770 |     0.8
   10 |   0.8703 |     29.987 |   0.9726 |     32.803 |     0.9
   11 |   0.8383 |     28.924 |   0.9564 |     32.147 |     1.0
   12 |   0.8048 |     27.934 |   0.9658 |     31.804 |     1.1
   13 |   0.7659 |     26.497 |   0.9573 |     31.242 |     1.2
   14 |   0.7321 |     25.006 |   0.9572 |     32.428 |     1.3
   15 |   0.7048 |     24.356 |   0.9652 |     30.462 |     1.4
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 343,330

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6332 |     62.830 |   1.9332 |     47.909 |     0.1
    2 |   1.8402 |     47.518 |   1.5798 |     45.256 |     0.3
    3 |   1.6013 |     46.252 |   1.4844 |     45.256 |     0.5
    4 |   1.5077 |     46.009 |   1.4237 |     44.850 |     0.6
    5 |   1.4564 |     45.949 |   1.3823 |     44.538 |     0.8
    6 |   1.4111 |     45.421 |   1.3479 |     44.288 |     0.9
    7 |   1.3767 |     44.507 |   1.3173 |     42.385 |     1.1
    8 |   1.3516 |     43.989 |   1.2982 |     41.792 |     1.2
    9 |   1.3291 |     43.549 |   1.2762 |     41.167 |     1.4
   10 |   1.3103 |     42.889 |   1.2617 |     41.323 |     1.6
   11 |   1.2863 |     42.101 |   1.2337 |     39.419 |     1.7
   12 |   1.2653 |     41.733 |   1.2159 |     39.357 |     1.9
   13 |   1.2501 |     41.502 |   1.1990 |     39.263 |     2.0
   14 |   1.2330 |     40.709 |   1.1930 |     39.107 |     2.2
   15 |   1.2208 |     40.676 |   1.1847 |     39.139 |     2.3
   16 |   1.2037 |     39.999 |   1.1766 |     39.263 |     2.5
   17 |   1.1924 |     39.883 |   1.1704 |     38.826 |     2.6
   18 |   1.1800 |     39.581 |   1.1483 |     38.233 |     2.8
   19 |   1.1669 |     39.493 |   1.1387 |     38.296 |     3.0
   20 |   1.1564 |     39.234 |   1.1271 |     37.953 |     3.1
   21 |   1.1457 |     38.942 |   1.1260 |     37.640 |     3.3
   22 |   1.1327 |     38.601 |   1.1154 |     38.077 |     3.4
   23 |   1.1232 |     38.122 |   1.1067 |     37.110 |     3.6
   24 |   1.1181 |     37.924 |   1.0998 |     37.578 |     3.7
   25 |   1.1088 |     37.605 |   1.1001 |     37.328 |     3.9
   26 |   1.0995 |     37.472 |   1.0820 |     36.610 |     4.1
   27 |   1.0882 |     37.159 |   1.0818 |     36.298 |     4.2
   28 |   1.0819 |     36.950 |   1.0796 |     36.579 |     4.4
   29 |   1.0716 |     36.680 |   1.0652 |     36.548 |     4.5
   30 |   1.0610 |     36.207 |   1.0637 |     36.049 |     4.7
   31 |   1.0573 |     36.030 |   1.0499 |     35.300 |     4.8
   32 |   1.0498 |     35.783 |   1.0503 |     35.549 |     5.0
   33 |   1.0387 |     35.001 |   1.0548 |     35.643 |     5.1
   34 |   1.0296 |     35.001 |   1.0403 |     35.768 |     5.3
   35 |   1.0258 |     34.913 |   1.0378 |     35.144 |     5.5
   36 |   1.0176 |     34.836 |   1.0349 |     34.988 |     5.6
   37 |   1.0124 |     34.335 |   1.0365 |     34.800 |     5.8
   38 |   1.0044 |     34.473 |   1.0407 |     35.424 |     5.9
   39 |   0.9956 |     34.016 |   1.0180 |     33.895 |     6.1
   40 |   0.9894 |     33.768 |   1.0200 |     34.114 |     6.2
   41 |   0.9849 |     33.410 |   1.0100 |     33.833 |     6.4
   42 |   0.9770 |     32.926 |   1.0245 |     34.582 |     6.5
   43 |   0.9674 |     32.838 |   1.0135 |     33.864 |     6.7
   44 |   0.9643 |     32.810 |   1.0133 |     34.114 |     6.9
   45 |   0.9584 |     32.574 |   1.0166 |     33.926 |     7.0
   46 |   0.9508 |     32.425 |   1.0046 |     33.645 |     7.2
   47 |   0.9489 |     32.183 |   1.0026 |     33.458 |     7.3
   48 |   0.9391 |     32.056 |   0.9981 |     33.521 |     7.5
   49 |   0.9346 |     31.880 |   1.0060 |     33.052 |     7.6
   50 |   0.9248 |     31.423 |   1.0147 |     33.801 |     7.8
   51 |   0.9172 |     31.352 |   0.9934 |     32.522 |     7.9
   52 |   0.9159 |     31.137 |   1.0084 |     33.302 |     8.1
   53 |   0.9110 |     30.796 |   0.9989 |     33.333 |     8.3
   54 |   0.8990 |     30.548 |   0.9943 |     32.678 |     8.4
   55 |   0.8965 |     30.664 |   0.9978 |     32.834 |     8.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.2
Trainable parameters: 343,330

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7936 |     66.133 |   2.1629 |     48.283 |     0.1
    2 |   2.0969 |     48.949 |   1.7046 |     45.256 |     0.3
    3 |   1.7576 |     46.791 |   1.5593 |     45.256 |     0.5
    4 |   1.6179 |     46.329 |   1.4817 |     44.850 |     0.6
    5 |   1.5357 |     46.037 |   1.4323 |     44.788 |     0.8
    6 |   1.4818 |     45.905 |   1.3945 |     44.632 |     0.9
    7 |   1.4436 |     45.542 |   1.3660 |     44.039 |     1.1
    8 |   1.4185 |     45.591 |   1.3520 |     44.007 |     1.2
    9 |   1.3948 |     45.354 |   1.3274 |     43.602 |     1.4
   10 |   1.3743 |     45.002 |   1.3192 |     43.851 |     1.6
   11 |   1.3596 |     44.859 |   1.3006 |     43.383 |     1.7
   12 |   1.3448 |     44.435 |   1.2823 |     42.915 |     1.9
   13 |   1.3338 |     44.468 |   1.2749 |     42.385 |     2.0
   14 |   1.3190 |     43.769 |   1.2656 |     41.792 |     2.2
   15 |   1.3082 |     43.692 |   1.2603 |     42.260 |     2.3
   16 |   1.2994 |     43.758 |   1.2391 |     41.292 |     2.5
   17 |   1.2890 |     43.158 |   1.2380 |     40.949 |     2.6
   18 |   1.2800 |     43.263 |   1.2322 |     41.042 |     2.8
   19 |   1.2696 |     42.723 |   1.2189 |     40.387 |     3.0
   20 |   1.2652 |     42.812 |   1.2060 |     40.262 |     3.1
   21 |   1.2559 |     42.784 |   1.2086 |     39.794 |     3.3
   22 |   1.2432 |     41.980 |   1.1870 |     38.702 |     3.4
   23 |   1.2374 |     41.953 |   1.1820 |     38.826 |     3.6
   24 |   1.2315 |     41.969 |   1.1806 |     39.513 |     3.7
   25 |   1.2222 |     41.782 |   1.1703 |     38.233 |     3.9
   26 |   1.2141 |     41.045 |   1.1602 |     38.608 |     4.0
   27 |   1.2102 |     41.375 |   1.1520 |     37.859 |     4.2
   28 |   1.2014 |     40.951 |   1.1489 |     37.609 |     4.4
   29 |   1.1930 |     40.599 |   1.1375 |     37.547 |     4.5
   30 |   1.1893 |     40.522 |   1.1336 |     37.297 |     4.7
   31 |   1.1819 |     40.318 |   1.1298 |     37.360 |     4.8
   32 |   1.1735 |     39.839 |   1.1191 |     37.016 |     5.0
   33 |   1.1707 |     40.230 |   1.1158 |     37.297 |     5.1
   34 |   1.1579 |     39.531 |   1.1106 |     37.422 |     5.3
   35 |   1.1558 |     39.630 |   1.1164 |     36.985 |     5.4
   36 |   1.1482 |     39.410 |   1.1036 |     37.172 |     5.6
   37 |   1.1440 |     39.234 |   1.0998 |     36.860 |     5.8
   38 |   1.1398 |     38.992 |   1.1021 |     36.548 |     5.9
   39 |   1.1318 |     38.849 |   1.0959 |     37.016 |     6.1
   40 |   1.1290 |     39.063 |   1.0845 |     36.049 |     6.2
   41 |   1.1237 |     38.546 |   1.0937 |     36.236 |     6.4
   42 |   1.1206 |     38.623 |   1.0935 |     36.704 |     6.5
   43 |   1.1157 |     38.474 |   1.0809 |     36.236 |     6.7
   44 |   1.1058 |     38.320 |   1.0809 |     35.924 |     6.9
   45 |   1.1041 |     38.034 |   1.0804 |     35.861 |     7.0
   46 |   1.0969 |     37.704 |   1.0871 |     36.361 |     7.2
   47 |   1.0923 |     37.671 |   1.0730 |     35.830 |     7.3
   48 |   1.0908 |     37.406 |   1.0579 |     35.424 |     7.5
   49 |   1.0818 |     37.164 |   1.0797 |     35.518 |     7.6
   50 |   1.0830 |     37.230 |   1.0751 |     35.861 |     7.8
   51 |   1.0694 |     36.674 |   1.0692 |     35.424 |     7.9
   52 |   1.0744 |     37.186 |   1.0596 |     35.019 |     8.1
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 2
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 779,938

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.5130 |     47.259 |   1.2585 |     43.508 |     0.1
    2 |   1.2587 |     43.478 |   1.1978 |     40.793 |     0.2
    3 |   1.2038 |     42.118 |   1.1488 |     40.075 |     0.3
    4 |   1.1561 |     40.406 |   1.1053 |     39.170 |     0.4
    5 |   1.1144 |     39.118 |   1.0983 |     38.233 |     0.5
    6 |   1.0922 |     38.502 |   1.0673 |     38.546 |     0.6
    7 |   1.0657 |     37.423 |   1.0420 |     36.454 |     0.7
    8 |   1.0388 |     36.795 |   1.0461 |     36.205 |     0.8
    9 |   1.0069 |     35.425 |   1.0269 |     36.298 |     0.9
   10 |   0.9882 |     34.654 |   1.0027 |     35.144 |     1.0
   11 |   0.9651 |     33.944 |   0.9831 |     34.207 |     1.1
   12 |   0.9488 |     33.454 |   0.9743 |     33.084 |     1.2
   13 |   0.9175 |     32.100 |   0.9596 |     33.052 |     1.3
   14 |   0.8969 |     31.231 |   0.9916 |     33.521 |     1.4
   15 |   0.8789 |     30.867 |   0.9855 |     33.240 |     1.5
   16 |   0.8673 |     30.196 |   0.9755 |     33.177 |     1.6
   17 |   0.8372 |     29.425 |   0.9734 |     32.303 |     1.7
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 4
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.1
Trainable parameters: 525,666

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.7172 |     49.130 |   1.3495 |     44.444 |     0.1
    2 |   1.3482 |     45.266 |   1.2717 |     42.946 |     0.2
    3 |   1.2787 |     44.413 |   1.2289 |     43.851 |     0.4
    4 |   1.2385 |     43.720 |   1.1884 |     42.572 |     0.5
    5 |   1.2123 |     43.246 |   1.1681 |     41.355 |     0.6
    6 |   1.1939 |     42.823 |   1.1545 |     40.918 |     0.7
    7 |   1.1766 |     42.190 |   1.1462 |     41.042 |     0.8
    8 |   1.1645 |     41.887 |   1.1234 |     40.169 |     1.0
    9 |   1.1549 |     41.903 |   1.1168 |     40.949 |     1.1
   10 |   1.1465 |     41.557 |   1.1096 |     40.449 |     1.2
   11 |   1.1340 |     41.469 |   1.1038 |     40.200 |     1.3
   12 |   1.1269 |     41.017 |   1.1069 |     40.137 |     1.4
   13 |   1.1249 |     41.254 |   1.1063 |     40.543 |     1.6
   14 |   1.1176 |     40.973 |   1.0927 |     39.825 |     1.7
   15 |   1.1120 |     40.764 |   1.0950 |     40.169 |     1.8
   16 |   1.1169 |     40.852 |   1.0914 |     39.888 |     1.9
   17 |   1.1060 |     40.665 |   1.0831 |     39.856 |     2.0
   18 |   1.1027 |     40.720 |   1.0798 |     39.794 |     2.2
   19 |   1.1003 |     40.472 |   1.0755 |     39.607 |     2.3
   20 |   1.0965 |     40.819 |   1.0848 |     40.231 |     2.4
   21 |   1.0948 |     40.384 |   1.0845 |     40.418 |     2.5
   22 |   1.0895 |     40.406 |   1.0877 |     40.637 |     2.6
   23 |   1.0927 |     40.516 |   1.0856 |     39.638 |     2.8
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 1,177,506

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1708 |     55.917 |   1.5120 |     44.944 |     0.1
    2 |   1.4939 |     45.905 |   1.3689 |     44.725 |     0.2
    3 |   1.3914 |     44.551 |   1.2999 |     42.821 |     0.4
    4 |   1.3297 |     43.747 |   1.2693 |     41.417 |     0.5
    5 |   1.2877 |     42.707 |   1.2308 |     40.855 |     0.6
    6 |   1.2535 |     42.107 |   1.2069 |     41.511 |     0.7
    7 |   1.2212 |     41.001 |   1.1659 |     38.889 |     0.8
    8 |   1.1969 |     40.434 |   1.1430 |     38.171 |     1.0
    9 |   1.1741 |     39.724 |   1.1281 |     38.171 |     1.1
   10 |   1.1481 |     38.926 |   1.1016 |     37.079 |     1.2
   11 |   1.1315 |     38.524 |   1.0903 |     36.236 |     1.3
   12 |   1.1073 |     37.583 |   1.0715 |     35.955 |     1.5
   13 |   1.0918 |     37.126 |   1.0687 |     35.581 |     1.6
   14 |   1.0753 |     36.707 |   1.0510 |     34.894 |     1.7
   15 |   1.0569 |     35.810 |   1.0487 |     34.894 |     1.8
   16 |   1.0420 |     35.480 |   1.0431 |     34.988 |     2.0
   17 |   1.0220 |     34.539 |   1.0293 |     34.363 |     2.1
   18 |   1.0079 |     34.225 |   1.0174 |     33.864 |     2.2
   19 |   0.9938 |     33.460 |   1.0118 |     33.052 |     2.3
   20 |   0.9833 |     33.278 |   1.0079 |     33.614 |     2.4
   21 |   0.9636 |     32.645 |   1.0019 |     33.677 |     2.6
   22 |   0.9534 |     32.332 |   1.0054 |     33.302 |     2.7
   23 |   0.9345 |     31.924 |   0.9970 |     32.928 |     2.8
   24 |   0.9232 |     31.319 |   0.9929 |     32.584 |     2.9
   25 |   0.9098 |     30.983 |   0.9753 |     32.210 |     3.1
   26 |   0.9023 |     30.647 |   0.9874 |     32.584 |     3.2
   27 |   0.8881 |     30.119 |   0.9854 |     32.397 |     3.3
   28 |   0.8777 |     29.635 |   0.9672 |     32.147 |     3.4
   29 |   0.8642 |     29.161 |   0.9580 |     31.648 |     3.5
   30 |   0.8539 |     28.820 |   0.9593 |     31.024 |     3.7
   31 |   0.8368 |     28.132 |   0.9683 |     31.742 |     3.8
   32 |   0.8252 |     27.895 |   0.9574 |     30.899 |     3.9
   33 |   0.8140 |     27.312 |   0.9675 |     31.211 |     4.0
   34 |   0.7988 |     27.207 |   0.9484 |     31.117 |     4.2
   35 |   0.7965 |     26.882 |   0.9540 |     31.367 |     4.3
   36 |   0.7819 |     26.321 |   0.9532 |     31.149 |     4.4
   37 |   0.7693 |     26.029 |   0.9530 |     30.961 |     4.5
   38 |   0.7583 |     25.385 |   0.9625 |     30.899 |     4.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 358,946

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6638 |     48.888 |   1.3255 |     44.600 |     0.1
    2 |   1.2814 |     44.033 |   1.2210 |     42.665 |     0.2
    3 |   1.2087 |     42.151 |   1.1640 |     39.732 |     0.3
    4 |   1.1507 |     40.417 |   1.1499 |     39.232 |     0.3
    5 |   1.1069 |     38.777 |   1.1280 |     38.421 |     0.4
    6 |   1.0708 |     37.401 |   1.0900 |     38.046 |     0.5
    7 |   1.0337 |     36.212 |   1.0467 |     35.893 |     0.6
    8 |   1.0039 |     35.337 |   1.0333 |     35.019 |     0.7
    9 |   0.9778 |     34.098 |   1.0246 |     35.737 |     0.8
   10 |   0.9510 |     33.300 |   1.0090 |     34.894 |     0.9
   11 |   0.9208 |     31.886 |   0.9881 |     33.833 |     1.0
   12 |   0.8944 |     31.110 |   0.9814 |     33.271 |     1.1
   13 |   0.8622 |     30.251 |   0.9742 |     33.365 |     1.1
   14 |   0.8375 |     29.167 |   0.9509 |     31.898 |     1.2
   15 |   0.8188 |     28.451 |   0.9448 |     32.584 |     1.3
   16 |   0.7869 |     27.378 |   0.9497 |     31.648 |     1.4
   17 |   0.7572 |     26.062 |   0.9333 |     31.086 |     1.5
   18 |   0.7288 |     25.374 |   0.9277 |     30.993 |     1.6
   19 |   0.7040 |     24.092 |   0.9425 |     30.243 |     1.7
   20 |   0.6783 |     23.558 |   0.9243 |     30.743 |     1.8
   21 |   0.6569 |     22.710 |   0.9213 |     29.994 |     1.8
   22 |   0.6348 |     21.780 |   0.9251 |     30.119 |     1.9
   23 |   0.6051 |     20.949 |   0.9451 |     30.087 |     2.0
   24 |   0.5949 |     20.498 |   0.9417 |     30.087 |     2.1
   25 |   0.5626 |     19.457 |   0.9365 |     29.432 |     2.2
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 1,179,298

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.5672 |     49.240 |   1.3022 |     44.663 |     0.1
    2 |   1.2573 |     43.648 |   1.1938 |     42.135 |     0.3
    3 |   1.1887 |     41.606 |   1.1651 |     40.886 |     0.4
    4 |   1.1522 |     40.709 |   1.1223 |     39.139 |     0.6
    5 |   1.1202 |     39.795 |   1.1165 |     39.263 |     0.7
    6 |   1.0998 |     39.036 |   1.1206 |     39.794 |     0.8
    7 |   1.0966 |     38.788 |   1.0844 |     37.672 |     1.0
    8 |   1.0707 |     38.375 |   1.0784 |     38.514 |     1.1
    9 |   1.0567 |     38.348 |   1.0685 |     38.202 |     1.3
   10 |   1.0449 |     37.500 |   1.0540 |     37.547 |     1.4
   11 |   1.0454 |     37.715 |   1.0721 |     38.951 |     1.5
   12 |   1.0494 |     38.111 |   1.0615 |     38.202 |     1.7
   13 |   1.0261 |     37.241 |   1.0483 |     36.829 |     1.8
   14 |   1.0113 |     36.366 |   1.0405 |     36.954 |     2.0
   15 |   1.0058 |     36.322 |   1.0216 |     36.486 |     2.1
   16 |   1.0042 |     36.652 |   1.0281 |     35.893 |     2.2
   17 |   0.9941 |     36.262 |   1.0199 |     36.891 |     2.4
   18 |   0.9883 |     35.986 |   1.0266 |     35.737 |     2.5
   19 |   0.9812 |     35.777 |   1.0163 |     36.392 |     2.7
   20 |   0.9746 |     35.474 |   1.0328 |     36.517 |     2.8
   21 |   0.9713 |     35.574 |   1.0217 |     36.829 |     3.0
   22 |   0.9748 |     35.590 |   1.0112 |     36.486 |     3.1
   23 |   0.9733 |     35.469 |   1.0363 |     36.923 |     3.2
   24 |   0.9740 |     35.205 |   1.0128 |     36.361 |     3.4
   25 |   0.9681 |     35.414 |   1.0050 |     36.142 |     3.5
   26 |   0.9591 |     34.946 |   1.0158 |     37.422 |     3.7
   27 |   0.9602 |     35.293 |   1.0207 |     36.923 |     3.8
   28 |   0.9581 |     35.073 |   1.0375 |     36.829 |     3.9
   29 |   0.9539 |     34.715 |   1.0073 |     36.205 |     4.1
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 343,330

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9044 |     52.708 |   1.3781 |     45.256 |     0.1
    2 |   1.3855 |     46.285 |   1.2992 |     43.602 |     0.2
    3 |   1.3220 |     45.404 |   1.2529 |     43.040 |     0.3
    4 |   1.2818 |     44.744 |   1.2062 |     42.509 |     0.4
    5 |   1.2530 |     44.011 |   1.1771 |     40.293 |     0.5
    6 |   1.2292 |     43.131 |   1.1658 |     39.825 |     0.6
    7 |   1.2151 |     42.894 |   1.1431 |     39.981 |     0.8
    8 |   1.1980 |     42.107 |   1.1466 |     39.794 |     0.9
    9 |   1.1831 |     41.584 |   1.1308 |     40.293 |     1.0
   10 |   1.1739 |     41.309 |   1.1324 |     39.981 |     1.1
   11 |   1.1556 |     40.533 |   1.1221 |     39.388 |     1.2
   12 |   1.1481 |     40.533 |   1.1168 |     39.232 |     1.3
   13 |   1.1430 |     40.428 |   1.0930 |     37.516 |     1.4
   14 |   1.1278 |     39.878 |   1.0932 |     38.639 |     1.5
   15 |   1.1198 |     39.410 |   1.0946 |     38.577 |     1.6
   16 |   1.1095 |     39.124 |   1.0974 |     39.014 |     1.7
   17 |   1.1014 |     39.025 |   1.0879 |     39.014 |     1.8
   18 |   1.0889 |     38.810 |   1.0850 |     38.983 |     1.9
   19 |   1.0835 |     38.166 |   1.0898 |     38.327 |     2.1
   20 |   1.0724 |     38.072 |   1.0811 |     38.421 |     2.2
   21 |   1.0622 |     37.720 |   1.0845 |     37.984 |     2.3
   22 |   1.0525 |     37.390 |   1.0746 |     37.422 |     2.4
   23 |   1.0466 |     37.098 |   1.0623 |     37.859 |     2.5
   24 |   1.0375 |     36.614 |   1.0755 |     37.360 |     2.6
   25 |   1.0319 |     36.427 |   1.0662 |     36.735 |     2.7
   26 |   1.0258 |     36.416 |   1.0582 |     37.672 |     2.8
   27 |   1.0151 |     35.865 |   1.0598 |     37.703 |     2.9
   28 |   1.0075 |     35.728 |   1.0734 |     38.140 |     3.0
   29 |   0.9991 |     35.051 |   1.0775 |     38.514 |     3.1
   30 |   0.9896 |     34.852 |   1.0662 |     36.985 |     3.2
   31 |   0.9819 |     34.583 |   1.0576 |     37.079 |     3.4
   32 |   0.9767 |     34.291 |   1.0609 |     38.546 |     3.5
   33 |   0.9719 |     34.065 |   1.0746 |     37.921 |     3.6
   34 |   0.9626 |     34.192 |   1.0835 |     37.859 |     3.7
   35 |   0.9602 |     33.603 |   1.0620 |     37.921 |     3.8
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 2
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 292,194

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3535 |     53.710 |   1.8017 |     45.225 |     0.1
    2 |   1.6510 |     45.949 |   1.5487 |     45.037 |     0.2
    3 |   1.4968 |     45.365 |   1.4504 |     43.789 |     0.2
    4 |   1.4219 |     44.314 |   1.3895 |     43.571 |     0.3
    5 |   1.3703 |     43.808 |   1.3475 |     42.541 |     0.4
    6 |   1.3310 |     42.828 |   1.3066 |     41.698 |     0.5
    7 |   1.2958 |     42.168 |   1.2868 |     41.323 |     0.6
    8 |   1.2698 |     41.777 |   1.2566 |     40.730 |     0.6
    9 |   1.2428 |     40.687 |   1.2284 |     40.106 |     0.7
   10 |   1.2190 |     40.610 |   1.2111 |     39.482 |     0.8
   11 |   1.1965 |     39.960 |   1.1927 |     38.577 |     0.9
   12 |   1.1770 |     39.344 |   1.1762 |     38.233 |     1.0
   13 |   1.1576 |     38.788 |   1.1605 |     37.734 |     1.0
   14 |   1.1370 |     37.924 |   1.1434 |     38.015 |     1.1
   15 |   1.1183 |     37.638 |   1.1314 |     37.547 |     1.2
   16 |   1.1041 |     37.362 |   1.1184 |     36.954 |     1.3
   17 |   1.0859 |     36.619 |   1.1032 |     36.517 |     1.4
   18 |   1.0705 |     36.113 |   1.1052 |     37.141 |     1.5
   19 |   1.0571 |     35.788 |   1.0792 |     35.955 |     1.5
   20 |   1.0420 |     35.111 |   1.0748 |     36.049 |     1.6
   21 |   1.0295 |     34.753 |   1.0585 |     34.831 |     1.7
   22 |   1.0146 |     34.275 |   1.0569 |     34.956 |     1.8
   23 |   1.0024 |     33.867 |   1.0617 |     35.393 |     1.9
   24 |   0.9912 |     33.344 |   1.0365 |     34.769 |     1.9
   25 |   0.9779 |     32.899 |   1.0310 |     33.770 |     2.0
   26 |   0.9654 |     32.365 |   1.0353 |     34.519 |     2.1
   27 |   0.9548 |     32.001 |   1.0228 |     33.458 |     2.2
   28 |   0.9415 |     31.611 |   1.0168 |     33.458 |     2.3
   29 |   0.9298 |     31.121 |   1.0122 |     33.396 |     2.3
   30 |   0.9186 |     30.768 |   1.0163 |     32.990 |     2.4
   31 |   0.9093 |     30.422 |   0.9995 |     33.614 |     2.5
   32 |   0.9028 |     30.367 |   0.9939 |     32.990 |     2.6
   33 |   0.8882 |     29.750 |   0.9978 |     32.772 |     2.7
   34 |   0.8772 |     29.365 |   0.9838 |     32.584 |     2.7
   35 |   0.8644 |     28.809 |   0.9874 |     32.397 |     2.8
   36 |   0.8575 |     28.561 |   0.9853 |     31.898 |     2.9
   37 |   0.8503 |     28.506 |   0.9658 |     32.116 |     3.0
   38 |   0.8349 |     27.835 |   0.9624 |     31.273 |     3.1
   39 |   0.8244 |     27.378 |   0.9593 |     31.273 |     3.2
   40 |   0.8154 |     27.290 |   0.9686 |     32.116 |     3.2
   41 |   0.8076 |     26.739 |   0.9512 |     31.024 |     3.3
   42 |   0.8010 |     26.734 |   0.9580 |     31.648 |     3.4
   43 |   0.7895 |     26.249 |   0.9573 |     31.586 |     3.5
   44 |   0.7820 |     25.974 |   0.9472 |     31.492 |     3.6
   45 |   0.7691 |     25.655 |   0.9521 |     31.398 |     3.6
   46 |   0.7585 |     25.182 |   0.9509 |     30.680 |     3.7
   47 |   0.7531 |     25.033 |   0.9435 |     31.117 |     3.8
   48 |   0.7479 |     25.028 |   0.9441 |     31.398 |     3.9
   49 |   0.7357 |     24.439 |   0.9394 |     30.774 |     4.0
   50 |   0.7267 |     23.927 |   0.9570 |     31.710 |     4.0
   51 |   0.7158 |     23.718 |   0.9559 |     31.055 |     4.1
   52 |   0.7060 |     23.244 |   0.9460 |     31.024 |     4.2
   53 |   0.6998 |     23.096 |   0.9484 |     30.805 |     4.3
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 393,570

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.7149 |     48.976 |   1.3443 |     43.945 |     0.1
    2 |   1.3000 |     44.000 |   1.2316 |     42.353 |     0.2
    3 |   1.2293 |     42.889 |   1.1800 |     41.074 |     0.3
    4 |   1.1824 |     41.408 |   1.1506 |     40.169 |     0.4
    5 |   1.1414 |     40.219 |   1.1289 |     40.106 |     0.5
    6 |   1.1107 |     39.250 |   1.0969 |     38.327 |     0.6
    7 |   1.0775 |     38.276 |   1.0919 |     37.984 |     0.7
    8 |   1.0601 |     38.045 |   1.0673 |     37.797 |     0.8
    9 |   1.0303 |     36.757 |   1.0602 |     37.609 |     0.9
   10 |   1.0062 |     35.975 |   1.0292 |     36.017 |     1.1
   11 |   0.9884 |     35.507 |   1.0336 |     36.610 |     1.2
   12 |   0.9644 |     34.704 |   1.0065 |     35.955 |     1.3
   13 |   0.9441 |     33.295 |   0.9836 |     34.831 |     1.4
   14 |   0.9189 |     32.409 |   0.9889 |     34.114 |     1.5
   15 |   0.8920 |     31.671 |   0.9791 |     34.488 |     1.6
   16 |   0.8794 |     30.917 |   0.9855 |     33.958 |     1.7
   17 |   0.8573 |     30.212 |   0.9776 |     33.833 |     1.8
   18 |   0.8442 |     29.816 |   0.9512 |     33.052 |     1.9
   19 |   0.8221 |     28.743 |   0.9560 |     32.803 |     2.0
   20 |   0.7942 |     27.961 |   0.9431 |     32.241 |     2.1
   21 |   0.7819 |     27.257 |   0.9813 |     32.896 |     2.2
   22 |   0.7604 |     26.745 |   0.9623 |     31.929 |     2.3
   23 |   0.7504 |     26.591 |   0.9575 |     32.022 |     2.4
   24 |   0.7302 |     25.622 |   0.9514 |     31.679 |     2.5
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 343,330

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4653 |     62.054 |   1.8365 |     45.818 |     0.1
    2 |   1.6640 |     45.729 |   1.5458 |     43.883 |     0.2
    3 |   1.4917 |     44.435 |   1.4367 |     43.102 |     0.3
    4 |   1.4100 |     43.879 |   1.3717 |     42.946 |     0.4
    5 |   1.3524 |     42.591 |   1.3201 |     41.230 |     0.5
    6 |   1.3136 |     42.256 |   1.2922 |     40.980 |     0.6
    7 |   1.2787 |     41.458 |   1.2566 |     40.980 |     0.7
    8 |   1.2468 |     40.841 |   1.2355 |     39.763 |     0.8
    9 |   1.2197 |     40.357 |   1.2098 |     40.106 |     0.9
   10 |   1.1965 |     39.548 |   1.1887 |     39.045 |     1.0
   11 |   1.1746 |     39.003 |   1.1597 |     37.984 |     1.1
   12 |   1.1554 |     38.392 |   1.1546 |     37.921 |     1.2
   13 |   1.1380 |     38.205 |   1.1434 |     38.171 |     1.3
   14 |   1.1226 |     37.731 |   1.1367 |     37.859 |     1.4
   15 |   1.1053 |     37.159 |   1.1133 |     36.330 |     1.5
   16 |   1.0873 |     36.361 |   1.1123 |     37.016 |     1.6
   17 |   1.0706 |     35.871 |   1.0891 |     36.080 |     1.7
   18 |   1.0576 |     35.403 |   1.0799 |     35.955 |     1.8
   19 |   1.0427 |     34.863 |   1.0759 |     35.456 |     1.9
   20 |   1.0307 |     34.115 |   1.0595 |     35.300 |     2.0
   21 |   1.0157 |     33.509 |   1.0634 |     35.175 |     2.1
   22 |   1.0005 |     33.053 |   1.0447 |     34.582 |     2.2
   23 |   0.9922 |     32.816 |   1.0410 |     33.958 |     2.3
   24 |   0.9766 |     32.381 |   1.0297 |     34.457 |     2.4
   25 |   0.9667 |     31.935 |   1.0292 |     34.488 |     2.5
   26 |   0.9555 |     31.550 |   1.0144 |     33.770 |     2.6
   27 |   0.9378 |     30.383 |   1.0218 |     34.457 |     2.7
   28 |   0.9318 |     30.856 |   1.0241 |     34.488 |     2.8
   29 |   0.9188 |     30.190 |   1.0071 |     33.115 |     2.9
   30 |   0.9043 |     29.690 |   1.0070 |     33.365 |     3.0
   31 |   0.8982 |     29.585 |   0.9859 |     33.084 |     3.1
   32 |   0.8833 |     28.781 |   1.0024 |     33.177 |     3.2
   33 |   0.8738 |     28.704 |   0.9837 |     32.522 |     3.3
   34 |   0.8637 |     28.269 |   0.9913 |     33.146 |     3.4
   35 |   0.8565 |     28.143 |   0.9857 |     32.990 |     3.5
   36 |   0.8388 |     27.328 |   0.9794 |     32.896 |     3.6
   37 |   0.8344 |     27.180 |   0.9863 |     33.458 |     3.7
   38 |   0.8289 |     27.147 |   0.9687 |     32.491 |     3.8
   39 |   0.8155 |     26.673 |   0.9772 |     33.052 |     3.9
   40 |   0.8015 |     26.349 |   0.9693 |     32.959 |     4.0
   41 |   0.7954 |     26.233 |   0.9653 |     32.397 |     4.1
   42 |   0.7845 |     25.473 |   0.9762 |     32.615 |     4.2
   43 |   0.7730 |     25.198 |   0.9650 |     32.335 |     4.3
   44 |   0.7651 |     25.000 |   0.9719 |     32.709 |     4.4
   45 |   0.7581 |     24.483 |   0.9591 |     32.054 |     4.5
   46 |   0.7474 |     24.582 |   0.9561 |     31.648 |     4.6
   47 |   0.7351 |     24.031 |   0.9613 |     32.428 |     4.7
   48 |   0.7283 |     23.464 |   0.9674 |     32.179 |     4.8
   49 |   0.7201 |     23.497 |   0.9776 |     31.617 |     4.9
   50 |   0.7057 |     22.969 |   0.9901 |     32.022 |     5.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 1,179,298

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.9948 |     52.477 |   1.4903 |     44.944 |     0.2
    2 |   1.4199 |     44.821 |   1.3483 |     43.664 |     0.3
    3 |   1.3240 |     43.186 |   1.2722 |     41.511 |     0.5
    4 |   1.2615 |     41.980 |   1.2446 |     40.855 |     0.7
    5 |   1.2105 |     40.533 |   1.1902 |     39.014 |     0.9
    6 |   1.1680 |     39.520 |   1.1594 |     38.358 |     1.0
    7 |   1.1334 |     38.414 |   1.1286 |     37.235 |     1.2
    8 |   1.1022 |     37.247 |   1.1078 |     36.985 |     1.4
    9 |   1.0709 |     36.162 |   1.0639 |     35.050 |     1.6
   10 |   1.0475 |     35.254 |   1.0544 |     35.487 |     1.8
   11 |   1.0159 |     33.774 |   1.0357 |     33.739 |     1.9
   12 |   0.9872 |     33.157 |   1.0199 |     33.396 |     2.1
   13 |   0.9620 |     32.106 |   0.9998 |     32.928 |     2.3
   14 |   0.9322 |     30.746 |   0.9982 |     32.865 |     2.5
   15 |   0.9102 |     30.229 |   0.9870 |     32.459 |     2.6
   16 |   0.8856 |     29.381 |   0.9632 |     31.617 |     2.8
   17 |   0.8584 |     28.501 |   0.9376 |     31.398 |     3.0
   18 |   0.8298 |     27.229 |   0.9566 |     32.428 |     3.2
   19 |   0.8043 |     26.310 |   0.9260 |     29.994 |     3.3
   20 |   0.7816 |     25.638 |   0.9330 |     30.337 |     3.5
   21 |   0.7576 |     25.017 |   0.9339 |     30.680 |     3.7
   22 |   0.7318 |     24.213 |   0.9157 |     29.963 |     3.9
   23 |   0.7084 |     23.261 |   0.9220 |     29.869 |     4.1
   24 |   0.6890 |     22.479 |   0.9178 |     30.119 |     4.2
   25 |   0.6637 |     21.758 |   0.9127 |     29.370 |     4.4
   26 |   0.6414 |     21.158 |   0.9105 |     29.120 |     4.6
   27 |   0.6211 |     20.327 |   0.9188 |     29.120 |     4.8
   28 |   0.6016 |     19.567 |   0.9156 |     28.870 |     4.9
   29 |   0.5730 |     18.445 |   0.9458 |     30.025 |     5.1
   30 |   0.5528 |     17.856 |   0.9262 |     28.745 |     5.3
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 64
Encoder layers: 4
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 327,522

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.9231 |     75.594 |   2.2873 |     52.778 |     0.1
    2 |   2.2289 |     52.686 |   1.7623 |     47.878 |     0.2
    3 |   1.8366 |     47.639 |   1.5810 |     46.130 |     0.3
    4 |   1.6485 |     46.472 |   1.4946 |     44.944 |     0.4
    5 |   1.5629 |     46.186 |   1.4559 |     44.944 |     0.6
    6 |   1.5132 |     46.307 |   1.4259 |     44.944 |     0.7
    7 |   1.4766 |     46.164 |   1.4019 |     44.975 |     0.8
    8 |   1.4500 |     46.136 |   1.3847 |     44.944 |     0.9
    9 |   1.4319 |     46.031 |   1.3685 |     44.913 |     1.0
   10 |   1.4140 |     45.734 |   1.3533 |     44.663 |     1.1
   11 |   1.3971 |     45.591 |   1.3362 |     44.101 |     1.2
   12 |   1.3801 |     45.343 |   1.3209 |     44.070 |     1.3
   13 |   1.3689 |     45.354 |   1.3144 |     43.258 |     1.5
   14 |   1.3586 |     45.002 |   1.2991 |     42.541 |     1.6
   15 |   1.3456 |     44.903 |   1.2902 |     42.884 |     1.7
   16 |   1.3370 |     44.589 |   1.2793 |     41.885 |     1.8
   17 |   1.3245 |     44.111 |   1.2700 |     41.355 |     1.9
   18 |   1.3162 |     44.077 |   1.2599 |     40.855 |     2.0
   19 |   1.3066 |     43.648 |   1.2480 |     40.605 |     2.1
   20 |   1.2981 |     43.637 |   1.2471 |     40.699 |     2.2
   21 |   1.2862 |     43.191 |   1.2428 |     40.668 |     2.4
   22 |   1.2775 |     42.740 |   1.2424 |     41.261 |     2.5
   23 |   1.2750 |     43.120 |   1.2300 |     40.918 |     2.6
   24 |   1.2641 |     42.476 |   1.2241 |     40.481 |     2.7
   25 |   1.2563 |     42.261 |   1.2113 |     40.012 |     2.8
   26 |   1.2530 |     42.322 |   1.2201 |     40.574 |     2.9
   27 |   1.2403 |     42.096 |   1.2032 |     39.763 |     3.0
   28 |   1.2337 |     41.606 |   1.1908 |     40.231 |     3.2
   29 |   1.2257 |     41.435 |   1.1914 |     39.638 |     3.3
   30 |   1.2233 |     41.491 |   1.1915 |     40.512 |     3.4
   31 |   1.2133 |     41.199 |   1.1859 |     40.137 |     3.5
   32 |   1.2092 |     41.391 |   1.2036 |     40.824 |     3.6
   33 |   1.2038 |     41.045 |   1.1995 |     40.169 |     3.7
   34 |   1.1964 |     40.676 |   1.1708 |     39.357 |     3.8
   35 |   1.1917 |     41.083 |   1.1695 |     38.514 |     3.9
   36 |   1.1924 |     40.803 |   1.1765 |     39.607 |     4.1
   37 |   1.1826 |     40.280 |   1.1673 |     39.326 |     4.2
   38 |   1.1789 |     40.103 |   1.1513 |     38.670 |     4.3
   39 |   1.1718 |     39.740 |   1.1615 |     39.326 |     4.4
   40 |   1.1651 |     40.015 |   1.1483 |     39.201 |     4.5
   41 |   1.1592 |     39.773 |   1.1657 |     39.232 |     4.6
   42 |   1.1556 |     39.404 |   1.1403 |     38.202 |     4.7
   43 |   1.1513 |     39.162 |   1.1598 |     39.139 |     4.8
   44 |   1.1476 |     39.118 |   1.1587 |     38.795 |     5.0
   45 |   1.1449 |     39.443 |   1.1476 |     38.171 |     5.1
   46 |   1.1401 |     39.316 |   1.1308 |     37.703 |     5.2
   47 |   1.1365 |     38.711 |   1.1323 |     37.765 |     5.3
   48 |   1.1321 |     39.030 |   1.1213 |     37.953 |     5.4
   49 |   1.1279 |     38.414 |   1.1352 |     38.390 |     5.5
   50 |   1.1218 |     38.689 |   1.1372 |     38.109 |     5.6
   51 |   1.1169 |     38.425 |   1.1268 |     37.890 |     5.7
   52 |   1.1086 |     37.979 |   1.1253 |     37.578 |     5.9
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.2
Trainable parameters: 425,698

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.7928 |     51.051 |   1.3880 |     44.881 |     0.1
    2 |   1.3783 |     45.916 |   1.2973 |     43.477 |     0.2
    3 |   1.3136 |     44.655 |   1.2358 |     40.886 |     0.3
    4 |   1.2647 |     43.351 |   1.1787 |     40.012 |     0.4
    5 |   1.2184 |     41.892 |   1.1531 |     39.451 |     0.5
    6 |   1.1843 |     40.874 |   1.1311 |     39.139 |     0.6
    7 |   1.1546 |     39.702 |   1.1212 |     38.140 |     0.7
    8 |   1.1248 |     38.282 |   1.1046 |     37.297 |     0.8
    9 |   1.1028 |     37.759 |   1.1013 |     38.171 |     0.9
   10 |   1.0813 |     36.851 |   1.0784 |     36.610 |     1.0
   11 |   1.0598 |     36.421 |   1.0795 |     36.111 |     1.2
   12 |   1.0351 |     35.530 |   1.0571 |     35.893 |     1.3
   13 |   1.0114 |     34.665 |   1.0451 |     36.111 |     1.4
   14 |   0.9896 |     34.005 |   1.0642 |     36.174 |     1.5
   15 |   0.9820 |     33.746 |   1.0396 |     34.831 |     1.6
   16 |   0.9568 |     32.882 |   1.0343 |     34.894 |     1.7
   17 |   0.9335 |     32.183 |   1.0361 |     34.270 |     1.8
   18 |   0.9171 |     31.451 |   1.0259 |     34.332 |     1.9
   19 |   0.8926 |     30.812 |   1.0291 |     33.958 |     2.0
   20 |   0.8782 |     30.124 |   1.0409 |     34.582 |     2.1
   21 |   0.8593 |     29.431 |   1.0362 |     34.769 |     2.2
   22 |   0.8349 |     28.611 |   1.0324 |     33.365 |     2.3
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 4
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 392,162

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6290 |     47.947 |   1.3226 |     44.070 |     0.1
    2 |   1.2875 |     44.380 |   1.2429 |     41.916 |     0.2
    3 |   1.2139 |     42.503 |   1.1803 |     41.854 |     0.3
    4 |   1.1786 |     41.980 |   1.1683 |     43.009 |     0.4
    5 |   1.1530 |     41.584 |   1.1493 |     41.042 |     0.5
    6 |   1.1305 |     40.858 |   1.1249 |     41.386 |     0.6
    7 |   1.1182 |     41.138 |   1.1229 |     41.604 |     0.7
    8 |   1.1080 |     40.929 |   1.1006 |     39.732 |     0.8
    9 |   1.0981 |     40.632 |   1.0978 |     40.637 |     0.9
   10 |   1.0940 |     40.434 |   1.0959 |     40.044 |     1.0
   11 |   1.0875 |     40.258 |   1.1001 |     40.137 |     1.1
   12 |   1.0860 |     40.659 |   1.0842 |     40.044 |     1.1
   13 |   1.0778 |     39.977 |   1.0871 |     40.730 |     1.2
   14 |   1.0702 |     39.718 |   1.0776 |     39.232 |     1.3
   15 |   1.0591 |     38.832 |   1.0643 |     38.639 |     1.4
   16 |   1.0467 |     38.441 |   1.0487 |     37.484 |     1.5
   17 |   1.0394 |     38.260 |   1.0619 |     37.921 |     1.6
   18 |   1.0275 |     37.902 |   1.0410 |     37.516 |     1.7
   19 |   1.0173 |     37.208 |   1.0357 |     37.079 |     1.8
   20 |   1.0102 |     37.137 |   1.0317 |     36.923 |     1.9
   21 |   1.0028 |     36.977 |   1.0287 |     36.735 |     2.0
   22 |   0.9912 |     36.372 |   1.0188 |     36.767 |     2.1
   23 |   0.9825 |     36.355 |   1.0109 |     36.767 |     2.2
   24 |   0.9776 |     36.223 |   1.0021 |     36.017 |     2.3
   25 |   0.9648 |     35.441 |   0.9928 |     35.581 |     2.4
   26 |   0.9586 |     35.194 |   1.0026 |     36.330 |     2.5
   27 |   0.9492 |     34.919 |   0.9900 |     34.675 |     2.6
   28 |   0.9389 |     34.357 |   0.9965 |     36.392 |     2.7
   29 |   0.9287 |     33.856 |   0.9796 |     35.300 |     2.8
   30 |   0.9179 |     33.647 |   0.9751 |     34.426 |     2.9
   31 |   0.9097 |     33.328 |   0.9730 |     34.207 |     3.0
   32 |   0.8960 |     32.805 |   0.9653 |     34.270 |     3.1
   33 |   0.8857 |     32.260 |   0.9648 |     33.645 |     3.2
   34 |   0.8822 |     32.062 |   0.9645 |     34.238 |     3.3
   35 |   0.8681 |     31.401 |   0.9595 |     33.427 |     3.4
   36 |   0.8646 |     31.412 |   0.9650 |     34.301 |     3.5
   37 |   0.8534 |     30.675 |   0.9678 |     33.926 |     3.6
   38 |   0.8402 |     30.339 |   0.9358 |     32.615 |     3.6
   39 |   0.8323 |     29.926 |   0.9599 |     32.709 |     3.7
   40 |   0.8303 |     29.739 |   0.9461 |     33.021 |     3.8
   41 |   0.8101 |     29.123 |   0.9268 |     32.022 |     3.9
   42 |   0.8116 |     29.431 |   0.9410 |     32.491 |     4.0
   43 |   0.7937 |     28.429 |   0.9484 |     32.272 |     4.1
   44 |   0.7887 |     28.358 |   0.9317 |     32.022 |     4.2
   45 |   0.7825 |     28.391 |   0.9551 |     32.678 |     4.3
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 425,698

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5311 |     61.939 |   1.7625 |     45.381 |     0.1
    2 |   1.7152 |     46.136 |   1.5338 |     44.944 |     0.2
    3 |   1.5572 |     45.971 |   1.4567 |     44.819 |     0.3
    4 |   1.4815 |     45.630 |   1.4013 |     44.132 |     0.4
    5 |   1.4306 |     44.881 |   1.3664 |     43.820 |     0.5
    6 |   1.3935 |     44.617 |   1.3333 |     43.290 |     0.6
    7 |   1.3638 |     44.237 |   1.3068 |     42.697 |     0.7
    8 |   1.3358 |     43.808 |   1.2845 |     42.572 |     0.8
    9 |   1.3125 |     43.417 |   1.2633 |     42.228 |     0.9
   10 |   1.2895 |     42.806 |   1.2386 |     41.136 |     1.0
   11 |   1.2743 |     42.641 |   1.2187 |     40.231 |     1.1
   12 |   1.2542 |     41.859 |   1.1995 |     39.732 |     1.3
   13 |   1.2385 |     41.612 |   1.1867 |     39.763 |     1.4
   14 |   1.2238 |     41.364 |   1.1730 |     39.388 |     1.5
   15 |   1.2063 |     40.797 |   1.1544 |     38.983 |     1.6
   16 |   1.1967 |     40.555 |   1.1435 |     37.765 |     1.7
   17 |   1.1824 |     39.867 |   1.1292 |     38.452 |     1.8
   18 |   1.1678 |     39.762 |   1.1186 |     37.984 |     1.9
   19 |   1.1579 |     39.476 |   1.1092 |     37.016 |     2.0
   20 |   1.1511 |     39.157 |   1.1038 |     37.953 |     2.1
   21 |   1.1358 |     38.595 |   1.0941 |     36.767 |     2.2
   22 |   1.1305 |     38.540 |   1.0926 |     36.954 |     2.3
   23 |   1.1234 |     38.606 |   1.0877 |     36.423 |     2.4
   24 |   1.1112 |     37.880 |   1.0745 |     36.735 |     2.5
   25 |   1.1056 |     38.116 |   1.0653 |     36.080 |     2.6
   26 |   1.0959 |     37.770 |   1.0681 |     36.017 |     2.7
   27 |   1.0891 |     37.461 |   1.0561 |     34.800 |     2.8
   28 |   1.0825 |     37.236 |   1.0497 |     35.050 |     2.9
   29 |   1.0700 |     36.922 |   1.0437 |     35.144 |     3.0
   30 |   1.0673 |     36.581 |   1.0435 |     35.674 |     3.1
   31 |   1.0633 |     36.658 |   1.0324 |     34.426 |     3.2
   32 |   1.0542 |     36.135 |   1.0297 |     34.613 |     3.3
   33 |   1.0429 |     35.937 |   1.0251 |     34.613 |     3.4
   34 |   1.0377 |     35.750 |   1.0276 |     34.301 |     3.6
   35 |   1.0338 |     35.403 |   1.0160 |     34.488 |     3.7
   36 |   1.0208 |     35.045 |   1.0100 |     33.739 |     3.8
   37 |   1.0158 |     34.847 |   1.0040 |     33.240 |     3.9
   38 |   1.0112 |     34.786 |   1.0053 |     33.583 |     4.0
   39 |   1.0033 |     34.434 |   0.9989 |     33.084 |     4.1
   40 |   0.9987 |     34.043 |   0.9921 |     32.959 |     4.2
   41 |   0.9876 |     33.658 |   0.9926 |     33.521 |     4.3
   42 |   0.9858 |     33.823 |   0.9888 |     33.052 |     4.4
   43 |   0.9805 |     33.311 |   0.9828 |     32.522 |     4.5
   44 |   0.9740 |     33.361 |   0.9818 |     32.241 |     4.6
   45 |   0.9655 |     33.036 |   0.9767 |     32.210 |     4.7
   46 |   0.9578 |     32.876 |   0.9743 |     31.898 |     4.8
   47 |   0.9530 |     32.524 |   0.9687 |     31.991 |     4.9
   48 |   0.9506 |     32.497 |   0.9657 |     32.335 |     5.0
   49 |   0.9415 |     32.106 |   0.9630 |     31.773 |     5.1
   50 |   0.9325 |     31.858 |   0.9605 |     31.804 |     5.2
   51 |   0.9346 |     31.974 |   0.9574 |     31.648 |     5.3
   52 |   0.9227 |     31.280 |   0.9635 |     31.929 |     5.4
   53 |   0.9196 |     31.335 |   0.9653 |     32.491 |     5.5
   54 |   0.9145 |     31.456 |   0.9527 |     31.929 |     5.6
   55 |   0.9053 |     30.939 |   0.9507 |     31.554 |     5.8
   56 |   0.9028 |     31.082 |   0.9502 |     31.866 |     5.9
   57 |   0.8956 |     30.268 |   0.9468 |     31.211 |     6.0
   58 |   0.8939 |     30.603 |   0.9414 |     31.117 |     6.1
   59 |   0.8901 |     30.477 |   0.9446 |     30.899 |     6.2
   60 |   0.8826 |     30.190 |   0.9496 |     31.242 |     6.3
   61 |   0.8763 |     30.003 |   0.9364 |     30.961 |     6.4
   62 |   0.8741 |     29.552 |   0.9343 |     30.712 |     6.5
   63 |   0.8687 |     29.701 |   0.9314 |     30.743 |     6.6
   64 |   0.8639 |     29.447 |   0.9298 |     30.368 |     6.7
   65 |   0.8591 |     29.387 |   0.9290 |     30.493 |     6.8
   66 |   0.8587 |     29.189 |   0.9271 |     30.462 |     6.9
   67 |   0.8523 |     29.150 |   0.9281 |     30.400 |     7.0
   68 |   0.8484 |     28.737 |   0.9245 |     29.900 |     7.1
   69 |   0.8413 |     28.671 |   0.9315 |     30.556 |     7.2
   70 |   0.8346 |     28.479 |   0.9247 |     30.243 |     7.3
   71 |   0.8309 |     28.473 |   0.9253 |     29.463 |     7.4
   72 |   0.8256 |     28.325 |   0.9170 |     29.931 |     7.5
   73 |   0.8208 |     27.890 |   0.9186 |     29.401 |     7.6
   74 |   0.8247 |     28.231 |   0.9265 |     30.337 |     7.7
   75 |   0.8157 |     27.873 |   0.9188 |     29.713 |     7.8
   76 |   0.8092 |     27.543 |   0.9183 |     29.432 |     7.9
   77 |   0.8081 |     27.648 |   0.9150 |     29.120 |     8.1
   78 |   0.8032 |     27.510 |   0.9225 |     29.806 |     8.2
   79 |   0.7980 |     26.987 |   0.9134 |     29.151 |     8.3
   80 |   0.7964 |     26.915 |   0.9108 |     29.744 |     8.4
   81 |   0.7923 |     27.026 |   0.9113 |     29.994 |     8.5
   82 |   0.7856 |     26.789 |   0.9084 |     29.213 |     8.6
   83 |   0.7783 |     26.365 |   0.9099 |     29.026 |     8.7
   84 |   0.7847 |     26.937 |   0.9108 |     29.650 |     8.8
   85 |   0.7743 |     26.216 |   0.9137 |     29.713 |     8.9
   86 |   0.7690 |     26.382 |   0.9062 |     29.526 |     9.0
   87 |   0.7726 |     26.563 |   0.9161 |     29.370 |     9.1
   88 |   0.7639 |     26.062 |   0.9016 |     28.839 |     9.2
   89 |   0.7549 |     25.798 |   0.9075 |     29.401 |     9.3
   90 |   0.7507 |     25.694 |   0.9127 |     29.526 |     9.4
   91 |   0.7483 |     25.473 |   0.9125 |     29.931 |     9.5
   92 |   0.7500 |     25.550 |   0.9009 |     28.995 |     9.6
   93 |   0.7402 |     25.160 |   0.9054 |     29.931 |     9.7
   94 |   0.7368 |     25.000 |   0.9047 |     29.057 |     9.8
   95 |   0.7348 |     25.017 |   0.9046 |     29.619 |     9.9
   96 |   0.7331 |     25.099 |   0.9023 |     29.370 |    10.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 64
Encoder layers: 4
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 748,834

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6293 |     48.960 |   1.3135 |     44.288 |     0.1
    2 |   1.3227 |     44.512 |   1.2438 |     41.948 |     0.3
    3 |   1.2654 |     43.670 |   1.2139 |     41.136 |     0.4
    4 |   1.2341 |     43.241 |   1.1833 |     40.949 |     0.5
    5 |   1.2042 |     42.360 |   1.1605 |     40.075 |     0.6
    6 |   1.1822 |     41.298 |   1.1388 |     39.825 |     0.8
    7 |   1.1631 |     40.593 |   1.1817 |     40.793 |     0.9
    8 |   1.1455 |     40.406 |   1.1260 |     39.201 |     1.0
    9 |   1.1327 |     39.768 |   1.1335 |     38.795 |     1.2
   10 |   1.1192 |     39.300 |   1.1015 |     38.858 |     1.3
   11 |   1.1131 |     39.647 |   1.0921 |     38.483 |     1.4
   12 |   1.0972 |     38.601 |   1.1087 |     38.920 |     1.6
   13 |   1.0977 |     38.827 |   1.1234 |     38.296 |     1.7
   14 |   1.0823 |     38.491 |   1.1771 |     41.105 |     1.8
   15 |   1.0738 |     38.221 |   1.1171 |     38.390 |     2.0
   16 |   1.0791 |     38.298 |   1.0853 |     38.546 |     2.1
   17 |   1.0803 |     38.480 |   1.0984 |     38.109 |     2.2
   18 |   1.0848 |     38.298 |   1.1042 |     38.452 |     2.4
   19 |   1.0900 |     38.909 |   1.1053 |     38.951 |     2.5
   20 |   1.0922 |     39.707 |   1.1497 |     40.824 |     2.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 343,330

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.8460 |     51.673 |   1.3763 |     44.975 |     0.1
    2 |   1.3893 |     46.312 |   1.2972 |     44.663 |     0.2
    3 |   1.3314 |     45.696 |   1.2536 |     43.539 |     0.3
    4 |   1.2949 |     44.810 |   1.2223 |     42.790 |     0.4
    5 |   1.2623 |     44.430 |   1.2033 |     41.948 |     0.5
    6 |   1.2491 |     44.166 |   1.1878 |     41.854 |     0.6
    7 |   1.2330 |     43.670 |   1.1806 |     41.230 |     0.7
    8 |   1.2210 |     43.456 |   1.1695 |     41.511 |     0.9
    9 |   1.2086 |     43.224 |   1.1582 |     40.855 |     1.0
   10 |   1.1991 |     42.718 |   1.1467 |     41.448 |     1.1
   11 |   1.1922 |     42.685 |   1.1502 |     41.199 |     1.2
   12 |   1.1808 |     42.536 |   1.1408 |     40.637 |     1.3
   13 |   1.1719 |     42.311 |   1.1422 |     41.261 |     1.4
   14 |   1.1727 |     42.591 |   1.1289 |     40.543 |     1.5
   15 |   1.1670 |     42.311 |   1.1282 |     40.231 |     1.6
   16 |   1.1595 |     42.019 |   1.1256 |     40.481 |     1.7
   17 |   1.1533 |     41.788 |   1.1186 |     40.418 |     1.8
   18 |   1.1456 |     41.705 |   1.1250 |     40.668 |     1.9
   19 |   1.1473 |     41.386 |   1.1255 |     40.231 |     2.0
   20 |   1.1423 |     41.909 |   1.1138 |     40.325 |     2.1
   21 |   1.1385 |     41.568 |   1.1112 |     39.856 |     2.3
   22 |   1.1381 |     41.634 |   1.1126 |     40.200 |     2.4
   23 |   1.1337 |     41.749 |   1.1087 |     40.325 |     2.5
   24 |   1.1337 |     41.402 |   1.1035 |     40.169 |     2.6
   25 |   1.1294 |     41.430 |   1.1000 |     40.356 |     2.7
   26 |   1.1280 |     41.221 |   1.0999 |     40.387 |     2.8
   27 |   1.1242 |     41.496 |   1.0959 |     40.200 |     2.9
   28 |   1.1189 |     40.885 |   1.0951 |     39.981 |     3.0
   29 |   1.1195 |     41.094 |   1.0961 |     39.763 |     3.1
   30 |   1.1158 |     40.825 |   1.0894 |     39.825 |     3.2
   31 |   1.1120 |     41.078 |   1.0900 |     39.794 |     3.3
   32 |   1.1068 |     40.478 |   1.0917 |     39.544 |     3.4
   33 |   1.1004 |     40.054 |   1.0738 |     38.670 |     3.5
   34 |   1.0900 |     39.724 |   1.0634 |     37.672 |     3.6
   35 |   1.0844 |     39.261 |   1.0675 |     38.764 |     3.7
   36 |   1.0836 |     39.421 |   1.0646 |     38.764 |     3.9
   37 |   1.0833 |     39.465 |   1.0565 |     38.077 |     4.0
   38 |   1.0769 |     38.931 |   1.0536 |     37.984 |     4.1
   39 |   1.0710 |     38.882 |   1.0516 |     37.422 |     4.2
   40 |   1.0626 |     38.612 |   1.0425 |     37.422 |     4.3
   41 |   1.0595 |     38.474 |   1.0362 |     37.016 |     4.4
   42 |   1.0527 |     38.436 |   1.0424 |     37.235 |     4.5
   43 |   1.0468 |     37.957 |   1.0348 |     37.235 |     4.6
   44 |   1.0435 |     37.940 |   1.0163 |     36.298 |     4.7
   45 |   1.0400 |     37.858 |   1.0271 |     36.673 |     4.8
   46 |   1.0339 |     37.263 |   1.0276 |     36.517 |     4.9
   47 |   1.0292 |     37.362 |   1.0194 |     36.205 |     5.0
   48 |   1.0211 |     37.148 |   1.0300 |     37.640 |     5.1
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 814,242

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2621 |     57.150 |   1.5456 |     45.256 |     0.1
    2 |   1.5292 |     46.290 |   1.3972 |     44.507 |     0.2
    3 |   1.4170 |     45.448 |   1.3295 |     43.633 |     0.3
    4 |   1.3552 |     44.600 |   1.2790 |     42.072 |     0.4
    5 |   1.3088 |     43.428 |   1.2477 |     41.667 |     0.5
    6 |   1.2730 |     42.663 |   1.2140 |     40.418 |     0.7
    7 |   1.2406 |     41.513 |   1.1847 |     39.232 |     0.8
    8 |   1.2159 |     41.083 |   1.1617 |     38.421 |     0.9
    9 |   1.1931 |     40.599 |   1.1361 |     37.703 |     1.0
   10 |   1.1673 |     39.845 |   1.1186 |     36.954 |     1.1
   11 |   1.1475 |     39.080 |   1.0983 |     35.705 |     1.2
   12 |   1.1228 |     38.408 |   1.0851 |     35.861 |     1.3
   13 |   1.1089 |     37.819 |   1.0751 |     36.174 |     1.4
   14 |   1.0947 |     37.274 |   1.0648 |     36.017 |     1.5
   15 |   1.0809 |     36.856 |   1.0532 |     35.112 |     1.6
   16 |   1.0633 |     35.915 |   1.0477 |     35.019 |     1.8
   17 |   1.0452 |     35.546 |   1.0336 |     34.988 |     1.9
   18 |   1.0295 |     34.858 |   1.0257 |     34.894 |     2.0
   19 |   1.0183 |     34.924 |   1.0119 |     33.864 |     2.1
   20 |   1.0024 |     33.840 |   1.0073 |     33.396 |     2.2
   21 |   0.9883 |     33.587 |   0.9962 |     33.521 |     2.3
   22 |   0.9716 |     32.899 |   1.0025 |     33.989 |     2.4
   23 |   0.9621 |     32.530 |   1.0003 |     33.084 |     2.5
   24 |   0.9472 |     31.974 |   0.9897 |     33.115 |     2.6
   25 |   0.9382 |     31.666 |   0.9872 |     32.428 |     2.7
   26 |   0.9258 |     31.077 |   0.9819 |     33.052 |     2.9
   27 |   0.9095 |     30.455 |   0.9954 |     32.740 |     3.0
   28 |   0.9012 |     30.064 |   0.9622 |     31.835 |     3.1
   29 |   0.8911 |     29.926 |   0.9588 |     31.804 |     3.2
   30 |   0.8729 |     28.969 |   0.9629 |     32.022 |     3.3
   31 |   0.8617 |     28.935 |   0.9626 |     31.586 |     3.4
   32 |   0.8536 |     28.457 |   0.9606 |     31.710 |     3.5
   33 |   0.8458 |     28.209 |   0.9516 |     31.211 |     3.6
   34 |   0.8305 |     27.906 |   0.9471 |     31.180 |     3.7
   35 |   0.8189 |     27.306 |   0.9512 |     31.055 |     3.8
   36 |   0.8121 |     27.251 |   0.9471 |     30.868 |     4.0
   37 |   0.7979 |     26.772 |   0.9560 |     30.774 |     4.1
   38 |   0.7900 |     26.293 |   0.9705 |     31.742 |     4.2
   39 |   0.7832 |     26.079 |   0.9627 |     31.336 |     4.3
   40 |   0.7722 |     26.029 |   0.9585 |     30.961 |     4.4
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 4
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 458,914

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.7022 |     49.081 |   1.3460 |     44.819 |     0.2
    2 |   1.3296 |     45.107 |   1.2682 |     43.508 |     0.3
    3 |   1.2698 |     44.347 |   1.2206 |     41.667 |     0.5
    4 |   1.2222 |     43.378 |   1.1646 |     41.854 |     0.6
    5 |   1.1984 |     42.707 |   1.1554 |     41.542 |     0.8
    6 |   1.1763 |     42.212 |   1.1370 |     41.573 |     1.0
    7 |   1.1597 |     41.678 |   1.1211 |     40.481 |     1.1
    8 |   1.1504 |     41.573 |   1.1204 |     41.230 |     1.3
    9 |   1.1417 |     41.491 |   1.1068 |     40.668 |     1.5
   10 |   1.1333 |     41.480 |   1.1027 |     40.231 |     1.6
   11 |   1.1259 |     41.199 |   1.1015 |     39.856 |     1.8
   12 |   1.1186 |     41.006 |   1.0962 |     39.888 |     1.9
   13 |   1.1160 |     41.001 |   1.0890 |     40.293 |     2.1
   14 |   1.1120 |     40.951 |   1.0949 |     40.200 |     2.3
   15 |   1.1083 |     40.731 |   1.0788 |     40.543 |     2.4
   16 |   1.0989 |     40.571 |   1.0869 |     39.576 |     2.6
   17 |   1.0978 |     40.593 |   1.0841 |     40.512 |     2.7
   18 |   1.0969 |     40.483 |   1.0829 |     40.481 |     2.9
   19 |   1.0972 |     40.648 |   1.0793 |     40.699 |     3.1
   20 |   1.0925 |     40.764 |   1.0747 |     39.732 |     3.2
   21 |   1.0870 |     40.329 |   1.0751 |     39.919 |     3.4
   22 |   1.0872 |     40.516 |   1.0759 |     39.700 |     3.5
   23 |   1.0871 |     40.450 |   1.0707 |     39.638 |     3.7
   24 |   1.0847 |     40.313 |   1.0780 |     40.293 |     3.9
   25 |   1.0816 |     40.522 |   1.0666 |     39.482 |     4.0
   26 |   1.0789 |     40.604 |   1.0645 |     39.981 |     4.2
   27 |   1.0786 |     40.412 |   1.0671 |     39.856 |     4.3
   28 |   1.0781 |     40.302 |   1.0733 |     40.325 |     4.5
   29 |   1.0768 |     40.296 |   1.0682 |     39.763 |     4.7
   30 |   1.0736 |     40.236 |   1.0672 |     40.137 |     4.8
   31 |   1.0688 |     39.949 |   1.0503 |     38.483 |     5.0
   32 |   1.0607 |     39.300 |   1.0472 |     38.514 |     5.1
   33 |   1.0532 |     38.810 |   1.0480 |     39.513 |     5.3
   34 |   1.0495 |     38.397 |   1.0427 |     38.109 |     5.5
   35 |   1.0395 |     37.990 |   1.0382 |     37.890 |     5.6
   36 |   1.0356 |     37.957 |   1.0448 |     38.202 |     5.8
   37 |   1.0263 |     37.803 |   1.0289 |     36.829 |     5.9
   38 |   1.0214 |     37.307 |   1.0253 |     37.422 |     6.1
   39 |   1.0137 |     37.236 |   1.0229 |     37.328 |     6.3
   40 |   1.0149 |     37.693 |   1.0307 |     37.422 |     6.4
   41 |   1.0126 |     37.137 |   1.0146 |     37.422 |     6.6
   42 |   1.0031 |     37.225 |   1.0153 |     37.047 |     6.8
   43 |   1.0010 |     36.999 |   1.0169 |     37.422 |     6.9
   44 |   0.9994 |     36.999 |   1.0133 |     37.047 |     7.1
   45 |   0.9938 |     37.032 |   1.0175 |     37.079 |     7.2
   46 |   0.9880 |     36.746 |   1.0097 |     36.267 |     7.4
   47 |   0.9875 |     36.625 |   1.0115 |     36.486 |     7.6
   48 |   0.9794 |     36.240 |   0.9988 |     36.860 |     7.7
   49 |   0.9762 |     35.981 |   1.0090 |     36.486 |     7.9
   50 |   0.9738 |     35.926 |   1.0002 |     36.205 |     8.0
   51 |   0.9657 |     35.755 |   0.9997 |     36.860 |     8.2
   52 |   0.9616 |     35.552 |   0.9930 |     35.893 |     8.4
   53 |   0.9567 |     35.386 |   0.9836 |     35.581 |     8.5
   54 |   0.9583 |     35.194 |   0.9907 |     36.049 |     8.7
   55 |   0.9515 |     35.029 |   0.9876 |     35.081 |     8.8
   56 |   0.9442 |     34.561 |   0.9847 |     35.424 |     9.0
   57 |   0.9409 |     34.748 |   0.9846 |     35.393 |     9.2
   58 |   0.9386 |     34.665 |   0.9733 |     34.956 |     9.3
   59 |   0.9338 |     34.522 |   0.9811 |     34.894 |     9.5
   60 |   0.9312 |     34.104 |   0.9851 |     34.707 |     9.6
   61 |   0.9278 |     33.917 |   0.9752 |     34.925 |     9.8
   62 |   0.9252 |     33.812 |   0.9763 |     35.206 |    10.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.2
Trainable parameters: 1,179,298

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4347 |     60.568 |   1.6763 |     46.099 |     0.2
    2 |   1.6761 |     46.648 |   1.4569 |     44.881 |     0.4
    3 |   1.5028 |     46.087 |   1.3822 |     44.819 |     0.6
    4 |   1.4276 |     45.756 |   1.3340 |     43.258 |     0.8
    5 |   1.3863 |     45.272 |   1.3015 |     42.322 |     0.9
    6 |   1.3509 |     44.281 |   1.2928 |     42.416 |     1.1
    7 |   1.3293 |     43.747 |   1.2696 |     41.792 |     1.3
    8 |   1.3054 |     43.285 |   1.2403 |     40.200 |     1.5
    9 |   1.2842 |     42.801 |   1.2346 |     40.574 |     1.7
   10 |   1.2627 |     42.162 |   1.2159 |     40.137 |     1.9
   11 |   1.2462 |     41.892 |   1.2041 |     40.418 |     2.1
   12 |   1.2321 |     41.529 |   1.1939 |     39.139 |     2.3
   13 |   1.2159 |     40.637 |   1.1674 |     39.357 |     2.5
   14 |   1.1988 |     40.456 |   1.1800 |     39.139 |     2.7
   15 |   1.1867 |     40.059 |   1.1428 |     38.577 |     2.9
   16 |   1.1672 |     39.526 |   1.1424 |     38.202 |     3.1
   17 |   1.1612 |     39.355 |   1.1384 |     38.296 |     3.2
   18 |   1.1425 |     38.650 |   1.1241 |     37.953 |     3.4
   19 |   1.1328 |     38.496 |   1.1519 |     38.390 |     3.6
   20 |   1.1187 |     38.017 |   1.1313 |     37.297 |     3.8
   21 |   1.1125 |     38.061 |   1.1317 |     37.828 |     4.0
   22 |   1.0973 |     37.280 |   1.1084 |     37.141 |     4.2
   23 |   1.0827 |     36.911 |   1.1020 |     37.391 |     4.4
   24 |   1.0747 |     36.652 |   1.1071 |     36.579 |     4.6
   25 |   1.0615 |     36.251 |   1.1309 |     38.046 |     4.8
   26 |   1.0482 |     35.463 |   1.1002 |     36.361 |     5.0
   27 |   1.0449 |     35.563 |   1.1003 |     36.423 |     5.2
   28 |   1.0292 |     34.803 |   1.1097 |     36.205 |     5.4
   29 |   1.0230 |     34.478 |   1.0897 |     35.518 |     5.5
   30 |   1.0115 |     34.280 |   1.0843 |     35.581 |     5.7
   31 |   1.0008 |     33.658 |   1.0964 |     36.330 |     5.9
   32 |   0.9927 |     33.416 |   1.0860 |     35.362 |     6.1
   33 |   0.9803 |     33.058 |   1.0890 |     35.237 |     6.3
   34 |   0.9761 |     32.882 |   1.0788 |     34.894 |     6.5
   35 |   0.9636 |     32.502 |   1.0947 |     35.237 |     6.7
   36 |   0.9604 |     32.695 |   1.0595 |     34.582 |     6.9
   37 |   0.9449 |     31.594 |   1.0638 |     34.270 |     7.1
   38 |   0.9399 |     31.231 |   1.0600 |     34.613 |     7.3
   39 |   0.9269 |     31.544 |   1.0799 |     34.644 |     7.5
   40 |   0.9203 |     30.956 |   1.0569 |     34.176 |     7.6
   41 |   0.9139 |     30.779 |   1.0628 |     33.989 |     7.8
   42 |   0.9024 |     30.455 |   1.0681 |     34.301 |     8.0
   43 |   0.8927 |     29.805 |   1.0813 |     34.395 |     8.2
   44 |   0.8847 |     29.723 |   1.0670 |     34.051 |     8.4
   45 |   0.8757 |     29.535 |   1.0512 |     33.677 |     8.6
   46 |   0.8634 |     29.145 |   1.1076 |     34.238 |     8.8
   47 |   0.8584 |     28.941 |   1.0827 |     34.145 |     9.0
   48 |   0.8581 |     29.200 |   1.0911 |     33.989 |     9.2
   49 |   0.8382 |     28.126 |   1.0813 |     33.427 |     9.4
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 2
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 779,938

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1110 |     53.451 |   1.5483 |     46.223 |     0.1
    2 |   1.4544 |     45.184 |   1.3865 |     43.414 |     0.2
    3 |   1.3439 |     43.450 |   1.3128 |     44.663 |     0.3
    4 |   1.2775 |     42.013 |   1.2484 |     40.325 |     0.4
    5 |   1.2234 |     40.643 |   1.2180 |     40.637 |     0.5
    6 |   1.1809 |     39.845 |   1.1726 |     37.734 |     0.6
    7 |   1.1471 |     38.667 |   1.1534 |     38.171 |     0.7
    8 |   1.1171 |     37.830 |   1.1096 |     37.516 |     0.8
    9 |   1.0848 |     36.713 |   1.0913 |     37.360 |     0.8
   10 |   1.0550 |     35.711 |   1.0753 |     37.047 |     0.9
   11 |   1.0246 |     34.544 |   1.0589 |     35.674 |     1.0
   12 |   1.0027 |     33.933 |   1.0657 |     36.954 |     1.1
   13 |   0.9782 |     32.744 |   1.0258 |     34.519 |     1.2
   14 |   0.9507 |     31.836 |   1.0263 |     34.145 |     1.3
   15 |   0.9292 |     31.286 |   1.0256 |     34.800 |     1.4
   16 |   0.9066 |     30.664 |   1.0084 |     33.770 |     1.5
   17 |   0.8821 |     29.717 |   0.9949 |     33.115 |     1.6
   18 |   0.8603 |     28.765 |   0.9977 |     32.990 |     1.7
   19 |   0.8358 |     28.027 |   0.9764 |     32.054 |     1.8
   20 |   0.8136 |     27.240 |   0.9787 |     32.397 |     1.9
   21 |   0.7984 |     26.871 |   0.9765 |     32.647 |     2.0
   22 |   0.7750 |     25.804 |   0.9600 |     30.993 |     2.1
   23 |   0.7514 |     25.231 |   0.9656 |     31.367 |     2.2
   24 |   0.7305 |     24.213 |   0.9625 |     31.586 |     2.3
   25 |   0.7080 |     23.558 |   0.9644 |     31.742 |     2.4
   26 |   0.6909 |     22.710 |   0.9660 |     30.805 |     2.5
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 2
Attention heads: 4
Activation: relu
Dropout: 0.2
Trainable parameters: 226,146

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.8109 |     50.589 |   1.3763 |     44.975 |     0.1
    2 |   1.3745 |     45.432 |   1.2721 |     42.853 |     0.2
    3 |   1.3018 |     44.523 |   1.2330 |     43.227 |     0.3
    4 |   1.2591 |     43.714 |   1.2065 |     41.823 |     0.3
    5 |   1.2290 |     42.536 |   1.1645 |     39.700 |     0.4
    6 |   1.2008 |     41.667 |   1.1348 |     39.107 |     0.5
    7 |   1.1772 |     40.830 |   1.1199 |     38.390 |     0.6
    8 |   1.1519 |     40.021 |   1.0942 |     37.640 |     0.7
    9 |   1.1297 |     39.294 |   1.0846 |     36.517 |     0.8
   10 |   1.1096 |     38.474 |   1.0643 |     36.548 |     0.9
   11 |   1.0926 |     37.709 |   1.0500 |     35.830 |     0.9
   12 |   1.0721 |     37.390 |   1.0339 |     35.268 |     1.0
   13 |   1.0533 |     36.295 |   1.0264 |     35.175 |     1.1
   14 |   1.0364 |     35.865 |   1.0409 |     35.362 |     1.2
   15 |   1.0166 |     35.447 |   1.0127 |     34.644 |     1.3
   16 |   1.0038 |     34.968 |   1.0154 |     34.488 |     1.4
   17 |   0.9896 |     34.280 |   1.0147 |     34.301 |     1.5
   18 |   0.9764 |     34.054 |   0.9873 |     33.052 |     1.5
   19 |   0.9606 |     33.399 |   0.9780 |     33.365 |     1.6
   20 |   0.9482 |     32.832 |   0.9904 |     33.302 |     1.7
   21 |   0.9281 |     32.095 |   0.9958 |     33.021 |     1.8
   22 |   0.9168 |     31.561 |   0.9753 |     32.553 |     1.9
   23 |   0.8984 |     30.978 |   0.9879 |     32.709 |     2.0
   24 |   0.8788 |     30.086 |   0.9700 |     31.929 |     2.1
   25 |   0.8678 |     30.268 |   0.9717 |     31.554 |     2.1
   26 |   0.8548 |     29.695 |   0.9715 |     31.211 |     2.2
   27 |   0.8425 |     29.117 |   0.9805 |     31.866 |     2.3
   28 |   0.8300 |     28.721 |   1.0500 |     33.052 |     2.4
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 64
Encoder layers: 2
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.1
Trainable parameters: 193,122

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8510 |     69.397 |   2.2326 |     52.965 |     0.1
    2 |   2.0687 |     49.972 |   1.7245 |     46.754 |     0.1
    3 |   1.7130 |     46.885 |   1.5446 |     44.913 |     0.2
    4 |   1.5728 |     46.235 |   1.4734 |     44.788 |     0.2
    5 |   1.5048 |     46.054 |   1.4303 |     44.944 |     0.3
    6 |   1.4570 |     45.888 |   1.4001 |     44.819 |     0.4
    7 |   1.4236 |     45.624 |   1.3723 |     44.600 |     0.4
    8 |   1.3956 |     45.360 |   1.3429 |     44.070 |     0.5
    9 |   1.3699 |     44.969 |   1.3167 |     43.633 |     0.5
   10 |   1.3486 |     44.446 |   1.2944 |     42.072 |     0.6
   11 |   1.3290 |     44.259 |   1.2739 |     42.385 |     0.7
   12 |   1.3113 |     43.720 |   1.2569 |     41.667 |     0.7
   13 |   1.2939 |     43.544 |   1.2430 |     41.417 |     0.8
   14 |   1.2816 |     43.169 |   1.2266 |     40.980 |     0.8
   15 |   1.2681 |     42.696 |   1.2230 |     41.292 |     0.9
   16 |   1.2566 |     42.476 |   1.2041 |     40.886 |     1.0
   17 |   1.2485 |     42.234 |   1.1956 |     40.387 |     1.0
   18 |   1.2333 |     42.146 |   1.1879 |     40.293 |     1.1
   19 |   1.2263 |     41.997 |   1.1786 |     40.044 |     1.1
   20 |   1.2170 |     41.518 |   1.1740 |     39.950 |     1.2
   21 |   1.2068 |     40.825 |   1.1610 |     38.983 |     1.3
   22 |   1.2004 |     41.045 |   1.1571 |     39.544 |     1.3
   23 |   1.1932 |     40.852 |   1.1468 |     38.983 |     1.4
   24 |   1.1818 |     40.758 |   1.1409 |     38.764 |     1.5
   25 |   1.1776 |     40.247 |   1.1319 |     37.797 |     1.5
   26 |   1.1698 |     40.021 |   1.1323 |     38.327 |     1.6
   27 |   1.1639 |     39.999 |   1.1245 |     37.984 |     1.6
   28 |   1.1592 |     39.905 |   1.1182 |     37.235 |     1.7
   29 |   1.1521 |     39.636 |   1.1144 |     37.297 |     1.8
   30 |   1.1461 |     39.465 |   1.1110 |     37.360 |     1.8
   31 |   1.1428 |     39.245 |   1.1086 |     37.079 |     1.9
   32 |   1.1362 |     39.019 |   1.1020 |     37.360 |     1.9
   33 |   1.1298 |     38.711 |   1.0967 |     36.954 |     2.0
   34 |   1.1264 |     38.590 |   1.0925 |     36.735 |     2.1
   35 |   1.1178 |     38.485 |   1.0911 |     37.110 |     2.1
   36 |   1.1121 |     38.083 |   1.0869 |     36.548 |     2.2
   37 |   1.1087 |     38.205 |   1.0791 |     36.673 |     2.2
   38 |   1.1041 |     37.896 |   1.0801 |     35.955 |     2.3
   39 |   1.0986 |     37.737 |   1.0716 |     35.955 |     2.4
   40 |   1.0987 |     37.913 |   1.0805 |     36.049 |     2.4
   41 |   1.0911 |     37.241 |   1.0683 |     36.236 |     2.5
   42 |   1.0882 |     37.214 |   1.0640 |     35.737 |     2.5
   43 |   1.0852 |     36.977 |   1.0719 |     36.486 |     2.6
   44 |   1.0770 |     36.911 |   1.0579 |     35.393 |     2.7
   45 |   1.0759 |     36.862 |   1.0628 |     35.393 |     2.7
   46 |   1.0732 |     36.939 |   1.0653 |     36.049 |     2.8
   47 |   1.0638 |     36.487 |   1.0522 |     35.237 |     2.8
   48 |   1.0590 |     36.229 |   1.0582 |     35.893 |     2.9
   49 |   1.0567 |     35.915 |   1.0459 |     35.175 |     3.0
   50 |   1.0523 |     36.278 |   1.0531 |     35.268 |     3.0
   51 |   1.0483 |     35.728 |   1.0444 |     35.237 |     3.1
   52 |   1.0480 |     35.970 |   1.0421 |     35.112 |     3.1
   53 |   1.0431 |     35.623 |   1.0373 |     34.831 |     3.2
   54 |   1.0379 |     35.607 |   1.0400 |     35.081 |     3.3
   55 |   1.0356 |     35.524 |   1.0365 |     34.707 |     3.3
   56 |   1.0266 |     35.293 |   1.0356 |     34.644 |     3.4
   57 |   1.0265 |     34.935 |   1.0351 |     34.613 |     3.4
   58 |   1.0266 |     34.935 |   1.0329 |     33.989 |     3.5
   59 |   1.0203 |     34.858 |   1.0330 |     34.457 |     3.6
   60 |   1.0155 |     34.456 |   1.0378 |     34.738 |     3.6
   61 |   1.0107 |     34.192 |   1.0255 |     33.895 |     3.7
   62 |   1.0060 |     34.451 |   1.0225 |     34.363 |     3.7
   63 |   1.0032 |     34.005 |   1.0324 |     34.457 |     3.8
   64 |   1.0039 |     33.977 |   1.0217 |     33.864 |     3.9
   65 |   0.9993 |     33.999 |   1.0198 |     34.332 |     3.9
   66 |   0.9892 |     33.284 |   1.0226 |     34.051 |     4.0
   67 |   0.9870 |     33.708 |   1.0151 |     33.833 |     4.1
   68 |   0.9887 |     33.592 |   1.0183 |     33.926 |     4.1
   69 |   0.9831 |     33.686 |   1.0192 |     33.365 |     4.2
   70 |   0.9797 |     33.080 |   1.0212 |     33.895 |     4.2
   71 |   0.9789 |     33.185 |   1.0177 |     33.645 |     4.3
   72 |   0.9739 |     32.910 |   1.0077 |     33.458 |     4.4
   73 |   0.9662 |     32.662 |   1.0107 |     33.770 |     4.4
   74 |   0.9683 |     32.970 |   1.0101 |     33.146 |     4.5
   75 |   0.9598 |     32.700 |   1.0125 |     33.489 |     4.5
   76 |   0.9579 |     32.447 |   1.0101 |     32.990 |     4.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 64
Encoder layers: 4
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 285,538

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7335 |     65.219 |   2.0960 |     48.252 |     0.1
    2 |   1.8505 |     46.637 |   1.6873 |     45.006 |     0.2
    3 |   1.6081 |     45.789 |   1.5380 |     44.944 |     0.3
    4 |   1.4988 |     45.041 |   1.4677 |     44.913 |     0.4
    5 |   1.4336 |     44.799 |   1.4068 |     44.195 |     0.5
    6 |   1.3864 |     44.364 |   1.3603 |     43.664 |     0.6
    7 |   1.3494 |     43.956 |   1.3332 |     43.196 |     0.7
    8 |   1.3191 |     43.323 |   1.3039 |     42.572 |     0.8
    9 |   1.2949 |     42.861 |   1.2854 |     42.634 |     0.9
   10 |   1.2709 |     42.173 |   1.2669 |     42.072 |     1.0
   11 |   1.2480 |     41.837 |   1.2388 |     41.167 |     1.1
   12 |   1.2287 |     41.122 |   1.2189 |     39.638 |     1.2
   13 |   1.2064 |     40.434 |   1.2030 |     39.544 |     1.3
   14 |   1.1894 |     39.949 |   1.1913 |     40.449 |     1.4
   15 |   1.1710 |     39.360 |   1.1701 |     37.360 |     1.5
   16 |   1.1539 |     39.052 |   1.1611 |     38.452 |     1.6
   17 |   1.1369 |     38.386 |   1.1492 |     39.107 |     1.7
   18 |   1.1249 |     37.929 |   1.1273 |     37.016 |     1.8
   19 |   1.1080 |     37.489 |   1.1254 |     37.110 |     1.8
   20 |   1.0959 |     36.895 |   1.1037 |     36.423 |     1.9
   21 |   1.0805 |     36.322 |   1.1080 |     37.203 |     2.0
   22 |   1.0647 |     35.744 |   1.0983 |     36.236 |     2.1
   23 |   1.0539 |     35.370 |   1.0848 |     36.486 |     2.2
   24 |   1.0407 |     34.484 |   1.0683 |     35.612 |     2.3
   25 |   1.0298 |     34.462 |   1.0640 |     35.112 |     2.4
   26 |   1.0171 |     33.834 |   1.0499 |     35.144 |     2.5
   27 |   1.0029 |     33.179 |   1.0490 |     34.800 |     2.6
   28 |   0.9943 |     33.058 |   1.0527 |     34.988 |     2.7
   29 |   0.9838 |     32.552 |   1.0390 |     34.270 |     2.8
   30 |   0.9681 |     32.122 |   1.0270 |     33.989 |     2.9
   31 |   0.9646 |     31.693 |   1.0231 |     33.801 |     3.0
   32 |   0.9460 |     31.148 |   1.0187 |     33.895 |     3.1
   33 |   0.9351 |     30.950 |   1.0197 |     34.238 |     3.2
   34 |   0.9228 |     30.141 |   1.0177 |     33.864 |     3.3
   35 |   0.9156 |     30.097 |   1.0019 |     33.146 |     3.4
   36 |   0.9065 |     30.069 |   0.9941 |     32.553 |     3.5
   37 |   0.8909 |     29.211 |   0.9941 |     32.896 |     3.6
   38 |   0.8829 |     29.057 |   0.9860 |     32.709 |     3.7
   39 |   0.8706 |     28.908 |   0.9999 |     32.928 |     3.8
   40 |   0.8648 |     28.325 |   0.9881 |     32.896 |     3.9
   41 |   0.8523 |     28.187 |   0.9760 |     32.553 |     4.0
   42 |   0.8449 |     27.631 |   0.9719 |     32.241 |     4.1
   43 |   0.8357 |     27.504 |   0.9679 |     32.272 |     4.2
   44 |   0.8228 |     27.224 |   0.9702 |     31.898 |     4.3
   45 |   0.8136 |     26.745 |   0.9689 |     31.804 |     4.4
   46 |   0.8028 |     26.453 |   0.9673 |     31.866 |     4.5
   47 |   0.7951 |     26.161 |   0.9674 |     32.241 |     4.6
   48 |   0.7876 |     26.084 |   0.9800 |     31.742 |     4.7
   49 |   0.7752 |     25.644 |   0.9647 |     31.773 |     4.8
   50 |   0.7669 |     25.303 |   0.9618 |     31.273 |     4.9
   51 |   0.7615 |     25.330 |   0.9552 |     30.743 |     5.0
   52 |   0.7527 |     24.950 |   0.9669 |     31.554 |     5.1
   53 |   0.7383 |     24.224 |   0.9692 |     31.055 |     5.2
   54 |   0.7327 |     24.317 |   0.9627 |     31.523 |     5.3
   55 |   0.7248 |     23.993 |   0.9526 |     30.649 |     5.4
   56 |   0.7130 |     23.861 |   0.9624 |     30.836 |     5.5
   57 |   0.7059 |     23.497 |   0.9542 |     30.680 |     5.6
   58 |   0.6960 |     23.068 |   0.9617 |     31.211 |     5.6
   59 |   0.6869 |     22.683 |   0.9516 |     30.368 |     5.7
   60 |   0.6769 |     22.430 |   0.9641 |     30.868 |     5.8
   61 |   0.6705 |     22.253 |   0.9813 |     30.805 |     5.9
   62 |   0.6638 |     21.989 |   0.9617 |     30.462 |     6.0
   63 |   0.6542 |     21.675 |   0.9805 |     31.773 |     6.1
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 293,090

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7872 |     66.402 |   2.0869 |     47.753 |     0.1
    2 |   2.0128 |     49.433 |   1.6352 |     46.130 |     0.2
    3 |   1.6955 |     46.659 |   1.5136 |     44.663 |     0.3
    4 |   1.5775 |     46.202 |   1.4513 |     44.507 |     0.4
    5 |   1.5073 |     46.042 |   1.4163 |     43.321 |     0.5
    6 |   1.4617 |     45.701 |   1.3796 |     43.664 |     0.6
    7 |   1.4317 |     45.399 |   1.3591 |     43.071 |     0.7
    8 |   1.4100 |     45.239 |   1.3361 |     42.697 |     0.8
    9 |   1.3876 |     45.030 |   1.3199 |     42.978 |     0.9
   10 |   1.3723 |     45.090 |   1.3111 |     42.978 |     1.0
   11 |   1.3569 |     44.573 |   1.2922 |     42.385 |     1.1
   12 |   1.3416 |     44.463 |   1.2827 |     41.698 |     1.2
   13 |   1.3281 |     44.044 |   1.2722 |     42.447 |     1.3
   14 |   1.3207 |     43.978 |   1.2586 |     41.417 |     1.4
   15 |   1.3110 |     43.808 |   1.2529 |     41.448 |     1.5
   16 |   1.3000 |     43.544 |   1.2370 |     40.512 |     1.6
   17 |   1.2908 |     43.268 |   1.2361 |     40.949 |     1.7
   18 |   1.2838 |     43.076 |   1.2254 |     40.262 |     1.8
   19 |   1.2711 |     42.944 |   1.2149 |     40.512 |     1.9
   20 |   1.2680 |     42.646 |   1.2084 |     40.231 |     2.0
   21 |   1.2626 |     42.740 |   1.2043 |     39.856 |     2.1
   22 |   1.2485 |     42.245 |   1.1952 |     40.075 |     2.2
   23 |   1.2431 |     42.107 |   1.1876 |     39.607 |     2.3
   24 |   1.2356 |     41.914 |   1.1792 |     39.388 |     2.4
   25 |   1.2234 |     41.557 |   1.1819 |     39.482 |     2.5
   26 |   1.2214 |     41.826 |   1.1727 |     39.232 |     2.6
   27 |   1.2125 |     41.408 |   1.1657 |     39.045 |     2.7
   28 |   1.2053 |     41.292 |   1.1581 |     39.170 |     2.8
   29 |   1.2024 |     41.061 |   1.1658 |     39.139 |     2.9
   30 |   1.1947 |     40.918 |   1.1450 |     38.327 |     3.0
   31 |   1.1926 |     40.803 |   1.1548 |     38.358 |     3.1
   32 |   1.1831 |     40.803 |   1.1427 |     38.296 |     3.2
   33 |   1.1762 |     40.285 |   1.1608 |     39.139 |     3.3
   34 |   1.1690 |     40.081 |   1.1466 |     38.764 |     3.4
   35 |   1.1646 |     39.922 |   1.1438 |     38.046 |     3.5
   36 |   1.1633 |     40.054 |   1.1565 |     38.702 |     3.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 2
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 276,386

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6520 |     48.778 |   1.3053 |     43.727 |     0.1
    2 |   1.2706 |     43.384 |   1.2336 |     43.071 |     0.2
    3 |   1.1843 |     41.320 |   1.1513 |     39.669 |     0.3
    4 |   1.1282 |     39.504 |   1.1037 |     38.358 |     0.3
    5 |   1.0818 |     37.489 |   1.0578 |     36.735 |     0.4
    6 |   1.0267 |     35.783 |   1.0641 |     35.955 |     0.5
    7 |   0.9953 |     34.264 |   1.0237 |     35.237 |     0.6
    8 |   0.9426 |     32.640 |   1.0036 |     34.551 |     0.7
    9 |   0.9116 |     31.451 |   0.9656 |     31.804 |     0.8
   10 |   0.8639 |     29.855 |   0.9881 |     34.145 |     0.9
   11 |   0.8345 |     28.611 |   0.9486 |     31.742 |     0.9
   12 |   0.7962 |     27.378 |   0.9214 |     31.024 |     1.0
   13 |   0.7600 |     26.002 |   0.9241 |     31.242 |     1.1
   14 |   0.7209 |     24.664 |   0.9453 |     30.805 |     1.2
   15 |   0.6877 |     23.541 |   0.9492 |     31.398 |     1.3
   16 |   0.6489 |     22.127 |   0.9603 |     31.617 |     1.4
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 256
Encoder layers: 4
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 1,442,466

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.5191 |     47.859 |   1.2760 |     44.007 |     0.1
    2 |   1.2505 |     43.566 |   1.2157 |     41.542 |     0.3
    3 |   1.2022 |     42.812 |   1.1720 |     43.258 |     0.4
    4 |   1.1642 |     41.986 |   1.1545 |     41.511 |     0.6
    5 |   1.1438 |     41.683 |   1.1418 |     41.760 |     0.7
    6 |   1.1316 |     41.215 |   1.1325 |     41.074 |     0.9
    7 |   1.1167 |     41.061 |   1.1030 |     40.855 |     1.0
    8 |   1.1099 |     41.045 |   1.1109 |     40.605 |     1.2
    9 |   1.1064 |     40.830 |   1.1163 |     40.293 |     1.3
   10 |   1.0991 |     40.775 |   1.1049 |     40.137 |     1.5
   11 |   1.0902 |     40.533 |   1.0934 |     41.386 |     1.6
   12 |   1.0889 |     40.461 |   1.0885 |     39.888 |     1.7
   13 |   1.0838 |     40.296 |   1.0953 |     40.356 |     1.9
   14 |   1.0822 |     40.423 |   1.0924 |     40.481 |     2.0
   15 |   1.0794 |     40.307 |   1.0818 |     40.356 |     2.2
   16 |   1.0735 |     40.329 |   1.0818 |     40.418 |     2.3
   17 |   1.0703 |     40.230 |   1.0739 |     40.418 |     2.5
   18 |   1.0701 |     40.175 |   1.0833 |     40.605 |     2.6
   19 |   1.0671 |     40.230 |   1.0757 |     39.919 |     2.8
   20 |   1.0699 |     40.263 |   1.0819 |     40.668 |     2.9
   21 |   1.0690 |     40.313 |   1.0679 |     39.888 |     3.1
   22 |   1.0648 |     39.867 |   1.0705 |     39.856 |     3.2
   23 |   1.0649 |     39.911 |   1.0742 |     40.075 |     3.4
   24 |   1.0612 |     39.845 |   1.0685 |     40.762 |     3.5
   25 |   1.0624 |     40.142 |   1.0799 |     40.449 |     3.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 64
Encoder layers: 4
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 898,274

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4165 |     59.748 |   1.6734 |     44.944 |     0.1
    2 |   1.6752 |     46.890 |   1.4621 |     44.881 |     0.3
    3 |   1.5020 |     46.279 |   1.3873 |     44.164 |     0.4
    4 |   1.4327 |     45.789 |   1.3479 |     43.571 |     0.6
    5 |   1.3864 |     45.360 |   1.3150 |     43.290 |     0.7
    6 |   1.3538 |     44.958 |   1.2860 |     41.979 |     0.8
    7 |   1.3316 |     44.116 |   1.2741 |     42.260 |     1.0
    8 |   1.3099 |     43.555 |   1.2468 |     41.105 |     1.1
    9 |   1.2875 |     43.549 |   1.2246 |     41.042 |     1.3
   10 |   1.2679 |     42.839 |   1.2073 |     41.011 |     1.4
   11 |   1.2508 |     41.991 |   1.2065 |     40.418 |     1.5
   12 |   1.2370 |     41.623 |   1.1809 |     39.201 |     1.7
   13 |   1.2222 |     41.292 |   1.1845 |     39.919 |     1.8
   14 |   1.2069 |     41.111 |   1.1630 |     38.639 |     2.0
   15 |   1.1982 |     40.472 |   1.1538 |     38.733 |     2.1
   16 |   1.1839 |     40.302 |   1.1545 |     39.201 |     2.2
   17 |   1.1766 |     40.214 |   1.1436 |     38.296 |     2.4
   18 |   1.1636 |     39.691 |   1.1431 |     38.577 |     2.5
   19 |   1.1537 |     39.437 |   1.1388 |     38.265 |     2.6
   20 |   1.1437 |     39.041 |   1.1319 |     38.109 |     2.8
   21 |   1.1334 |     38.617 |   1.1257 |     38.077 |     2.9
   22 |   1.1238 |     38.419 |   1.1225 |     37.578 |     3.1
   23 |   1.1097 |     38.034 |   1.1185 |     37.203 |     3.2
   24 |   1.1042 |     37.676 |   1.1013 |     36.954 |     3.3
   25 |   1.0932 |     37.192 |   1.1096 |     37.203 |     3.5
   26 |   1.0853 |     37.329 |   1.1037 |     37.047 |     3.6
   27 |   1.0764 |     36.757 |   1.1050 |     36.954 |     3.8
   28 |   1.0687 |     36.696 |   1.0984 |     36.673 |     3.9
   29 |   1.0602 |     36.295 |   1.0797 |     35.768 |     4.0
   30 |   1.0513 |     36.096 |   1.0891 |     36.298 |     4.2
   31 |   1.0470 |     35.816 |   1.0990 |     36.392 |     4.3
   32 |   1.0381 |     35.761 |   1.1218 |     36.486 |     4.5
   33 |   1.0241 |     35.040 |   1.1220 |     36.517 |     4.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 256
Encoder layers: 4
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 1,243,682

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3497 |     58.999 |   1.5872 |     44.881 |     0.1
    2 |   1.5993 |     46.290 |   1.4164 |     45.069 |     0.3
    3 |   1.4613 |     45.916 |   1.3553 |     44.819 |     0.4
    4 |   1.4037 |     45.701 |   1.3172 |     43.820 |     0.6
    5 |   1.3640 |     44.760 |   1.2854 |     42.010 |     0.7
    6 |   1.3243 |     43.802 |   1.2761 |     42.728 |     0.9
    7 |   1.3034 |     43.367 |   1.2309 |     40.699 |     1.0
    8 |   1.2787 |     42.745 |   1.2144 |     40.418 |     1.2
    9 |   1.2586 |     42.289 |   1.1935 |     40.137 |     1.3
   10 |   1.2425 |     42.019 |   1.1759 |     39.794 |     1.5
   11 |   1.2222 |     41.292 |   1.1694 |     39.669 |     1.6
   12 |   1.2045 |     40.924 |   1.1520 |     38.951 |     1.8
   13 |   1.1961 |     40.599 |   1.1365 |     37.921 |     1.9
   14 |   1.1791 |     39.949 |   1.1220 |     37.141 |     2.1
   15 |   1.1636 |     39.498 |   1.1248 |     37.453 |     2.2
   16 |   1.1518 |     39.311 |   1.1046 |     36.423 |     2.4
   17 |   1.1418 |     38.920 |   1.1151 |     37.297 |     2.5
   18 |   1.1322 |     38.716 |   1.0950 |     37.047 |     2.7
   19 |   1.1204 |     38.408 |   1.0884 |     36.142 |     2.8
   20 |   1.1105 |     37.979 |   1.0906 |     36.423 |     3.0
   21 |   1.0981 |     37.599 |   1.0800 |     35.643 |     3.1
   22 |   1.0865 |     37.093 |   1.0720 |     35.830 |     3.3
   23 |   1.0786 |     36.707 |   1.0681 |     35.924 |     3.4
   24 |   1.0695 |     36.129 |   1.0628 |     35.549 |     3.6
   25 |   1.0584 |     35.915 |   1.0641 |     35.019 |     3.7
   26 |   1.0447 |     35.733 |   1.0779 |     35.737 |     3.9
   27 |   1.0434 |     35.276 |   1.0603 |     35.050 |     4.0
   28 |   1.0344 |     35.133 |   1.0613 |     35.081 |     4.2
   29 |   1.0259 |     34.594 |   1.0573 |     35.331 |     4.3
   30 |   1.0159 |     34.484 |   1.0391 |     34.301 |     4.5
   31 |   1.0040 |     34.148 |   1.0473 |     34.332 |     4.6
   32 |   0.9911 |     33.399 |   1.0686 |     34.894 |     4.8
   33 |   0.9827 |     32.998 |   1.0795 |     35.237 |     4.9
   34 |   0.9748 |     33.163 |   1.0370 |     34.332 |     5.0
   35 |   0.9680 |     32.981 |   1.0533 |     34.176 |     5.2
   36 |   0.9582 |     32.689 |   1.0654 |     34.519 |     5.3
   37 |   0.9534 |     32.106 |   1.0238 |     33.864 |     5.5
   38 |   0.9381 |     31.743 |   1.0476 |     33.895 |     5.6
   39 |   0.9358 |     31.930 |   1.0528 |     33.833 |     5.8
   40 |   0.9225 |     31.115 |   1.0463 |     34.207 |     5.9
   41 |   0.9172 |     31.269 |   1.0391 |     33.396 |     6.1
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 64
Encoder layers: 2
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.1
Trainable parameters: 235,106

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.7833 |     50.297 |   1.3537 |     43.695 |     0.1
    2 |   1.3375 |     44.595 |   1.2500 |     42.759 |     0.1
    3 |   1.2618 |     42.993 |   1.2177 |     40.886 |     0.2
    4 |   1.2165 |     41.369 |   1.1609 |     38.358 |     0.3
    5 |   1.1713 |     40.170 |   1.1440 |     39.669 |     0.3
    6 |   1.1410 |     39.459 |   1.1084 |     37.266 |     0.4
    7 |   1.1076 |     38.419 |   1.0804 |     36.923 |     0.5
    8 |   1.0758 |     37.131 |   1.0647 |     36.517 |     0.5
    9 |   1.0457 |     35.893 |   1.0510 |     34.894 |     0.6
   10 |   1.0260 |     35.320 |   1.0607 |     35.456 |     0.7
   11 |   0.9986 |     34.352 |   1.0364 |     34.363 |     0.7
   12 |   0.9761 |     33.427 |   1.0225 |     33.989 |     0.8
   13 |   0.9510 |     32.854 |   1.0188 |     34.176 |     0.9
   14 |   0.9264 |     31.666 |   1.0103 |     33.770 |     0.9
   15 |   0.9128 |     31.302 |   1.0344 |     34.082 |     1.0
   16 |   0.8878 |     30.592 |   1.0487 |     33.677 |     1.1
   17 |   0.8730 |     29.695 |   1.0235 |     32.896 |     1.2
   18 |   0.8567 |     29.282 |   1.0274 |     33.240 |     1.2
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 343,330

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5552 |     58.851 |   1.8965 |     45.037 |     0.1
    2 |   1.7116 |     46.120 |   1.5915 |     45.225 |     0.3
    3 |   1.5366 |     45.663 |   1.4888 |     44.288 |     0.4
    4 |   1.4529 |     44.837 |   1.4215 |     43.134 |     0.6
    5 |   1.3995 |     44.127 |   1.3751 |     43.664 |     0.7
    6 |   1.3573 |     43.687 |   1.3450 |     43.009 |     0.8
    7 |   1.3239 |     42.602 |   1.3064 |     41.792 |     1.0
    8 |   1.2928 |     41.969 |   1.2778 |     41.261 |     1.1
    9 |   1.2684 |     41.579 |   1.2546 |     40.637 |     1.3
   10 |   1.2446 |     40.907 |   1.2380 |     40.044 |     1.4
   11 |   1.2238 |     40.439 |   1.2131 |     39.544 |     1.6
   12 |   1.2034 |     39.905 |   1.2085 |     39.607 |     1.7
   13 |   1.1824 |     39.702 |   1.1766 |     38.639 |     1.8
   14 |   1.1687 |     39.404 |   1.1699 |     38.702 |     2.0
   15 |   1.1501 |     38.915 |   1.1545 |     37.640 |     2.1
   16 |   1.1371 |     38.469 |   1.1456 |     38.639 |     2.3
   17 |   1.1204 |     38.326 |   1.1353 |     37.953 |     2.4
   18 |   1.1091 |     38.072 |   1.1220 |     37.422 |     2.5
   19 |   1.0929 |     37.324 |   1.1151 |     37.672 |     2.7
   20 |   1.0786 |     36.718 |   1.1005 |     37.453 |     2.8
   21 |   1.0652 |     36.212 |   1.0898 |     36.049 |     3.0
   22 |   1.0509 |     35.689 |   1.0707 |     35.331 |     3.1
   23 |   1.0389 |     35.232 |   1.0677 |     34.956 |     3.3
   24 |   1.0238 |     34.489 |   1.0660 |     35.019 |     3.4
   25 |   1.0072 |     33.961 |   1.0610 |     35.300 |     3.5
   26 |   0.9950 |     33.548 |   1.0407 |     34.894 |     3.7
   27 |   0.9839 |     33.058 |   1.0328 |     33.833 |     3.8
   28 |   0.9673 |     32.199 |   1.0384 |     34.738 |     4.0
   29 |   0.9562 |     31.963 |   1.0201 |     33.801 |     4.1
   30 |   0.9464 |     31.605 |   1.0188 |     33.552 |     4.3
   31 |   0.9308 |     30.906 |   1.0441 |     34.332 |     4.4
   32 |   0.9221 |     30.956 |   0.9997 |     32.834 |     4.5
   33 |   0.9155 |     30.576 |   1.0087 |     32.772 |     4.7
   34 |   0.9006 |     30.003 |   0.9910 |     32.709 |     4.8
   35 |   0.8859 |     29.403 |   0.9763 |     32.335 |     5.0
   36 |   0.8793 |     29.453 |   0.9760 |     32.865 |     5.1
   37 |   0.8621 |     28.963 |   0.9742 |     31.492 |     5.2
   38 |   0.8486 |     28.253 |   0.9646 |     31.804 |     5.4
   39 |   0.8362 |     27.978 |   0.9811 |     32.491 |     5.5
   40 |   0.8285 |     27.758 |   0.9504 |     31.898 |     5.7
   41 |   0.8196 |     27.620 |   0.9476 |     31.149 |     5.8
   42 |   0.8056 |     26.640 |   0.9567 |     31.149 |     6.0
   43 |   0.7925 |     26.271 |   0.9508 |     31.710 |     6.1
   44 |   0.7875 |     26.249 |   0.9537 |     30.868 |     6.2
   45 |   0.7738 |     25.727 |   0.9533 |     30.868 |     6.4
   46 |   0.7622 |     25.484 |   0.9415 |     30.462 |     6.5
   47 |   0.7519 |     25.022 |   0.9502 |     30.680 |     6.7
   48 |   0.7425 |     24.681 |   0.9343 |     30.056 |     6.8
   49 |   0.7314 |     24.488 |   0.9407 |     30.556 |     7.0
   50 |   0.7236 |     24.092 |   0.9401 |     29.869 |     7.1
   51 |   0.7116 |     23.668 |   0.9417 |     30.087 |     7.2
   52 |   0.7033 |     23.239 |   0.9376 |     29.900 |     7.4
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 64
Encoder layers: 4
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 748,834

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2424 |     56.440 |   1.6259 |     46.161 |     0.1
    2 |   1.5159 |     46.136 |   1.4274 |     44.476 |     0.2
    3 |   1.3881 |     44.336 |   1.3406 |     43.383 |     0.4
    4 |   1.3238 |     43.533 |   1.2858 |     41.542 |     0.5
    5 |   1.2763 |     42.179 |   1.2461 |     40.418 |     0.6
    6 |   1.2381 |     40.852 |   1.2257 |     41.011 |     0.7
    7 |   1.2078 |     40.456 |   1.1928 |     38.951 |     0.9
    8 |   1.1785 |     39.680 |   1.1813 |     39.576 |     1.0
    9 |   1.1513 |     38.904 |   1.1608 |     38.670 |     1.1
   10 |   1.1226 |     37.698 |   1.1323 |     37.547 |     1.2
   11 |   1.0972 |     36.862 |   1.1272 |     37.453 |     1.3
   12 |   1.0689 |     35.887 |   1.0975 |     36.642 |     1.5
   13 |   1.0440 |     34.990 |   1.0803 |     36.298 |     1.6
   14 |   1.0156 |     33.983 |   1.0752 |     35.955 |     1.7
   15 |   0.9935 |     33.047 |   1.0653 |     35.612 |     1.8
   16 |   0.9665 |     31.979 |   1.0531 |     35.300 |     2.0
   17 |   0.9396 |     31.181 |   1.0578 |     35.175 |     2.1
   18 |   0.9162 |     29.893 |   1.0400 |     34.707 |     2.2
   19 |   0.8867 |     28.930 |   1.0375 |     33.708 |     2.3
   20 |   0.8614 |     27.730 |   1.0361 |     33.958 |     2.5
   21 |   0.8360 |     27.070 |   1.0566 |     34.738 |     2.6
   22 |   0.8109 |     26.145 |   1.0469 |     33.926 |     2.7
   23 |   0.7875 |     25.534 |   1.0440 |     33.739 |     2.8
   24 |   0.7613 |     24.609 |   1.0445 |     34.020 |     2.9
   25 |   0.7404 |     23.707 |   1.0355 |     33.208 |     3.1
   26 |   0.7089 |     22.435 |   1.0491 |     32.834 |     3.2
   27 |   0.6807 |     21.466 |   1.0528 |     33.396 |     3.3
   28 |   0.6553 |     20.415 |   1.0581 |     33.583 |     3.4
   29 |   0.6317 |     20.151 |   1.0667 |     33.084 |     3.6
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 2
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 292,194

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5021 |     58.746 |   1.9071 |     46.473 |     0.1
    2 |   1.7198 |     46.395 |   1.5947 |     44.913 |     0.1
    3 |   1.5378 |     45.883 |   1.4821 |     44.164 |     0.2
    4 |   1.4517 |     44.677 |   1.4186 |     43.258 |     0.2
    5 |   1.3960 |     43.626 |   1.3705 |     42.759 |     0.3
    6 |   1.3524 |     42.911 |   1.3298 |     42.010 |     0.4
    7 |   1.3179 |     42.608 |   1.3036 |     42.478 |     0.4
    8 |   1.2895 |     42.168 |   1.2776 |     41.355 |     0.5
    9 |   1.2618 |     41.281 |   1.2498 |     39.669 |     0.5
   10 |   1.2365 |     40.709 |   1.2327 |     39.856 |     0.6
   11 |   1.2117 |     40.076 |   1.2039 |     39.576 |     0.6
   12 |   1.1896 |     39.239 |   1.1891 |     40.075 |     0.7
   13 |   1.1704 |     39.190 |   1.1757 |     39.107 |     0.8
   14 |   1.1545 |     38.804 |   1.1596 |     38.077 |     0.8
   15 |   1.1381 |     38.606 |   1.1511 |     38.109 |     0.9
   16 |   1.1229 |     37.990 |   1.1441 |     38.608 |     0.9
   17 |   1.1099 |     37.511 |   1.1253 |     37.797 |     1.0
   18 |   1.0978 |     37.269 |   1.1245 |     37.703 |     1.1
   19 |   1.0839 |     37.027 |   1.1214 |     37.859 |     1.1
   20 |   1.0714 |     36.168 |   1.1072 |     37.797 |     1.2
   21 |   1.0640 |     36.201 |   1.0975 |     36.642 |     1.2
   22 |   1.0522 |     35.854 |   1.0873 |     36.330 |     1.3
   23 |   1.0404 |     35.375 |   1.0866 |     36.735 |     1.4
   24 |   1.0290 |     34.968 |   1.0828 |     36.454 |     1.4
   25 |   1.0168 |     34.341 |   1.0721 |     36.205 |     1.5
   26 |   1.0039 |     33.988 |   1.0742 |     36.860 |     1.5
   27 |   0.9952 |     33.653 |   1.0575 |     35.955 |     1.6
   28 |   0.9848 |     33.135 |   1.0573 |     36.330 |     1.6
   29 |   0.9729 |     32.970 |   1.0470 |     35.518 |     1.7
   30 |   0.9659 |     32.590 |   1.0524 |     35.674 |     1.8
   31 |   0.9577 |     32.084 |   1.0327 |     34.894 |     1.8
   32 |   0.9471 |     31.715 |   1.0389 |     36.111 |     1.9
   33 |   0.9372 |     31.423 |   1.0282 |     35.175 |     1.9
   34 |   0.9305 |     31.082 |   1.0236 |     34.644 |     2.0
   35 |   0.9199 |     30.680 |   1.0377 |     34.831 |     2.1
   36 |   0.9091 |     30.449 |   1.0149 |     34.582 |     2.1
   37 |   0.9007 |     30.256 |   1.0177 |     34.956 |     2.2
   38 |   0.8917 |     30.053 |   1.0147 |     34.644 |     2.2
   39 |   0.8857 |     29.618 |   1.0020 |     33.801 |     2.3
   40 |   0.8749 |     29.271 |   0.9998 |     34.082 |     2.3
   41 |   0.8696 |     28.969 |   1.0058 |     33.271 |     2.4
   42 |   0.8573 |     28.446 |   1.0239 |     34.707 |     2.5
   43 |   0.8483 |     28.104 |   0.9902 |     33.677 |     2.5
   44 |   0.8433 |     28.363 |   0.9906 |     33.614 |     2.6
   45 |   0.8320 |     27.890 |   0.9894 |     33.365 |     2.6
   46 |   0.8275 |     27.747 |   0.9901 |     33.708 |     2.7
   47 |   0.8180 |     27.213 |   0.9996 |     33.115 |     2.8
   48 |   0.8103 |     27.103 |   0.9787 |     33.115 |     2.8
   49 |   0.8012 |     26.657 |   1.0002 |     32.865 |     2.9
   50 |   0.7956 |     26.167 |   0.9920 |     33.521 |     2.9
   51 |   0.7859 |     26.035 |   0.9837 |     32.428 |     3.0
   52 |   0.7794 |     25.936 |   0.9902 |     32.834 |     3.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 4
Decoder layers: 4
Attention heads: 2
Activation: relu
Dropout: 0.0
Trainable parameters: 525,666

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6109 |     47.386 |   1.3288 |     44.632 |     0.1
    2 |   1.2767 |     44.138 |   1.2198 |     42.853 |     0.2
    3 |   1.2027 |     42.415 |   1.1604 |     39.357 |     0.3
    4 |   1.1678 |     41.375 |   1.1438 |     39.513 |     0.4
    5 |   1.1309 |     40.081 |   1.1251 |     39.014 |     0.5
    6 |   1.0997 |     38.942 |   1.1070 |     38.421 |     0.7
    7 |   1.0800 |     38.320 |   1.0843 |     37.079 |     0.8
    8 |   1.0660 |     38.331 |   1.0686 |     37.921 |     0.9
    9 |   1.0398 |     37.561 |   1.0616 |     36.798 |     1.0
   10 |   1.0272 |     37.021 |   1.0638 |     38.140 |     1.1
   11 |   1.0089 |     36.465 |   1.0270 |     37.016 |     1.2
   12 |   0.9991 |     36.372 |   1.0262 |     36.923 |     1.3
   13 |   0.9875 |     35.750 |   1.0141 |     36.330 |     1.4
   14 |   0.9756 |     35.265 |   1.0332 |     37.016 |     1.5
   15 |   0.9564 |     34.676 |   1.0105 |     35.737 |     1.6
   16 |   0.9510 |     34.368 |   1.0181 |     37.328 |     1.8
   17 |   0.9450 |     33.988 |   1.0114 |     36.080 |     1.9
   18 |   0.9214 |     33.218 |   0.9798 |     34.145 |     2.0
   19 |   0.9218 |     33.218 |   0.9986 |     36.642 |     2.1
   20 |   0.9015 |     32.535 |   0.9720 |     34.426 |     2.2
   21 |   0.8855 |     31.853 |   0.9712 |     33.708 |     2.3
   22 |   0.8804 |     31.726 |   0.9633 |     34.051 |     2.4
   23 |   0.8646 |     30.818 |   0.9685 |     34.207 |     2.5
   24 |   0.8603 |     30.972 |   0.9674 |     33.895 |     2.6
   25 |   0.8449 |     30.367 |   0.9719 |     34.613 |     2.7
   26 |   0.8472 |     30.444 |   0.9995 |     34.551 |     2.9
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.2
Trainable parameters: 358,946

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7222 |     65.533 |   1.8683 |     48.315 |     0.1
    2 |   1.8528 |     46.978 |   1.5651 |     45.256 |     0.1
    3 |   1.6310 |     46.384 |   1.4771 |     44.975 |     0.2
    4 |   1.5415 |     46.334 |   1.4244 |     44.975 |     0.3
    5 |   1.4802 |     45.855 |   1.3904 |     44.600 |     0.4
    6 |   1.4401 |     45.404 |   1.3587 |     43.976 |     0.4
    7 |   1.4083 |     45.173 |   1.3353 |     43.602 |     0.5
    8 |   1.3841 |     44.810 |   1.3120 |     43.383 |     0.6
    9 |   1.3623 |     44.336 |   1.2950 |     42.322 |     0.6
   10 |   1.3436 |     43.907 |   1.2771 |     41.854 |     0.7
   11 |   1.3310 |     43.775 |   1.2638 |     41.698 |     0.8
   12 |   1.3118 |     43.681 |   1.2477 |     41.323 |     0.9
   13 |   1.2957 |     42.955 |   1.2348 |     41.292 |     0.9
   14 |   1.2802 |     42.536 |   1.2221 |     40.855 |     1.0
   15 |   1.2728 |     42.591 |   1.2124 |     40.169 |     1.1
   16 |   1.2582 |     42.305 |   1.2020 |     40.169 |     1.1
   17 |   1.2478 |     41.881 |   1.1923 |     39.638 |     1.2
   18 |   1.2379 |     41.634 |   1.1879 |     40.137 |     1.3
   19 |   1.2311 |     41.276 |   1.1736 |     39.076 |     1.4
   20 |   1.2221 |     41.177 |   1.1632 |     39.139 |     1.4
   21 |   1.2116 |     41.012 |   1.1593 |     38.826 |     1.5
   22 |   1.2060 |     40.885 |   1.1517 |     39.014 |     1.6
   23 |   1.2000 |     40.786 |   1.1500 |     39.295 |     1.6
   24 |   1.1862 |     40.401 |   1.1373 |     38.889 |     1.7
   25 |   1.1830 |     40.274 |   1.1355 |     38.546 |     1.8
   26 |   1.1732 |     39.982 |   1.1316 |     38.858 |     1.8
   27 |   1.1688 |     39.663 |   1.1319 |     38.858 |     1.9
   28 |   1.1595 |     39.448 |   1.1208 |     38.265 |     2.0
   29 |   1.1565 |     39.437 |   1.1124 |     37.953 |     2.1
   30 |   1.1539 |     39.432 |   1.1127 |     38.109 |     2.1
   31 |   1.1422 |     39.135 |   1.1063 |     37.391 |     2.2
   32 |   1.1404 |     39.228 |   1.0994 |     37.672 |     2.3
   33 |   1.1357 |     38.970 |   1.0989 |     37.578 |     2.3
   34 |   1.1295 |     38.606 |   1.0970 |     37.141 |     2.4
   35 |   1.1254 |     38.694 |   1.0970 |     37.422 |     2.5
   36 |   1.1191 |     37.918 |   1.0895 |     37.328 |     2.6
   37 |   1.1144 |     38.221 |   1.0841 |     37.016 |     2.6
   38 |   1.1105 |     38.205 |   1.0825 |     36.829 |     2.7
   39 |   1.1045 |     37.995 |   1.0814 |     36.829 |     2.8
   40 |   1.1000 |     37.764 |   1.0747 |     36.704 |     2.8
   41 |   1.0914 |     37.423 |   1.0754 |     36.829 |     2.9
   42 |   1.0921 |     37.748 |   1.0717 |     36.517 |     3.0
   43 |   1.0832 |     37.401 |   1.0711 |     36.610 |     3.1
   44 |   1.0817 |     37.071 |   1.0674 |     36.205 |     3.1
   45 |   1.0750 |     36.834 |   1.0734 |     36.829 |     3.2
   46 |   1.0774 |     37.214 |   1.0695 |     35.955 |     3.3
   47 |   1.0679 |     36.845 |   1.0731 |     36.330 |     3.3
   48 |   1.0620 |     36.548 |   1.0652 |     36.330 |     3.4
   49 |   1.0619 |     36.520 |   1.0649 |     35.955 |     3.5
   50 |   1.0622 |     36.229 |   1.0628 |     35.986 |     3.6
   51 |   1.0543 |     36.449 |   1.0539 |     35.331 |     3.6
   52 |   1.0456 |     35.700 |   1.0539 |     35.237 |     3.7
   53 |   1.0404 |     35.513 |   1.0630 |     35.612 |     3.8
   54 |   1.0349 |     35.436 |   1.0680 |     35.612 |     3.8
   55 |   1.0362 |     35.695 |   1.0468 |     35.081 |     3.9
   56 |   1.0353 |     35.403 |   1.0448 |     34.956 |     4.0
   57 |   1.0305 |     34.957 |   1.0398 |     34.613 |     4.1
   58 |   1.0264 |     35.051 |   1.0521 |     35.050 |     4.1
   59 |   1.0229 |     34.726 |   1.0365 |     34.207 |     4.2
   60 |   1.0179 |     34.753 |   1.0456 |     34.613 |     4.3
   61 |   1.0135 |     34.676 |   1.0476 |     35.081 |     4.3
   62 |   1.0118 |     34.704 |   1.0409 |     34.270 |     4.4
   63 |   0.9993 |     33.972 |   1.0425 |     34.301 |     4.5
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 128
Encoder layers: 4
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 1,179,298

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2039 |     55.279 |   1.5518 |     44.944 |     0.2
    2 |   1.5416 |     45.932 |   1.4031 |     44.632 |     0.4
    3 |   1.4253 |     44.997 |   1.3288 |     42.884 |     0.6
    4 |   1.3628 |     43.978 |   1.2950 |     43.009 |     0.8
    5 |   1.3189 |     43.175 |   1.2425 |     40.200 |     0.9
    6 |   1.2794 |     42.250 |   1.2199 |     40.262 |     1.1
    7 |   1.2474 |     41.518 |   1.1906 |     39.794 |     1.3
    8 |   1.2205 |     40.847 |   1.1634 |     38.452 |     1.5
    9 |   1.1935 |     40.170 |   1.1494 |     38.639 |     1.7
   10 |   1.1741 |     39.559 |   1.1267 |     37.484 |     1.9
   11 |   1.1547 |     38.915 |   1.1182 |     37.360 |     2.1
   12 |   1.1334 |     38.392 |   1.1031 |     36.985 |     2.3
   13 |   1.1137 |     37.715 |   1.0877 |     36.298 |     2.5
   14 |   1.0974 |     37.379 |   1.0675 |     36.205 |     2.7
   15 |   1.0771 |     36.399 |   1.0612 |     35.081 |     2.9
   16 |   1.0614 |     35.920 |   1.0835 |     36.205 |     3.1
   17 |   1.0456 |     35.326 |   1.0693 |     35.424 |     3.2
   18 |   1.0320 |     34.858 |   1.0420 |     34.238 |     3.4
   19 |   1.0139 |     34.076 |   1.0421 |     34.769 |     3.6
   20 |   0.9966 |     33.449 |   1.0331 |     33.833 |     3.8
   21 |   0.9818 |     33.251 |   1.0283 |     33.614 |     4.0
   22 |   0.9713 |     32.623 |   1.0169 |     32.928 |     4.2
   23 |   0.9584 |     32.255 |   1.0213 |     33.677 |     4.4
   24 |   0.9421 |     31.809 |   1.0081 |     32.428 |     4.6
   25 |   0.9269 |     31.352 |   1.0165 |     33.396 |     4.8
   26 |   0.9134 |     30.702 |   0.9987 |     32.896 |     5.0
   27 |   0.8976 |     30.091 |   0.9925 |     32.678 |     5.2
   28 |   0.8868 |     29.827 |   0.9859 |     32.210 |     5.4
   29 |   0.8727 |     29.326 |   0.9908 |     32.147 |     5.5
   30 |   0.8593 |     28.671 |   0.9816 |     32.272 |     5.7
   31 |   0.8463 |     28.275 |   0.9924 |     32.491 |     5.9
   32 |   0.8297 |     28.027 |   0.9932 |     32.054 |     6.1
   33 |   0.8168 |     27.829 |   0.9745 |     31.617 |     6.3
   34 |   0.8103 |     27.427 |   0.9905 |     31.648 |     6.5
   35 |   0.7944 |     26.734 |   0.9995 |     32.022 |     6.7
   36 |   0.7810 |     26.167 |   1.0156 |     32.772 |     6.9
   37 |   0.7619 |     25.248 |   0.9901 |     31.242 |     7.1
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 64
Feedforward dimension: 64
Encoder layers: 2
Decoder layers: 3
Attention heads: 4
Activation: relu
Dropout: 0.1
Trainable parameters: 235,106

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.8001 |     50.396 |   1.3468 |     44.413 |     0.1
    2 |   1.3306 |     44.226 |   1.2465 |     41.916 |     0.2
    3 |   1.2478 |     42.090 |   1.1954 |     39.763 |     0.3
    4 |   1.2004 |     41.094 |   1.1343 |     38.639 |     0.4
    5 |   1.1540 |     39.603 |   1.1069 |     37.890 |     0.5
    6 |   1.1171 |     38.513 |   1.0723 |     37.547 |     0.6
    7 |   1.0842 |     37.616 |   1.0552 |     36.080 |     0.6
    8 |   1.0527 |     36.185 |   1.0482 |     36.236 |     0.7
    9 |   1.0230 |     35.150 |   1.0311 |     35.549 |     0.8
   10 |   1.0041 |     34.676 |   1.0175 |     34.301 |     0.9
   11 |   0.9656 |     33.306 |   0.9999 |     34.582 |     1.0
   12 |   0.9437 |     32.585 |   1.0135 |     34.051 |     1.1
   13 |   0.9262 |     31.957 |   0.9815 |     33.552 |     1.2
   14 |   0.8918 |     30.493 |   0.9954 |     33.770 |     1.3
   15 |   0.8731 |     30.042 |   0.9820 |     33.333 |     1.4
   16 |   0.8421 |     29.073 |   0.9923 |     32.615 |     1.5
   17 |   0.8302 |     28.468 |   0.9739 |     31.773 |     1.6
   18 |   0.8017 |     27.466 |   0.9898 |     32.522 |     1.7
   19 |   0.7726 |     26.679 |   0.9844 |     32.054 |     1.8
   20 |   0.7612 |     26.288 |   0.9872 |     32.522 |     1.9
   21 |   0.7298 |     24.813 |   1.0147 |     33.084 |     2.0
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 64
Encoder layers: 4
Decoder layers: 4
Attention heads: 4
Activation: relu
Dropout: 0.0
Trainable parameters: 1,047,714

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   1.6345 |     49.367 |   1.3136 |     45.225 |     0.2
    2 |   1.2620 |     44.391 |   1.2082 |     42.572 |     0.3
    3 |   1.1911 |     42.135 |   1.1779 |     40.387 |     0.5
    4 |   1.1527 |     40.626 |   1.1527 |     40.356 |     0.7
    5 |   1.1171 |     39.768 |   1.1275 |     39.950 |     0.9
    6 |   1.0878 |     38.727 |   1.0886 |     38.546 |     1.0
    7 |   1.0565 |     37.456 |   1.0653 |     37.297 |     1.2
    8 |   1.0390 |     37.054 |   1.0732 |     38.514 |     1.4
    9 |   1.0223 |     36.548 |   1.0326 |     35.456 |     1.6
   10 |   0.9973 |     35.557 |   1.0252 |     35.861 |     1.7
   11 |   0.9822 |     35.535 |   1.0409 |     37.391 |     1.9
   12 |   0.9635 |     34.731 |   1.0214 |     35.924 |     2.1
   13 |   0.9557 |     34.385 |   0.9909 |     35.331 |     2.3
   14 |   0.9283 |     33.108 |   1.0022 |     34.707 |     2.4
   15 |   0.9167 |     32.678 |   0.9781 |     34.082 |     2.6
   16 |   0.9006 |     32.161 |   0.9571 |     32.990 |     2.8
   17 |   0.8833 |     31.181 |   0.9738 |     33.645 |     3.0
   18 |   0.8716 |     31.055 |   0.9586 |     33.052 |     3.1
   19 |   0.8499 |     30.317 |   0.9414 |     32.553 |     3.3
   20 |   0.8382 |     29.800 |   0.9578 |     32.459 |     3.5
   21 |   0.8222 |     28.792 |   0.9478 |     32.272 |     3.7
   22 |   0.8079 |     28.363 |   0.9687 |     33.271 |     3.8
   23 |   0.7949 |     27.972 |   0.9337 |     30.930 |     4.0
   24 |   0.7704 |     27.191 |   0.9496 |     30.868 |     4.2
   25 |   0.7579 |     26.607 |   0.9566 |     32.179 |     4.3
   26 |   0.7397 |     25.908 |   0.9494 |     30.868 |     4.5
   27 |   0.7219 |     25.121 |   0.9203 |     30.961 |     4.7
   28 |   0.7000 |     24.675 |   0.9378 |     30.868 |     4.9
   29 |   0.6951 |     24.191 |   0.9231 |     30.556 |     5.0
   30 |   0.6876 |     24.169 |   0.9724 |     31.804 |     5.2
   31 |   0.6704 |     23.558 |   0.9432 |     30.306 |     5.4
Early stopping

Model: Seq2Seq Transformer
Source index: <Seq2Seq Index with 45 items>
Target index: <Seq2Seq Index with 34 items>
Max sequence length: 800
Embedding dimension: 128
Feedforward dimension: 256
Encoder layers: 2
Decoder layers: 3
Attention heads: 2
Activation: relu
Dropout: 0.1
Trainable parameters: 978,722

Training started
X_train.shape: torch.Size([3028, 649])
Y_train.shape: torch.Size([3028, 7])
X_dev.shape: torch.Size([534, 702])
Y_dev.shape: torch.Size([534, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0446 |     52.703 |   1.5057 |     45.568 |     0.1
    2 |   1.4826 |     46.059 |   1.3769 |     45.162 |     0.2
    3 |   1.3876 |     44.931 |   1.3093 |     43.227 |     0.3
    4 |   1.3350 |     43.863 |   1.2666 |     41.698 |     0.4
    5 |   1.2928 |     42.663 |   1.2350 |     40.793 |     0.5
    6 |   1.2573 |     41.634 |   1.1961 |     39.732 |     0.5
    7 |   1.2247 |     40.869 |   1.1724 |     39.451 |     0.6
    8 |   1.1985 |     40.313 |   1.1500 |     38.670 |     0.7
    9 |   1.1745 |     39.619 |   1.1292 |     38.077 |     0.8
   10 |   1.1567 |     39.228 |   1.1141 |     37.765 |     0.9
   11 |   1.1347 |     38.447 |   1.1124 |     37.734 |     1.0
   12 |   1.1179 |     37.654 |   1.0890 |     36.829 |     1.1
   13 |   1.0962 |     36.828 |   1.0709 |     36.111 |     1.2
   14 |   1.0805 |     36.361 |   1.0559 |     35.268 |     1.3
   15 |   1.0665 |     35.728 |   1.0479 |     34.519 |     1.4
   16 |   1.0464 |     35.326 |   1.0425 |     34.675 |     1.5
   17 |   1.0274 |     34.671 |   1.0285 |     34.082 |     1.5
   18 |   1.0168 |     33.917 |   1.0229 |     34.332 |     1.6
   19 |   0.9989 |     33.664 |   1.0319 |     34.363 |     1.7
   20 |   0.9771 |     32.887 |   1.0040 |     32.865 |     1.8
   21 |   0.9749 |     33.058 |   1.0005 |     33.146 |     1.9
   22 |   0.9586 |     32.260 |   0.9933 |     32.959 |     2.0
   23 |   0.9350 |     31.418 |   0.9944 |     33.146 |     2.1
   24 |   0.9293 |     31.484 |   0.9893 |     33.208 |     2.2
   25 |   0.9112 |     30.482 |   0.9795 |     32.615 |     2.3
   26 |   0.9056 |     30.262 |   0.9954 |     33.333 |     2.4
   27 |   0.8933 |     30.179 |   0.9733 |     32.865 |     2.5
   28 |   0.8795 |     29.442 |   0.9759 |     32.584 |     2.5
   29 |   0.8642 |     28.985 |   0.9605 |     32.428 |     2.6
   30 |   0.8560 |     28.512 |   0.9669 |     32.397 |     2.7
   31 |   0.8404 |     28.148 |   0.9636 |     32.303 |     2.8
   32 |   0.8326 |     27.901 |   0.9681 |     32.147 |     2.9
   33 |   0.8248 |     27.736 |   0.9542 |     31.866 |     3.0
   34 |   0.8090 |     27.328 |   0.9568 |     31.773 |     3.1
   35 |   0.7990 |     26.800 |   0.9445 |     31.492 |     3.2
   36 |   0.7857 |     26.442 |   0.9403 |     31.461 |     3.3
   37 |   0.7798 |     26.376 |   0.9707 |     32.366 |     3.4
   38 |   0.7722 |     26.007 |   0.9498 |     31.180 |     3.5
   39 |   0.7611 |     25.523 |   0.9570 |     31.305 |     3.5
   40 |   0.7501 |     25.209 |   0.9361 |     30.743 |     3.6
   41 |   0.7355 |     24.868 |   0.9522 |     31.086 |     3.7
   42 |   0.7242 |     24.444 |   0.9597 |     31.554 |     3.8
   43 |   0.7205 |     24.015 |   0.9509 |     30.805 |     3.9
   44 |   0.7095 |     23.861 |   0.9462 |     30.961 |     4.0
Early stopping

