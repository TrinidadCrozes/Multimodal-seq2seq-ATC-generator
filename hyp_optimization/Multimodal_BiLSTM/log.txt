Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 825,442

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5081 |     93.487 |   3.4099 |     82.445 |     0.0
    2 |   3.2564 |     82.017 |   3.0752 |     82.445 |     0.1
    3 |   2.9257 |     81.838 |   2.8126 |     81.893 |     0.1
    4 |   2.7287 |     76.940 |   2.6650 |     67.341 |     0.1
    5 |   2.6002 |     66.661 |   2.5500 |     66.912 |     0.2
    6 |   2.4914 |     60.685 |   2.4480 |     58.824 |     0.2
    7 |   2.4045 |     59.157 |   2.3668 |     58.824 |     0.3
    8 |   2.3314 |     58.761 |   2.3030 |     58.824 |     0.3
    9 |   2.2740 |     58.155 |   2.2504 |     58.824 |     0.3
   10 |   2.2253 |     57.363 |   2.2028 |     58.824 |     0.4
   11 |   2.1827 |     57.065 |   2.1572 |     58.272 |     0.4
   12 |   2.1379 |     56.513 |   2.1157 |     56.587 |     0.4
   13 |   2.0970 |     56.166 |   2.0762 |     54.381 |     0.5
   14 |   2.0599 |     55.960 |   2.0388 |     54.075 |     0.5
   15 |   2.0218 |     55.521 |   2.0035 |     53.799 |     0.5
   16 |   1.9873 |     55.353 |   1.9691 |     53.830 |     0.6
   17 |   1.9530 |     55.050 |   1.9359 |     53.799 |     0.6
   18 |   1.9193 |     54.302 |   1.9038 |     53.554 |     0.7
   19 |   1.8914 |     52.281 |   1.8727 |     48.805 |     0.7
   20 |   1.8577 |     49.973 |   1.8426 |     48.346 |     0.7
   21 |   1.8294 |     49.285 |   1.8138 |     48.346 |     0.8
   22 |   1.8022 |     49.025 |   1.7859 |     48.346 |     0.8
   23 |   1.7763 |     48.797 |   1.7587 |     48.346 |     0.8
   24 |   1.7495 |     48.727 |   1.7329 |     48.346 |     0.9
   25 |   1.7241 |     48.082 |   1.7083 |     47.181 |     0.9
   26 |   1.6999 |     47.058 |   1.6854 |     45.466 |     0.9
   27 |   1.6783 |     46.402 |   1.6636 |     45.466 |     1.0
   28 |   1.6585 |     46.299 |   1.6429 |     45.466 |     1.0
   29 |   1.6367 |     46.240 |   1.6238 |     45.466 |     1.0
   30 |   1.6200 |     46.218 |   1.6066 |     45.466 |     1.1
   31 |   1.6049 |     46.223 |   1.5908 |     45.466 |     1.1
   32 |   1.5887 |     46.218 |   1.5764 |     45.466 |     1.2
   33 |   1.5747 |     46.213 |   1.5631 |     45.466 |     1.2
   34 |   1.5634 |     46.213 |   1.5506 |     45.466 |     1.2
   35 |   1.5527 |     46.213 |   1.5394 |     45.466 |     1.3
   36 |   1.5389 |     46.213 |   1.5299 |     45.466 |     1.3
   37 |   1.5290 |     46.213 |   1.5206 |     45.466 |     1.3
   38 |   1.5187 |     46.218 |   1.5124 |     45.466 |     1.4
   39 |   1.5130 |     46.213 |   1.5042 |     45.466 |     1.4
   40 |   1.5033 |     46.213 |   1.4966 |     45.466 |     1.4
   41 |   1.4962 |     46.223 |   1.4897 |     45.466 |     1.5
   42 |   1.4913 |     46.207 |   1.4818 |     45.466 |     1.5
   43 |   1.4828 |     46.218 |   1.4753 |     45.466 |     1.5
   44 |   1.4761 |     46.218 |   1.4693 |     45.466 |     1.6
   45 |   1.4688 |     46.364 |   1.4632 |     45.466 |     1.6
   46 |   1.4673 |     46.169 |   1.4569 |     45.466 |     1.7
   47 |   1.4564 |     46.207 |   1.4509 |     45.466 |     1.7
   48 |   1.4530 |     46.131 |   1.4452 |     45.466 |     1.7
   49 |   1.4438 |     46.164 |   1.4401 |     45.466 |     1.8
   50 |   1.4400 |     46.175 |   1.4338 |     45.037 |     1.8
   51 |   1.4309 |     46.175 |   1.4276 |     44.853 |     1.8
   52 |   1.4245 |     46.077 |   1.4205 |     44.822 |     1.9
   53 |   1.4168 |     46.045 |   1.4143 |     44.853 |     1.9
   54 |   1.4079 |     46.055 |   1.4079 |     45.006 |     1.9
   55 |   1.3999 |     45.931 |   1.4014 |     44.730 |     2.0
   56 |   1.3965 |     45.828 |   1.3955 |     45.037 |     2.0
   57 |   1.3853 |     45.557 |   1.3898 |     44.332 |     2.1
   58 |   1.3816 |     45.562 |   1.3848 |     44.393 |     2.1
   59 |   1.3764 |     45.026 |   1.3814 |     44.914 |     2.1
   60 |   1.3684 |     45.346 |   1.3752 |     44.761 |     2.2
   61 |   1.3640 |     45.031 |   1.3711 |     44.393 |     2.2
   62 |   1.3565 |     44.815 |   1.3675 |     43.964 |     2.2
   63 |   1.3517 |     44.630 |   1.3624 |     44.547 |     2.3
   64 |   1.3443 |     44.565 |   1.3594 |     43.290 |     2.3
   65 |   1.3432 |     44.571 |   1.3577 |     43.321 |     2.4
   66 |   1.3394 |     44.175 |   1.3512 |     43.321 |     2.4
   67 |   1.3321 |     44.208 |   1.3495 |     43.627 |     2.4
   68 |   1.3264 |     44.132 |   1.3478 |     43.444 |     2.5
   69 |   1.3215 |     43.742 |   1.3440 |     43.995 |     2.5
   70 |   1.3152 |     43.671 |   1.3399 |     44.026 |     2.5
   71 |   1.3133 |     43.455 |   1.3393 |     43.229 |     2.6
   72 |   1.3062 |     43.048 |   1.3342 |     43.658 |     2.6
   73 |   1.3014 |     42.788 |   1.3321 |     43.137 |     2.7
   74 |   1.2968 |     42.664 |   1.3275 |     43.536 |     2.7
   75 |   1.2937 |     42.474 |   1.3276 |     43.229 |     2.7
   76 |   1.2896 |     42.219 |   1.3245 |     42.923 |     2.8
   77 |   1.2838 |     41.976 |   1.3211 |     43.107 |     2.8
   78 |   1.2797 |     41.927 |   1.3184 |     43.076 |     2.8
   79 |   1.2742 |     41.472 |   1.3174 |     42.494 |     2.9
   80 |   1.2746 |     41.602 |   1.3158 |     42.770 |     2.9
   81 |   1.2675 |     41.239 |   1.3106 |     42.096 |     2.9
   82 |   1.2619 |     40.897 |   1.3073 |     42.402 |     3.0
   83 |   1.2572 |     40.968 |   1.3044 |     42.525 |     3.0
   84 |   1.2575 |     40.767 |   1.3034 |     42.279 |     3.0
   85 |   1.2536 |     40.670 |   1.3045 |     41.575 |     3.1
   86 |   1.2480 |     40.404 |   1.2978 |     41.513 |     3.1
   87 |   1.2431 |     40.274 |   1.2963 |     41.268 |     3.2
   88 |   1.2406 |     39.971 |   1.2969 |     41.391 |     3.2
   89 |   1.2374 |     39.922 |   1.2943 |     41.146 |     3.2
   90 |   1.2338 |     40.101 |   1.2885 |     40.962 |     3.3
   91 |   1.2294 |     39.494 |   1.2899 |     41.176 |     3.3
   92 |   1.2256 |     39.434 |   1.2877 |     40.870 |     3.3
   93 |   1.2206 |     39.429 |   1.2815 |     40.809 |     3.4
   94 |   1.2132 |     39.077 |   1.2792 |     40.870 |     3.4
   95 |   1.2113 |     39.153 |   1.2789 |     40.748 |     3.4
   96 |   1.2073 |     39.066 |   1.2791 |     40.931 |     3.5
   97 |   1.2054 |     38.827 |   1.2774 |     40.870 |     3.5
   98 |   1.2012 |     38.627 |   1.2727 |     40.901 |     3.5
   99 |   1.1961 |     38.600 |   1.2739 |     40.564 |     3.6
  100 |   1.1930 |     38.372 |   1.2706 |     40.533 |     3.6
  101 |   1.1880 |     38.362 |   1.2667 |     40.288 |     3.7
  102 |   1.1856 |     38.166 |   1.2628 |     40.411 |     3.7
  103 |   1.1818 |     38.248 |   1.2610 |     40.533 |     3.7
  104 |   1.1785 |     38.064 |   1.2648 |     40.533 |     3.8
  105 |   1.1758 |     38.064 |   1.2567 |     40.257 |     3.8
  106 |   1.1724 |     37.961 |   1.2552 |     40.165 |     3.8
  107 |   1.1663 |     37.771 |   1.2564 |     40.227 |     3.9
  108 |   1.1597 |     37.825 |   1.2533 |     40.196 |     3.9
  109 |   1.1575 |     37.668 |   1.2524 |     40.165 |     3.9
  110 |   1.1525 |     37.473 |   1.2498 |     40.074 |     4.0
  111 |   1.1491 |     37.262 |   1.2541 |     40.288 |     4.0
  112 |   1.1460 |     37.235 |   1.2471 |     40.288 |     4.0
  113 |   1.1409 |     37.104 |   1.2444 |     40.135 |     4.1
  114 |   1.1362 |     37.034 |   1.2455 |     39.920 |     4.1
  115 |   1.1309 |     36.828 |   1.2436 |     40.074 |     4.2
  116 |   1.1361 |     36.611 |   1.2402 |     39.767 |     4.2
  117 |   1.1263 |     36.546 |   1.2391 |     39.737 |     4.2
  118 |   1.1249 |     36.292 |   1.2334 |     39.675 |     4.3
  119 |   1.1217 |     36.563 |   1.2346 |     39.706 |     4.3
  120 |   1.1183 |     36.389 |   1.2323 |     39.706 |     4.3
  121 |   1.1111 |     36.151 |   1.2328 |     39.614 |     4.4
  122 |   1.1115 |     36.243 |   1.2317 |     39.369 |     4.4
  123 |   1.1041 |     35.912 |   1.2258 |     39.706 |     4.4
  124 |   1.1003 |     36.021 |   1.2248 |     39.338 |     4.5
  125 |   1.0983 |     35.809 |   1.2233 |     39.308 |     4.5
  126 |   1.0959 |     35.652 |   1.2214 |     39.185 |     4.5
  127 |   1.0937 |     35.365 |   1.2257 |     39.583 |     4.6
  128 |   1.0959 |     35.625 |   1.2188 |     39.338 |     4.6
  129 |   1.0848 |     35.409 |   1.2189 |     39.461 |     4.7
  130 |   1.0811 |     35.241 |   1.2215 |     39.553 |     4.7
  131 |   1.0768 |     35.322 |   1.2242 |     39.645 |     4.7
  132 |   1.0730 |     35.008 |   1.2169 |     39.614 |     4.8
  133 |   1.0686 |     34.845 |   1.2176 |     39.308 |     4.8
  134 |   1.0670 |     35.029 |   1.2165 |     39.185 |     4.8
  135 |   1.0657 |     34.818 |   1.2133 |     39.583 |     4.9
  136 |   1.0614 |     34.422 |   1.2128 |     38.971 |     4.9
  137 |   1.0583 |     34.655 |   1.2204 |     39.369 |     4.9
  138 |   1.0551 |     34.531 |   1.2157 |     39.032 |     5.0
  139 |   1.0517 |     34.471 |   1.2122 |     38.848 |     5.0
  140 |   1.0477 |     34.460 |   1.2099 |     39.032 |     5.0
  141 |   1.0454 |     34.162 |   1.2071 |     38.909 |     5.1
  142 |   1.0429 |     34.173 |   1.2036 |     39.154 |     5.1
  143 |   1.0423 |     34.265 |   1.2049 |     38.848 |     5.2
  144 |   1.0341 |     33.908 |   1.2064 |     39.185 |     5.2
  145 |   1.0352 |     33.691 |   1.2038 |     38.940 |     5.2
  146 |   1.0398 |     34.309 |   1.2002 |     38.909 |     5.3
  147 |   1.0301 |     33.621 |   1.1984 |     38.450 |     5.3
  148 |   1.0275 |     33.826 |   1.1956 |     38.940 |     5.3
  149 |   1.0238 |     33.626 |   1.2056 |     38.848 |     5.4
  150 |   1.0179 |     33.322 |   1.2003 |     38.695 |     5.4
  151 |   1.0215 |     33.360 |   1.1922 |     38.450 |     5.4
  152 |   1.0140 |     33.160 |   1.1910 |     38.297 |     5.5
  153 |   1.0144 |     33.274 |   1.1955 |     38.542 |     5.5
  154 |   1.0088 |     33.182 |   1.1842 |     38.021 |     5.5
  155 |   1.0082 |     32.932 |   1.1856 |     37.960 |     5.6
  156 |   1.0026 |     32.846 |   1.1918 |     38.419 |     5.6
  157 |   0.9997 |     32.943 |   1.1863 |     38.388 |     5.7
  158 |   0.9980 |     32.683 |   1.1932 |     38.205 |     5.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 411,138

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5044 |     64.879 |   1.9451 |     53.768 |     0.0
    2 |   1.7353 |     49.680 |   1.5536 |     45.466 |     0.0
    3 |   1.4980 |     46.272 |   1.4424 |     45.466 |     0.1
    4 |   1.4313 |     46.213 |   1.4159 |     45.987 |     0.1
    5 |   1.4016 |     46.148 |   1.3845 |     45.833 |     0.1
    6 |   1.3770 |     46.153 |   1.3612 |     44.516 |     0.1
    7 |   1.3602 |     45.720 |   1.3450 |     44.975 |     0.2
    8 |   1.3420 |     45.302 |   1.3313 |     44.608 |     0.2
    9 |   1.3256 |     44.869 |   1.3170 |     44.148 |     0.2
   10 |   1.3052 |     44.175 |   1.3015 |     43.137 |     0.2
   11 |   1.2863 |     43.059 |   1.2790 |     41.912 |     0.3
   12 |   1.2557 |     42.192 |   1.2572 |     41.115 |     0.3
   13 |   1.2268 |     41.298 |   1.2369 |     40.717 |     0.3
   14 |   1.2078 |     40.550 |   1.2230 |     40.043 |     0.3
   15 |   1.1871 |     39.852 |   1.2074 |     39.400 |     0.3
   16 |   1.1657 |     38.893 |   1.1888 |     39.062 |     0.4
   17 |   1.1458 |     38.649 |   1.1806 |     39.369 |     0.4
   18 |   1.1372 |     38.199 |   1.1911 |     39.093 |     0.4
   19 |   1.1127 |     37.570 |   1.1580 |     38.480 |     0.4
   20 |   1.0919 |     36.985 |   1.1410 |     37.132 |     0.5
   21 |   1.0688 |     35.950 |   1.1220 |     37.469 |     0.5
   22 |   1.0521 |     35.333 |   1.1164 |     36.581 |     0.5
   23 |   1.0240 |     34.260 |   1.1153 |     36.121 |     0.5
   24 |   1.0065 |     33.490 |   1.1040 |     36.887 |     0.5
   25 |   0.9916 |     32.954 |   1.0784 |     34.835 |     0.6
   26 |   0.9674 |     32.309 |   1.0695 |     34.620 |     0.6
   27 |   0.9565 |     31.730 |   1.0616 |     34.038 |     0.6
   28 |   0.9319 |     30.879 |   1.0485 |     34.130 |     0.6
   29 |   0.9133 |     30.261 |   1.0421 |     33.395 |     0.7
   30 |   0.9013 |     29.665 |   1.0371 |     33.211 |     0.7
   31 |   0.8931 |     29.687 |   1.0479 |     33.364 |     0.7
   32 |   0.8753 |     28.950 |   1.0207 |     32.475 |     0.7
   33 |   0.8435 |     28.105 |   1.0209 |     31.985 |     0.7
   34 |   0.8243 |     27.102 |   1.0094 |     32.138 |     0.8
   35 |   0.8039 |     26.284 |   1.0011 |     32.230 |     0.8
   36 |   0.8006 |     26.512 |   1.0078 |     32.261 |     0.8
   37 |   0.7786 |     25.591 |   0.9838 |     30.607 |     0.8
   38 |   0.7626 |     24.919 |   0.9839 |     31.219 |     0.9
   39 |   0.7471 |     24.724 |   0.9952 |     31.281 |     0.9
   40 |   0.7454 |     24.794 |   0.9748 |     31.464 |     0.9
   41 |   0.7233 |     23.808 |   0.9851 |     31.679 |     0.9
   42 |   0.7115 |     23.494 |   0.9761 |     30.270 |     0.9
   43 |   0.6911 |     22.502 |   0.9647 |     30.362 |     1.0
   44 |   0.6786 |     21.928 |   0.9701 |     30.239 |     1.0
   45 |   0.6890 |     22.892 |   0.9689 |     29.320 |     1.0
   46 |   0.6618 |     21.966 |   0.9602 |     29.197 |     1.0
   47 |   0.6521 |     21.283 |   0.9664 |     29.044 |     1.1
   48 |   0.6247 |     20.487 |   0.9647 |     29.167 |     1.1
   49 |   0.6330 |     20.714 |   0.9805 |     29.565 |     1.1
   50 |   0.6216 |     20.714 |   0.9404 |     29.197 |     1.1
   51 |   0.6155 |     20.259 |   0.9539 |     29.718 |     1.1
   52 |   0.6006 |     19.354 |   0.9728 |     29.105 |     1.2
   53 |   0.5885 |     19.224 |   0.9622 |     28.401 |     1.2
   54 |   0.5779 |     18.850 |   0.9749 |     29.289 |     1.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,457,762

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5197 |     68.048 |   2.0130 |     58.824 |     0.1
    2 |   1.7888 |     50.818 |   1.5845 |     45.466 |     0.2
    3 |   1.5178 |     46.245 |   1.4571 |     45.466 |     0.3
    4 |   1.4421 |     46.272 |   1.4245 |     45.987 |     0.3
    5 |   1.4203 |     46.245 |   1.4095 |     45.987 |     0.4
    6 |   1.4095 |     46.175 |   1.4004 |     45.987 |     0.5
    7 |   1.4030 |     46.348 |   1.3945 |     45.466 |     0.6
    8 |   1.3970 |     46.240 |   1.3903 |     45.466 |     0.7
    9 |   1.3927 |     46.121 |   1.3804 |     44.884 |     0.8
   10 |   1.3825 |     45.801 |   1.3692 |     44.301 |     0.8
   11 |   1.3666 |     44.831 |   1.3405 |     44.056 |     0.9
   12 |   1.3463 |     44.788 |   1.3357 |     43.964 |     1.0
   13 |   1.3421 |     44.500 |   1.3298 |     43.627 |     1.1
   14 |   1.3335 |     44.446 |   1.3247 |     44.056 |     1.2
   15 |   1.3225 |     44.284 |   1.3129 |     43.260 |     1.3
   16 |   1.3175 |     44.002 |   1.3133 |     43.566 |     1.3
   17 |   1.3153 |     43.921 |   1.3060 |     42.923 |     1.4
   18 |   1.3089 |     43.866 |   1.2996 |     43.260 |     1.5
   19 |   1.2983 |     43.509 |   1.2940 |     41.881 |     1.6
   20 |   1.2923 |     43.216 |   1.2790 |     42.218 |     1.7
   21 |   1.2853 |     43.086 |   1.2818 |     42.126 |     1.8
   22 |   1.2816 |     42.718 |   1.2774 |     41.789 |     1.9
   23 |   1.2743 |     42.848 |   1.2674 |     41.513 |     2.0
   24 |   1.2696 |     42.572 |   1.2664 |     41.697 |     2.0
   25 |   1.2685 |     42.203 |   1.2677 |     42.004 |     2.1
   26 |   1.2621 |     42.593 |   1.2730 |     41.728 |     2.2
   27 |   1.2584 |     42.143 |   1.2597 |     41.605 |     2.3
   28 |   1.2501 |     41.959 |   1.2673 |     41.667 |     2.4
   29 |   1.2429 |     41.959 |   1.2569 |     41.605 |     2.5
   30 |   1.2496 |     42.111 |   1.2402 |     41.207 |     2.6
   31 |   1.2430 |     41.721 |   1.2499 |     41.360 |     2.7
   32 |   1.2328 |     41.634 |   1.2415 |     41.330 |     2.7
   33 |   1.2255 |     41.331 |   1.2417 |     41.330 |     2.8
   34 |   1.2193 |     40.968 |   1.2385 |     40.809 |     2.9
   35 |   1.2188 |     40.800 |   1.2433 |     40.533 |     3.0
   36 |   1.2125 |     40.762 |   1.2247 |     40.074 |     3.1
   37 |   1.2010 |     40.491 |   1.2117 |     39.828 |     3.2
   38 |   1.1937 |     39.933 |   1.2162 |     39.767 |     3.3
   39 |   1.1938 |     39.987 |   1.2085 |     39.982 |     3.4
   40 |   1.1808 |     39.608 |   1.1973 |     39.400 |     3.4
   41 |   1.1749 |     39.207 |   1.2005 |     39.369 |     3.5
   42 |   1.1796 |     39.494 |   1.1979 |     39.522 |     3.6
   43 |   1.1657 |     39.023 |   1.2005 |     39.828 |     3.7
   44 |   1.1681 |     39.001 |   1.2005 |     39.430 |     3.8
   45 |   1.1593 |     38.849 |   1.1892 |     38.971 |     3.9
   46 |   1.1507 |     38.638 |   1.1738 |     38.664 |     4.0
   47 |   1.1454 |     38.649 |   1.1816 |     38.756 |     4.0
   48 |   1.1365 |     37.966 |   1.1718 |     38.817 |     4.1
   49 |   1.1262 |     37.939 |   1.1749 |     38.327 |     4.2
   50 |   1.1292 |     37.700 |   1.1693 |     38.174 |     4.3
   51 |   1.1179 |     37.809 |   1.1657 |     37.929 |     4.4
   52 |   1.1138 |     37.370 |   1.1659 |     38.603 |     4.5
   53 |   1.1141 |     37.505 |   1.1789 |     39.001 |     4.6
   54 |   1.1072 |     37.321 |   1.1678 |     37.898 |     4.6
   55 |   1.0996 |     37.110 |   1.1691 |     38.634 |     4.7
   56 |   1.0883 |     36.899 |   1.1540 |     37.960 |     4.8
   57 |   1.0818 |     36.725 |   1.1563 |     37.929 |     4.9
   58 |   1.0831 |     36.465 |   1.1461 |     37.255 |     5.0
   59 |   1.0809 |     36.308 |   1.1550 |     38.174 |     5.1
   60 |   1.0669 |     36.162 |   1.1484 |     37.990 |     5.2
   61 |   1.0721 |     36.411 |   1.1297 |     37.010 |     5.3
   62 |   1.0655 |     36.162 |   1.1463 |     38.235 |     5.3
   63 |   1.0613 |     36.194 |   1.1360 |     37.776 |     5.4
   64 |   1.0479 |     35.474 |   1.1386 |     37.960 |     5.5
   65 |   1.0457 |     35.224 |   1.1258 |     36.703 |     5.6
   66 |   1.0420 |     35.176 |   1.1354 |     37.194 |     5.7
   67 |   1.0451 |     35.549 |   1.1272 |     37.286 |     5.8
   68 |   1.0383 |     35.295 |   1.1153 |     36.489 |     5.9
   69 |   1.0364 |     34.997 |   1.1336 |     36.826 |     5.9
   70 |   1.0444 |     35.734 |   1.1355 |     36.489 |     6.0
   71 |   1.0381 |     35.354 |   1.1332 |     37.531 |     6.1
   72 |   1.0292 |     34.975 |   1.1233 |     36.857 |     6.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 2,502,306

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2118 |     84.617 |   2.8860 |     83.333 |     0.1
    2 |   2.6924 |     71.771 |   2.5633 |     66.391 |     0.2
    3 |   2.4575 |     60.539 |   2.3830 |     58.824 |     0.2
    4 |   2.2966 |     58.718 |   2.2397 |     58.548 |     0.3
    5 |   2.1652 |     57.813 |   2.1223 |     57.874 |     0.4
    6 |   2.0593 |     56.702 |   2.0207 |     56.434 |     0.5
    7 |   1.9665 |     54.757 |   1.9325 |     51.287 |     0.6
    8 |   1.8884 |     49.252 |   1.8604 |     48.376 |     0.7
    9 |   1.8171 |     48.678 |   1.7862 |     48.131 |     0.7
   10 |   1.7474 |     48.467 |   1.7194 |     48.223 |     0.8
   11 |   1.6875 |     48.477 |   1.6636 |     48.591 |     0.9
   12 |   1.6353 |     47.930 |   1.6145 |     46.354 |     1.0
   13 |   1.5875 |     46.424 |   1.5716 |     45.650 |     1.1
   14 |   1.5475 |     46.256 |   1.5359 |     45.588 |     1.2
   15 |   1.5114 |     46.207 |   1.5017 |     45.588 |     1.2
   16 |   1.4816 |     45.990 |   1.4757 |     45.190 |     1.3
   17 |   1.4556 |     45.275 |   1.4517 |     44.638 |     1.4
   18 |   1.4285 |     44.988 |   1.4286 |     44.301 |     1.5
   19 |   1.4064 |     44.690 |   1.4131 |     44.240 |     1.6
   20 |   1.3860 |     44.305 |   1.3935 |     43.842 |     1.6
   21 |   1.3647 |     43.839 |   1.3812 |     43.719 |     1.7
   22 |   1.3482 |     43.466 |   1.3644 |     43.229 |     1.8
   23 |   1.3335 |     42.983 |   1.3521 |     42.770 |     1.9
   24 |   1.3169 |     42.474 |   1.3394 |     42.310 |     2.0
   25 |   1.2992 |     41.369 |   1.3259 |     41.483 |     2.0
   26 |   1.2849 |     40.480 |   1.3151 |     41.023 |     2.1
   27 |   1.2710 |     40.057 |   1.3044 |     40.257 |     2.2
   28 |   1.2547 |     39.434 |   1.2933 |     40.288 |     2.3
   29 |   1.2404 |     38.779 |   1.2841 |     39.277 |     2.4
   30 |   1.2247 |     38.161 |   1.2715 |     39.737 |     2.5
   31 |   1.2110 |     37.803 |   1.2647 |     38.909 |     2.5
   32 |   1.1953 |     37.440 |   1.2487 |     38.787 |     2.6
   33 |   1.1847 |     37.018 |   1.2391 |     38.940 |     2.7
   34 |   1.1704 |     36.964 |   1.2296 |     38.756 |     2.8
   35 |   1.1560 |     36.508 |   1.2210 |     38.174 |     2.8
   36 |   1.1424 |     36.097 |   1.2157 |     38.419 |     2.9
   37 |   1.1256 |     35.533 |   1.2039 |     37.960 |     3.0
   38 |   1.1165 |     35.154 |   1.2023 |     37.990 |     3.1
   39 |   1.1036 |     34.910 |   1.1851 |     37.408 |     3.2
   40 |   1.0848 |     34.341 |   1.1771 |     37.500 |     3.2
   41 |   1.0709 |     34.151 |   1.1760 |     37.408 |     3.3
   42 |   1.0594 |     33.686 |   1.1591 |     36.581 |     3.4
   43 |   1.0513 |     33.236 |   1.1598 |     36.366 |     3.5
   44 |   1.0371 |     32.710 |   1.1500 |     36.183 |     3.6
   45 |   1.0245 |     32.466 |   1.1402 |     35.907 |     3.6
   46 |   1.0067 |     31.702 |   1.1383 |     35.631 |     3.7
   47 |   0.9961 |     31.101 |   1.1323 |     35.325 |     3.8
   48 |   0.9828 |     30.651 |   1.1322 |     35.600 |     3.9
   49 |   0.9710 |     30.483 |   1.1174 |     34.651 |     4.0
   50 |   0.9648 |     29.849 |   1.1131 |     35.018 |     4.0
   51 |   0.9478 |     29.156 |   1.1122 |     35.049 |     4.1
   52 |   0.9349 |     28.858 |   1.1018 |     34.222 |     4.2
   53 |   0.9251 |     28.565 |   1.0997 |     34.191 |     4.3
   54 |   0.9171 |     28.186 |   1.0936 |     34.191 |     4.4
   55 |   0.9035 |     27.411 |   1.0915 |     34.283 |     4.4
   56 |   0.8913 |     27.238 |   1.0786 |     33.027 |     4.5
   57 |   0.8850 |     26.680 |   1.0825 |     33.946 |     4.6
   58 |   0.8755 |     26.701 |   1.0858 |     33.609 |     4.7
   59 |   0.8634 |     25.981 |   1.0721 |     33.027 |     4.8
   60 |   0.8547 |     25.710 |   1.0642 |     33.119 |     4.8
   61 |   0.8419 |     25.260 |   1.0644 |     32.812 |     4.9
   62 |   0.8358 |     24.984 |   1.0628 |     32.843 |     5.0
   63 |   0.8255 |     24.848 |   1.0627 |     32.629 |     5.1
   64 |   0.8144 |     24.409 |   1.0555 |     32.414 |     5.2
   65 |   0.8052 |     24.339 |   1.0541 |     32.016 |     5.2
   66 |   0.7962 |     23.949 |   1.0503 |     32.047 |     5.3
   67 |   0.7840 |     23.450 |   1.0556 |     32.782 |     5.4
   68 |   0.7792 |     23.358 |   1.0468 |     32.292 |     5.5
   69 |   0.7666 |     22.844 |   1.0471 |     32.414 |     5.6
   70 |   0.7596 |     22.746 |   1.0372 |     31.740 |     5.6
   71 |   0.7495 |     22.437 |   1.0368 |     31.648 |     5.7
   72 |   0.7400 |     22.096 |   1.0368 |     31.893 |     5.8
   73 |   0.7320 |     21.901 |   1.0389 |     31.587 |     5.9
   74 |   0.7268 |     21.565 |   1.0291 |     31.311 |     5.9
   75 |   0.7161 |     21.256 |   1.0319 |     31.556 |     6.0
   76 |   0.7139 |     21.191 |   1.0307 |     31.311 |     6.1
   77 |   0.6997 |     20.736 |   1.0289 |     31.710 |     6.2
   78 |   0.6898 |     20.400 |   1.0331 |     31.526 |     6.3
   79 |   0.6879 |     20.373 |   1.0250 |     31.434 |     6.3
   80 |   0.6757 |     19.912 |   1.0298 |     31.127 |     6.4
   81 |   0.6648 |     19.788 |   1.0209 |     31.250 |     6.5
   82 |   0.6589 |     19.468 |   1.0196 |     30.729 |     6.6
   83 |   0.6570 |     19.322 |   1.0245 |     30.790 |     6.7
   84 |   0.6459 |     19.235 |   1.0210 |     30.852 |     6.7
   85 |   0.6362 |     18.726 |   1.0214 |     30.545 |     6.8
   86 |   0.6337 |     18.493 |   1.0161 |     30.208 |     6.9
   87 |   0.6330 |     18.585 |   1.0078 |     30.392 |     7.0
   88 |   0.6225 |     18.048 |   1.0121 |     30.331 |     7.1
   89 |   0.6166 |     17.864 |   1.0114 |     30.607 |     7.1
   90 |   0.6044 |     17.485 |   1.0081 |     30.239 |     7.2
   91 |   0.6064 |     17.523 |   1.0196 |     30.239 |     7.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 418,978

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8449 |     74.783 |   2.2914 |     58.824 |     0.0
    2 |   2.0798 |     57.093 |   1.9085 |     53.401 |     0.0
    3 |   1.7831 |     49.572 |   1.6661 |     48.346 |     0.1
    4 |   1.6051 |     47.692 |   1.5366 |     45.466 |     0.1
    5 |   1.5106 |     46.218 |   1.4727 |     45.466 |     0.1
    6 |   1.4639 |     46.294 |   1.4392 |     45.466 |     0.1
    7 |   1.4353 |     46.288 |   1.4214 |     45.741 |     0.1
    8 |   1.4244 |     46.256 |   1.4112 |     45.987 |     0.2
    9 |   1.4148 |     46.191 |   1.4069 |     45.987 |     0.2
   10 |   1.4110 |     46.229 |   1.4006 |     45.987 |     0.2
   11 |   1.4023 |     46.175 |   1.3967 |     45.466 |     0.2
   12 |   1.4014 |     46.213 |   1.3905 |     45.987 |     0.2
   13 |   1.3931 |     46.261 |   1.3841 |     45.987 |     0.3
   14 |   1.3859 |     46.337 |   1.3761 |     45.558 |     0.3
   15 |   1.3815 |     46.386 |   1.3699 |     45.772 |     0.3
   16 |   1.3727 |     46.234 |   1.3649 |     45.466 |     0.3
   17 |   1.3621 |     45.785 |   1.3546 |     45.006 |     0.3
   18 |   1.3497 |     45.633 |   1.3433 |     44.730 |     0.4
   19 |   1.3406 |     45.627 |   1.3407 |     44.975 |     0.4
   20 |   1.3330 |     45.638 |   1.3324 |     45.282 |     0.4
   21 |   1.3257 |     45.394 |   1.3265 |     44.485 |     0.4
   22 |   1.3236 |     45.378 |   1.3163 |     44.485 |     0.4
   23 |   1.3171 |     45.216 |   1.3135 |     44.271 |     0.5
   24 |   1.3079 |     44.901 |   1.3067 |     44.271 |     0.5
   25 |   1.3056 |     44.809 |   1.3013 |     44.118 |     0.5
   26 |   1.3003 |     44.685 |   1.3025 |     44.577 |     0.5
   27 |   1.2952 |     44.896 |   1.2953 |     44.118 |     0.5
   28 |   1.2897 |     44.652 |   1.2952 |     44.148 |     0.6
   29 |   1.2856 |     44.620 |   1.2871 |     43.658 |     0.6
   30 |   1.2816 |     44.332 |   1.2806 |     43.964 |     0.6
   31 |   1.2711 |     44.213 |   1.2771 |     43.934 |     0.6
   32 |   1.2673 |     43.856 |   1.2707 |     43.260 |     0.6
   33 |   1.2593 |     43.742 |   1.2670 |     43.474 |     0.7
   34 |   1.2548 |     43.281 |   1.2572 |     42.647 |     0.7
   35 |   1.2462 |     43.119 |   1.2548 |     43.260 |     0.7
   36 |   1.2406 |     42.902 |   1.2517 |     42.770 |     0.7
   37 |   1.2327 |     42.810 |   1.2447 |     42.555 |     0.7
   38 |   1.2200 |     42.322 |   1.2385 |     42.494 |     0.7
   39 |   1.2130 |     42.030 |   1.2253 |     41.881 |     0.8
   40 |   1.2017 |     41.396 |   1.2172 |     41.544 |     0.8
   41 |   1.1886 |     40.637 |   1.2003 |     40.717 |     0.8
   42 |   1.1796 |     40.280 |   1.1911 |     40.625 |     0.8
   43 |   1.1663 |     39.619 |   1.1845 |     39.982 |     0.8
   44 |   1.1556 |     39.321 |   1.1783 |     39.890 |     0.9
   45 |   1.1537 |     39.223 |   1.1768 |     39.185 |     0.9
   46 |   1.1425 |     38.215 |   1.1725 |     38.725 |     0.9
   47 |   1.1337 |     38.112 |   1.1580 |     38.419 |     0.9
   48 |   1.1231 |     37.673 |   1.1596 |     38.971 |     0.9
   49 |   1.1207 |     37.706 |   1.1468 |     38.480 |     1.0
   50 |   1.1104 |     37.310 |   1.1488 |     38.266 |     1.0
   51 |   1.1020 |     36.741 |   1.1381 |     37.592 |     1.0
   52 |   1.0820 |     35.896 |   1.1403 |     37.898 |     1.0
   53 |   1.0828 |     36.308 |   1.1276 |     37.377 |     1.0
   54 |   1.0804 |     36.254 |   1.1224 |     37.040 |     1.1
   55 |   1.0662 |     35.804 |   1.1225 |     36.673 |     1.1
   56 |   1.0705 |     35.761 |   1.1102 |     36.857 |     1.1
   57 |   1.0585 |     35.549 |   1.1036 |     36.949 |     1.1
   58 |   1.0435 |     35.203 |   1.1019 |     36.581 |     1.1
   59 |   1.0462 |     34.921 |   1.0999 |     36.336 |     1.2
   60 |   1.0293 |     34.596 |   1.0983 |     36.612 |     1.2
   61 |   1.0194 |     34.000 |   1.1085 |     36.857 |     1.2
   62 |   1.0129 |     34.016 |   1.0937 |     36.795 |     1.2
   63 |   1.0127 |     33.973 |   1.0960 |     36.673 |     1.2
   64 |   1.0075 |     33.799 |   1.0910 |     36.550 |     1.3
   65 |   1.0005 |     33.518 |   1.0804 |     35.662 |     1.3
   66 |   0.9921 |     33.404 |   1.0971 |     36.550 |     1.3
   67 |   0.9940 |     33.626 |   1.0710 |     35.784 |     1.3
   68 |   0.9948 |     33.686 |   1.0881 |     36.183 |     1.3
   69 |   0.9899 |     33.539 |   1.0684 |     35.539 |     1.4
   70 |   0.9739 |     32.775 |   1.0804 |     35.692 |     1.4
   71 |   0.9662 |     32.559 |   1.0717 |     35.325 |     1.4
   72 |   0.9646 |     32.531 |   1.0562 |     34.988 |     1.4
   73 |   0.9537 |     32.082 |   1.0756 |     35.141 |     1.4
   74 |   0.9621 |     32.271 |   1.0717 |     35.723 |     1.5
   75 |   0.9499 |     32.049 |   1.0616 |     35.110 |     1.5
   76 |   0.9437 |     31.627 |   1.0544 |     34.681 |     1.5
   77 |   0.9383 |     31.394 |   1.0572 |     34.835 |     1.5
   78 |   0.9292 |     31.031 |   1.0695 |     35.600 |     1.5
   79 |   0.9221 |     31.014 |   1.0825 |     35.202 |     1.6
   80 |   0.9245 |     31.058 |   1.0689 |     34.926 |     1.6
   81 |   0.9199 |     30.890 |   1.0513 |     34.222 |     1.6
   82 |   0.9127 |     30.624 |   1.0570 |     35.080 |     1.6
   83 |   0.9180 |     30.570 |   1.0583 |     34.344 |     1.6
   84 |   0.9032 |     30.429 |   1.0486 |     34.161 |     1.7
   85 |   0.9019 |     30.370 |   1.0471 |     34.161 |     1.7
   86 |   0.9025 |     30.142 |   1.0481 |     35.049 |     1.7
   87 |   0.8967 |     30.218 |   1.0591 |     34.835 |     1.7
   88 |   0.8961 |     30.250 |   1.0329 |     34.406 |     1.7
   89 |   0.8801 |     29.367 |   1.0373 |     34.436 |     1.8
   90 |   0.8777 |     29.595 |   1.0367 |     33.915 |     1.8
   91 |   0.8815 |     29.746 |   1.0491 |     34.375 |     1.8
   92 |   0.8758 |     29.367 |   1.0389 |     33.762 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 976,322

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7932 |     73.570 |   2.4025 |     59.007 |     0.0
    2 |   2.2186 |     58.420 |   2.0487 |     57.567 |     0.1
    3 |   1.9129 |     54.102 |   1.7592 |     48.529 |     0.1
    4 |   1.6607 |     47.800 |   1.5614 |     45.466 |     0.2
    5 |   1.5303 |     46.213 |   1.4792 |     45.466 |     0.2
    6 |   1.4685 |     46.137 |   1.4418 |     45.711 |     0.2
    7 |   1.4391 |     46.186 |   1.4177 |     45.895 |     0.3
    8 |   1.4171 |     46.045 |   1.3976 |     45.221 |     0.3
    9 |   1.3979 |     45.487 |   1.3831 |     43.995 |     0.4
   10 |   1.3772 |     44.658 |   1.3577 |     44.547 |     0.4
   11 |   1.3569 |     44.246 |   1.3432 |     43.474 |     0.5
   12 |   1.3413 |     43.774 |   1.3245 |     43.505 |     0.5
   13 |   1.3216 |     43.401 |   1.3088 |     42.555 |     0.5
   14 |   1.3037 |     43.021 |   1.2986 |     42.831 |     0.6
   15 |   1.2910 |     42.729 |   1.2871 |     41.728 |     0.6
   16 |   1.2740 |     42.268 |   1.2721 |     41.942 |     0.7
   17 |   1.2538 |     42.106 |   1.2544 |     41.483 |     0.7
   18 |   1.2420 |     41.661 |   1.2439 |     41.360 |     0.7
   19 |   1.2302 |     41.027 |   1.2391 |     40.533 |     0.8
   20 |   1.2108 |     40.691 |   1.2282 |     39.737 |     0.8
   21 |   1.1910 |     39.700 |   1.2114 |     38.909 |     0.9
   22 |   1.1799 |     39.689 |   1.1991 |     38.388 |     0.9
   23 |   1.1641 |     38.898 |   1.1964 |     39.032 |     0.9
   24 |   1.1515 |     38.513 |   1.1810 |     38.419 |     1.0
   25 |   1.1459 |     38.616 |   1.1677 |     37.898 |     1.0
   26 |   1.1351 |     37.803 |   1.1578 |     37.776 |     1.1
   27 |   1.1263 |     37.538 |   1.1711 |     38.664 |     1.1
   28 |   1.1144 |     37.375 |   1.1526 |     37.623 |     1.2
   29 |   1.1083 |     36.980 |   1.1496 |     37.255 |     1.2
   30 |   1.1078 |     36.828 |   1.1434 |     37.132 |     1.2
   31 |   1.0886 |     36.297 |   1.1337 |     36.795 |     1.3
   32 |   1.0780 |     35.907 |   1.1168 |     36.520 |     1.3
   33 |   1.0676 |     35.365 |   1.1098 |     35.938 |     1.4
   34 |   1.0615 |     35.105 |   1.1094 |     36.581 |     1.4
   35 |   1.0532 |     34.325 |   1.1051 |     36.489 |     1.4
   36 |   1.0426 |     34.222 |   1.0912 |     34.743 |     1.5
   37 |   1.0363 |     33.843 |   1.0844 |     35.846 |     1.5
   38 |   1.0158 |     33.707 |   1.0925 |     34.743 |     1.6
   39 |   1.0162 |     33.442 |   1.0792 |     34.835 |     1.6
   40 |   1.0123 |     33.106 |   1.0735 |     34.191 |     1.6
   41 |   1.0020 |     32.515 |   1.0656 |     33.946 |     1.7
   42 |   0.9864 |     32.407 |   1.0536 |     34.130 |     1.7
   43 |   0.9758 |     31.919 |   1.0601 |     34.222 |     1.8
   44 |   0.9752 |     31.778 |   1.0572 |     34.222 |     1.8
   45 |   0.9685 |     31.827 |   1.0397 |     32.812 |     1.8
   46 |   0.9675 |     31.713 |   1.0463 |     33.609 |     1.9
   47 |   0.9531 |     31.361 |   1.0414 |     33.272 |     1.9
   48 |   0.9433 |     31.003 |   1.0451 |     34.038 |     2.0
   49 |   0.9440 |     30.966 |   1.0271 |     33.058 |     2.0
   50 |   0.9320 |     30.657 |   1.0445 |     33.762 |     2.1
   51 |   0.9281 |     30.451 |   1.0323 |     32.935 |     2.1
   52 |   0.9178 |     30.169 |   1.0287 |     32.935 |     2.1
   53 |   0.9124 |     29.817 |   1.0278 |     33.180 |     2.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 398,754

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8414 |     75.016 |   2.2887 |     58.824 |     0.0
    2 |   2.0568 |     56.773 |   1.8871 |     48.346 |     0.0
    3 |   1.7480 |     48.402 |   1.6289 |     45.466 |     0.0
    4 |   1.5725 |     46.240 |   1.5186 |     45.987 |     0.1
    5 |   1.4945 |     46.288 |   1.4645 |     45.466 |     0.1
    6 |   1.4559 |     46.321 |   1.4367 |     45.466 |     0.1
    7 |   1.4324 |     46.234 |   1.4196 |     45.466 |     0.1
    8 |   1.4204 |     46.131 |   1.4105 |     45.925 |     0.1
    9 |   1.4073 |     46.034 |   1.3982 |     45.650 |     0.2
   10 |   1.3975 |     45.996 |   1.3890 |     45.006 |     0.2
   11 |   1.3875 |     45.720 |   1.3801 |     45.159 |     0.2
   12 |   1.3791 |     45.893 |   1.3728 |     45.558 |     0.2
   13 |   1.3707 |     45.443 |   1.3642 |     44.332 |     0.2
   14 |   1.3582 |     45.470 |   1.3621 |     44.761 |     0.2
   15 |   1.3481 |     45.021 |   1.3452 |     44.485 |     0.3
   16 |   1.3395 |     44.961 |   1.3486 |     45.833 |     0.3
   17 |   1.3293 |     44.685 |   1.3322 |     43.873 |     0.3
   18 |   1.3190 |     43.807 |   1.3239 |     43.229 |     0.3
   19 |   1.3069 |     43.254 |   1.3198 |     43.444 |     0.3
   20 |   1.2927 |     42.842 |   1.2984 |     43.352 |     0.3
   21 |   1.2735 |     42.339 |   1.2935 |     43.229 |     0.4
   22 |   1.2620 |     42.181 |   1.2760 |     41.973 |     0.4
   23 |   1.2500 |     41.992 |   1.2728 |     42.310 |     0.4
   24 |   1.2407 |     41.840 |   1.2713 |     42.126 |     0.4
   25 |   1.2298 |     41.607 |   1.2752 |     42.739 |     0.4
   26 |   1.2189 |     41.342 |   1.2530 |     42.218 |     0.4
   27 |   1.2065 |     41.060 |   1.2455 |     42.525 |     0.5
   28 |   1.2001 |     40.886 |   1.2431 |     41.759 |     0.5
   29 |   1.1919 |     40.708 |   1.2338 |     41.360 |     0.5
   30 |   1.1810 |     40.117 |   1.2322 |     41.728 |     0.5
   31 |   1.1646 |     39.770 |   1.2257 |     41.636 |     0.5
   32 |   1.1571 |     39.602 |   1.2327 |     42.157 |     0.5
   33 |   1.1566 |     39.700 |   1.2261 |     40.839 |     0.6
   34 |   1.1412 |     39.234 |   1.2165 |     40.502 |     0.6
   35 |   1.1246 |     38.687 |   1.2086 |     40.656 |     0.6
   36 |   1.1104 |     38.139 |   1.2080 |     39.951 |     0.6
   37 |   1.1011 |     37.939 |   1.1942 |     39.859 |     0.6
   38 |   1.0909 |     37.337 |   1.1963 |     39.706 |     0.6
   39 |   1.0813 |     37.381 |   1.1902 |     39.154 |     0.7
   40 |   1.0696 |     36.893 |   1.1918 |     39.583 |     0.7
   41 |   1.0691 |     36.720 |   1.2000 |     39.491 |     0.7
   42 |   1.0604 |     36.335 |   1.1762 |     38.358 |     0.7
   43 |   1.0492 |     35.891 |   1.1745 |     38.051 |     0.7
   44 |   1.0319 |     35.625 |   1.1813 |     38.480 |     0.7
   45 |   1.0284 |     35.094 |   1.1878 |     39.308 |     0.8
   46 |   1.0194 |     34.943 |   1.1863 |     38.480 |     0.8
   47 |   1.0150 |     34.970 |   1.1736 |     38.971 |     0.8
   48 |   1.0021 |     34.298 |   1.1801 |     38.450 |     0.8
   49 |   0.9985 |     34.244 |   1.1640 |     37.745 |     0.8
   50 |   0.9847 |     33.832 |   1.1618 |     38.297 |     0.8
   51 |   0.9776 |     33.621 |   1.1614 |     38.419 |     0.9
   52 |   0.9786 |     33.604 |   1.1872 |     38.572 |     0.9
   53 |   0.9989 |     33.908 |   1.1788 |     38.542 |     0.9
   54 |   0.9707 |     33.290 |   1.1684 |     38.542 |     0.9
   55 |   0.9568 |     32.802 |   1.1912 |     38.603 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,141,858

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5138 |     67.062 |   1.9046 |     53.799 |     0.0
    2 |   1.6886 |     48.727 |   1.5270 |     45.466 |     0.1
    3 |   1.4804 |     46.359 |   1.4372 |     45.466 |     0.1
    4 |   1.4285 |     46.272 |   1.4120 |     45.987 |     0.2
    5 |   1.4115 |     46.554 |   1.4013 |     45.987 |     0.2
    6 |   1.3983 |     46.234 |   1.3895 |     45.466 |     0.2
    7 |   1.3890 |     46.251 |   1.3727 |     45.466 |     0.3
    8 |   1.3776 |     46.115 |   1.3617 |     45.987 |     0.3
    9 |   1.3562 |     45.752 |   1.3407 |     43.995 |     0.4
   10 |   1.3339 |     44.522 |   1.3264 |     43.689 |     0.4
   11 |   1.3119 |     44.072 |   1.2991 |     42.862 |     0.5
   12 |   1.2904 |     43.801 |   1.2841 |     42.800 |     0.5
   13 |   1.2771 |     43.466 |   1.2712 |     41.850 |     0.5
   14 |   1.2639 |     43.113 |   1.2561 |     42.402 |     0.6
   15 |   1.2467 |     42.376 |   1.2512 |     41.636 |     0.6
   16 |   1.2345 |     42.089 |   1.2338 |     40.993 |     0.7
   17 |   1.2195 |     41.683 |   1.2225 |     41.360 |     0.7
   18 |   1.2056 |     41.103 |   1.2111 |     41.146 |     0.7
   19 |   1.1863 |     40.550 |   1.1964 |     39.920 |     0.8
   20 |   1.1725 |     40.057 |   1.1892 |     39.645 |     0.8
   21 |   1.1508 |     39.169 |   1.1816 |     39.859 |     0.9
   22 |   1.1438 |     38.947 |   1.1644 |     38.817 |     0.9
   23 |   1.1193 |     37.852 |   1.1470 |     38.235 |     1.0
   24 |   1.1076 |     37.668 |   1.1391 |     38.603 |     1.0
   25 |   1.0947 |     37.262 |   1.1268 |     37.194 |     1.0
   26 |   1.0654 |     35.891 |   1.1008 |     35.815 |     1.1
   27 |   1.0497 |     35.241 |   1.1076 |     37.224 |     1.1
   28 |   1.0410 |     35.116 |   1.0804 |     35.754 |     1.2
   29 |   1.0234 |     34.412 |   1.0690 |     35.355 |     1.2
   30 |   1.0052 |     33.664 |   1.0600 |     35.509 |     1.2
   31 |   0.9908 |     32.824 |   1.0520 |     34.314 |     1.3
   32 |   0.9828 |     32.932 |   1.0503 |     34.130 |     1.3
   33 |   0.9612 |     32.158 |   1.0428 |     33.762 |     1.4
   34 |   0.9536 |     31.838 |   1.0374 |     34.283 |     1.4
   35 |   0.9389 |     31.404 |   1.0266 |     33.946 |     1.5
   36 |   0.9325 |     31.442 |   1.0283 |     33.946 |     1.5
   37 |   0.9146 |     30.711 |   1.0084 |     33.456 |     1.5
   38 |   0.9023 |     30.342 |   1.0105 |     33.548 |     1.6
   39 |   0.8884 |     29.763 |   1.0065 |     32.843 |     1.6
   40 |   0.8750 |     29.275 |   1.0114 |     33.640 |     1.7
   41 |   0.8644 |     28.982 |   0.9949 |     33.058 |     1.7
   42 |   0.8521 |     28.766 |   0.9990 |     33.027 |     1.8
   43 |   0.8447 |     28.386 |   0.9833 |     32.016 |     1.8
   44 |   0.8338 |     27.888 |   0.9794 |     32.200 |     1.8
   45 |   0.8209 |     27.498 |   0.9694 |     31.863 |     1.9
   46 |   0.8126 |     27.010 |   0.9647 |     31.495 |     1.9
   47 |   0.8070 |     26.999 |   0.9662 |     30.607 |     2.0
   48 |   0.7930 |     26.371 |   0.9585 |     30.852 |     2.0
   49 |   0.7826 |     26.149 |   0.9631 |     31.250 |     2.0
   50 |   0.7785 |     26.338 |   0.9659 |     31.526 |     2.1
   51 |   0.7741 |     26.187 |   0.9636 |     31.036 |     2.1
   52 |   0.7649 |     25.721 |   0.9576 |     30.790 |     2.2
   53 |   0.7500 |     25.152 |   0.9507 |     30.913 |     2.2
   54 |   0.7385 |     24.626 |   0.9447 |     30.147 |     2.3
   55 |   0.7269 |     24.686 |   0.9414 |     29.994 |     2.3
   56 |   0.7255 |     24.361 |   0.9490 |     30.545 |     2.3
   57 |   0.7202 |     24.296 |   0.9294 |     29.534 |     2.4
   58 |   0.6939 |     23.179 |   0.9451 |     29.688 |     2.4
   59 |   0.7004 |     23.878 |   0.9277 |     29.933 |     2.5
   60 |   0.6853 |     22.979 |   0.9434 |     29.504 |     2.5
   61 |   0.6841 |     23.087 |   0.9392 |     29.320 |     2.5
   62 |   0.6627 |     22.291 |   0.9205 |     28.339 |     2.6
   63 |   0.6650 |     22.128 |   0.9150 |     28.401 |     2.6
   64 |   0.6561 |     21.977 |   0.9197 |     28.891 |     2.7
   65 |   0.6359 |     21.429 |   0.9263 |     29.197 |     2.7
   66 |   0.6456 |     21.581 |   0.9322 |     28.707 |     2.8
   67 |   0.6331 |     21.554 |   0.9073 |     27.727 |     2.8
   68 |   0.6289 |     21.186 |   0.9223 |     28.615 |     2.8
   69 |   0.6300 |     21.424 |   0.9270 |     28.493 |     2.9
   70 |   0.6224 |     21.299 |   0.9289 |     28.431 |     2.9
   71 |   0.6197 |     20.682 |   0.9414 |     29.075 |     3.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 1,393,186

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2602 |     62.603 |   1.6237 |     48.346 |     0.0
    2 |   1.4877 |     46.614 |   1.4259 |     45.466 |     0.1
    3 |   1.4172 |     46.288 |   1.3980 |     45.466 |     0.1
    4 |   1.4032 |     46.321 |   1.3961 |     45.527 |     0.1
    5 |   1.3975 |     46.348 |   1.3828 |     45.496 |     0.2
    6 |   1.3776 |     45.709 |   1.3546 |     44.516 |     0.2
    7 |   1.3561 |     45.124 |   1.3409 |     44.485 |     0.2
    8 |   1.3433 |     45.145 |   1.3257 |     44.087 |     0.3
    9 |   1.3333 |     44.842 |   1.3251 |     43.719 |     0.3
   10 |   1.3273 |     44.939 |   1.3241 |     43.719 |     0.4
   11 |   1.3208 |     44.240 |   1.3231 |     44.424 |     0.4
   12 |   1.3078 |     44.278 |   1.2999 |     43.229 |     0.4
   13 |   1.3018 |     43.904 |   1.2940 |     42.831 |     0.5
   14 |   1.2868 |     43.314 |   1.2870 |     42.892 |     0.5
   15 |   1.2726 |     42.826 |   1.2816 |     42.433 |     0.5
   16 |   1.2642 |     42.658 |   1.2738 |     42.555 |     0.6
   17 |   1.2582 |     42.057 |   1.2610 |     41.667 |     0.6
   18 |   1.2482 |     41.873 |   1.2581 |     41.483 |     0.6
   19 |   1.2319 |     41.369 |   1.2402 |     41.023 |     0.7
   20 |   1.2302 |     41.688 |   1.2550 |     41.973 |     0.7
   21 |   1.2283 |     41.407 |   1.2381 |     40.931 |     0.7
   22 |   1.2037 |     40.789 |   1.2286 |     40.625 |     0.8
   23 |   1.1904 |     40.350 |   1.2151 |     40.380 |     0.8
   24 |   1.1777 |     39.911 |   1.2223 |     40.472 |     0.8
   25 |   1.1613 |     39.196 |   1.1969 |     40.104 |     0.9
   26 |   1.1481 |     38.367 |   1.1878 |     39.553 |     0.9
   27 |   1.1328 |     38.405 |   1.1757 |     39.093 |     0.9
   28 |   1.1130 |     37.527 |   1.1729 |     38.787 |     1.0
   29 |   1.1084 |     37.576 |   1.1631 |     38.511 |     1.0
   30 |   1.0978 |     36.839 |   1.1507 |     38.450 |     1.0
   31 |   1.0791 |     36.351 |   1.1351 |     37.623 |     1.1
   32 |   1.0721 |     36.254 |   1.1348 |     37.224 |     1.1
   33 |   1.0550 |     35.799 |   1.1489 |     37.868 |     1.2
   34 |   1.0355 |     34.840 |   1.1493 |     38.511 |     1.2
   35 |   1.0325 |     34.850 |   1.1419 |     37.623 |     1.2
   36 |   1.0311 |     35.035 |   1.1330 |     37.316 |     1.3
   37 |   0.9874 |     33.496 |   1.1051 |     36.857 |     1.3
   38 |   0.9718 |     32.992 |   1.1072 |     36.029 |     1.3
   39 |   0.9528 |     32.726 |   1.1174 |     36.305 |     1.4
   40 |   0.9418 |     32.309 |   1.1134 |     36.703 |     1.4
   41 |   0.9311 |     31.529 |   1.0988 |     36.121 |     1.4
   42 |   0.9289 |     31.957 |   1.0905 |     36.275 |     1.5
   43 |   0.8992 |     30.754 |   1.1025 |     35.784 |     1.5
   44 |   0.8737 |     30.061 |   1.0973 |     35.080 |     1.5
   45 |   0.8570 |     29.356 |   1.0960 |     36.213 |     1.6
   46 |   0.8538 |     29.394 |   1.1037 |     36.029 |     1.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 2,587,426

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5712 |     68.964 |   1.9986 |     58.854 |     0.1
    2 |   1.7623 |     49.290 |   1.5849 |     45.496 |     0.2
    3 |   1.5134 |     46.305 |   1.4544 |     45.466 |     0.2
    4 |   1.4421 |     46.158 |   1.4200 |     45.987 |     0.3
    5 |   1.4172 |     46.234 |   1.4049 |     45.466 |     0.4
    6 |   1.4090 |     46.332 |   1.3985 |     45.435 |     0.5
    7 |   1.4006 |     46.386 |   1.3942 |     45.006 |     0.6
    8 |   1.3968 |     46.137 |   1.3888 |     45.466 |     0.6
    9 |   1.3962 |     46.066 |   1.3876 |     45.312 |     0.7
   10 |   1.3903 |     45.757 |   1.3766 |     44.792 |     0.8
   11 |   1.3833 |     45.302 |   1.3625 |     43.781 |     0.9
   12 |   1.3645 |     44.685 |   1.3480 |     43.842 |     1.0
   13 |   1.3539 |     44.446 |   1.3430 |     43.627 |     1.0
   14 |   1.3468 |     44.457 |   1.3385 |     43.750 |     1.1
   15 |   1.3471 |     44.479 |   1.3358 |     43.719 |     1.2
   16 |   1.3361 |     44.343 |   1.3303 |     44.118 |     1.3
   17 |   1.3292 |     44.099 |   1.3177 |     43.474 |     1.4
   18 |   1.3198 |     44.034 |   1.3174 |     43.474 |     1.5
   19 |   1.3133 |     43.829 |   1.3058 |     42.616 |     1.5
   20 |   1.3057 |     43.401 |   1.3081 |     43.719 |     1.6
   21 |   1.3042 |     43.428 |   1.2991 |     42.555 |     1.7
   22 |   1.2985 |     43.433 |   1.3002 |     42.770 |     1.8
   23 |   1.2910 |     43.298 |   1.2900 |     42.770 |     1.9
   24 |   1.2872 |     43.227 |   1.2901 |     42.218 |     1.9
   25 |   1.2801 |     43.233 |   1.2781 |     42.371 |     2.0
   26 |   1.2736 |     43.059 |   1.2866 |     42.586 |     2.1
   27 |   1.2642 |     42.577 |   1.2780 |     42.923 |     2.2
   28 |   1.2600 |     42.566 |   1.2574 |     42.004 |     2.3
   29 |   1.2547 |     42.474 |   1.2588 |     41.973 |     2.3
   30 |   1.2499 |     42.577 |   1.2598 |     41.697 |     2.4
   31 |   1.2450 |     42.062 |   1.2517 |     41.544 |     2.5
   32 |   1.2430 |     42.236 |   1.2572 |     41.881 |     2.6
   33 |   1.2435 |     42.274 |   1.2631 |     42.157 |     2.7
   34 |   1.2405 |     41.927 |   1.2570 |     41.422 |     2.7
   35 |   1.2274 |     41.580 |   1.2422 |     41.575 |     2.8
   36 |   1.2161 |     41.385 |   1.2432 |     40.901 |     2.9
   37 |   1.2217 |     41.130 |   1.2218 |     40.564 |     3.0
   38 |   1.2154 |     41.060 |   1.2265 |     40.380 |     3.1
   39 |   1.2110 |     40.865 |   1.2359 |     40.778 |     3.1
   40 |   1.2093 |     40.561 |   1.2206 |     40.380 |     3.2
   41 |   1.2017 |     40.567 |   1.2127 |     40.257 |     3.3
   42 |   1.1816 |     40.171 |   1.2216 |     40.043 |     3.4
   43 |   1.1821 |     39.884 |   1.2135 |     38.388 |     3.5
   44 |   1.1749 |     39.721 |   1.2061 |     39.828 |     3.5
   45 |   1.1662 |     39.142 |   1.2124 |     39.400 |     3.6
   46 |   1.1575 |     39.082 |   1.1986 |     39.491 |     3.7
   47 |   1.1585 |     39.142 |   1.2010 |     39.859 |     3.8
   48 |   1.1521 |     38.844 |   1.1894 |     38.664 |     3.9
   49 |   1.1445 |     38.708 |   1.1849 |     39.277 |     3.9
   50 |   1.1338 |     38.172 |   1.1741 |     38.971 |     4.0
   51 |   1.1163 |     37.446 |   1.1732 |     38.297 |     4.1
   52 |   1.1194 |     37.191 |   1.1865 |     38.051 |     4.2
   53 |   1.1180 |     37.608 |   1.1882 |     38.664 |     4.3
   54 |   1.1205 |     37.554 |   1.1912 |     38.480 |     4.3
   55 |   1.1105 |     37.012 |   1.1720 |     38.572 |     4.4
   56 |   1.0987 |     36.714 |   1.1699 |     37.806 |     4.5
   57 |   1.1051 |     37.191 |   1.1700 |     38.082 |     4.6
   58 |   1.1080 |     36.980 |   1.1629 |     38.205 |     4.7
   59 |   1.0907 |     36.384 |   1.1553 |     37.316 |     4.7
   60 |   1.0828 |     36.059 |   1.1667 |     37.439 |     4.8
   61 |   1.0821 |     36.118 |   1.1517 |     37.561 |     4.9
   62 |   1.0749 |     35.631 |   1.1413 |     37.194 |     5.0
   63 |   1.0606 |     35.777 |   1.1450 |     36.703 |     5.1
   64 |   1.0451 |     35.121 |   1.1521 |     37.439 |     5.1
   65 |   1.0413 |     34.823 |   1.1514 |     37.684 |     5.2
   66 |   1.0431 |     34.937 |   1.1370 |     37.132 |     5.3
   67 |   1.0424 |     34.970 |   1.1499 |     37.531 |     5.4
   68 |   1.0331 |     34.986 |   1.1593 |     38.327 |     5.5
   69 |   1.0280 |     34.748 |   1.1482 |     37.868 |     5.5
   70 |   1.0146 |     34.271 |   1.1466 |     36.673 |     5.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,177,122

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4638 |     66.119 |   2.0334 |     57.843 |     0.0
    2 |   1.7511 |     49.648 |   1.5303 |     45.496 |     0.1
    3 |   1.4689 |     46.131 |   1.4146 |     45.404 |     0.1
    4 |   1.4008 |     45.850 |   1.3758 |     44.547 |     0.2
    5 |   1.3634 |     44.863 |   1.3421 |     43.627 |     0.2
    6 |   1.3245 |     43.634 |   1.3063 |     43.260 |     0.3
    7 |   1.2844 |     43.146 |   1.2796 |     41.483 |     0.3
    8 |   1.2507 |     42.171 |   1.2544 |     41.575 |     0.3
    9 |   1.2224 |     41.076 |   1.2147 |     40.012 |     0.4
   10 |   1.1840 |     39.602 |   1.1986 |     39.093 |     0.4
   11 |   1.1535 |     38.513 |   1.1628 |     37.592 |     0.5
   12 |   1.1146 |     36.801 |   1.1416 |     36.857 |     0.5
   13 |   1.0928 |     35.983 |   1.1296 |     35.815 |     0.6
   14 |   1.0674 |     35.018 |   1.0898 |     35.294 |     0.6
   15 |   1.0363 |     34.027 |   1.0806 |     34.926 |     0.6
   16 |   1.0081 |     32.802 |   1.0640 |     34.773 |     0.7
   17 |   0.9822 |     31.887 |   1.0482 |     33.058 |     0.7
   18 |   0.9549 |     31.025 |   1.0435 |     34.069 |     0.8
   19 |   0.9280 |     29.941 |   1.0280 |     33.303 |     0.8
   20 |   0.9038 |     29.470 |   1.0091 |     32.047 |     0.9
   21 |   0.8889 |     28.798 |   0.9943 |     31.893 |     0.9
   22 |   0.8648 |     28.213 |   0.9960 |     31.710 |     0.9
   23 |   0.8444 |     27.276 |   0.9838 |     31.618 |     1.0
   24 |   0.8167 |     26.588 |   0.9839 |     31.005 |     1.0
   25 |   0.7916 |     25.704 |   0.9642 |     31.863 |     1.1
   26 |   0.7808 |     25.336 |   0.9772 |     31.158 |     1.1
   27 |   0.7688 |     25.184 |   0.9641 |     30.729 |     1.1
   28 |   0.7411 |     24.138 |   0.9554 |     30.576 |     1.2
   29 |   0.7243 |     23.542 |   0.9456 |     30.116 |     1.2
   30 |   0.7166 |     23.380 |   0.9485 |     29.871 |     1.3
   31 |   0.7167 |     23.727 |   0.9409 |     30.055 |     1.3
   32 |   0.6776 |     22.117 |   0.9458 |     29.350 |     1.4
   33 |   0.6677 |     21.792 |   0.9220 |     28.339 |     1.4
   34 |   0.6447 |     20.850 |   0.9441 |     29.534 |     1.4
   35 |   0.6398 |     20.806 |   0.9391 |     29.136 |     1.5
   36 |   0.6171 |     20.264 |   0.9250 |     28.462 |     1.5
   37 |   0.6141 |     19.874 |   0.9344 |     28.738 |     1.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 367,586

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6117 |     70.232 |   2.0314 |     58.487 |     0.0
    2 |   1.8045 |     50.883 |   1.5897 |     45.466 |     0.0
    3 |   1.5239 |     46.321 |   1.4562 |     45.466 |     0.1
    4 |   1.4468 |     46.326 |   1.4193 |     45.466 |     0.1
    5 |   1.4203 |     46.278 |   1.4063 |     45.466 |     0.1
    6 |   1.4049 |     46.283 |   1.3864 |     45.466 |     0.1
    7 |   1.3844 |     46.386 |   1.3672 |     45.466 |     0.1
    8 |   1.3674 |     45.779 |   1.3512 |     44.792 |     0.1
    9 |   1.3472 |     45.226 |   1.3380 |     44.056 |     0.2
   10 |   1.3332 |     45.270 |   1.3239 |     44.118 |     0.2
   11 |   1.3210 |     45.156 |   1.3147 |     43.842 |     0.2
   12 |   1.3055 |     44.690 |   1.3010 |     43.903 |     0.2
   13 |   1.2884 |     44.538 |   1.2935 |     44.853 |     0.2
   14 |   1.2784 |     44.273 |   1.2803 |     43.107 |     0.3
   15 |   1.2656 |     43.899 |   1.2685 |     42.831 |     0.3
   16 |   1.2545 |     43.135 |   1.2653 |     44.179 |     0.3
   17 |   1.2356 |     42.409 |   1.2379 |     41.513 |     0.3
   18 |   1.2145 |     41.472 |   1.2212 |     41.513 |     0.3
   19 |   1.1935 |     40.735 |   1.2108 |     40.564 |     0.3
   20 |   1.1752 |     39.787 |   1.1938 |     40.227 |     0.4
   21 |   1.1602 |     39.570 |   1.1898 |     40.012 |     0.4
   22 |   1.1387 |     38.947 |   1.1896 |     39.338 |     0.4
   23 |   1.1290 |     38.594 |   1.1640 |     39.706 |     0.4
   24 |   1.1178 |     37.944 |   1.1464 |     39.737 |     0.4
   25 |   1.0941 |     37.592 |   1.1427 |     38.542 |     0.5
   26 |   1.0823 |     36.915 |   1.1233 |     38.480 |     0.5
   27 |   1.0712 |     36.785 |   1.1246 |     38.297 |     0.5
   28 |   1.0581 |     36.281 |   1.1234 |     37.806 |     0.5
   29 |   1.0400 |     35.690 |   1.1080 |     38.388 |     0.5
   30 |   1.0309 |     35.371 |   1.0958 |     37.255 |     0.5
   31 |   1.0185 |     34.970 |   1.0897 |     37.500 |     0.6
   32 |   1.0039 |     34.200 |   1.0889 |     37.347 |     0.6
   33 |   0.9953 |     34.141 |   1.0676 |     36.091 |     0.6
   34 |   0.9723 |     33.252 |   1.0682 |     36.550 |     0.6
   35 |   0.9580 |     33.252 |   1.0631 |     36.152 |     0.6
   36 |   0.9470 |     32.651 |   1.0531 |     35.754 |     0.7
   37 |   0.9427 |     32.575 |   1.0566 |     35.999 |     0.7
   38 |   0.9406 |     32.434 |   1.0588 |     35.754 |     0.7
   39 |   0.9181 |     31.681 |   1.0525 |     35.386 |     0.7
   40 |   0.9173 |     32.017 |   1.0420 |     36.121 |     0.7
   41 |   0.9003 |     30.911 |   1.0447 |     35.417 |     0.7
   42 |   0.8851 |     30.873 |   1.0352 |     35.417 |     0.8
   43 |   0.8841 |     30.521 |   1.0373 |     35.386 |     0.8
   44 |   0.8707 |     30.109 |   1.0283 |     34.191 |     0.8
   45 |   0.8603 |     29.616 |   1.0283 |     34.436 |     0.8
   46 |   0.8526 |     29.676 |   1.0110 |     34.161 |     0.8
   47 |   0.8424 |     29.031 |   1.0094 |     32.751 |     0.8
   48 |   0.8446 |     28.782 |   1.0167 |     34.099 |     0.9
   49 |   0.8273 |     28.603 |   1.0172 |     32.782 |     0.9
   50 |   0.8202 |     28.040 |   1.0056 |     32.567 |     0.9
   51 |   0.8098 |     27.926 |   1.0138 |     33.180 |     0.9
   52 |   0.7997 |     27.400 |   1.0101 |     32.567 |     0.9
   53 |   0.7985 |     27.617 |   0.9866 |     31.955 |     1.0
   54 |   0.7875 |     26.761 |   0.9844 |     32.016 |     1.0
   55 |   0.7783 |     26.582 |   0.9929 |     31.863 |     1.0
   56 |   0.7690 |     26.067 |   0.9838 |     31.863 |     1.0
   57 |   0.7574 |     25.726 |   0.9720 |     30.729 |     1.0
   58 |   0.7533 |     25.639 |   0.9750 |     30.821 |     1.0
   59 |   0.7404 |     25.179 |   0.9670 |     30.821 |     1.1
   60 |   0.7506 |     25.656 |   0.9671 |     31.556 |     1.1
   61 |   0.7392 |     25.141 |   0.9650 |     31.464 |     1.1
   62 |   0.7225 |     24.615 |   0.9615 |     30.944 |     1.1
   63 |   0.7125 |     24.046 |   0.9728 |     30.637 |     1.1
   64 |   0.7102 |     23.922 |   0.9732 |     30.300 |     1.2
   65 |   0.6969 |     23.635 |   0.9783 |     30.331 |     1.2
   66 |   0.6905 |     23.353 |   0.9751 |     30.423 |     1.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,242,370

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2304 |     60.679 |   1.6168 |     45.466 |     0.0
    2 |   1.4915 |     46.229 |   1.4293 |     45.466 |     0.1
    3 |   1.4181 |     46.272 |   1.3985 |     45.466 |     0.1
    4 |   1.4000 |     46.207 |   1.3869 |     45.987 |     0.1
    5 |   1.3895 |     46.169 |   1.3715 |     45.466 |     0.2
    6 |   1.3702 |     46.018 |   1.3607 |     45.037 |     0.2
    7 |   1.3496 |     45.335 |   1.3307 |     43.382 |     0.2
    8 |   1.3258 |     44.327 |   1.2985 |     42.739 |     0.3
    9 |   1.2942 |     43.639 |   1.2804 |     41.789 |     0.3
   10 |   1.2733 |     42.913 |   1.2503 |     41.422 |     0.4
   11 |   1.2394 |     41.954 |   1.2403 |     40.196 |     0.4
   12 |   1.2090 |     40.979 |   1.2062 |     40.441 |     0.4
   13 |   1.1857 |     40.014 |   1.1848 |     39.430 |     0.5
   14 |   1.1598 |     39.386 |   1.1671 |     38.205 |     0.5
   15 |   1.1320 |     38.399 |   1.1590 |     39.338 |     0.5
   16 |   1.0995 |     37.614 |   1.1153 |     37.286 |     0.6
   17 |   1.0756 |     36.947 |   1.1077 |     37.500 |     0.6
   18 |   1.0517 |     36.037 |   1.0818 |     35.723 |     0.6
   19 |   1.0204 |     34.905 |   1.0570 |     35.141 |     0.7
   20 |   0.9902 |     33.474 |   1.0359 |     34.069 |     0.7
   21 |   0.9662 |     32.970 |   1.0196 |     34.099 |     0.7
   22 |   0.9420 |     32.401 |   1.0163 |     33.425 |     0.8
   23 |   0.9191 |     31.285 |   0.9793 |     32.874 |     0.8
   24 |   0.8824 |     29.963 |   0.9875 |     32.935 |     0.8
   25 |   0.8618 |     29.172 |   0.9520 |     31.311 |     0.9
   26 |   0.8408 |     28.587 |   0.9447 |     31.464 |     0.9
   27 |   0.8044 |     27.054 |   0.9380 |     30.607 |     0.9
   28 |   0.7952 |     26.810 |   0.9420 |     30.208 |     1.0
   29 |   0.7733 |     26.019 |   0.9193 |     29.504 |     1.0
   30 |   0.7491 |     25.222 |   0.9152 |     28.952 |     1.1
   31 |   0.7248 |     24.247 |   0.9020 |     29.075 |     1.1
   32 |   0.6991 |     23.369 |   0.9171 |     29.596 |     1.1
   33 |   0.6815 |     23.114 |   0.9062 |     29.136 |     1.2
   34 |   0.6618 |     22.134 |   0.8936 |     28.493 |     1.2
   35 |   0.6465 |     21.928 |   0.9065 |     28.493 |     1.2
   36 |   0.6217 |     21.066 |   0.8889 |     28.493 |     1.3
   37 |   0.5994 |     20.432 |   0.8805 |     27.390 |     1.3
   38 |   0.6033 |     20.579 |   0.8916 |     28.125 |     1.3
   39 |   0.5710 |     19.230 |   0.8838 |     27.849 |     1.4
   40 |   0.5529 |     18.823 |   0.8888 |     28.339 |     1.4
   41 |   0.5383 |     18.319 |   0.9122 |     27.359 |     1.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,013,858

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8188 |     72.659 |   2.3287 |     58.824 |     0.0
    2 |   2.1427 |     57.862 |   1.9684 |     53.799 |     0.1
    3 |   1.8482 |     51.365 |   1.7029 |     48.346 |     0.1
    4 |   1.6363 |     47.708 |   1.5540 |     45.466 |     0.2
    5 |   1.5252 |     46.229 |   1.4793 |     45.466 |     0.2
    6 |   1.4725 |     46.337 |   1.4470 |     45.466 |     0.2
    7 |   1.4441 |     46.234 |   1.4271 |     45.466 |     0.3
    8 |   1.4281 |     46.283 |   1.4149 |     45.466 |     0.3
    9 |   1.4197 |     46.283 |   1.4095 |     45.987 |     0.4
   10 |   1.4142 |     46.126 |   1.4034 |     45.466 |     0.4
   11 |   1.4093 |     46.234 |   1.4003 |     44.914 |     0.5
   12 |   1.4023 |     45.980 |   1.3966 |     45.129 |     0.5
   13 |   1.4020 |     45.817 |   1.3911 |     44.485 |     0.5
   14 |   1.3929 |     45.221 |   1.3797 |     44.761 |     0.6
   15 |   1.3806 |     45.183 |   1.3680 |     44.669 |     0.6
   16 |   1.3736 |     45.189 |   1.3755 |     44.700 |     0.7
   17 |   1.3656 |     45.096 |   1.3527 |     44.700 |     0.7
   18 |   1.3577 |     45.004 |   1.3501 |     44.485 |     0.7
   19 |   1.3525 |     45.010 |   1.3396 |     43.689 |     0.8
   20 |   1.3447 |     44.544 |   1.3342 |     43.995 |     0.8
   21 |   1.3374 |     44.132 |   1.3274 |     43.352 |     0.9
   22 |   1.3292 |     44.018 |   1.3194 |     43.168 |     0.9
   23 |   1.3270 |     43.563 |   1.3212 |     42.616 |     0.9
   24 |   1.3199 |     43.460 |   1.3182 |     42.494 |     1.0
   25 |   1.3098 |     43.200 |   1.3069 |     42.647 |     1.0
   26 |   1.3078 |     43.000 |   1.3085 |     42.310 |     1.1
   27 |   1.3061 |     42.826 |   1.3011 |     42.433 |     1.1
   28 |   1.3036 |     42.897 |   1.3063 |     42.310 |     1.2
   29 |   1.2988 |     42.918 |   1.2936 |     42.310 |     1.2
   30 |   1.2921 |     42.756 |   1.2879 |     41.973 |     1.2
   31 |   1.2827 |     42.528 |   1.2906 |     42.341 |     1.3
   32 |   1.2879 |     42.702 |   1.2823 |     42.279 |     1.3
   33 |   1.2796 |     42.436 |   1.2841 |     41.942 |     1.4
   34 |   1.2712 |     42.360 |   1.2852 |     41.820 |     1.4
   35 |   1.2680 |     42.246 |   1.2751 |     41.697 |     1.4
   36 |   1.2611 |     42.311 |   1.2770 |     41.391 |     1.5
   37 |   1.2575 |     41.997 |   1.2616 |     40.962 |     1.5
   38 |   1.2512 |     41.948 |   1.2654 |     41.360 |     1.6
   39 |   1.2557 |     42.019 |   1.2716 |     41.636 |     1.6
   40 |   1.2444 |     41.921 |   1.2573 |     41.759 |     1.7
   41 |   1.2429 |     41.986 |   1.2623 |     41.575 |     1.7
   42 |   1.2434 |     41.954 |   1.2726 |     41.789 |     1.7
   43 |   1.2363 |     41.721 |   1.2604 |     41.452 |     1.8
   44 |   1.2400 |     41.851 |   1.2536 |     41.115 |     1.8
   45 |   1.2351 |     41.824 |   1.2483 |     41.115 |     1.9
   46 |   1.2315 |     41.732 |   1.2544 |     41.207 |     1.9
   47 |   1.2236 |     41.390 |   1.2402 |     40.717 |     1.9
   48 |   1.2197 |     41.190 |   1.2369 |     41.575 |     2.0
   49 |   1.2197 |     41.336 |   1.2424 |     40.839 |     2.0
   50 |   1.2203 |     41.081 |   1.2446 |     41.238 |     2.1
   51 |   1.2133 |     41.022 |   1.2332 |     40.594 |     2.1
   52 |   1.2120 |     40.762 |   1.2366 |     40.533 |     2.2
   53 |   1.2026 |     40.838 |   1.2423 |     40.533 |     2.2
   54 |   1.2028 |     40.789 |   1.2331 |     40.778 |     2.2
   55 |   1.2003 |     40.583 |   1.2371 |     40.870 |     2.3
   56 |   1.2018 |     40.485 |   1.2325 |     41.483 |     2.3
   57 |   1.2132 |     40.908 |   1.2219 |     40.656 |     2.4
   58 |   1.2042 |     40.442 |   1.2226 |     40.502 |     2.4
   59 |   1.1917 |     40.318 |   1.2112 |     39.920 |     2.4
   60 |   1.1756 |     39.900 |   1.2090 |     39.706 |     2.5
   61 |   1.1746 |     39.998 |   1.2105 |     39.951 |     2.5
   62 |   1.1821 |     39.776 |   1.2094 |     39.859 |     2.6
   63 |   1.1688 |     39.402 |   1.2081 |     39.461 |     2.6
   64 |   1.1604 |     39.055 |   1.1995 |     39.277 |     2.6
   65 |   1.1582 |     39.245 |   1.2066 |     39.216 |     2.7
   66 |   1.1557 |     39.001 |   1.1968 |     39.522 |     2.7
   67 |   1.1522 |     38.985 |   1.2222 |     40.839 |     2.8
   68 |   1.1601 |     39.060 |   1.1982 |     40.074 |     2.8
   69 |   1.1522 |     39.033 |   1.2025 |     39.369 |     2.9
   70 |   1.1340 |     38.470 |   1.1906 |     38.971 |     2.9
   71 |   1.1301 |     38.172 |   1.2120 |     39.522 |     2.9
   72 |   1.1318 |     38.047 |   1.1899 |     39.400 |     3.0
   73 |   1.1285 |     38.188 |   1.1944 |     39.093 |     3.0
   74 |   1.1288 |     37.841 |   1.1889 |     39.154 |     3.1
   75 |   1.1334 |     38.080 |   1.1810 |     39.124 |     3.1
   76 |   1.1129 |     37.587 |   1.1814 |     38.909 |     3.1
   77 |   1.1216 |     37.717 |   1.1852 |     39.338 |     3.2
   78 |   1.1147 |     37.446 |   1.1727 |     38.909 |     3.2
   79 |   1.1108 |     37.218 |   1.1835 |     39.369 |     3.3
   80 |   1.0998 |     36.682 |   1.1848 |     39.614 |     3.3
   81 |   1.0954 |     36.763 |   1.1688 |     38.450 |     3.3
   82 |   1.0923 |     36.877 |   1.1782 |     38.297 |     3.4
   83 |   1.0958 |     36.741 |   1.1734 |     38.848 |     3.4
   84 |   1.0961 |     36.834 |   1.1746 |     38.021 |     3.5
   85 |   1.0835 |     36.292 |   1.1626 |     37.745 |     3.5
   86 |   1.0742 |     36.091 |   1.1587 |     38.664 |     3.6
   87 |   1.0845 |     36.156 |   1.1616 |     38.695 |     3.6
   88 |   1.0712 |     35.761 |   1.1673 |     38.817 |     3.6
   89 |   1.0869 |     36.601 |   1.1814 |     39.216 |     3.7
   90 |   1.0882 |     36.660 |   1.1609 |     38.664 |     3.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 300,770

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8068 |     71.749 |   2.4157 |     63.940 |     0.0
    2 |   2.1838 |     58.929 |   1.9509 |     57.353 |     0.0
    3 |   1.8233 |     51.187 |   1.6806 |     48.346 |     0.1
    4 |   1.6050 |     46.760 |   1.5244 |     45.956 |     0.1
    5 |   1.5030 |     46.148 |   1.4650 |     45.404 |     0.1
    6 |   1.4565 |     45.785 |   1.4341 |     44.638 |     0.1
    7 |   1.4306 |     45.622 |   1.4159 |     44.975 |     0.2
    8 |   1.4152 |     45.378 |   1.4024 |     44.700 |     0.2
    9 |   1.4039 |     45.140 |   1.3905 |     44.669 |     0.2
   10 |   1.3903 |     44.934 |   1.3901 |     44.853 |     0.2
   11 |   1.3859 |     44.977 |   1.3731 |     44.363 |     0.3
   12 |   1.3736 |     44.907 |   1.3650 |     44.240 |     0.3
   13 |   1.3605 |     44.945 |   1.3515 |     44.210 |     0.3
   14 |   1.3499 |     44.473 |   1.3393 |     44.026 |     0.3
   15 |   1.3311 |     44.213 |   1.3237 |     43.229 |     0.3
   16 |   1.3168 |     43.487 |   1.3119 |     42.953 |     0.4
   17 |   1.3044 |     43.081 |   1.3003 |     41.605 |     0.4
   18 |   1.2895 |     42.420 |   1.2861 |     41.207 |     0.4
   19 |   1.2718 |     42.030 |   1.2775 |     40.778 |     0.4
   20 |   1.2594 |     41.791 |   1.2684 |     41.452 |     0.5
   21 |   1.2460 |     41.477 |   1.2528 |     40.533 |     0.5
   22 |   1.2328 |     41.130 |   1.2415 |     40.411 |     0.5
   23 |   1.2199 |     40.513 |   1.2221 |     39.522 |     0.5
   24 |   1.2044 |     40.231 |   1.2140 |     39.369 |     0.5
   25 |   1.1908 |     39.721 |   1.2033 |     38.450 |     0.6
   26 |   1.1772 |     39.505 |   1.1996 |     39.185 |     0.6
   27 |   1.1645 |     39.174 |   1.1932 |     38.082 |     0.6
   28 |   1.1486 |     38.708 |   1.1786 |     38.297 |     0.6
   29 |   1.1359 |     37.890 |   1.1689 |     37.347 |     0.7
   30 |   1.1272 |     37.728 |   1.1573 |     37.132 |     0.7
   31 |   1.1110 |     37.321 |   1.1582 |     37.255 |     0.7
   32 |   1.1027 |     36.996 |   1.1464 |     36.029 |     0.7
   33 |   1.0925 |     36.709 |   1.1382 |     36.642 |     0.8
   34 |   1.0859 |     36.313 |   1.1424 |     36.121 |     0.8
   35 |   1.0767 |     35.869 |   1.1162 |     36.183 |     0.8
   36 |   1.0609 |     35.425 |   1.1123 |     35.539 |     0.8
   37 |   1.0453 |     35.148 |   1.1035 |     35.325 |     0.8
   38 |   1.0363 |     34.780 |   1.0990 |     35.325 |     0.9
   39 |   1.0314 |     34.108 |   1.0908 |     35.202 |     0.9
   40 |   1.0198 |     34.141 |   1.0891 |     35.417 |     0.9
   41 |   1.0214 |     34.038 |   1.0736 |     35.018 |     0.9
   42 |   1.0114 |     33.588 |   1.0839 |     35.172 |     1.0
   43 |   0.9957 |     33.322 |   1.0797 |     35.018 |     1.0
   44 |   0.9946 |     33.209 |   1.0721 |     34.896 |     1.0
   45 |   0.9920 |     33.041 |   1.0668 |     35.172 |     1.0
   46 |   0.9739 |     32.347 |   1.0569 |     34.222 |     1.0
   47 |   0.9700 |     32.374 |   1.0658 |     34.436 |     1.1
   48 |   0.9584 |     31.865 |   1.0574 |     34.988 |     1.1
   49 |   0.9554 |     31.892 |   1.0556 |     33.793 |     1.1
   50 |   0.9449 |     31.610 |   1.0437 |     33.701 |     1.1
   51 |   0.9289 |     31.182 |   1.0444 |     33.333 |     1.2
   52 |   0.9258 |     30.873 |   1.0531 |     33.915 |     1.2
   53 |   0.9215 |     30.743 |   1.0474 |     33.854 |     1.2
   54 |   0.9145 |     30.575 |   1.0445 |     33.425 |     1.2
   55 |   0.9015 |     29.898 |   1.0366 |     33.058 |     1.2
   56 |   0.8959 |     29.671 |   1.0387 |     33.456 |     1.3
   57 |   0.8958 |     29.746 |   1.0325 |     32.904 |     1.3
   58 |   0.8891 |     29.546 |   1.0418 |     33.425 |     1.3
   59 |   0.8851 |     29.627 |   1.0278 |     32.904 |     1.3
   60 |   0.8799 |     29.215 |   1.0270 |     33.241 |     1.4
   61 |   0.8717 |     28.912 |   1.0219 |     32.598 |     1.4
   62 |   0.8640 |     29.129 |   1.0208 |     32.537 |     1.4
   63 |   0.8622 |     28.847 |   1.0298 |     33.180 |     1.4
   64 |   0.8548 |     28.684 |   1.0242 |     32.843 |     1.4
   65 |   0.8481 |     28.349 |   1.0289 |     32.690 |     1.5
   66 |   0.8427 |     28.235 |   1.0229 |     32.108 |     1.5
   67 |   0.8611 |     28.982 |   1.0172 |     31.955 |     1.5
   68 |   0.8436 |     28.267 |   1.0251 |     32.843 |     1.5
   69 |   0.8306 |     27.752 |   1.0193 |     32.475 |     1.6
   70 |   0.8241 |     27.140 |   1.0100 |     32.108 |     1.6
   71 |   0.8255 |     27.720 |   1.0211 |     32.200 |     1.6
   72 |   0.8099 |     27.108 |   1.0131 |     32.567 |     1.6
   73 |   0.8106 |     27.270 |   1.0130 |     31.955 |     1.6
   74 |   0.8045 |     26.886 |   1.0100 |     31.771 |     1.7
   75 |   0.8004 |     26.718 |   1.0024 |     31.710 |     1.7
   76 |   0.8060 |     26.907 |   1.0125 |     32.230 |     1.7
   77 |   0.7893 |     26.615 |   1.0139 |     31.893 |     1.7
   78 |   0.7916 |     26.322 |   1.0273 |     32.108 |     1.8
   79 |   0.7881 |     26.463 |   1.0051 |     31.526 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 934,306

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2899 |     62.077 |   1.6870 |     48.346 |     0.0
    2 |   1.5142 |     46.776 |   1.4299 |     45.466 |     0.0
    3 |   1.4199 |     46.359 |   1.4019 |     45.558 |     0.1
    4 |   1.3990 |     46.218 |   1.3812 |     45.466 |     0.1
    5 |   1.3788 |     46.321 |   1.3633 |     44.822 |     0.1
    6 |   1.3629 |     45.822 |   1.3431 |     44.761 |     0.1
    7 |   1.3443 |     45.292 |   1.3209 |     43.015 |     0.1
    8 |   1.3077 |     43.801 |   1.2865 |     42.188 |     0.2
    9 |   1.2805 |     43.222 |   1.2643 |     42.218 |     0.2
   10 |   1.2531 |     42.674 |   1.2546 |     42.004 |     0.2
   11 |   1.2239 |     41.434 |   1.2410 |     41.422 |     0.2
   12 |   1.2008 |     40.534 |   1.2067 |     39.491 |     0.2
   13 |   1.1663 |     39.732 |   1.2031 |     40.288 |     0.3
   14 |   1.1447 |     38.833 |   1.1735 |     38.787 |     0.3
   15 |   1.1159 |     37.933 |   1.1698 |     38.297 |     0.3
   16 |   1.0905 |     37.132 |   1.1548 |     37.745 |     0.3
   17 |   1.0698 |     36.433 |   1.1548 |     38.205 |     0.3
   18 |   1.0428 |     35.544 |   1.1212 |     36.765 |     0.4
   19 |   1.0045 |     34.038 |   1.0834 |     36.550 |     0.4
   20 |   0.9740 |     33.220 |   1.0627 |     35.907 |     0.4
   21 |   0.9480 |     31.849 |   1.0811 |     36.244 |     0.4
   22 |   0.9190 |     30.716 |   1.0359 |     33.915 |     0.4
   23 |   0.8867 |     29.654 |   1.0217 |     32.782 |     0.4
   24 |   0.8530 |     28.771 |   1.0285 |     33.732 |     0.5
   25 |   0.8333 |     27.698 |   0.9976 |     31.526 |     0.5
   26 |   0.8073 |     26.745 |   0.9917 |     32.200 |     0.5
   27 |   0.7884 |     25.834 |   0.9856 |     31.526 |     0.5
   28 |   0.7502 |     24.561 |   0.9824 |     31.434 |     0.5
   29 |   0.7143 |     23.510 |   0.9811 |     31.281 |     0.6
   30 |   0.6882 |     22.410 |   0.9804 |     29.902 |     0.6
   31 |   0.6584 |     21.559 |   0.9673 |     30.178 |     0.6
   32 |   0.6335 |     20.541 |   0.9783 |     30.362 |     0.6
   33 |   0.6082 |     19.826 |   0.9614 |     28.830 |     0.6
   34 |   0.6128 |     19.901 |   0.9507 |     28.830 |     0.7
   35 |   0.5754 |     18.915 |   0.9696 |     29.044 |     0.7
   36 |   0.5450 |     17.680 |   0.9497 |     28.493 |     0.7
   37 |   0.5160 |     16.764 |   0.9717 |     29.320 |     0.7
   38 |   0.4983 |     16.163 |   0.9766 |     29.228 |     0.7
   39 |   0.4863 |     16.006 |   0.9870 |     28.860 |     0.8
   40 |   0.4651 |     15.415 |   0.9692 |     27.574 |     0.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 255,330

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3850 |     84.439 |   3.1004 |     80.974 |     0.0
    2 |   2.7141 |     72.280 |   2.4397 |     59.130 |     0.0
    3 |   2.2986 |     58.555 |   2.1967 |     57.108 |     0.0
    4 |   2.1142 |     56.480 |   2.0502 |     53.585 |     0.1
    5 |   1.9906 |     50.585 |   1.9331 |     49.357 |     0.1
    6 |   1.8793 |     49.019 |   1.8313 |     48.376 |     0.1
    7 |   1.7906 |     48.645 |   1.7461 |     48.162 |     0.1
    8 |   1.7148 |     48.429 |   1.6735 |     48.346 |     0.1
    9 |   1.6471 |     48.369 |   1.6131 |     48.192 |     0.1
   10 |   1.5918 |     48.255 |   1.5651 |     48.070 |     0.1
   11 |   1.5479 |     46.608 |   1.5266 |     45.037 |     0.1
   12 |   1.5137 |     45.590 |   1.4934 |     45.037 |     0.2
   13 |   1.4811 |     45.644 |   1.4675 |     45.037 |     0.2
   14 |   1.4541 |     45.568 |   1.4453 |     44.975 |     0.2
   15 |   1.4309 |     45.595 |   1.4253 |     44.945 |     0.2
   16 |   1.4152 |     45.541 |   1.4088 |     44.975 |     0.2
   17 |   1.3983 |     45.622 |   1.3922 |     45.037 |     0.2
   18 |   1.3806 |     45.248 |   1.3776 |     45.251 |     0.2
   19 |   1.3653 |     44.993 |   1.3666 |     45.619 |     0.3
   20 |   1.3499 |     44.755 |   1.3585 |     45.374 |     0.3
   21 |   1.3393 |     44.408 |   1.3448 |     44.547 |     0.3
   22 |   1.3250 |     43.942 |   1.3362 |     44.056 |     0.3
   23 |   1.3161 |     43.547 |   1.3287 |     42.739 |     0.3
   24 |   1.3055 |     42.924 |   1.3184 |     43.199 |     0.3
   25 |   1.2950 |     42.620 |   1.3105 |     42.678 |     0.3
   26 |   1.2857 |     42.214 |   1.3045 |     42.525 |     0.4
   27 |   1.2764 |     42.084 |   1.2993 |     42.433 |     0.4
   28 |   1.2693 |     41.618 |   1.2912 |     42.034 |     0.4
   29 |   1.2607 |     41.331 |   1.2853 |     41.146 |     0.4
   30 |   1.2529 |     41.255 |   1.2801 |     41.268 |     0.4
   31 |   1.2444 |     40.995 |   1.2734 |     40.717 |     0.4
   32 |   1.2390 |     40.735 |   1.2678 |     40.870 |     0.4
   33 |   1.2320 |     40.372 |   1.2631 |     40.288 |     0.4
   34 |   1.2216 |     40.177 |   1.2592 |     40.717 |     0.5
   35 |   1.2156 |     40.090 |   1.2523 |     39.951 |     0.5
   36 |   1.2051 |     39.667 |   1.2495 |     40.257 |     0.5
   37 |   1.2015 |     39.472 |   1.2458 |     40.411 |     0.5
   38 |   1.1924 |     39.071 |   1.2390 |     39.553 |     0.5
   39 |   1.1826 |     38.849 |   1.2386 |     39.920 |     0.5
   40 |   1.1773 |     38.275 |   1.2271 |     39.553 |     0.5
   41 |   1.1718 |     38.172 |   1.2234 |     39.308 |     0.6
   42 |   1.1634 |     37.782 |   1.2161 |     39.430 |     0.6
   43 |   1.1599 |     37.522 |   1.2111 |     39.032 |     0.6
   44 |   1.1496 |     37.077 |   1.2064 |     38.909 |     0.6
   45 |   1.1426 |     36.752 |   1.2017 |     39.154 |     0.6
   46 |   1.1349 |     36.324 |   1.1961 |     38.542 |     0.6
   47 |   1.1258 |     35.858 |   1.1901 |     38.143 |     0.6
   48 |   1.1219 |     35.566 |   1.1811 |     38.235 |     0.7
   49 |   1.1142 |     35.409 |   1.1791 |     37.960 |     0.7
   50 |   1.1064 |     34.883 |   1.1769 |     37.776 |     0.7
   51 |   1.0966 |     34.905 |   1.1677 |     37.653 |     0.7
   52 |   1.0898 |     34.357 |   1.1628 |     37.500 |     0.7
   53 |   1.0855 |     34.254 |   1.1574 |     37.377 |     0.7
   54 |   1.0732 |     34.021 |   1.1507 |     37.500 |     0.7
   55 |   1.0685 |     33.859 |   1.1425 |     36.857 |     0.7
   56 |   1.0603 |     33.599 |   1.1391 |     36.550 |     0.8
   57 |   1.0528 |     33.290 |   1.1331 |     36.336 |     0.8
   58 |   1.0433 |     33.176 |   1.1243 |     36.366 |     0.8
   59 |   1.0381 |     33.008 |   1.1210 |     36.060 |     0.8
   60 |   1.0256 |     32.867 |   1.1180 |     36.152 |     0.8
   61 |   1.0224 |     32.493 |   1.1118 |     35.999 |     0.8
   62 |   1.0142 |     32.093 |   1.1055 |     35.631 |     0.8
   63 |   1.0103 |     32.385 |   1.1035 |     35.509 |     0.9
   64 |   1.0011 |     31.946 |   1.0945 |     35.325 |     0.9
   65 |   0.9960 |     31.795 |   1.0887 |     34.589 |     0.9
   66 |   0.9843 |     31.334 |   1.0857 |     34.773 |     0.9
   67 |   0.9789 |     31.166 |   1.0807 |     35.172 |     0.9
   68 |   0.9737 |     31.177 |   1.0763 |     34.835 |     0.9
   69 |   0.9655 |     31.014 |   1.0702 |     34.344 |     0.9
   70 |   0.9577 |     30.657 |   1.0672 |     34.375 |     0.9
   71 |   0.9501 |     30.283 |   1.0623 |     34.375 |     1.0
   72 |   0.9450 |     30.050 |   1.0576 |     34.406 |     1.0
   73 |   0.9406 |     30.158 |   1.0579 |     34.007 |     1.0
   74 |   0.9326 |     29.730 |   1.0515 |     33.977 |     1.0
   75 |   0.9249 |     29.481 |   1.0494 |     33.732 |     1.0
   76 |   0.9200 |     29.150 |   1.0419 |     33.885 |     1.0
   77 |   0.9123 |     29.183 |   1.0450 |     33.701 |     1.0
   78 |   0.9082 |     29.069 |   1.0318 |     33.180 |     1.1
   79 |   0.8997 |     28.674 |   1.0301 |     33.333 |     1.1
   80 |   0.8943 |     28.462 |   1.0310 |     32.996 |     1.1
   81 |   0.8905 |     28.479 |   1.0219 |     32.659 |     1.1
   82 |   0.8832 |     27.926 |   1.0214 |     33.150 |     1.1
   83 |   0.8801 |     27.899 |   1.0167 |     32.629 |     1.1
   84 |   0.8739 |     27.742 |   1.0215 |     32.353 |     1.1
   85 |   0.8673 |     27.476 |   1.0118 |     32.261 |     1.2
   86 |   0.8655 |     27.221 |   1.0062 |     32.077 |     1.2
   87 |   0.8564 |     27.156 |   1.0042 |     31.924 |     1.2
   88 |   0.8503 |     26.875 |   0.9990 |     31.955 |     1.2
   89 |   0.8444 |     26.588 |   0.9996 |     31.985 |     1.2
   90 |   0.8419 |     26.550 |   0.9982 |     31.832 |     1.2
   91 |   0.8374 |     26.371 |   0.9933 |     31.710 |     1.2
   92 |   0.8330 |     26.252 |   0.9861 |     31.464 |     1.2
   93 |   0.8283 |     26.149 |   0.9849 |     31.005 |     1.3
   94 |   0.8215 |     25.699 |   1.0016 |     32.169 |     1.3
   95 |   0.8185 |     25.553 |   0.9878 |     31.403 |     1.3
   96 |   0.8099 |     25.287 |   0.9835 |     31.403 |     1.3
   97 |   0.8065 |     25.168 |   0.9754 |     31.127 |     1.3
   98 |   0.8027 |     25.152 |   0.9775 |     31.250 |     1.3
   99 |   0.8015 |     25.000 |   0.9738 |     30.729 |     1.3
  100 |   0.7945 |     24.854 |   0.9661 |     31.066 |     1.4
  101 |   0.7871 |     24.577 |   0.9642 |     30.699 |     1.4
  102 |   0.7824 |     24.556 |   0.9725 |     30.699 |     1.4
  103 |   0.7818 |     24.588 |   0.9641 |     30.453 |     1.4
  104 |   0.7771 |     24.155 |   0.9603 |     30.362 |     1.4
  105 |   0.7722 |     24.361 |   0.9602 |     30.453 |     1.4
  106 |   0.7679 |     24.149 |   0.9554 |     29.994 |     1.4
  107 |   0.7616 |     23.813 |   0.9642 |     30.270 |     1.4
  108 |   0.7607 |     23.667 |   0.9531 |     29.841 |     1.5
  109 |   0.7544 |     23.450 |   0.9495 |     29.534 |     1.5
  110 |   0.7544 |     23.553 |   0.9467 |     29.350 |     1.5
  111 |   0.7471 |     23.234 |   0.9527 |     29.350 |     1.5
  112 |   0.7416 |     23.163 |   0.9488 |     29.504 |     1.5
  113 |   0.7369 |     22.865 |   0.9491 |     29.504 |     1.5
  114 |   0.7312 |     22.762 |   0.9454 |     29.504 |     1.5
  115 |   0.7313 |     22.627 |   0.9479 |     29.320 |     1.6
  116 |   0.7290 |     22.459 |   0.9394 |     29.044 |     1.6
  117 |   0.7246 |     22.291 |   0.9421 |     28.922 |     1.6
  118 |   0.7205 |     22.285 |   0.9362 |     28.830 |     1.6
  119 |   0.7119 |     22.080 |   0.9471 |     28.983 |     1.6
  120 |   0.7131 |     22.020 |   0.9340 |     28.922 |     1.6
  121 |   0.7064 |     22.107 |   0.9359 |     28.738 |     1.6
  122 |   0.7062 |     21.895 |   0.9324 |     28.922 |     1.6
  123 |   0.6989 |     21.771 |   0.9324 |     28.768 |     1.7
  124 |   0.6975 |     21.538 |   0.9259 |     28.431 |     1.7
  125 |   0.6938 |     21.424 |   0.9339 |     28.431 |     1.7
  126 |   0.6906 |     21.343 |   0.9228 |     28.002 |     1.7
  127 |   0.6903 |     21.532 |   0.9285 |     28.217 |     1.7
  128 |   0.6829 |     21.110 |   0.9204 |     28.094 |     1.7
  129 |   0.6788 |     21.050 |   0.9313 |     28.248 |     1.7
  130 |   0.6752 |     20.882 |   0.9280 |     28.431 |     1.8
  131 |   0.6781 |     20.703 |   0.9218 |     27.696 |     1.8
  132 |   0.6673 |     20.199 |   0.9273 |     27.880 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,163,938

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1104 |     81.101 |   2.5175 |     62.898 |     0.0
    2 |   2.2571 |     58.100 |   2.0795 |     55.270 |     0.1
    3 |   1.9809 |     53.002 |   1.8746 |     48.346 |     0.1
    4 |   1.8127 |     48.591 |   1.7370 |     48.070 |     0.1
    5 |   1.6914 |     48.385 |   1.6300 |     48.039 |     0.2
    6 |   1.5961 |     48.266 |   1.5506 |     47.947 |     0.2
    7 |   1.5273 |     47.470 |   1.4901 |     45.037 |     0.3
    8 |   1.4735 |     45.790 |   1.4447 |     44.914 |     0.3
    9 |   1.4369 |     45.514 |   1.4118 |     44.945 |     0.3
   10 |   1.3988 |     45.048 |   1.3822 |     44.730 |     0.4
   11 |   1.3682 |     44.506 |   1.3566 |     43.964 |     0.4
   12 |   1.3442 |     43.617 |   1.3339 |     43.168 |     0.4
   13 |   1.3170 |     42.599 |   1.3139 |     42.371 |     0.5
   14 |   1.2970 |     41.900 |   1.2931 |     40.717 |     0.5
   15 |   1.2795 |     40.588 |   1.2801 |     39.767 |     0.6
   16 |   1.2554 |     40.036 |   1.2643 |     39.951 |     0.6
   17 |   1.2380 |     39.391 |   1.2454 |     38.756 |     0.6
   18 |   1.2188 |     38.622 |   1.2300 |     38.327 |     0.7
   19 |   1.2008 |     38.172 |   1.2196 |     37.929 |     0.7
   20 |   1.1805 |     37.646 |   1.1975 |     37.316 |     0.7
   21 |   1.1624 |     36.844 |   1.1875 |     37.377 |     0.8
   22 |   1.1431 |     36.281 |   1.1757 |     37.255 |     0.8
   23 |   1.1250 |     35.696 |   1.1588 |     36.642 |     0.9
   24 |   1.1084 |     35.409 |   1.1525 |     36.366 |     0.9
   25 |   1.0899 |     34.666 |   1.1326 |     35.938 |     0.9
   26 |   1.0755 |     34.162 |   1.1209 |     35.539 |     1.0
   27 |   1.0580 |     33.783 |   1.1073 |     35.202 |     1.0
   28 |   1.0384 |     33.052 |   1.0944 |     35.110 |     1.0
   29 |   1.0273 |     32.786 |   1.0880 |     34.528 |     1.1
   30 |   1.0081 |     32.044 |   1.0762 |     34.161 |     1.1
   31 |   0.9946 |     31.491 |   1.0639 |     34.467 |     1.1
   32 |   0.9797 |     31.236 |   1.0534 |     33.793 |     1.2
   33 |   0.9624 |     30.478 |   1.0482 |     33.211 |     1.2
   34 |   0.9537 |     30.061 |   1.0375 |     33.027 |     1.3
   35 |   0.9376 |     29.237 |   1.0274 |     32.874 |     1.3
   36 |   0.9273 |     29.010 |   1.0189 |     32.843 |     1.3
   37 |   0.9122 |     28.479 |   1.0087 |     32.138 |     1.4
   38 |   0.8959 |     27.920 |   1.0034 |     32.261 |     1.4
   39 |   0.8796 |     27.541 |   0.9930 |     31.495 |     1.4
   40 |   0.8673 |     26.923 |   0.9892 |     31.434 |     1.5
   41 |   0.8538 |     26.414 |   0.9813 |     31.127 |     1.5
   42 |   0.8424 |     26.116 |   0.9715 |     30.882 |     1.6
   43 |   0.8257 |     25.352 |   0.9639 |     30.637 |     1.6
   44 |   0.8146 |     25.217 |   0.9627 |     30.668 |     1.6
   45 |   0.8026 |     24.702 |   0.9558 |     30.607 |     1.7
   46 |   0.7927 |     24.453 |   0.9502 |     29.749 |     1.7
   47 |   0.7788 |     24.041 |   0.9420 |     29.228 |     1.7
   48 |   0.7695 |     23.450 |   0.9425 |     29.442 |     1.8
   49 |   0.7576 |     22.903 |   0.9372 |     29.350 |     1.8
   50 |   0.7428 |     22.621 |   0.9274 |     28.493 |     1.8
   51 |   0.7305 |     22.264 |   0.9263 |     29.167 |     1.9
   52 |   0.7166 |     21.738 |   0.9303 |     28.738 |     1.9
   53 |   0.7099 |     21.917 |   0.9246 |     28.830 |     2.0
   54 |   0.6991 |     21.278 |   0.9222 |     28.554 |     2.0
   55 |   0.6892 |     20.925 |   0.9137 |     28.707 |     2.0
   56 |   0.6766 |     20.508 |   0.9159 |     28.615 |     2.1
   57 |   0.6713 |     20.449 |   0.9152 |     28.156 |     2.1
   58 |   0.6603 |     20.075 |   0.9089 |     28.278 |     2.1
   59 |   0.6563 |     19.853 |   0.9064 |     28.401 |     2.2
   60 |   0.6379 |     19.121 |   0.8992 |     27.390 |     2.2
   61 |   0.6326 |     19.143 |   0.9013 |     27.757 |     2.3
   62 |   0.6239 |     19.007 |   0.9008 |     27.757 |     2.3
   63 |   0.6112 |     18.552 |   0.8938 |     27.543 |     2.3
   64 |   0.6063 |     18.428 |   0.8965 |     27.206 |     2.4
   65 |   0.6003 |     18.130 |   0.8909 |     27.053 |     2.4
   66 |   0.5900 |     17.962 |   0.8943 |     27.145 |     2.4
   67 |   0.5825 |     17.707 |   0.8983 |     27.298 |     2.5
   68 |   0.5728 |     17.257 |   0.9012 |     26.777 |     2.5
   69 |   0.5650 |     17.176 |   0.8921 |     26.961 |     2.5
   70 |   0.5603 |     16.992 |   0.8801 |     26.869 |     2.6
   71 |   0.5498 |     16.596 |   0.8892 |     26.593 |     2.6
   72 |   0.5416 |     16.201 |   0.8911 |     26.042 |     2.7
   73 |   0.5361 |     16.379 |   0.8910 |     26.532 |     2.7
   74 |   0.5299 |     16.147 |   0.8823 |     26.134 |     2.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 296,258

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4669 |     89.494 |   3.3537 |     84.222 |     0.0
    2 |   3.1274 |     84.693 |   2.8940 |     86.366 |     0.0
    3 |   2.7674 |     72.984 |   2.6787 |     70.251 |     0.1
    4 |   2.6145 |     60.268 |   2.5615 |     59.865 |     0.1
    5 |   2.5195 |     61.747 |   2.4763 |     62.377 |     0.1
    6 |   2.4401 |     62.300 |   2.4018 |     59.712 |     0.1
    7 |   2.3740 |     60.799 |   2.3398 |     58.915 |     0.2
    8 |   2.3145 |     59.439 |   2.2820 |     58.824 |     0.2
    9 |   2.2602 |     58.881 |   2.2259 |     58.824 |     0.2
   10 |   2.2083 |     58.583 |   2.1724 |     58.701 |     0.2
   11 |   2.1575 |     58.225 |   2.1202 |     58.701 |     0.3
   12 |   2.1094 |     58.230 |   2.0720 |     58.701 |     0.3
   13 |   2.0616 |     57.504 |   2.0257 |     57.077 |     0.3
   14 |   2.0185 |     54.763 |   1.9824 |     52.757 |     0.3
   15 |   1.9766 |     52.498 |   1.9401 |     52.175 |     0.3
   16 |   1.9373 |     51.653 |   1.8991 |     52.022 |     0.4
   17 |   1.8962 |     51.235 |   1.8591 |     50.582 |     0.4
   18 |   1.8605 |     50.824 |   1.8218 |     50.061 |     0.4
   19 |   1.8235 |     50.509 |   1.7861 |     49.449 |     0.4
   20 |   1.7866 |     50.309 |   1.7522 |     48.866 |     0.5
   21 |   1.7546 |     49.669 |   1.7180 |     48.407 |     0.5
   22 |   1.7197 |     49.496 |   1.6856 |     48.192 |     0.5
   23 |   1.6894 |     49.144 |   1.6575 |     48.131 |     0.5
   24 |   1.6606 |     48.244 |   1.6327 |     45.619 |     0.5
   25 |   1.6374 |     47.047 |   1.6102 |     45.067 |     0.6
   26 |   1.6165 |     46.500 |   1.5909 |     45.129 |     0.6
   27 |   1.5975 |     46.310 |   1.5734 |     45.129 |     0.6
   28 |   1.5813 |     46.245 |   1.5574 |     45.098 |     0.6
   29 |   1.5654 |     46.121 |   1.5429 |     45.221 |     0.7
   30 |   1.5494 |     46.012 |   1.5305 |     45.282 |     0.7
   31 |   1.5369 |     46.001 |   1.5181 |     45.312 |     0.7
   32 |   1.5250 |     45.996 |   1.5048 |     45.190 |     0.7
   33 |   1.5088 |     46.012 |   1.4942 |     45.190 |     0.8
   34 |   1.4985 |     45.860 |   1.4836 |     45.251 |     0.8
   35 |   1.4888 |     45.963 |   1.4748 |     45.251 |     0.8
   36 |   1.4745 |     45.768 |   1.4645 |     45.098 |     0.8
   37 |   1.4679 |     45.703 |   1.4567 |     45.251 |     0.8
   38 |   1.4611 |     45.893 |   1.4495 |     45.067 |     0.9
   39 |   1.4503 |     45.741 |   1.4426 |     45.251 |     0.9
   40 |   1.4427 |     45.698 |   1.4350 |     44.975 |     0.9
   41 |   1.4341 |     45.627 |   1.4287 |     45.190 |     0.9
   42 |   1.4269 |     45.633 |   1.4225 |     45.067 |     1.0
   43 |   1.4201 |     45.692 |   1.4153 |     44.884 |     1.0
   44 |   1.4144 |     45.535 |   1.4120 |     45.190 |     1.0
   45 |   1.4067 |     45.519 |   1.4048 |     45.098 |     1.0
   46 |   1.4004 |     45.438 |   1.4009 |     45.190 |     1.0
   47 |   1.3950 |     45.524 |   1.3941 |     45.098 |     1.1
   48 |   1.3895 |     45.600 |   1.3893 |     44.914 |     1.1
   49 |   1.3831 |     45.259 |   1.3837 |     44.792 |     1.1
   50 |   1.3790 |     45.216 |   1.3801 |     44.792 |     1.1
   51 |   1.3697 |     45.210 |   1.3757 |     44.669 |     1.2
   52 |   1.3647 |     44.988 |   1.3701 |     44.761 |     1.2
   53 |   1.3575 |     44.853 |   1.3660 |     44.669 |     1.2
   54 |   1.3523 |     44.630 |   1.3602 |     44.669 |     1.2
   55 |   1.3514 |     44.322 |   1.3568 |     44.516 |     1.3
   56 |   1.3429 |     44.289 |   1.3510 |     44.608 |     1.3
   57 |   1.3393 |     44.116 |   1.3481 |     44.516 |     1.3
   58 |   1.3321 |     43.991 |   1.3450 |     44.301 |     1.3
   59 |   1.3290 |     43.877 |   1.3402 |     44.118 |     1.3
   60 |   1.3233 |     43.818 |   1.3365 |     43.964 |     1.4
   61 |   1.3174 |     43.601 |   1.3317 |     43.597 |     1.4
   62 |   1.3130 |     43.260 |   1.3272 |     43.658 |     1.4
   63 |   1.3073 |     43.059 |   1.3240 |     43.658 |     1.4
   64 |   1.3032 |     43.027 |   1.3209 |     43.444 |     1.5
   65 |   1.2955 |     42.897 |   1.3173 |     43.352 |     1.5
   66 |   1.2923 |     42.501 |   1.3128 |     43.536 |     1.5
   67 |   1.2857 |     42.376 |   1.3083 |     42.739 |     1.5
   68 |   1.2823 |     42.252 |   1.3067 |     42.678 |     1.5
   69 |   1.2750 |     41.759 |   1.3031 |     43.199 |     1.6
   70 |   1.2719 |     41.883 |   1.2976 |     41.973 |     1.6
   71 |   1.2671 |     41.629 |   1.2934 |     42.126 |     1.6
   72 |   1.2621 |     41.547 |   1.2914 |     41.881 |     1.6
   73 |   1.2577 |     41.277 |   1.2860 |     41.422 |     1.7
   74 |   1.2488 |     40.881 |   1.2817 |     41.636 |     1.7
   75 |   1.2472 |     40.870 |   1.2810 |     41.728 |     1.7
   76 |   1.2421 |     40.686 |   1.2774 |     41.667 |     1.7
   77 |   1.2399 |     40.708 |   1.2734 |     41.575 |     1.7
   78 |   1.2320 |     40.334 |   1.2686 |     41.268 |     1.8
   79 |   1.2275 |     40.144 |   1.2624 |     41.360 |     1.8
   80 |   1.2185 |     40.036 |   1.2609 |     40.993 |     1.8
   81 |   1.2178 |     39.792 |   1.2572 |     40.870 |     1.8
   82 |   1.2117 |     39.754 |   1.2541 |     40.870 |     1.9
   83 |   1.2086 |     39.234 |   1.2516 |     40.074 |     1.9
   84 |   1.2006 |     38.920 |   1.2464 |     40.625 |     1.9
   85 |   1.1954 |     38.730 |   1.2445 |     40.227 |     1.9
   86 |   1.1910 |     38.665 |   1.2391 |     39.982 |     2.0
   87 |   1.1896 |     38.546 |   1.2372 |     39.859 |     2.0
   88 |   1.1818 |     38.362 |   1.2333 |     39.767 |     2.0
   89 |   1.1775 |     38.042 |   1.2291 |     39.737 |     2.0
   90 |   1.1723 |     38.064 |   1.2274 |     39.706 |     2.0
   91 |   1.1692 |     37.625 |   1.2210 |     39.277 |     2.1
   92 |   1.1603 |     37.467 |   1.2190 |     39.491 |     2.1
   93 |   1.1538 |     37.419 |   1.2166 |     39.216 |     2.1
   94 |   1.1522 |     37.305 |   1.2099 |     38.817 |     2.1
   95 |   1.1496 |     37.094 |   1.2093 |     39.032 |     2.2
   96 |   1.1453 |     36.974 |   1.2059 |     38.787 |     2.2
   97 |   1.1361 |     36.628 |   1.2015 |     38.971 |     2.2
   98 |   1.1327 |     36.546 |   1.1988 |     38.634 |     2.2
   99 |   1.1309 |     36.216 |   1.1961 |     38.725 |     2.2
  100 |   1.1240 |     36.048 |   1.1921 |     38.940 |     2.3
  101 |   1.1238 |     36.227 |   1.1894 |     38.205 |     2.3
  102 |   1.1151 |     35.685 |   1.1862 |     37.960 |     2.3
  103 |   1.1094 |     35.398 |   1.1824 |     37.714 |     2.3
  104 |   1.1037 |     35.490 |   1.1822 |     38.021 |     2.4
  105 |   1.1027 |     35.457 |   1.1797 |     38.021 |     2.4
  106 |   1.0971 |     35.111 |   1.1752 |     37.439 |     2.4
  107 |   1.0975 |     34.980 |   1.1709 |     37.408 |     2.4
  108 |   1.0869 |     34.352 |   1.1669 |     37.316 |     2.4
  109 |   1.0805 |     34.477 |   1.1678 |     36.826 |     2.5
  110 |   1.0774 |     34.417 |   1.1642 |     36.703 |     2.5
  111 |   1.0761 |     34.292 |   1.1615 |     36.489 |     2.5
  112 |   1.0707 |     34.168 |   1.1610 |     36.275 |     2.5
  113 |   1.0667 |     33.853 |   1.1584 |     36.612 |     2.6
  114 |   1.0620 |     33.528 |   1.1540 |     36.336 |     2.6
  115 |   1.0585 |     33.545 |   1.1514 |     35.907 |     2.6
  116 |   1.0514 |     32.987 |   1.1520 |     36.550 |     2.6
  117 |   1.0486 |     33.382 |   1.1433 |     35.447 |     2.6
  118 |   1.0452 |     33.182 |   1.1443 |     36.244 |     2.7
  119 |   1.0402 |     32.607 |   1.1404 |     35.447 |     2.7
  120 |   1.0351 |     32.802 |   1.1415 |     35.570 |     2.7
  121 |   1.0347 |     32.824 |   1.1406 |     35.662 |     2.7
  122 |   1.0291 |     32.542 |   1.1390 |     35.417 |     2.8
  123 |   1.0250 |     32.542 |   1.1322 |     35.202 |     2.8
  124 |   1.0216 |     32.531 |   1.1334 |     35.386 |     2.8
  125 |   1.0187 |     32.353 |   1.1271 |     35.172 |     2.8
  126 |   1.0111 |     32.065 |   1.1257 |     35.355 |     2.8
  127 |   1.0086 |     31.876 |   1.1247 |     35.386 |     2.9
  128 |   1.0053 |     31.681 |   1.1237 |     35.202 |     2.9
  129 |   0.9992 |     31.659 |   1.1238 |     35.539 |     2.9
  130 |   1.0014 |     31.518 |   1.1182 |     35.202 |     2.9
  131 |   0.9968 |     31.366 |   1.1180 |     35.080 |     3.0
  132 |   0.9901 |     31.675 |   1.1153 |     35.325 |     3.0
  133 |   0.9915 |     31.312 |   1.1151 |     35.447 |     3.0
  134 |   0.9844 |     31.090 |   1.1092 |     34.865 |     3.0
  135 |   0.9792 |     31.117 |   1.1093 |     35.263 |     3.0
  136 |   0.9772 |     31.058 |   1.1077 |     35.018 |     3.1
  137 |   0.9768 |     31.025 |   1.1069 |     35.080 |     3.1
  138 |   0.9746 |     30.792 |   1.1039 |     34.773 |     3.1
  139 |   0.9707 |     30.716 |   1.1038 |     34.865 |     3.1
  140 |   0.9590 |     30.256 |   1.1049 |     34.804 |     3.2
  141 |   0.9582 |     30.478 |   1.0998 |     35.141 |     3.2
  142 |   0.9559 |     30.104 |   1.1029 |     34.712 |     3.2
  143 |   0.9525 |     30.164 |   1.0954 |     34.712 |     3.2
  144 |   0.9517 |     30.196 |   1.0934 |     34.620 |     3.2
  145 |   0.9435 |     29.855 |   1.0966 |     34.528 |     3.3
  146 |   0.9402 |     29.708 |   1.0910 |     34.559 |     3.3
  147 |   0.9392 |     29.801 |   1.0908 |     34.589 |     3.3
  148 |   0.9374 |     29.779 |   1.0927 |     34.528 |     3.3
  149 |   0.9353 |     29.638 |   1.0877 |     34.283 |     3.4
  150 |   0.9305 |     29.492 |   1.0865 |     34.161 |     3.4
  151 |   0.9255 |     29.053 |   1.0831 |     33.824 |     3.4
  152 |   0.9237 |     29.129 |   1.0818 |     33.854 |     3.4
  153 |   0.9151 |     28.842 |   1.0814 |     34.130 |     3.4
  154 |   0.9165 |     28.939 |   1.0841 |     33.824 |     3.5
  155 |   0.9159 |     28.896 |   1.0814 |     34.038 |     3.5
  156 |   0.9093 |     28.581 |   1.0748 |     33.609 |     3.5
  157 |   0.9106 |     28.695 |   1.0759 |     33.364 |     3.5
  158 |   0.9038 |     28.376 |   1.0754 |     33.548 |     3.6
  159 |   0.8968 |     28.327 |   1.0802 |     34.069 |     3.6
  160 |   0.8959 |     28.094 |   1.0797 |     33.824 |     3.6
  161 |   0.8911 |     27.899 |   1.0735 |     33.303 |     3.6
  162 |   0.8982 |     28.099 |   1.0696 |     33.395 |     3.6
  163 |   0.8923 |     28.186 |   1.0726 |     33.241 |     3.7
  164 |   0.8872 |     27.785 |   1.0687 |     32.782 |     3.7
  165 |   0.8804 |     27.471 |   1.0645 |     32.904 |     3.7
  166 |   0.8807 |     27.454 |   1.0678 |     33.578 |     3.7
  167 |   0.8767 |     27.492 |   1.0648 |     32.721 |     3.8
  168 |   0.8763 |     27.487 |   1.0634 |     32.874 |     3.8
  169 |   0.8708 |     27.297 |   1.0630 |     32.567 |     3.8
  170 |   0.8682 |     27.276 |   1.0647 |     32.874 |     3.8
  171 |   0.8649 |     27.200 |   1.0569 |     32.567 |     3.8
  172 |   0.8590 |     27.026 |   1.0586 |     32.966 |     3.9
  173 |   0.8643 |     27.119 |   1.0612 |     32.414 |     3.9
  174 |   0.8599 |     26.989 |   1.0561 |     32.690 |     3.9
  175 |   0.8531 |     26.999 |   1.0591 |     32.721 |     3.9
  176 |   0.8503 |     26.723 |   1.0567 |     32.721 |     4.0
  177 |   0.8494 |     26.701 |   1.0501 |     32.414 |     4.0
  178 |   0.8437 |     26.436 |   1.0550 |     32.353 |     4.0
  179 |   0.8463 |     26.295 |   1.0559 |     32.721 |     4.0
  180 |   0.8365 |     26.273 |   1.0505 |     32.016 |     4.0
  181 |   0.8361 |     26.138 |   1.0481 |     32.200 |     4.1
  182 |   0.8360 |     26.246 |   1.0504 |     32.353 |     4.1
  183 |   0.8310 |     26.122 |   1.0494 |     32.230 |     4.1
  184 |   0.8319 |     26.290 |   1.0500 |     32.414 |     4.1
  185 |   0.8318 |     26.084 |   1.0466 |     32.414 |     4.2
  186 |   0.8275 |     26.057 |   1.0500 |     32.047 |     4.2
  187 |   0.8250 |     25.753 |   1.0455 |     32.384 |     4.2
  188 |   0.8215 |     25.276 |   1.0450 |     31.955 |     4.2
  189 |   0.8181 |     25.845 |   1.0411 |     32.292 |     4.2
  190 |   0.8135 |     25.591 |   1.0392 |     31.863 |     4.3
  191 |   0.8156 |     25.807 |   1.0413 |     32.353 |     4.3
  192 |   0.8091 |     25.173 |   1.0388 |     31.924 |     4.3
  193 |   0.8094 |     25.368 |   1.0413 |     32.138 |     4.3
  194 |   0.8049 |     25.125 |   1.0390 |     31.924 |     4.4
  195 |   0.8033 |     25.163 |   1.0393 |     31.587 |     4.4
  196 |   0.7990 |     24.848 |   1.0399 |     31.985 |     4.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 398,754

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5053 |     99.978 |   3.4637 |     87.531 |     0.0
    2 |   3.2980 |     83.393 |   3.0788 |     83.333 |     0.0
    3 |   2.9230 |     83.333 |   2.8152 |     83.333 |     0.0
    4 |   2.7136 |     75.114 |   2.6340 |     67.341 |     0.1
    5 |   2.5400 |     65.534 |   2.4762 |     58.824 |     0.1
    6 |   2.4008 |     59.049 |   2.3585 |     58.824 |     0.1
    7 |   2.3028 |     59.049 |   2.2780 |     58.824 |     0.1
    8 |   2.2330 |     59.049 |   2.2185 |     58.824 |     0.1
    9 |   2.1816 |     59.049 |   2.1692 |     58.824 |     0.1
   10 |   2.1359 |     59.049 |   2.1255 |     58.824 |     0.2
   11 |   2.0939 |     59.049 |   2.0835 |     58.824 |     0.2
   12 |   2.0540 |     58.490 |   2.0420 |     58.150 |     0.2
   13 |   2.0143 |     55.229 |   2.0025 |     53.799 |     0.2
   14 |   1.9781 |     54.010 |   1.9647 |     53.768 |     0.2
   15 |   1.9404 |     53.181 |   1.9282 |     50.490 |     0.2
   16 |   1.9072 |     48.803 |   1.8940 |     48.346 |     0.3
   17 |   1.8748 |     48.607 |   1.8629 |     48.376 |     0.3
   18 |   1.8472 |     48.618 |   1.8339 |     48.346 |     0.3
   19 |   1.8233 |     48.607 |   1.8062 |     48.346 |     0.3
   20 |   1.7930 |     48.613 |   1.7795 |     48.376 |     0.3
   21 |   1.7667 |     48.607 |   1.7537 |     48.376 |     0.3
   22 |   1.7437 |     48.613 |   1.7290 |     48.346 |     0.4
   23 |   1.7186 |     48.607 |   1.7054 |     48.346 |     0.4
   24 |   1.6987 |     48.607 |   1.6838 |     48.346 |     0.4
   25 |   1.6763 |     48.607 |   1.6636 |     48.346 |     0.4
   26 |   1.6605 |     48.607 |   1.6454 |     48.346 |     0.4
   27 |   1.6401 |     48.607 |   1.6287 |     48.162 |     0.4
   28 |   1.6241 |     46.240 |   1.6133 |     45.466 |     0.5
   29 |   1.6095 |     46.213 |   1.5988 |     45.466 |     0.5
   30 |   1.5952 |     46.202 |   1.5853 |     45.466 |     0.5
   31 |   1.5820 |     46.213 |   1.5729 |     45.466 |     0.5
   32 |   1.5729 |     46.213 |   1.5617 |     45.466 |     0.5
   33 |   1.5618 |     46.213 |   1.5510 |     45.466 |     0.5
   34 |   1.5483 |     46.213 |   1.5408 |     45.466 |     0.6
   35 |   1.5393 |     46.213 |   1.5307 |     45.466 |     0.6
   36 |   1.5288 |     46.207 |   1.5209 |     45.466 |     0.6
   37 |   1.5189 |     46.218 |   1.5115 |     45.466 |     0.6
   38 |   1.5110 |     46.207 |   1.5021 |     45.466 |     0.6
   39 |   1.5010 |     46.202 |   1.4932 |     45.466 |     0.6
   40 |   1.4914 |     46.213 |   1.4843 |     45.466 |     0.7
   41 |   1.4810 |     46.213 |   1.4751 |     45.466 |     0.7
   42 |   1.4739 |     46.207 |   1.4671 |     45.466 |     0.7
   43 |   1.4623 |     46.213 |   1.4575 |     45.466 |     0.7
   44 |   1.4519 |     46.207 |   1.4495 |     45.466 |     0.7
   45 |   1.4436 |     46.218 |   1.4416 |     45.466 |     0.7
   46 |   1.4335 |     46.213 |   1.4322 |     45.466 |     0.8
   47 |   1.4248 |     46.207 |   1.4246 |     45.466 |     0.8
   48 |   1.4172 |     46.213 |   1.4163 |     45.435 |     0.8
   49 |   1.4050 |     46.066 |   1.4104 |     45.435 |     0.8
   50 |   1.3980 |     45.557 |   1.4049 |     44.638 |     0.8
   51 |   1.3917 |     45.134 |   1.4025 |     44.118 |     0.8
   52 |   1.3838 |     44.912 |   1.3913 |     44.301 |     0.9
   53 |   1.3763 |     44.777 |   1.3866 |     44.056 |     0.9
   54 |   1.3684 |     44.690 |   1.3813 |     44.148 |     0.9
   55 |   1.3609 |     44.668 |   1.3771 |     44.240 |     0.9
   56 |   1.3552 |     44.484 |   1.3750 |     44.148 |     0.9
   57 |   1.3502 |     44.392 |   1.3666 |     44.148 |     0.9
   58 |   1.3427 |     44.235 |   1.3634 |     44.056 |     1.0
   59 |   1.3354 |     44.099 |   1.3621 |     43.750 |     1.0
   60 |   1.3321 |     43.823 |   1.3576 |     43.658 |     1.0
   61 |   1.3265 |     43.845 |   1.3514 |     43.781 |     1.0
   62 |   1.3177 |     43.509 |   1.3482 |     43.566 |     1.0
   63 |   1.3119 |     43.444 |   1.3441 |     43.750 |     1.0
   64 |   1.3062 |     43.135 |   1.3459 |     43.689 |     1.1
   65 |   1.3025 |     43.065 |   1.3361 |     43.352 |     1.1
   66 |   1.2924 |     42.913 |   1.3352 |     43.536 |     1.1
   67 |   1.2886 |     42.783 |   1.3292 |     43.382 |     1.1
   68 |   1.2871 |     42.870 |   1.3294 |     43.260 |     1.1
   69 |   1.2801 |     42.572 |   1.3233 |     43.229 |     1.1
   70 |   1.2683 |     42.485 |   1.3197 |     43.290 |     1.1
   71 |   1.2643 |     42.230 |   1.3178 |     43.168 |     1.2
   72 |   1.2575 |     42.133 |   1.3119 |     43.137 |     1.2
   73 |   1.2544 |     42.062 |   1.3160 |     42.923 |     1.2
   74 |   1.2486 |     41.948 |   1.3114 |     43.321 |     1.2
   75 |   1.2403 |     41.802 |   1.3066 |     43.536 |     1.2
   76 |   1.2325 |     41.656 |   1.3079 |     43.321 |     1.2
   77 |   1.2291 |     41.504 |   1.3005 |     43.290 |     1.3
   78 |   1.2233 |     41.374 |   1.3027 |     43.382 |     1.3
   79 |   1.2202 |     41.287 |   1.2973 |     42.862 |     1.3
   80 |   1.2157 |     41.163 |   1.2957 |     42.984 |     1.3
   81 |   1.2106 |     41.000 |   1.2946 |     43.015 |     1.3
   82 |   1.2041 |     40.924 |   1.2927 |     42.984 |     1.4
   83 |   1.1995 |     40.464 |   1.2951 |     42.678 |     1.4
   84 |   1.1932 |     40.182 |   1.2917 |     43.045 |     1.4
   85 |   1.1890 |     40.052 |   1.2891 |     43.015 |     1.4
   86 |   1.1875 |     39.803 |   1.2867 |     42.678 |     1.4
   87 |   1.1803 |     39.461 |   1.2871 |     42.831 |     1.4
   88 |   1.1760 |     39.310 |   1.2890 |     42.586 |     1.4
   89 |   1.1712 |     39.001 |   1.2898 |     42.586 |     1.5
   90 |   1.1689 |     39.006 |   1.2817 |     42.739 |     1.5
   91 |   1.1637 |     38.800 |   1.2849 |     42.371 |     1.5
   92 |   1.1591 |     38.551 |   1.2857 |     42.463 |     1.5
   93 |   1.1526 |     38.405 |   1.2871 |     42.525 |     1.5
   94 |   1.1471 |     38.307 |   1.2844 |     42.034 |     1.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 282,466

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4857 |     66.683 |   1.9085 |     52.359 |     0.0
    2 |   1.6725 |     48.169 |   1.5064 |     45.343 |     0.0
    3 |   1.4498 |     45.541 |   1.4042 |     45.159 |     0.0
    4 |   1.3733 |     44.652 |   1.3416 |     43.597 |     0.1
    5 |   1.3104 |     42.761 |   1.2977 |     42.188 |     0.1
    6 |   1.2545 |     40.908 |   1.2424 |     39.798 |     0.1
    7 |   1.2042 |     39.586 |   1.2216 |     39.645 |     0.1
    8 |   1.1666 |     38.546 |   1.1719 |     37.898 |     0.1
    9 |   1.1234 |     37.007 |   1.1458 |     37.561 |     0.1
   10 |   1.0842 |     35.826 |   1.1141 |     36.581 |     0.2
   11 |   1.0461 |     34.422 |   1.0955 |     35.018 |     0.2
   12 |   1.0078 |     32.694 |   1.0677 |     34.007 |     0.2
   13 |   0.9654 |     30.559 |   1.0412 |     34.069 |     0.2
   14 |   0.9324 |     29.410 |   1.0282 |     33.333 |     0.2
   15 |   0.8910 |     27.883 |   0.9971 |     32.261 |     0.2
   16 |   0.8645 |     27.124 |   0.9997 |     31.801 |     0.3
   17 |   0.8283 |     26.008 |   0.9842 |     31.434 |     0.3
   18 |   0.8007 |     24.978 |   0.9505 |     30.055 |     0.3
   19 |   0.7628 |     23.976 |   0.9523 |     30.545 |     0.3
   20 |   0.7331 |     22.892 |   0.9468 |     29.779 |     0.3
   21 |   0.7082 |     22.226 |   0.9399 |     29.228 |     0.3
   22 |   0.6811 |     21.381 |   0.9501 |     29.105 |     0.4
   23 |   0.6554 |     20.590 |   0.9412 |     29.167 |     0.4
   24 |   0.6362 |     19.983 |   0.9322 |     28.431 |     0.4
   25 |   0.6165 |     19.262 |   0.9195 |     27.880 |     0.4
   26 |   0.5962 |     18.818 |   0.9363 |     28.768 |     0.4
   27 |   0.5628 |     17.637 |   0.9228 |     27.543 |     0.4
   28 |   0.5520 |     17.089 |   0.9159 |     27.482 |     0.5
   29 |   0.5362 |     16.786 |   0.9372 |     28.064 |     0.5
   30 |   0.5175 |     16.212 |   0.9233 |     28.278 |     0.5
   31 |   0.5007 |     15.832 |   0.9313 |     28.064 |     0.5
   32 |   0.4831 |     15.307 |   0.9316 |     28.094 |     0.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 456,610

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2114 |     60.306 |   1.5731 |     45.650 |     0.0
    2 |   1.4511 |     45.335 |   1.3704 |     43.382 |     0.0
    3 |   1.3368 |     43.682 |   1.3129 |     41.942 |     0.0
    4 |   1.2734 |     42.003 |   1.2412 |     40.594 |     0.1
    5 |   1.2042 |     39.526 |   1.2142 |     39.154 |     0.1
    6 |   1.1497 |     37.614 |   1.1559 |     37.561 |     0.1
    7 |   1.0919 |     35.539 |   1.1171 |     35.386 |     0.1
    8 |   1.0328 |     33.463 |   1.0713 |     35.110 |     0.1
    9 |   0.9764 |     31.529 |   1.0352 |     33.303 |     0.1
   10 |   0.9270 |     30.169 |   1.0103 |     32.782 |     0.2
   11 |   0.8850 |     29.037 |   0.9886 |     32.077 |     0.2
   12 |   0.8379 |     27.427 |   0.9694 |     31.801 |     0.2
   13 |   0.7950 |     26.046 |   0.9774 |     31.924 |     0.2
   14 |   0.7567 |     24.859 |   0.9424 |     31.005 |     0.2
   15 |   0.7173 |     23.488 |   0.9511 |     30.239 |     0.2
   16 |   0.6855 |     22.285 |   0.9131 |     29.504 |     0.3
   17 |   0.6414 |     21.061 |   0.9208 |     29.136 |     0.3
   18 |   0.5993 |     19.891 |   0.9144 |     28.309 |     0.3
   19 |   0.5696 |     18.709 |   0.9094 |     28.401 |     0.3
   20 |   0.5393 |     17.815 |   0.9092 |     27.972 |     0.3
   21 |   0.5104 |     16.992 |   0.9072 |     28.401 |     0.3
   22 |   0.4910 |     15.979 |   0.9010 |     26.991 |     0.4
   23 |   0.4649 |     15.464 |   0.9139 |     27.849 |     0.4
   24 |   0.4416 |     14.754 |   0.9051 |     27.788 |     0.4
   25 |   0.4194 |     13.741 |   0.9119 |     26.961 |     0.4
   26 |   0.3987 |     13.307 |   0.9366 |     27.237 |     0.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,586,786

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2098 |     86.443 |   2.6993 |     83.333 |     0.0
    2 |   2.4352 |     62.776 |   2.2329 |     58.824 |     0.1
    3 |   2.1161 |     57.152 |   2.0050 |     58.824 |     0.1
    4 |   1.9381 |     53.744 |   1.8490 |     48.346 |     0.2
    5 |   1.8071 |     49.258 |   1.7355 |     48.346 |     0.2
    6 |   1.7060 |     48.797 |   1.6458 |     48.346 |     0.3
    7 |   1.6230 |     48.450 |   1.5730 |     45.466 |     0.3
    8 |   1.5604 |     46.353 |   1.5201 |     45.466 |     0.3
    9 |   1.5150 |     46.213 |   1.4839 |     45.466 |     0.4
   10 |   1.4840 |     46.207 |   1.4600 |     45.466 |     0.4
   11 |   1.4609 |     46.245 |   1.4430 |     45.466 |     0.5
   12 |   1.4444 |     46.213 |   1.4313 |     45.466 |     0.5
   13 |   1.4360 |     46.158 |   1.4225 |     45.466 |     0.6
   14 |   1.4279 |     46.288 |   1.4160 |     45.466 |     0.6
   15 |   1.4207 |     46.191 |   1.4115 |     45.466 |     0.6
   16 |   1.4158 |     46.272 |   1.4057 |     45.466 |     0.7
   17 |   1.4085 |     46.196 |   1.3997 |     45.466 |     0.7
   18 |   1.4031 |     46.175 |   1.3928 |     46.048 |     0.8
   19 |   1.3983 |     46.288 |   1.3858 |     45.466 |     0.8
   20 |   1.3906 |     46.310 |   1.3807 |     45.466 |     0.9
   21 |   1.3810 |     46.267 |   1.3710 |     45.282 |     0.9
   22 |   1.3739 |     45.844 |   1.3651 |     44.730 |     0.9
   23 |   1.3647 |     45.351 |   1.3601 |     43.995 |     1.0
   24 |   1.3578 |     44.901 |   1.3548 |     44.332 |     1.0
   25 |   1.3490 |     44.934 |   1.3513 |     44.393 |     1.1
   26 |   1.3443 |     44.956 |   1.3450 |     43.382 |     1.1
   27 |   1.3361 |     44.739 |   1.3415 |     43.781 |     1.2
   28 |   1.3312 |     44.565 |   1.3373 |     43.627 |     1.2
   29 |   1.3278 |     44.484 |   1.3377 |     43.658 |     1.2
   30 |   1.3196 |     44.235 |   1.3335 |     44.087 |     1.3
   31 |   1.3130 |     44.300 |   1.3321 |     43.873 |     1.3
   32 |   1.3113 |     44.045 |   1.3248 |     43.627 |     1.4
   33 |   1.3045 |     44.051 |   1.3213 |     43.597 |     1.4
   34 |   1.3015 |     43.834 |   1.3176 |     43.903 |     1.4
   35 |   1.2962 |     43.904 |   1.3122 |     43.597 |     1.5
   36 |   1.2886 |     43.753 |   1.3075 |     43.413 |     1.5
   37 |   1.2843 |     43.401 |   1.3094 |     43.321 |     1.6
   38 |   1.2789 |     43.303 |   1.3074 |     43.597 |     1.6
   39 |   1.2707 |     43.314 |   1.3009 |     43.444 |     1.7
   40 |   1.2633 |     42.880 |   1.2922 |     42.984 |     1.7
   41 |   1.2605 |     42.691 |   1.2907 |     42.800 |     1.7
   42 |   1.2554 |     42.387 |   1.2852 |     42.402 |     1.8
   43 |   1.2482 |     41.932 |   1.2780 |     42.341 |     1.8
   44 |   1.2380 |     41.867 |   1.2825 |     42.800 |     1.9
   45 |   1.2321 |     41.656 |   1.2702 |     42.463 |     1.9
   46 |   1.2233 |     41.445 |   1.2679 |     41.728 |     2.0
   47 |   1.2168 |     41.190 |   1.2623 |     41.881 |     2.0
   48 |   1.2047 |     40.876 |   1.2556 |     41.850 |     2.0
   49 |   1.2009 |     40.572 |   1.2490 |     41.942 |     2.1
   50 |   1.1927 |     40.355 |   1.2391 |     41.146 |     2.1
   51 |   1.1798 |     40.057 |   1.2375 |     40.901 |     2.2
   52 |   1.1768 |     39.673 |   1.2348 |     40.870 |     2.2
   53 |   1.1656 |     39.353 |   1.2284 |     40.656 |     2.3
   54 |   1.1570 |     38.855 |   1.2219 |     40.165 |     2.3
   55 |   1.1482 |     38.502 |   1.2172 |     40.533 |     2.3
   56 |   1.1427 |     38.313 |   1.2125 |     40.196 |     2.4
   57 |   1.1323 |     37.858 |   1.2135 |     39.951 |     2.4
   58 |   1.1245 |     37.500 |   1.2049 |     39.553 |     2.5
   59 |   1.1144 |     37.148 |   1.1984 |     39.001 |     2.5
   60 |   1.1095 |     36.769 |   1.1878 |     39.001 |     2.5
   61 |   1.0986 |     36.649 |   1.1924 |     39.124 |     2.6
   62 |   1.0910 |     36.113 |   1.1804 |     38.572 |     2.6
   63 |   1.0783 |     35.875 |   1.1745 |     38.327 |     2.7
   64 |   1.0725 |     35.441 |   1.1687 |     38.297 |     2.7
   65 |   1.0604 |     35.474 |   1.1725 |     37.806 |     2.8
   66 |   1.0529 |     34.856 |   1.1557 |     37.990 |     2.8
   67 |   1.0499 |     35.089 |   1.1603 |     37.439 |     2.8
   68 |   1.0349 |     34.482 |   1.1497 |     37.010 |     2.9
   69 |   1.0302 |     33.967 |   1.1456 |     37.316 |     2.9
   70 |   1.0220 |     33.913 |   1.1402 |     37.255 |     3.0
   71 |   1.0160 |     33.485 |   1.1375 |     36.673 |     3.0
   72 |   1.0076 |     33.333 |   1.1326 |     36.581 |     3.1
   73 |   0.9987 |     33.008 |   1.1268 |     37.255 |     3.1
   74 |   0.9914 |     32.873 |   1.1250 |     36.336 |     3.1
   75 |   0.9878 |     32.510 |   1.1274 |     36.765 |     3.2
   76 |   0.9819 |     32.288 |   1.1196 |     35.723 |     3.2
   77 |   0.9709 |     32.071 |   1.1188 |     36.489 |     3.3
   78 |   0.9585 |     31.849 |   1.1137 |     36.489 |     3.3
   79 |   0.9578 |     31.534 |   1.1096 |     35.938 |     3.4
   80 |   0.9500 |     31.404 |   1.1046 |     36.244 |     3.4
   81 |   0.9477 |     31.356 |   1.1086 |     36.029 |     3.4
   82 |   0.9383 |     30.966 |   1.0957 |     35.263 |     3.5
   83 |   0.9329 |     31.085 |   1.0961 |     35.417 |     3.5
   84 |   0.9257 |     30.684 |   1.0994 |     35.784 |     3.6
   85 |   0.9136 |     30.353 |   1.0959 |     35.080 |     3.6
   86 |   0.9106 |     30.072 |   1.0924 |     35.325 |     3.7
   87 |   0.9059 |     30.180 |   1.0955 |     35.723 |     3.7
   88 |   0.8979 |     29.551 |   1.0974 |     35.233 |     3.7
   89 |   0.8928 |     29.671 |   1.0872 |     35.233 |     3.8
   90 |   0.8861 |     29.503 |   1.0912 |     35.294 |     3.8
   91 |   0.8874 |     29.519 |   1.0723 |     34.743 |     3.9
   92 |   0.8743 |     29.297 |   1.0897 |     35.202 |     3.9
   93 |   0.8669 |     29.058 |   1.0804 |     35.141 |     4.0
   94 |   0.8670 |     28.690 |   1.0844 |     34.896 |     4.0
   95 |   0.8608 |     28.338 |   1.0748 |     34.559 |     4.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,847,330

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3035 |     83.919 |   2.9621 |     83.333 |     0.1
    2 |   2.7584 |     81.388 |   2.6248 |     66.268 |     0.1
    3 |   2.4913 |     60.062 |   2.4001 |     58.701 |     0.2
    4 |   2.3004 |     58.664 |   2.2432 |     57.782 |     0.2
    5 |   2.1711 |     57.504 |   2.1358 |     54.810 |     0.3
    6 |   2.0775 |     55.933 |   2.0459 |     54.963 |     0.4
    7 |   1.9957 |     54.643 |   1.9634 |     53.983 |     0.4
    8 |   1.9189 |     51.403 |   1.8899 |     48.958 |     0.5
    9 |   1.8545 |     48.819 |   1.8262 |     48.346 |     0.5
   10 |   1.7958 |     48.624 |   1.7646 |     48.346 |     0.6
   11 |   1.7394 |     48.613 |   1.7114 |     48.346 |     0.7
   12 |   1.6893 |     48.602 |   1.6645 |     48.346 |     0.7
   13 |   1.6465 |     48.602 |   1.6225 |     48.346 |     0.8
   14 |   1.6065 |     46.673 |   1.5847 |     45.466 |     0.8
   15 |   1.5719 |     46.207 |   1.5532 |     45.466 |     0.9
   16 |   1.5435 |     46.213 |   1.5281 |     45.466 |     1.0
   17 |   1.5194 |     46.213 |   1.5073 |     45.466 |     1.0
   18 |   1.5018 |     46.213 |   1.4895 |     45.466 |     1.1
   19 |   1.4834 |     46.213 |   1.4762 |     45.466 |     1.1
   20 |   1.4711 |     46.218 |   1.4627 |     45.466 |     1.2
   21 |   1.4571 |     46.213 |   1.4536 |     45.466 |     1.3
   22 |   1.4459 |     46.207 |   1.4409 |     45.466 |     1.3
   23 |   1.4361 |     46.213 |   1.4339 |     45.466 |     1.4
   24 |   1.4279 |     46.218 |   1.4206 |     45.466 |     1.4
   25 |   1.4146 |     46.213 |   1.4127 |     45.466 |     1.5
   26 |   1.4058 |     46.207 |   1.4042 |     45.466 |     1.6
   27 |   1.3986 |     46.202 |   1.3968 |     45.466 |     1.6
   28 |   1.3871 |     46.186 |   1.3887 |     45.466 |     1.7
   29 |   1.3790 |     46.142 |   1.3810 |     45.435 |     1.7
   30 |   1.3706 |     45.855 |   1.3738 |     44.577 |     1.8
   31 |   1.3638 |     45.454 |   1.3666 |     44.792 |     1.9
   32 |   1.3521 |     45.059 |   1.3613 |     44.577 |     1.9
   33 |   1.3435 |     44.668 |   1.3553 |     44.179 |     2.0
   34 |   1.3375 |     44.495 |   1.3502 |     43.658 |     2.0
   35 |   1.3268 |     43.910 |   1.3416 |     43.658 |     2.1
   36 |   1.3203 |     43.964 |   1.3362 |     43.229 |     2.2
   37 |   1.3106 |     43.438 |   1.3305 |     43.352 |     2.2
   38 |   1.3014 |     43.574 |   1.3293 |     43.566 |     2.3
   39 |   1.2952 |     43.270 |   1.3204 |     43.199 |     2.3
   40 |   1.2874 |     43.314 |   1.3221 |     44.087 |     2.4
   41 |   1.2817 |     42.935 |   1.3158 |     43.290 |     2.5
   42 |   1.2736 |     42.593 |   1.3085 |     42.708 |     2.5
   43 |   1.2650 |     42.311 |   1.3063 |     42.892 |     2.6
   44 |   1.2583 |     42.154 |   1.3017 |     42.310 |     2.6
   45 |   1.2496 |     41.715 |   1.2970 |     43.260 |     2.7
   46 |   1.2387 |     41.607 |   1.2959 |     42.647 |     2.8
   47 |   1.2367 |     41.466 |   1.2871 |     42.341 |     2.8
   48 |   1.2274 |     41.081 |   1.2904 |     42.831 |     2.9
   49 |   1.2183 |     40.637 |   1.2838 |     42.708 |     2.9
   50 |   1.2124 |     40.865 |   1.2829 |     43.045 |     3.0
   51 |   1.2034 |     40.513 |   1.2745 |     42.678 |     3.1
   52 |   1.1958 |     40.372 |   1.2764 |     41.973 |     3.1
   53 |   1.1859 |     39.998 |   1.2710 |     42.831 |     3.2
   54 |   1.1799 |     39.759 |   1.2739 |     41.728 |     3.2
   55 |   1.1703 |     39.299 |   1.2668 |     41.697 |     3.3
   56 |   1.1690 |     39.256 |   1.2533 |     41.330 |     3.4
   57 |   1.1531 |     39.012 |   1.2528 |     41.207 |     3.4
   58 |   1.1440 |     38.562 |   1.2496 |     41.207 |     3.5
   59 |   1.1343 |     38.242 |   1.2425 |     40.686 |     3.5
   60 |   1.1228 |     37.765 |   1.2402 |     41.023 |     3.6
   61 |   1.1161 |     37.587 |   1.2360 |     40.901 |     3.7
   62 |   1.1066 |     37.110 |   1.2263 |     40.564 |     3.7
   63 |   1.0964 |     36.682 |   1.2228 |     40.135 |     3.8
   64 |   1.0843 |     36.070 |   1.2226 |     40.196 |     3.8
   65 |   1.0792 |     35.793 |   1.2192 |     39.767 |     3.9
   66 |   1.0667 |     35.235 |   1.2111 |     39.645 |     4.0
   67 |   1.0548 |     34.823 |   1.2024 |     39.583 |     4.0
   68 |   1.0506 |     34.498 |   1.2063 |     39.645 |     4.1
   69 |   1.0404 |     34.168 |   1.2023 |     39.491 |     4.1
   70 |   1.0272 |     33.929 |   1.2040 |     39.154 |     4.2
   71 |   1.0225 |     33.355 |   1.1965 |     38.817 |     4.3
   72 |   1.0140 |     33.127 |   1.1911 |     39.583 |     4.3
   73 |   1.0059 |     32.943 |   1.1850 |     38.419 |     4.4
   74 |   0.9974 |     32.721 |   1.1874 |     38.603 |     4.4
   75 |   0.9862 |     32.163 |   1.1827 |     38.511 |     4.5
   76 |   0.9786 |     31.876 |   1.1798 |     38.021 |     4.6
   77 |   0.9694 |     31.291 |   1.1833 |     37.776 |     4.6
   78 |   0.9621 |     31.047 |   1.1781 |     37.531 |     4.7
   79 |   0.9610 |     30.868 |   1.1678 |     37.469 |     4.7
   80 |   0.9575 |     30.678 |   1.1695 |     36.949 |     4.8
   81 |   0.9436 |     30.169 |   1.1709 |     36.949 |     4.9
   82 |   0.9375 |     29.719 |   1.1714 |     36.612 |     4.9
   83 |   0.9282 |     29.345 |   1.1665 |     36.458 |     5.0
   84 |   0.9224 |     29.616 |   1.1629 |     36.244 |     5.0
   85 |   0.9136 |     29.047 |   1.1654 |     36.305 |     5.1
   86 |   0.9051 |     28.863 |   1.1638 |     35.876 |     5.2
   87 |   0.8963 |     28.414 |   1.1573 |     35.386 |     5.2
   88 |   0.8912 |     27.899 |   1.1636 |     35.631 |     5.3
   89 |   0.8853 |     27.660 |   1.1590 |     35.723 |     5.3
   90 |   0.8856 |     27.920 |   1.1488 |     35.417 |     5.4
   91 |   0.8712 |     27.715 |   1.1523 |     35.263 |     5.5
   92 |   0.8636 |     26.934 |   1.1574 |     34.375 |     5.5
   93 |   0.8593 |     26.566 |   1.1607 |     34.957 |     5.6
   94 |   0.8562 |     26.848 |   1.1546 |     35.233 |     5.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 325,442

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5759 |     67.739 |   2.0452 |     58.793 |     0.0
    2 |   1.8174 |     51.132 |   1.5988 |     45.466 |     0.0
    3 |   1.5183 |     46.397 |   1.4551 |     45.987 |     0.1
    4 |   1.4428 |     46.397 |   1.4147 |     45.987 |     0.1
    5 |   1.4096 |     46.278 |   1.3927 |     45.496 |     0.1
    6 |   1.3943 |     46.121 |   1.3725 |     45.466 |     0.1
    7 |   1.3719 |     45.806 |   1.3582 |     45.466 |     0.1
    8 |   1.3502 |     45.226 |   1.3378 |     44.516 |     0.1
    9 |   1.3347 |     44.565 |   1.3239 |     44.332 |     0.2
   10 |   1.3155 |     44.089 |   1.2978 |     43.168 |     0.2
   11 |   1.2957 |     43.503 |   1.2781 |     42.034 |     0.2
   12 |   1.2680 |     42.393 |   1.2618 |     40.931 |     0.2
   13 |   1.2578 |     41.732 |   1.2506 |     41.115 |     0.2
   14 |   1.2325 |     41.206 |   1.2353 |     40.380 |     0.3
   15 |   1.2160 |     40.540 |   1.2191 |     39.675 |     0.3
   16 |   1.2043 |     40.101 |   1.2093 |     39.430 |     0.3
   17 |   1.1877 |     39.738 |   1.2067 |     39.491 |     0.3
   18 |   1.1728 |     39.543 |   1.1875 |     38.909 |     0.3
   19 |   1.1614 |     39.142 |   1.1905 |     39.001 |     0.4
   20 |   1.1454 |     38.676 |   1.1769 |     39.124 |     0.4
   21 |   1.1342 |     38.464 |   1.1634 |     38.756 |     0.4
   22 |   1.1140 |     37.619 |   1.1525 |     38.572 |     0.4
   23 |   1.1031 |     37.419 |   1.1348 |     38.235 |     0.4
   24 |   1.0865 |     36.909 |   1.1320 |     38.143 |     0.4
   25 |   1.0791 |     36.416 |   1.1287 |     37.868 |     0.5
   26 |   1.0633 |     36.243 |   1.1031 |     36.826 |     0.5
   27 |   1.0432 |     35.549 |   1.0954 |     35.539 |     0.5
   28 |   1.0262 |     34.905 |   1.0799 |     35.325 |     0.5
   29 |   1.0103 |     34.347 |   1.0819 |     35.386 |     0.5
   30 |   0.9978 |     33.843 |   1.0738 |     34.926 |     0.6
   31 |   0.9775 |     33.106 |   1.0553 |     34.007 |     0.6
   32 |   0.9637 |     32.504 |   1.0440 |     34.436 |     0.6
   33 |   0.9434 |     32.071 |   1.0357 |     34.222 |     0.6
   34 |   0.9288 |     31.697 |   1.0242 |     33.946 |     0.6
   35 |   0.9171 |     31.231 |   1.0305 |     33.517 |     0.6
   36 |   0.9048 |     30.705 |   1.0048 |     32.537 |     0.7
   37 |   0.8909 |     30.256 |   1.0053 |     32.721 |     0.7
   38 |   0.8715 |     29.622 |   0.9969 |     31.985 |     0.7
   39 |   0.8625 |     29.199 |   0.9985 |     32.812 |     0.7
   40 |   0.8491 |     28.809 |   1.0020 |     32.874 |     0.7
   41 |   0.8359 |     28.283 |   0.9965 |     31.740 |     0.8
   42 |   0.8315 |     28.397 |   0.9860 |     31.985 |     0.8
   43 |   0.8095 |     27.389 |   0.9693 |     30.821 |     0.8
   44 |   0.8022 |     27.254 |   0.9785 |     31.403 |     0.8
   45 |   0.7838 |     26.712 |   0.9687 |     31.036 |     0.8
   46 |   0.7731 |     26.105 |   0.9795 |     30.821 |     0.8
   47 |   0.7626 |     26.029 |   0.9689 |     30.729 |     0.9
   48 |   0.7596 |     25.796 |   0.9674 |     29.841 |     0.9
   49 |   0.7548 |     25.547 |   0.9631 |     29.841 |     0.9
   50 |   0.7536 |     25.455 |   0.9740 |     30.453 |     0.9
   51 |   0.7356 |     24.789 |   0.9667 |     30.055 |     0.9
   52 |   0.7255 |     24.583 |   0.9776 |     29.688 |     1.0
   53 |   0.7072 |     23.851 |   0.9636 |     30.086 |     1.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 984,770

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4184 |     89.364 |   3.3034 |     83.272 |     0.0
    2 |   3.1296 |     83.301 |   2.9385 |     83.333 |     0.1
    3 |   2.8382 |     83.333 |   2.7642 |     83.333 |     0.1
    4 |   2.7026 |     76.365 |   2.6550 |     67.341 |     0.2
    5 |   2.6035 |     67.371 |   2.5602 |     67.004 |     0.2
    6 |   2.5115 |     66.759 |   2.4700 |     66.912 |     0.2
    7 |   2.4318 |     64.234 |   2.3962 |     60.907 |     0.3
    8 |   2.3632 |     61.151 |   2.3266 |     58.824 |     0.3
    9 |   2.3060 |     59.173 |   2.2702 |     58.793 |     0.4
   10 |   2.2512 |     58.431 |   2.2186 |     58.762 |     0.4
   11 |   2.2061 |     57.737 |   2.1732 |     58.824 |     0.5
   12 |   2.1637 |     57.103 |   2.1289 |     58.762 |     0.5
   13 |   2.1222 |     56.925 |   2.0881 |     58.272 |     0.5
   14 |   2.0851 |     56.692 |   2.0524 |     58.456 |     0.6
   15 |   2.0494 |     56.529 |   2.0183 |     54.779 |     0.6
   16 |   2.0158 |     56.204 |   1.9854 |     55.974 |     0.7
   17 |   1.9854 |     55.906 |   1.9536 |     54.565 |     0.7
   18 |   1.9527 |     55.760 |   1.9224 |     53.860 |     0.8
   19 |   1.9224 |     55.180 |   1.8923 |     53.952 |     0.8
   20 |   1.8912 |     53.825 |   1.8628 |     48.529 |     0.8
   21 |   1.8618 |     51.745 |   1.8348 |     48.223 |     0.9
   22 |   1.8371 |     50.287 |   1.8079 |     48.376 |     0.9
   23 |   1.8093 |     49.713 |   1.7794 |     48.346 |     1.0
   24 |   1.7811 |     49.485 |   1.7521 |     48.376 |     1.0
   25 |   1.7535 |     49.355 |   1.7241 |     48.346 |     1.0
   26 |   1.7239 |     49.133 |   1.6980 |     48.346 |     1.1
   27 |   1.7005 |     49.014 |   1.6738 |     48.346 |     1.1
   28 |   1.6768 |     48.819 |   1.6511 |     48.346 |     1.2
   29 |   1.6529 |     48.391 |   1.6308 |     48.346 |     1.2
   30 |   1.6325 |     47.551 |   1.6125 |     45.466 |     1.2
   31 |   1.6151 |     46.814 |   1.5960 |     45.466 |     1.3
   32 |   1.5999 |     46.543 |   1.5811 |     45.466 |     1.3
   33 |   1.5837 |     46.419 |   1.5674 |     45.466 |     1.4
   34 |   1.5700 |     46.288 |   1.5547 |     45.466 |     1.4
   35 |   1.5597 |     46.288 |   1.5437 |     45.466 |     1.5
   36 |   1.5456 |     46.213 |   1.5330 |     45.466 |     1.5
   37 |   1.5372 |     46.110 |   1.5230 |     45.466 |     1.5
   38 |   1.5269 |     46.137 |   1.5135 |     45.466 |     1.6
   39 |   1.5153 |     46.023 |   1.5037 |     45.466 |     1.6
   40 |   1.5075 |     45.958 |   1.4948 |     45.466 |     1.7
   41 |   1.4978 |     45.942 |   1.4871 |     45.435 |     1.7
   42 |   1.4908 |     45.709 |   1.4788 |     45.282 |     1.7
   43 |   1.4850 |     45.590 |   1.4724 |     45.374 |     1.8
   44 |   1.4781 |     45.573 |   1.4667 |     45.159 |     1.8
   45 |   1.4699 |     45.264 |   1.4598 |     44.975 |     1.9
   46 |   1.4614 |     45.324 |   1.4544 |     44.975 |     1.9
   47 |   1.4590 |     45.210 |   1.4484 |     44.884 |     2.0
   48 |   1.4511 |     45.183 |   1.4437 |     44.822 |     2.0
   49 |   1.4469 |     44.896 |   1.4384 |     44.730 |     2.0
   50 |   1.4424 |     44.923 |   1.4327 |     44.485 |     2.1
   51 |   1.4343 |     44.999 |   1.4295 |     44.210 |     2.1
   52 |   1.4286 |     44.923 |   1.4236 |     44.118 |     2.2
   53 |   1.4235 |     44.788 |   1.4184 |     44.485 |     2.2
   54 |   1.4183 |     44.728 |   1.4133 |     44.240 |     2.2
   55 |   1.4127 |     44.685 |   1.4076 |     44.148 |     2.3
   56 |   1.4057 |     44.603 |   1.4033 |     44.301 |     2.3
   57 |   1.4013 |     44.663 |   1.3995 |     44.026 |     2.4
   58 |   1.3948 |     44.603 |   1.3931 |     44.455 |     2.4
   59 |   1.3891 |     44.538 |   1.3868 |     44.210 |     2.4
   60 |   1.3857 |     44.495 |   1.3825 |     43.995 |     2.5
   61 |   1.3800 |     44.419 |   1.3775 |     43.964 |     2.5
   62 |   1.3759 |     44.397 |   1.3724 |     43.995 |     2.6
   63 |   1.3662 |     44.235 |   1.3663 |     43.903 |     2.6
   64 |   1.3631 |     44.246 |   1.3612 |     43.873 |     2.7
   65 |   1.3577 |     43.997 |   1.3569 |     43.566 |     2.7
   66 |   1.3515 |     43.861 |   1.3527 |     43.781 |     2.7
   67 |   1.3466 |     43.693 |   1.3486 |     43.505 |     2.8
   68 |   1.3386 |     43.520 |   1.3426 |     43.352 |     2.8
   69 |   1.3338 |     43.363 |   1.3370 |     43.045 |     2.9
   70 |   1.3304 |     43.227 |   1.3339 |     42.953 |     2.9
   71 |   1.3270 |     43.103 |   1.3293 |     42.433 |     2.9
   72 |   1.3193 |     42.723 |   1.3274 |     42.770 |     3.0
   73 |   1.3151 |     42.620 |   1.3221 |     42.555 |     3.0
   74 |   1.3089 |     42.544 |   1.3180 |     42.310 |     3.1
   75 |   1.3043 |     42.360 |   1.3135 |     42.310 |     3.1
   76 |   1.2954 |     42.208 |   1.3084 |     42.188 |     3.2
   77 |   1.2919 |     42.008 |   1.3082 |     41.759 |     3.2
   78 |   1.2877 |     41.883 |   1.3037 |     41.850 |     3.2
   79 |   1.2861 |     41.965 |   1.3030 |     41.544 |     3.3
   80 |   1.2840 |     41.900 |   1.2979 |     41.636 |     3.3
   81 |   1.2768 |     41.466 |   1.2957 |     41.422 |     3.4
   82 |   1.2770 |     41.645 |   1.2985 |     41.483 |     3.4
   83 |   1.2691 |     41.342 |   1.2880 |     41.422 |     3.4
   84 |   1.2657 |     41.461 |   1.2851 |     41.422 |     3.5
   85 |   1.2624 |     41.266 |   1.2848 |     41.238 |     3.5
   86 |   1.2596 |     41.119 |   1.2812 |     41.299 |     3.6
   87 |   1.2523 |     41.044 |   1.2774 |     40.870 |     3.6
   88 |   1.2488 |     40.924 |   1.2768 |     40.686 |     3.7
   89 |   1.2447 |     40.914 |   1.2710 |     40.625 |     3.7
   90 |   1.2405 |     40.838 |   1.2704 |     40.962 |     3.7
   91 |   1.2359 |     40.621 |   1.2706 |     40.839 |     3.8
   92 |   1.2344 |     40.529 |   1.2664 |     40.809 |     3.8
   93 |   1.2286 |     40.334 |   1.2664 |     41.115 |     3.9
   94 |   1.2237 |     40.437 |   1.2607 |     40.778 |     3.9
   95 |   1.2213 |     40.263 |   1.2602 |     40.778 |     3.9
   96 |   1.2175 |     40.220 |   1.2562 |     40.656 |     4.0
   97 |   1.2144 |     40.133 |   1.2547 |     40.717 |     4.0
   98 |   1.2143 |     39.949 |   1.2511 |     40.717 |     4.1
   99 |   1.2074 |     39.949 |   1.2477 |     40.748 |     4.1
  100 |   1.2072 |     39.727 |   1.2484 |     40.288 |     4.2
  101 |   1.2040 |     39.721 |   1.2476 |     40.717 |     4.2
  102 |   1.1972 |     39.548 |   1.2417 |     40.686 |     4.2
  103 |   1.1962 |     39.694 |   1.2443 |     40.656 |     4.3
  104 |   1.1917 |     39.521 |   1.2414 |     40.227 |     4.3
  105 |   1.1865 |     39.386 |   1.2414 |     40.441 |     4.4
  106 |   1.1836 |     39.234 |   1.2371 |     40.349 |     4.4
  107 |   1.1807 |     38.963 |   1.2340 |     40.227 |     4.4
  108 |   1.1788 |     39.245 |   1.2338 |     40.656 |     4.5
  109 |   1.1779 |     39.136 |   1.2299 |     40.319 |     4.5
  110 |   1.1686 |     38.822 |   1.2328 |     40.227 |     4.6
  111 |   1.1688 |     38.936 |   1.2261 |     40.074 |     4.6
  112 |   1.1615 |     38.540 |   1.2218 |     40.196 |     4.6
  113 |   1.1597 |     38.735 |   1.2238 |     40.196 |     4.7
  114 |   1.1550 |     38.855 |   1.2215 |     40.227 |     4.7
  115 |   1.1525 |     38.519 |   1.2241 |     39.828 |     4.8
  116 |   1.1501 |     38.399 |   1.2195 |     39.951 |     4.8
  117 |   1.1461 |     38.399 |   1.2204 |     40.502 |     4.9
  118 |   1.1490 |     38.432 |   1.2178 |     40.012 |     4.9
  119 |   1.1424 |     38.150 |   1.2132 |     39.706 |     4.9
  120 |   1.1408 |     38.259 |   1.2130 |     39.706 |     5.0
  121 |   1.1402 |     37.988 |   1.2106 |     39.369 |     5.0
  122 |   1.1330 |     37.993 |   1.2094 |     39.951 |     5.1
  123 |   1.1303 |     38.064 |   1.2101 |     39.522 |     5.1
  124 |   1.1304 |     37.939 |   1.2087 |     40.043 |     5.1
  125 |   1.1239 |     37.793 |   1.2031 |     39.522 |     5.2
  126 |   1.1215 |     37.657 |   1.2097 |     40.012 |     5.2
  127 |   1.1181 |     37.397 |   1.2032 |     39.246 |     5.3
  128 |   1.1189 |     37.467 |   1.2032 |     39.767 |     5.3
  129 |   1.1134 |     37.175 |   1.2028 |     39.583 |     5.4
  130 |   1.1122 |     37.164 |   1.2012 |     39.859 |     5.4
  131 |   1.1071 |     37.327 |   1.1958 |     39.277 |     5.4
  132 |   1.1056 |     37.067 |   1.1938 |     39.430 |     5.5
  133 |   1.1043 |     37.045 |   1.1955 |     39.032 |     5.5
  134 |   1.1034 |     37.045 |   1.1936 |     39.062 |     5.6
  135 |   1.1034 |     37.077 |   1.1984 |     39.277 |     5.6
  136 |   1.0961 |     36.731 |   1.1984 |     39.491 |     5.7
  137 |   1.0928 |     36.606 |   1.1916 |     39.154 |     5.7
  138 |   1.0867 |     36.563 |   1.1889 |     39.706 |     5.7
  139 |   1.0878 |     36.416 |   1.1905 |     39.246 |     5.8
  140 |   1.0837 |     36.395 |   1.1886 |     39.461 |     5.8
  141 |   1.0799 |     36.286 |   1.1831 |     39.032 |     5.9
  142 |   1.0813 |     36.232 |   1.1842 |     39.400 |     5.9
  143 |   1.0774 |     36.313 |   1.1853 |     39.001 |     5.9
  144 |   1.0745 |     36.167 |   1.1864 |     38.817 |     6.0
  145 |   1.0672 |     36.053 |   1.1892 |     39.185 |     6.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,357,538

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.0848 |     81.068 |   2.5036 |     59.926 |     0.0
    2 |   2.2537 |     57.965 |   2.0831 |     55.882 |     0.1
    3 |   1.9736 |     53.338 |   1.8729 |     48.652 |     0.1
    4 |   1.7955 |     48.662 |   1.7144 |     48.346 |     0.2
    5 |   1.6650 |     48.347 |   1.6040 |     48.100 |     0.2
    6 |   1.5716 |     48.244 |   1.5287 |     46.293 |     0.3
    7 |   1.5029 |     45.909 |   1.4713 |     45.190 |     0.3
    8 |   1.4517 |     45.709 |   1.4323 |     44.914 |     0.3
    9 |   1.4136 |     45.302 |   1.4017 |     44.608 |     0.4
   10 |   1.3838 |     44.923 |   1.3801 |     44.700 |     0.4
   11 |   1.3589 |     44.593 |   1.3557 |     44.271 |     0.5
   12 |   1.3365 |     44.067 |   1.3433 |     43.382 |     0.5
   13 |   1.3161 |     43.514 |   1.3166 |     43.474 |     0.6
   14 |   1.2914 |     42.192 |   1.2949 |     42.310 |     0.6
   15 |   1.2694 |     41.249 |   1.2781 |     40.962 |     0.6
   16 |   1.2483 |     39.917 |   1.2617 |     40.196 |     0.7
   17 |   1.2263 |     39.191 |   1.2411 |     39.308 |     0.7
   18 |   1.2061 |     38.855 |   1.2241 |     38.143 |     0.8
   19 |   1.1882 |     37.977 |   1.2093 |     38.174 |     0.8
   20 |   1.1670 |     37.446 |   1.1965 |     38.051 |     0.9
   21 |   1.1468 |     36.763 |   1.1794 |     37.561 |     0.9
   22 |   1.1245 |     35.912 |   1.1653 |     37.408 |     0.9
   23 |   1.1039 |     35.143 |   1.1505 |     36.887 |     1.0
   24 |   1.0856 |     34.471 |   1.1383 |     36.397 |     1.0
   25 |   1.0675 |     33.772 |   1.1241 |     36.183 |     1.1
   26 |   1.0515 |     33.420 |   1.1156 |     35.907 |     1.1
   27 |   1.0323 |     32.743 |   1.1016 |     35.263 |     1.1
   28 |   1.0131 |     32.223 |   1.0877 |     34.773 |     1.2
   29 |   0.9977 |     31.410 |   1.0831 |     34.988 |     1.2
   30 |   0.9828 |     31.177 |   1.0675 |     34.528 |     1.3
   31 |   0.9628 |     30.256 |   1.0558 |     33.517 |     1.3
   32 |   0.9453 |     29.638 |   1.0475 |     33.670 |     1.4
   33 |   0.9283 |     28.934 |   1.0387 |     33.456 |     1.4
   34 |   0.9137 |     28.435 |   1.0252 |     32.935 |     1.4
   35 |   0.8968 |     27.541 |   1.0191 |     32.138 |     1.5
   36 |   0.8872 |     27.205 |   1.0105 |     32.537 |     1.5
   37 |   0.8665 |     26.604 |   1.0029 |     32.108 |     1.6
   38 |   0.8514 |     25.872 |   0.9949 |     32.016 |     1.6
   39 |   0.8404 |     25.569 |   0.9875 |     31.679 |     1.7
   40 |   0.8233 |     24.967 |   0.9778 |     31.281 |     1.7
   41 |   0.8134 |     24.442 |   0.9812 |     31.189 |     1.7
   42 |   0.7928 |     24.052 |   0.9616 |     30.729 |     1.8
   43 |   0.7820 |     23.586 |   0.9527 |     30.515 |     1.8
   44 |   0.7660 |     23.239 |   0.9517 |     30.208 |     1.9
   45 |   0.7534 |     22.724 |   0.9429 |     30.147 |     1.9
   46 |   0.7406 |     22.459 |   0.9470 |     29.779 |     2.0
   47 |   0.7327 |     22.025 |   0.9318 |     29.933 |     2.0
   48 |   0.7219 |     22.036 |   0.9249 |     29.473 |     2.0
   49 |   0.7025 |     21.245 |   0.9210 |     29.259 |     2.1
   50 |   0.6940 |     20.953 |   0.9168 |     28.585 |     2.1
   51 |   0.6813 |     20.611 |   0.9183 |     28.768 |     2.2
   52 |   0.6665 |     19.999 |   0.9077 |     28.799 |     2.2
   53 |   0.6552 |     19.582 |   0.9041 |     28.401 |     2.2
   54 |   0.6473 |     19.517 |   0.9144 |     28.094 |     2.3
   55 |   0.6348 |     18.997 |   0.9043 |     28.156 |     2.3
   56 |   0.6258 |     18.991 |   0.8987 |     27.543 |     2.4
   57 |   0.6143 |     18.444 |   0.8874 |     27.574 |     2.4
   58 |   0.6026 |     18.065 |   0.8881 |     27.696 |     2.5
   59 |   0.5909 |     17.729 |   0.8943 |     27.482 |     2.5
   60 |   0.5847 |     17.534 |   0.8869 |     26.961 |     2.5
   61 |   0.5733 |     17.181 |   0.8852 |     27.053 |     2.6
   62 |   0.5640 |     17.046 |   0.8916 |     27.267 |     2.6
   63 |   0.5576 |     16.818 |   0.9001 |     27.574 |     2.7
   64 |   0.5478 |     16.450 |   0.8815 |     27.114 |     2.7
   65 |   0.5360 |     16.081 |   0.8837 |     26.991 |     2.8
   66 |   0.5343 |     15.838 |   0.8842 |     26.440 |     2.8
   67 |   0.5228 |     15.751 |   0.8855 |     26.808 |     2.8
   68 |   0.5134 |     15.280 |   0.8830 |     26.317 |     2.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,013,858

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8412 |     76.653 |   2.4053 |     58.824 |     0.0
    2 |   2.1823 |     57.499 |   1.9990 |     58.824 |     0.1
    3 |   1.8740 |     52.026 |   1.7326 |     48.346 |     0.1
    4 |   1.6516 |     47.648 |   1.5633 |     45.466 |     0.2
    5 |   1.5353 |     46.261 |   1.4947 |     45.466 |     0.2
    6 |   1.4850 |     46.343 |   1.4592 |     45.466 |     0.3
    7 |   1.4574 |     46.191 |   1.4395 |     45.987 |     0.3
    8 |   1.4393 |     46.435 |   1.4258 |     45.987 |     0.3
    9 |   1.4275 |     46.251 |   1.4156 |     45.466 |     0.4
   10 |   1.4185 |     46.223 |   1.4103 |     46.017 |     0.4
   11 |   1.4117 |     46.353 |   1.4043 |     45.833 |     0.5
   12 |   1.4061 |     46.256 |   1.4183 |     45.803 |     0.5
   13 |   1.4059 |     46.353 |   1.3963 |     45.466 |     0.5
   14 |   1.4030 |     46.240 |   1.3925 |     44.945 |     0.6
   15 |   1.3968 |     46.218 |   1.3894 |     45.833 |     0.6
   16 |   1.3959 |     46.175 |   1.3818 |     45.466 |     0.7
   17 |   1.3895 |     45.969 |   1.3796 |     45.221 |     0.7
   18 |   1.3869 |     45.920 |   1.3744 |     45.159 |     0.8
   19 |   1.3804 |     45.638 |   1.3635 |     45.037 |     0.8
   20 |   1.3727 |     45.286 |   1.3608 |     44.669 |     0.8
   21 |   1.3616 |     44.891 |   1.3507 |     43.873 |     0.9
   22 |   1.3543 |     44.847 |   1.3423 |     43.873 |     0.9
   23 |   1.3544 |     44.815 |   1.3395 |     43.719 |     1.0
   24 |   1.3480 |     44.766 |   1.3372 |     44.026 |     1.0
   25 |   1.3448 |     44.793 |   1.3289 |     43.689 |     1.1
   26 |   1.3368 |     44.620 |   1.3245 |     43.781 |     1.1
   27 |   1.3310 |     44.457 |   1.3223 |     43.536 |     1.1
   28 |   1.3262 |     44.479 |   1.3205 |     43.597 |     1.2
   29 |   1.3313 |     44.392 |   1.3266 |     44.026 |     1.2
   30 |   1.3235 |     44.257 |   1.3174 |     43.658 |     1.3
   31 |   1.3206 |     44.257 |   1.3161 |     43.352 |     1.3
   32 |   1.3205 |     44.116 |   1.3137 |     42.984 |     1.3
   33 |   1.3145 |     44.110 |   1.3049 |     42.984 |     1.4
   34 |   1.3079 |     43.574 |   1.3061 |     42.678 |     1.4
   35 |   1.3092 |     43.769 |   1.3060 |     43.413 |     1.5
   36 |   1.3025 |     43.455 |   1.2982 |     43.413 |     1.5
   37 |   1.2990 |     43.531 |   1.2933 |     42.616 |     1.6
   38 |   1.2965 |     43.314 |   1.2907 |     42.586 |     1.6
   39 |   1.2918 |     43.314 |   1.2893 |     42.678 |     1.6
   40 |   1.2868 |     43.189 |   1.2801 |     42.371 |     1.7
   41 |   1.2853 |     43.119 |   1.2817 |     42.433 |     1.7
   42 |   1.2850 |     43.195 |   1.2817 |     42.433 |     1.8
   43 |   1.2833 |     43.146 |   1.2778 |     42.433 |     1.8
   44 |   1.2769 |     43.016 |   1.2774 |     42.310 |     1.8
   45 |   1.2733 |     42.891 |   1.2740 |     42.402 |     1.9
   46 |   1.2682 |     42.620 |   1.2720 |     41.973 |     1.9
   47 |   1.2672 |     42.669 |   1.2700 |     41.912 |     2.0
   48 |   1.2660 |     42.712 |   1.2658 |     41.544 |     2.0
   49 |   1.2574 |     42.246 |   1.2605 |     41.605 |     2.1
   50 |   1.2528 |     41.927 |   1.2584 |     41.422 |     2.1
   51 |   1.2553 |     42.160 |   1.2612 |     40.962 |     2.1
   52 |   1.2514 |     42.160 |   1.2545 |     41.146 |     2.2
   53 |   1.2503 |     42.062 |   1.2469 |     40.472 |     2.2
   54 |   1.2428 |     41.883 |   1.2504 |     40.901 |     2.3
   55 |   1.2437 |     41.883 |   1.2428 |     40.993 |     2.3
   56 |   1.2457 |     41.612 |   1.2462 |     41.391 |     2.4
   57 |   1.2346 |     41.623 |   1.2419 |     41.207 |     2.4
   58 |   1.2342 |     41.553 |   1.2381 |     41.023 |     2.4
   59 |   1.2343 |     41.466 |   1.2415 |     40.686 |     2.5
   60 |   1.2280 |     41.374 |   1.2362 |     40.349 |     2.5
   61 |   1.2275 |     41.477 |   1.2335 |     40.625 |     2.6
   62 |   1.2270 |     41.266 |   1.2355 |     40.870 |     2.6
   63 |   1.2200 |     41.358 |   1.2316 |     40.901 |     2.6
   64 |   1.2153 |     40.935 |   1.2239 |     40.870 |     2.7
   65 |   1.2088 |     40.816 |   1.2213 |     40.349 |     2.7
   66 |   1.2107 |     40.773 |   1.2180 |     39.675 |     2.8
   67 |   1.2082 |     40.540 |   1.2114 |     39.645 |     2.8
   68 |   1.2066 |     40.653 |   1.2174 |     40.257 |     2.9
   69 |   1.1926 |     40.431 |   1.2140 |     39.583 |     2.9
   70 |   1.1995 |     40.355 |   1.2160 |     40.227 |     2.9
   71 |   1.1956 |     40.345 |   1.2160 |     39.706 |     3.0
   72 |   1.1823 |     39.857 |   1.2099 |     40.043 |     3.0
   73 |   1.1944 |     40.166 |   1.2216 |     40.227 |     3.1
   74 |   1.1926 |     40.166 |   1.2027 |     40.074 |     3.1
   75 |   1.1900 |     40.133 |   1.1934 |     39.185 |     3.1
   76 |   1.1825 |     39.857 |   1.2019 |     39.614 |     3.2
   77 |   1.1755 |     39.619 |   1.1900 |     39.246 |     3.2
   78 |   1.1742 |     39.559 |   1.1996 |     39.491 |     3.3
   79 |   1.1674 |     39.304 |   1.1974 |     39.185 |     3.3
   80 |   1.1648 |     39.364 |   1.2051 |     38.940 |     3.4
   81 |   1.1601 |     38.958 |   1.1973 |     38.603 |     3.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,526,306

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5161 |     68.498 |   1.9892 |     58.793 |     0.1
    2 |   1.7722 |     49.967 |   1.5759 |     45.466 |     0.2
    3 |   1.5146 |     46.375 |   1.4516 |     45.466 |     0.3
    4 |   1.4393 |     46.240 |   1.4174 |     45.466 |     0.3
    5 |   1.4166 |     46.267 |   1.4032 |     45.466 |     0.4
    6 |   1.4053 |     46.245 |   1.3952 |     45.466 |     0.5
    7 |   1.4013 |     46.169 |   1.3930 |     45.466 |     0.6
    8 |   1.4003 |     46.256 |   1.3901 |     45.956 |     0.7
    9 |   1.3970 |     46.110 |   1.3892 |     45.803 |     0.8
   10 |   1.3909 |     45.822 |   1.3831 |     45.374 |     0.9
   11 |   1.3834 |     45.270 |   1.3702 |     44.822 |     0.9
   12 |   1.3701 |     45.048 |   1.3605 |     44.485 |     1.0
   13 |   1.3634 |     44.571 |   1.3473 |     43.750 |     1.1
   14 |   1.3476 |     44.197 |   1.3396 |     43.352 |     1.2
   15 |   1.3412 |     44.007 |   1.3322 |     43.873 |     1.3
   16 |   1.3280 |     44.137 |   1.3224 |     43.750 |     1.4
   17 |   1.3218 |     44.002 |   1.3097 |     43.321 |     1.5
   18 |   1.3094 |     43.736 |   1.3113 |     43.352 |     1.5
   19 |   1.3043 |     43.476 |   1.2983 |     42.525 |     1.6
   20 |   1.2937 |     43.119 |   1.2972 |     42.371 |     1.7
   21 |   1.2904 |     43.021 |   1.2921 |     42.157 |     1.8
   22 |   1.2818 |     42.907 |   1.2800 |     41.850 |     1.9
   23 |   1.2743 |     42.669 |   1.2829 |     42.616 |     2.0
   24 |   1.2734 |     42.897 |   1.2806 |     42.126 |     2.1
   25 |   1.2742 |     42.756 |   1.2686 |     42.341 |     2.1
   26 |   1.2718 |     42.414 |   1.2721 |     42.494 |     2.2
   27 |   1.2651 |     42.241 |   1.2570 |     42.004 |     2.3
   28 |   1.2541 |     42.035 |   1.2573 |     41.360 |     2.4
   29 |   1.2462 |     41.656 |   1.2555 |     41.085 |     2.5
   30 |   1.2448 |     41.618 |   1.2545 |     40.962 |     2.6
   31 |   1.2329 |     41.347 |   1.2470 |     41.146 |     2.7
   32 |   1.2321 |     40.989 |   1.2375 |     40.441 |     2.8
   33 |   1.2345 |     41.428 |   1.2385 |     41.667 |     2.8
   34 |   1.2230 |     41.174 |   1.2335 |     41.085 |     2.9
   35 |   1.2173 |     40.908 |   1.2211 |     40.472 |     3.0
   36 |   1.2065 |     40.502 |   1.2243 |     40.533 |     3.1
   37 |   1.2032 |     40.404 |   1.2423 |     40.441 |     3.2
   38 |   1.2112 |     40.572 |   1.2162 |     40.502 |     3.3
   39 |   1.1943 |     40.166 |   1.2111 |     40.074 |     3.4
   40 |   1.1962 |     40.215 |   1.2193 |     40.104 |     3.4
   41 |   1.1928 |     40.003 |   1.2116 |     39.767 |     3.5
   42 |   1.1860 |     39.754 |   1.2026 |     39.859 |     3.6
   43 |   1.1774 |     39.662 |   1.1964 |     39.400 |     3.7
   44 |   1.1703 |     39.537 |   1.1884 |     39.737 |     3.8
   45 |   1.1689 |     39.673 |   1.1876 |     39.277 |     3.9
   46 |   1.1630 |     39.001 |   1.1839 |     38.848 |     4.0
   47 |   1.1583 |     39.104 |   1.1822 |     39.001 |     4.1
   48 |   1.1525 |     38.762 |   1.1706 |     38.297 |     4.1
   49 |   1.1448 |     38.670 |   1.1679 |     38.419 |     4.2
   50 |   1.1383 |     38.248 |   1.1748 |     38.940 |     4.3
   51 |   1.1307 |     38.009 |   1.1622 |     38.971 |     4.4
   52 |   1.1249 |     38.215 |   1.1713 |     39.185 |     4.5
   53 |   1.1194 |     37.809 |   1.1589 |     38.756 |     4.6
   54 |   1.1210 |     37.923 |   1.1521 |     38.143 |     4.7
   55 |   1.1200 |     37.765 |   1.1507 |     37.592 |     4.8
   56 |   1.1028 |     37.500 |   1.1433 |     37.500 |     4.8
   57 |   1.1004 |     37.218 |   1.1290 |     37.500 |     4.9
   58 |   1.0985 |     37.132 |   1.1569 |     38.235 |     5.0
   59 |   1.1012 |     37.300 |   1.1434 |     37.316 |     5.1
   60 |   1.0880 |     36.947 |   1.1427 |     37.806 |     5.2
   61 |   1.0898 |     37.050 |   1.1448 |     37.898 |     5.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 783,490

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1938 |     83.469 |   2.6769 |     70.558 |     0.0
    2 |   2.3917 |     60.528 |   2.1794 |     57.445 |     0.0
    3 |   2.0458 |     55.630 |   1.9551 |     54.013 |     0.1
    4 |   1.8758 |     49.686 |   1.8132 |     48.346 |     0.1
    5 |   1.7533 |     48.602 |   1.6982 |     48.376 |     0.1
    6 |   1.6580 |     48.607 |   1.6116 |     48.346 |     0.1
    7 |   1.5848 |     48.607 |   1.5484 |     46.814 |     0.1
    8 |   1.5295 |     46.240 |   1.5012 |     45.466 |     0.1
    9 |   1.4906 |     46.213 |   1.4697 |     45.496 |     0.2
   10 |   1.4616 |     46.326 |   1.4453 |     45.466 |     0.2
   11 |   1.4373 |     46.278 |   1.4219 |     45.466 |     0.2
   12 |   1.4168 |     46.272 |   1.4020 |     45.404 |     0.2
   13 |   1.3960 |     46.261 |   1.3852 |     45.037 |     0.2
   14 |   1.3756 |     45.519 |   1.3709 |     44.975 |     0.2
   15 |   1.3633 |     45.351 |   1.3597 |     44.455 |     0.3
   16 |   1.3469 |     45.042 |   1.3481 |     43.903 |     0.3
   17 |   1.3335 |     44.690 |   1.3400 |     43.413 |     0.3
   18 |   1.3246 |     44.300 |   1.3302 |     44.240 |     0.3
   19 |   1.3136 |     44.208 |   1.3259 |     43.566 |     0.3
   20 |   1.3038 |     44.007 |   1.3150 |     43.321 |     0.3
   21 |   1.2949 |     43.655 |   1.3110 |     43.382 |     0.4
   22 |   1.2843 |     43.281 |   1.3033 |     43.137 |     0.4
   23 |   1.2764 |     43.260 |   1.3001 |     42.647 |     0.4
   24 |   1.2667 |     42.848 |   1.2918 |     43.107 |     0.4
   25 |   1.2612 |     42.658 |   1.2869 |     42.892 |     0.4
   26 |   1.2513 |     42.295 |   1.2824 |     41.912 |     0.4
   27 |   1.2454 |     41.661 |   1.2791 |     41.697 |     0.5
   28 |   1.2334 |     41.455 |   1.2715 |     41.330 |     0.5
   29 |   1.2288 |     40.903 |   1.2682 |     41.391 |     0.5
   30 |   1.2170 |     40.545 |   1.2635 |     40.962 |     0.5
   31 |   1.2078 |     40.290 |   1.2561 |     40.809 |     0.5
   32 |   1.1927 |     40.133 |   1.2497 |     40.656 |     0.5
   33 |   1.1787 |     39.803 |   1.2428 |     40.441 |     0.6
   34 |   1.1694 |     39.489 |   1.2361 |     40.472 |     0.6
   35 |   1.1558 |     39.131 |   1.2306 |     40.288 |     0.6
   36 |   1.1442 |     38.985 |   1.2179 |     40.533 |     0.6
   37 |   1.1280 |     38.194 |   1.2108 |     40.257 |     0.6
   38 |   1.1163 |     37.711 |   1.2031 |     39.767 |     0.6
   39 |   1.1000 |     37.267 |   1.1946 |     39.430 |     0.7
   40 |   1.0892 |     36.628 |   1.1887 |     39.583 |     0.7
   41 |   1.0743 |     36.227 |   1.1878 |     38.542 |     0.7
   42 |   1.0600 |     35.696 |   1.1741 |     38.879 |     0.7
   43 |   1.0471 |     35.008 |   1.1702 |     38.603 |     0.7
   44 |   1.0357 |     34.645 |   1.1644 |     39.032 |     0.7
   45 |   1.0185 |     34.173 |   1.1643 |     38.664 |     0.8
   46 |   1.0055 |     33.550 |   1.1474 |     38.511 |     0.8
   47 |   0.9931 |     33.019 |   1.1556 |     37.592 |     0.8
   48 |   0.9851 |     32.808 |   1.1410 |     37.071 |     0.8
   49 |   0.9695 |     32.185 |   1.1357 |     37.500 |     0.8
   50 |   0.9572 |     31.876 |   1.1350 |     37.408 |     0.9
   51 |   0.9475 |     31.204 |   1.1592 |     38.480 |     0.9
   52 |   0.9309 |     30.819 |   1.1215 |     36.703 |     0.9
   53 |   0.9199 |     30.305 |   1.1215 |     37.408 |     0.9
   54 |   0.9085 |     29.616 |   1.1156 |     35.846 |     0.9
   55 |   0.9026 |     29.595 |   1.1328 |     36.949 |     0.9
   56 |   0.8893 |     28.890 |   1.1103 |     35.386 |     1.0
   57 |   0.8781 |     28.684 |   1.1099 |     35.846 |     1.0
   58 |   0.8693 |     28.159 |   1.1166 |     35.907 |     1.0
   59 |   0.8535 |     27.498 |   1.1063 |     35.692 |     1.0
   60 |   0.8478 |     27.297 |   1.1092 |     36.397 |     1.0
   61 |   0.8354 |     26.777 |   1.1018 |     34.681 |     1.0
   62 |   0.8245 |     26.322 |   1.0915 |     34.130 |     1.1
   63 |   0.8132 |     25.547 |   1.0869 |     34.161 |     1.1
   64 |   0.8064 |     25.488 |   1.0932 |     33.977 |     1.1
   65 |   0.7939 |     24.436 |   1.0862 |     33.885 |     1.1
   66 |   0.7853 |     24.247 |   1.0979 |     33.150 |     1.1
   67 |   0.7759 |     23.943 |   1.1047 |     33.333 |     1.1
   68 |   0.7674 |     23.651 |   1.0789 |     33.088 |     1.2
   69 |   0.7541 |     23.109 |   1.0997 |     33.885 |     1.2
   70 |   0.7452 |     23.011 |   1.0784 |     32.966 |     1.2
   71 |   0.7349 |     22.865 |   1.0765 |     32.812 |     1.2
   72 |   0.7274 |     22.426 |   1.0768 |     32.659 |     1.2
   73 |   0.7165 |     21.960 |   1.0727 |     32.292 |     1.2
   74 |   0.7081 |     21.727 |   1.0610 |     32.169 |     1.3
   75 |   0.6965 |     21.505 |   1.0716 |     31.955 |     1.3
   76 |   0.6901 |     21.223 |   1.0789 |     31.832 |     1.3
   77 |   0.6841 |     21.310 |   1.0613 |     31.618 |     1.3
   78 |   0.6711 |     20.665 |   1.0649 |     32.108 |     1.3
   79 |   0.6672 |     20.557 |   1.0577 |     31.495 |     1.3
   80 |   0.6578 |     20.292 |   1.0719 |     31.832 |     1.4
   81 |   0.6513 |     20.145 |   1.0634 |     31.710 |     1.4
   82 |   0.6440 |     19.798 |   1.0696 |     32.016 |     1.4
   83 |   0.6403 |     19.744 |   1.0693 |     30.974 |     1.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 449,922

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4739 |     90.269 |   3.2218 |     81.893 |     0.0
    2 |   2.8657 |     81.410 |   2.6798 |     81.893 |     0.0
    3 |   2.5512 |     64.174 |   2.4292 |     58.824 |     0.1
    4 |   2.3437 |     58.138 |   2.2552 |     58.824 |     0.1
    5 |   2.1999 |     57.152 |   2.1251 |     58.456 |     0.1
    6 |   2.0867 |     56.345 |   2.0159 |     54.320 |     0.1
    7 |   1.9833 |     54.557 |   1.9177 |     48.376 |     0.1
    8 |   1.8923 |     50.379 |   1.8366 |     48.376 |     0.1
    9 |   1.8177 |     49.269 |   1.7682 |     48.376 |     0.2
   10 |   1.7531 |     48.927 |   1.7057 |     48.346 |     0.2
   11 |   1.6941 |     48.716 |   1.6545 |     48.346 |     0.2
   12 |   1.6509 |     48.407 |   1.6127 |     48.346 |     0.2
   13 |   1.6083 |     46.830 |   1.5787 |     45.466 |     0.2
   14 |   1.5773 |     46.229 |   1.5483 |     45.466 |     0.2
   15 |   1.5489 |     46.223 |   1.5254 |     45.496 |     0.3
   16 |   1.5276 |     46.229 |   1.5072 |     45.466 |     0.3
   17 |   1.5116 |     46.213 |   1.4910 |     45.466 |     0.3
   18 |   1.4927 |     46.196 |   1.4776 |     45.466 |     0.3
   19 |   1.4807 |     46.115 |   1.4664 |     45.466 |     0.3
   20 |   1.4696 |     46.234 |   1.4567 |     45.466 |     0.4
   21 |   1.4614 |     46.180 |   1.4489 |     45.466 |     0.4
   22 |   1.4518 |     46.288 |   1.4433 |     45.466 |     0.4
   23 |   1.4460 |     46.158 |   1.4336 |     45.466 |     0.4
   24 |   1.4397 |     46.240 |   1.4278 |     45.466 |     0.4
   25 |   1.4327 |     46.023 |   1.4229 |     45.466 |     0.4
   26 |   1.4264 |     46.039 |   1.4175 |     45.312 |     0.5
   27 |   1.4205 |     45.844 |   1.4132 |     45.098 |     0.5
   28 |   1.4189 |     45.806 |   1.4074 |     44.792 |     0.5
   29 |   1.4112 |     45.606 |   1.4060 |     45.098 |     0.5
   30 |   1.4024 |     45.357 |   1.3994 |     44.730 |     0.5
   31 |   1.3961 |     45.346 |   1.3953 |     44.792 |     0.5
   32 |   1.3946 |     45.080 |   1.3900 |     44.792 |     0.6
   33 |   1.3859 |     45.156 |   1.3859 |     44.669 |     0.6
   34 |   1.3827 |     44.956 |   1.3820 |     44.730 |     0.6
   35 |   1.3742 |     45.053 |   1.3754 |     44.363 |     0.6
   36 |   1.3701 |     44.950 |   1.3720 |     45.129 |     0.6
   37 |   1.3620 |     45.042 |   1.3648 |     44.792 |     0.6
   38 |   1.3531 |     44.896 |   1.3594 |     44.669 |     0.7
   39 |   1.3485 |     44.777 |   1.3550 |     44.577 |     0.7
   40 |   1.3416 |     44.771 |   1.3478 |     44.577 |     0.7
   41 |   1.3333 |     44.565 |   1.3447 |     44.485 |     0.7
   42 |   1.3282 |     44.565 |   1.3425 |     44.516 |     0.7
   43 |   1.3194 |     44.295 |   1.3337 |     44.271 |     0.7
   44 |   1.3118 |     44.343 |   1.3281 |     44.363 |     0.8
   45 |   1.3046 |     44.078 |   1.3179 |     44.240 |     0.8
   46 |   1.2960 |     44.089 |   1.3096 |     44.210 |     0.8
   47 |   1.2894 |     44.018 |   1.3065 |     43.964 |     0.8
   48 |   1.2802 |     43.487 |   1.2967 |     43.597 |     0.8
   49 |   1.2757 |     43.493 |   1.2903 |     43.290 |     0.9
   50 |   1.2655 |     43.151 |   1.2841 |     43.015 |     0.9
   51 |   1.2598 |     42.577 |   1.2819 |     42.862 |     0.9
   52 |   1.2507 |     42.512 |   1.2772 |     43.015 |     0.9
   53 |   1.2470 |     42.084 |   1.2704 |     42.494 |     0.9
   54 |   1.2406 |     41.835 |   1.2663 |     42.494 |     0.9
   55 |   1.2361 |     41.824 |   1.2629 |     42.433 |     1.0
   56 |   1.2270 |     41.542 |   1.2589 |     42.096 |     1.0
   57 |   1.2217 |     41.233 |   1.2563 |     42.034 |     1.0
   58 |   1.2177 |     41.087 |   1.2507 |     42.218 |     1.0
   59 |   1.2101 |     40.773 |   1.2502 |     41.881 |     1.0
   60 |   1.2082 |     40.588 |   1.2409 |     41.330 |     1.0
   61 |   1.2020 |     40.621 |   1.2370 |     41.697 |     1.1
   62 |   1.1959 |     40.339 |   1.2366 |     41.789 |     1.1
   63 |   1.1899 |     40.372 |   1.2381 |     41.697 |     1.1
   64 |   1.1854 |     40.020 |   1.2372 |     41.636 |     1.1
   65 |   1.1808 |     39.992 |   1.2278 |     41.850 |     1.1
   66 |   1.1753 |     39.792 |   1.2214 |     40.686 |     1.1
   67 |   1.1690 |     39.456 |   1.2197 |     40.748 |     1.2
   68 |   1.1634 |     39.494 |   1.2170 |     40.993 |     1.2
   69 |   1.1614 |     39.407 |   1.2093 |     40.870 |     1.2
   70 |   1.1577 |     39.163 |   1.2091 |     40.839 |     1.2
   71 |   1.1505 |     39.071 |   1.2040 |     40.502 |     1.2
   72 |   1.1467 |     39.006 |   1.2044 |     40.809 |     1.2
   73 |   1.1419 |     38.827 |   1.1982 |     40.319 |     1.3
   74 |   1.1369 |     38.687 |   1.1988 |     40.135 |     1.3
   75 |   1.1316 |     38.605 |   1.1892 |     40.165 |     1.3
   76 |   1.1295 |     38.475 |   1.1899 |     40.135 |     1.3
   77 |   1.1216 |     38.280 |   1.1864 |     40.043 |     1.3
   78 |   1.1165 |     37.890 |   1.1802 |     39.583 |     1.3
   79 |   1.1126 |     37.728 |   1.1833 |     39.491 |     1.4
   80 |   1.1106 |     37.522 |   1.1776 |     38.971 |     1.4
   81 |   1.1049 |     37.256 |   1.1697 |     38.664 |     1.4
   82 |   1.0975 |     37.061 |   1.1676 |     38.634 |     1.4
   83 |   1.0951 |     36.747 |   1.1634 |     38.113 |     1.4
   84 |   1.0898 |     36.655 |   1.1626 |     38.205 |     1.5
   85 |   1.0827 |     36.042 |   1.1618 |     37.898 |     1.5
   86 |   1.0784 |     36.005 |   1.1569 |     37.898 |     1.5
   87 |   1.0733 |     35.804 |   1.1545 |     37.194 |     1.5
   88 |   1.0685 |     35.414 |   1.1578 |     37.469 |     1.5
   89 |   1.0636 |     35.560 |   1.1523 |     36.795 |     1.5
   90 |   1.0573 |     34.731 |   1.1452 |     37.071 |     1.6
   91 |   1.0542 |     34.813 |   1.1446 |     36.305 |     1.6
   92 |   1.0497 |     34.807 |   1.1441 |     36.765 |     1.6
   93 |   1.0418 |     34.525 |   1.1349 |     36.673 |     1.6
   94 |   1.0418 |     34.531 |   1.1352 |     36.642 |     1.6
   95 |   1.0331 |     34.217 |   1.1275 |     36.642 |     1.6
   96 |   1.0293 |     33.734 |   1.1333 |     36.336 |     1.7
   97 |   1.0242 |     33.702 |   1.1317 |     36.765 |     1.7
   98 |   1.0186 |     33.317 |   1.1267 |     35.846 |     1.7
   99 |   1.0193 |     33.344 |   1.1167 |     35.784 |     1.7
  100 |   1.0108 |     33.333 |   1.1192 |     36.121 |     1.7
  101 |   1.0070 |     32.949 |   1.1114 |     35.907 |     1.7
  102 |   1.0000 |     32.678 |   1.1126 |     35.815 |     1.8
  103 |   0.9958 |     32.412 |   1.1064 |     35.294 |     1.8
  104 |   0.9884 |     32.537 |   1.1117 |     35.600 |     1.8
  105 |   0.9900 |     32.396 |   1.1053 |     36.183 |     1.8
  106 |   0.9812 |     32.168 |   1.1039 |     35.846 |     1.8
  107 |   0.9762 |     31.968 |   1.0993 |     35.784 |     1.8
  108 |   0.9769 |     32.049 |   1.1009 |     35.600 |     1.9
  109 |   0.9690 |     31.730 |   1.0973 |     35.263 |     1.9
  110 |   0.9630 |     31.567 |   1.0929 |     35.202 |     1.9
  111 |   0.9604 |     31.269 |   1.0963 |     35.172 |     1.9
  112 |   0.9555 |     31.139 |   1.1037 |     35.233 |     1.9
  113 |   0.9537 |     30.895 |   1.0968 |     35.447 |     1.9
  114 |   0.9477 |     31.128 |   1.0902 |     34.835 |     2.0
  115 |   0.9452 |     31.025 |   1.0892 |     34.926 |     2.0
  116 |   0.9372 |     30.857 |   1.0854 |     35.355 |     2.0
  117 |   0.9330 |     30.359 |   1.0909 |     35.172 |     2.0
  118 |   0.9362 |     30.619 |   1.0796 |     35.172 |     2.0
  119 |   0.9300 |     30.435 |   1.0865 |     35.080 |     2.1
  120 |   0.9242 |     30.277 |   1.0822 |     35.447 |     2.1
  121 |   0.9223 |     30.137 |   1.0751 |     34.681 |     2.1
  122 |   0.9214 |     30.077 |   1.0793 |     35.325 |     2.1
  123 |   0.9169 |     29.925 |   1.0837 |     34.773 |     2.1
  124 |   0.9137 |     29.990 |   1.0785 |     34.865 |     2.1
  125 |   0.9065 |     29.584 |   1.0814 |     34.896 |     2.2
  126 |   0.9043 |     29.654 |   1.0718 |     34.620 |     2.2
  127 |   0.9012 |     29.373 |   1.0733 |     34.528 |     2.2
  128 |   0.9020 |     29.410 |   1.0734 |     34.559 |     2.2
  129 |   0.8951 |     29.286 |   1.0690 |     34.804 |     2.2
  130 |   0.8872 |     29.226 |   1.0655 |     34.498 |     2.2
  131 |   0.8863 |     29.107 |   1.0716 |     34.804 |     2.3
  132 |   0.8854 |     28.804 |   1.0676 |     34.161 |     2.3
  133 |   0.8796 |     28.771 |   1.0719 |     34.559 |     2.3
  134 |   0.8763 |     28.571 |   1.0746 |     34.344 |     2.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,610,242

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3975 |     92.832 |   3.2208 |     83.578 |     0.1
    2 |   3.0816 |     83.328 |   2.9524 |     83.578 |     0.1
    3 |   2.8576 |     80.456 |   2.7834 |     70.619 |     0.2
    4 |   2.7164 |     65.540 |   2.6668 |     65.778 |     0.3
    5 |   2.6111 |     64.288 |   2.5684 |     66.881 |     0.3
    6 |   2.5225 |     64.001 |   2.4850 |     59.804 |     0.4
    7 |   2.4490 |     61.850 |   2.4138 |     59.252 |     0.4
    8 |   2.3783 |     60.219 |   2.3424 |     58.824 |     0.5
    9 |   2.3173 |     58.680 |   2.2875 |     58.824 |     0.6
   10 |   2.2644 |     58.165 |   2.2375 |     58.732 |     0.6
   11 |   2.2193 |     57.618 |   2.1944 |     56.955 |     0.7
   12 |   2.1787 |     57.147 |   2.1544 |     57.812 |     0.8
   13 |   2.1386 |     56.767 |   2.1146 |     57.812 |     0.8
   14 |   2.1027 |     56.572 |   2.0766 |     57.138 |     0.9
   15 |   2.0666 |     56.356 |   2.0401 |     55.453 |     1.0
   16 |   2.0310 |     55.955 |   2.0061 |     54.688 |     1.0
   17 |   1.9992 |     55.613 |   1.9711 |     55.178 |     1.1
   18 |   1.9620 |     55.196 |   1.9344 |     54.197 |     1.1
   19 |   1.9250 |     54.264 |   1.8966 |     52.237 |     1.2
   20 |   1.8843 |     52.362 |   1.8570 |     49.234 |     1.3
   21 |   1.8441 |     50.840 |   1.8191 |     48.407 |     1.3
   22 |   1.8054 |     49.680 |   1.7796 |     48.836 |     1.4
   23 |   1.7683 |     49.274 |   1.7438 |     48.499 |     1.5
   24 |   1.7334 |     49.204 |   1.7116 |     48.438 |     1.5
   25 |   1.7032 |     48.629 |   1.6836 |     48.468 |     1.6
   26 |   1.6759 |     47.480 |   1.6558 |     45.711 |     1.6
   27 |   1.6471 |     46.651 |   1.6305 |     45.496 |     1.7
   28 |   1.6239 |     46.516 |   1.6076 |     45.619 |     1.8
   29 |   1.6006 |     46.364 |   1.5867 |     45.619 |     1.8
   30 |   1.5806 |     46.419 |   1.5674 |     45.619 |     1.9
   31 |   1.5603 |     46.348 |   1.5484 |     45.650 |     2.0
   32 |   1.5423 |     46.164 |   1.5318 |     45.496 |     2.0
   33 |   1.5248 |     45.936 |   1.5189 |     45.466 |     2.1
   34 |   1.5134 |     45.963 |   1.5043 |     45.312 |     2.2
   35 |   1.4947 |     45.676 |   1.4917 |     45.190 |     2.2
   36 |   1.4852 |     45.606 |   1.4800 |     45.006 |     2.3
   37 |   1.4693 |     45.600 |   1.4699 |     45.221 |     2.3
   38 |   1.4574 |     45.357 |   1.4590 |     44.761 |     2.4
   39 |   1.4467 |     45.189 |   1.4490 |     44.608 |     2.5
   40 |   1.4399 |     45.069 |   1.4397 |     44.608 |     2.5
   41 |   1.4265 |     45.053 |   1.4313 |     44.240 |     2.6
   42 |   1.4166 |     44.972 |   1.4224 |     43.995 |     2.7
   43 |   1.4097 |     44.690 |   1.4133 |     43.903 |     2.7
   44 |   1.3992 |     44.603 |   1.4072 |     43.903 |     2.8
   45 |   1.3909 |     44.408 |   1.4001 |     43.750 |     2.9
   46 |   1.3851 |     44.164 |   1.3939 |     43.719 |     2.9
   47 |   1.3745 |     44.089 |   1.3858 |     43.903 |     3.0
   48 |   1.3653 |     43.829 |   1.3791 |     43.321 |     3.0
   49 |   1.3568 |     43.563 |   1.3746 |     43.719 |     3.1
   50 |   1.3545 |     43.308 |   1.3693 |     43.505 |     3.2
   51 |   1.3435 |     43.021 |   1.3637 |     43.290 |     3.2
   52 |   1.3355 |     42.929 |   1.3576 |     43.199 |     3.3
   53 |   1.3304 |     42.485 |   1.3536 |     42.770 |     3.4
   54 |   1.3258 |     42.322 |   1.3475 |     42.249 |     3.4
   55 |   1.3186 |     42.257 |   1.3456 |     42.402 |     3.5
   56 |   1.3097 |     41.948 |   1.3394 |     42.279 |     3.5
   57 |   1.3062 |     41.656 |   1.3352 |     41.850 |     3.6
   58 |   1.3028 |     41.645 |   1.3293 |     42.034 |     3.7
   59 |   1.2966 |     41.477 |   1.3255 |     41.789 |     3.7
   60 |   1.2866 |     41.168 |   1.3219 |     42.065 |     3.8
   61 |   1.2839 |     41.195 |   1.3142 |     41.513 |     3.9
   62 |   1.2772 |     40.935 |   1.3105 |     41.268 |     3.9
   63 |   1.2695 |     40.632 |   1.3067 |     41.360 |     4.0
   64 |   1.2639 |     40.350 |   1.3029 |     40.748 |     4.1
   65 |   1.2558 |     39.933 |   1.2978 |     40.686 |     4.1
   66 |   1.2555 |     39.987 |   1.2950 |     40.778 |     4.2
   67 |   1.2463 |     39.629 |   1.2944 |     40.380 |     4.2
   68 |   1.2437 |     39.591 |   1.2879 |     40.594 |     4.3
   69 |   1.2405 |     39.608 |   1.2863 |     40.441 |     4.4
   70 |   1.2288 |     39.033 |   1.2805 |     40.533 |     4.4
   71 |   1.2265 |     38.958 |   1.2788 |     40.165 |     4.5
   72 |   1.2182 |     38.670 |   1.2750 |     39.951 |     4.6
   73 |   1.2114 |     38.757 |   1.2706 |     39.798 |     4.6
   74 |   1.2050 |     38.259 |   1.2675 |     39.430 |     4.7
   75 |   1.1981 |     38.009 |   1.2642 |     39.614 |     4.7
   76 |   1.1967 |     37.901 |   1.2621 |     39.369 |     4.8
   77 |   1.1906 |     37.695 |   1.2577 |     39.400 |     4.9
   78 |   1.1873 |     37.267 |   1.2550 |     39.491 |     4.9
   79 |   1.1822 |     37.430 |   1.2523 |     39.430 |     5.0
   80 |   1.1716 |     36.974 |   1.2490 |     38.940 |     5.1
   81 |   1.1739 |     37.018 |   1.2453 |     39.338 |     5.1
   82 |   1.1642 |     36.741 |   1.2434 |     39.001 |     5.2
   83 |   1.1643 |     36.844 |   1.2371 |     38.695 |     5.3
   84 |   1.1519 |     36.492 |   1.2373 |     38.909 |     5.3
   85 |   1.1462 |     36.313 |   1.2316 |     38.756 |     5.4
   86 |   1.1434 |     36.395 |   1.2316 |     38.848 |     5.4
   87 |   1.1412 |     35.934 |   1.2276 |     38.603 |     5.5
   88 |   1.1333 |     36.292 |   1.2221 |     38.235 |     5.6
   89 |   1.1296 |     35.880 |   1.2215 |     38.695 |     5.6
   90 |   1.1271 |     35.381 |   1.2200 |     38.297 |     5.7
   91 |   1.1207 |     35.468 |   1.2204 |     38.297 |     5.8
   92 |   1.1106 |     35.127 |   1.2123 |     37.684 |     5.8
   93 |   1.1086 |     35.246 |   1.2130 |     38.205 |     5.9
   94 |   1.1024 |     34.780 |   1.2073 |     37.837 |     5.9
   95 |   1.0989 |     34.850 |   1.2067 |     37.776 |     6.0
   96 |   1.0959 |     34.682 |   1.2013 |     37.868 |     6.1
   97 |   1.0889 |     34.585 |   1.1997 |     37.316 |     6.1
   98 |   1.0854 |     34.384 |   1.1976 |     37.102 |     6.2
   99 |   1.0770 |     34.021 |   1.1946 |     37.898 |     6.3
  100 |   1.0771 |     34.005 |   1.1945 |     37.898 |     6.3
  101 |   1.0713 |     33.875 |   1.1925 |     37.623 |     6.4
  102 |   1.0698 |     34.065 |   1.1914 |     37.469 |     6.5
  103 |   1.0675 |     33.648 |   1.1890 |     37.623 |     6.5
  104 |   1.0584 |     33.523 |   1.1896 |     37.898 |     6.6
  105 |   1.0576 |     33.436 |   1.1846 |     37.102 |     6.6
  106 |   1.0499 |     33.030 |   1.1821 |     37.224 |     6.7
  107 |   1.0479 |     33.138 |   1.1798 |     37.745 |     6.8
  108 |   1.0465 |     32.846 |   1.1784 |     37.010 |     6.8
  109 |   1.0405 |     32.786 |   1.1794 |     37.469 |     6.9
  110 |   1.0356 |     32.531 |   1.1740 |     37.653 |     7.0
  111 |   1.0275 |     32.624 |   1.1744 |     37.255 |     7.0
  112 |   1.0223 |     32.461 |   1.1761 |     37.071 |     7.1
  113 |   1.0257 |     32.439 |   1.1732 |     37.439 |     7.2
  114 |   1.0159 |     32.347 |   1.1674 |     36.918 |     7.2
  115 |   1.0136 |     31.963 |   1.1680 |     36.857 |     7.3
  116 |   1.0072 |     31.968 |   1.1658 |     36.397 |     7.3
  117 |   1.0092 |     32.033 |   1.1669 |     36.336 |     7.4
  118 |   1.0059 |     31.773 |   1.1606 |     36.428 |     7.5
  119 |   0.9987 |     31.621 |   1.1645 |     36.428 |     7.5
  120 |   0.9970 |     31.231 |   1.1662 |     36.397 |     7.6
  121 |   0.9908 |     31.318 |   1.1616 |     36.366 |     7.7
  122 |   0.9885 |     31.150 |   1.1585 |     35.876 |     7.7
  123 |   0.9860 |     30.955 |   1.1529 |     35.662 |     7.8
  124 |   0.9821 |     30.890 |   1.1523 |     35.478 |     7.8
  125 |   0.9787 |     31.085 |   1.1461 |     35.294 |     7.9
  126 |   0.9726 |     30.722 |   1.1490 |     35.141 |     8.0
  127 |   0.9694 |     30.765 |   1.1533 |     35.233 |     8.0
  128 |   0.9681 |     30.743 |   1.1486 |     35.509 |     8.1
  129 |   0.9616 |     30.597 |   1.1492 |     35.509 |     8.2
  130 |   0.9603 |     30.310 |   1.1461 |     35.386 |     8.2
  131 |   0.9517 |     30.050 |   1.1457 |     35.233 |     8.3
  132 |   0.9497 |     30.153 |   1.1451 |     35.049 |     8.4
  133 |   0.9454 |     30.017 |   1.1537 |     35.386 |     8.4
  134 |   0.9471 |     29.600 |   1.1443 |     35.233 |     8.5
  135 |   0.9432 |     29.806 |   1.1409 |     34.804 |     8.5
  136 |   0.9410 |     29.643 |   1.1382 |     35.202 |     8.6
  137 |   0.9395 |     29.405 |   1.1345 |     34.712 |     8.7
  138 |   0.9337 |     29.410 |   1.1371 |     34.804 |     8.7
  139 |   0.9288 |     29.075 |   1.1361 |     34.406 |     8.8
  140 |   0.9226 |     29.031 |   1.1331 |     34.926 |     8.9
  141 |   0.9232 |     28.684 |   1.1310 |     34.222 |     8.9
  142 |   0.9218 |     28.972 |   1.1336 |     34.651 |     9.0
  143 |   0.9164 |     28.869 |   1.1294 |     34.314 |     9.0
  144 |   0.9162 |     28.538 |   1.1285 |     34.436 |     9.1
  145 |   0.9126 |     28.836 |   1.1260 |     34.406 |     9.2
  146 |   0.9047 |     28.446 |   1.1259 |     34.344 |     9.2
  147 |   0.9184 |     28.917 |   1.1263 |     34.406 |     9.3
  148 |   0.8967 |     28.181 |   1.1272 |     34.222 |     9.4
  149 |   0.9008 |     28.446 |   1.1251 |     34.007 |     9.4
  150 |   0.8950 |     28.137 |   1.1273 |     34.069 |     9.5
  151 |   0.8950 |     28.007 |   1.1229 |     34.559 |     9.6
  152 |   0.8876 |     28.007 |   1.1280 |     34.130 |     9.6
  153 |   0.8815 |     27.606 |   1.1245 |     34.467 |     9.7
  154 |   0.8814 |     27.666 |   1.1189 |     34.099 |     9.7
  155 |   0.8795 |     27.200 |   1.1218 |     33.824 |     9.8
  156 |   0.8745 |     27.417 |   1.1154 |     33.977 |     9.9
  157 |   0.8747 |     26.913 |   1.1171 |     34.283 |     9.9
  158 |   0.8675 |     27.010 |   1.1197 |     33.487 |    10.0
  159 |   0.8730 |     27.173 |   1.1221 |     34.099 |    10.1
  160 |   0.8649 |     26.934 |   1.1200 |     33.854 |    10.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 306,626

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4769 |     92.832 |   3.3867 |     86.826 |     0.0
    2 |   3.1622 |     77.167 |   2.9476 |     77.819 |     0.0
    3 |   2.8124 |     72.692 |   2.6973 |     62.010 |     0.1
    4 |   2.6033 |     60.452 |   2.5170 |     58.915 |     0.1
    5 |   2.4476 |     59.070 |   2.3855 |     58.793 |     0.1
    6 |   2.3341 |     58.675 |   2.2848 |     58.609 |     0.1
    7 |   2.2454 |     58.501 |   2.2045 |     58.670 |     0.2
    8 |   2.1744 |     58.057 |   2.1381 |     58.578 |     0.2
    9 |   2.1118 |     57.569 |   2.0795 |     57.077 |     0.2
   10 |   2.0601 |     56.383 |   2.0282 |     55.944 |     0.2
   11 |   2.0118 |     54.188 |   1.9810 |     50.735 |     0.3
   12 |   1.9687 |     51.441 |   1.9363 |     48.928 |     0.3
   13 |   1.9252 |     50.135 |   1.8929 |     48.652 |     0.3
   14 |   1.8796 |     49.377 |   1.8513 |     48.192 |     0.3
   15 |   1.8442 |     49.090 |   1.8110 |     48.070 |     0.3
   16 |   1.8006 |     48.743 |   1.7730 |     48.039 |     0.4
   17 |   1.7640 |     48.586 |   1.7379 |     48.070 |     0.4
   18 |   1.7271 |     48.358 |   1.7052 |     48.039 |     0.4
   19 |   1.6993 |     48.369 |   1.6764 |     48.039 |     0.4
   20 |   1.6686 |     48.293 |   1.6502 |     48.009 |     0.5
   21 |   1.6452 |     48.277 |   1.6267 |     48.009 |     0.5
   22 |   1.6239 |     48.179 |   1.6049 |     48.039 |     0.5
   23 |   1.6010 |     48.039 |   1.5837 |     47.978 |     0.5
   24 |   1.5779 |     47.318 |   1.5647 |     45.374 |     0.5
   25 |   1.5589 |     46.272 |   1.5476 |     45.129 |     0.6
   26 |   1.5435 |     45.855 |   1.5312 |     45.159 |     0.6
   27 |   1.5251 |     45.839 |   1.5162 |     45.098 |     0.6
   28 |   1.5111 |     45.779 |   1.5031 |     45.098 |     0.6
   29 |   1.4966 |     45.665 |   1.4908 |     45.098 |     0.7
   30 |   1.4828 |     45.709 |   1.4791 |     45.129 |     0.7
   31 |   1.4716 |     45.709 |   1.4683 |     44.792 |     0.7
   32 |   1.4596 |     45.682 |   1.4574 |     45.558 |     0.7
   33 |   1.4477 |     45.562 |   1.4473 |     45.558 |     0.8
   34 |   1.4395 |     45.438 |   1.4374 |     45.404 |     0.8
   35 |   1.4290 |     45.384 |   1.4295 |     45.374 |     0.8
   36 |   1.4193 |     45.362 |   1.4207 |     45.404 |     0.8
   37 |   1.4093 |     45.118 |   1.4145 |     45.282 |     0.8
   38 |   1.4017 |     45.075 |   1.4045 |     45.374 |     0.9
   39 |   1.3960 |     45.004 |   1.3983 |     44.884 |     0.9
   40 |   1.3853 |     44.988 |   1.3899 |     45.067 |     0.9
   41 |   1.3775 |     44.728 |   1.3833 |     45.251 |     0.9
   42 |   1.3693 |     44.690 |   1.3771 |     45.190 |     1.0
   43 |   1.3609 |     44.403 |   1.3712 |     44.914 |     1.0
   44 |   1.3527 |     44.354 |   1.3656 |     45.282 |     1.0
   45 |   1.3467 |     44.137 |   1.3590 |     44.853 |     1.0
   46 |   1.3387 |     43.991 |   1.3546 |     44.975 |     1.0
   47 |   1.3323 |     43.866 |   1.3491 |     44.700 |     1.1
   48 |   1.3261 |     43.612 |   1.3409 |     44.148 |     1.1
   49 |   1.3180 |     43.471 |   1.3373 |     44.393 |     1.1
   50 |   1.3099 |     42.994 |   1.3316 |     43.566 |     1.1
   51 |   1.3067 |     42.788 |   1.3260 |     43.045 |     1.2
   52 |   1.2973 |     42.707 |   1.3210 |     43.811 |     1.2
   53 |   1.2920 |     42.534 |   1.3157 |     42.555 |     1.2
   54 |   1.2870 |     42.268 |   1.3126 |     42.647 |     1.2
   55 |   1.2781 |     41.883 |   1.3090 |     43.076 |     1.3
   56 |   1.2735 |     41.938 |   1.3054 |     42.188 |     1.3
   57 |   1.2669 |     41.347 |   1.2998 |     42.494 |     1.3
   58 |   1.2627 |     41.044 |   1.2975 |     41.697 |     1.3
   59 |   1.2546 |     40.453 |   1.2921 |     41.605 |     1.3
   60 |   1.2479 |     40.393 |   1.2891 |     41.728 |     1.4
   61 |   1.2389 |     40.258 |   1.2823 |     41.054 |     1.4
   62 |   1.2368 |     39.873 |   1.2831 |     41.299 |     1.4
   63 |   1.2288 |     39.564 |   1.2753 |     40.931 |     1.4
   64 |   1.2229 |     39.196 |   1.2693 |     40.441 |     1.5
   65 |   1.2175 |     39.077 |   1.2674 |     40.380 |     1.5
   66 |   1.2103 |     38.995 |   1.2630 |     39.951 |     1.5
   67 |   1.2096 |     38.551 |   1.2605 |     39.614 |     1.5
   68 |   1.2003 |     38.324 |   1.2554 |     39.920 |     1.5
   69 |   1.1995 |     38.275 |   1.2545 |     40.012 |     1.6
   70 |   1.1882 |     37.961 |   1.2477 |     39.093 |     1.6
   71 |   1.1852 |     37.711 |   1.2423 |     39.093 |     1.6
   72 |   1.1770 |     37.614 |   1.2451 |     38.971 |     1.6
   73 |   1.1709 |     37.348 |   1.2382 |     39.001 |     1.7
   74 |   1.1642 |     37.305 |   1.2342 |     39.001 |     1.7
   75 |   1.1604 |     37.002 |   1.2339 |     38.940 |     1.7
   76 |   1.1565 |     36.915 |   1.2298 |     38.940 |     1.7
   77 |   1.1523 |     36.671 |   1.2240 |     38.817 |     1.7
   78 |   1.1456 |     36.476 |   1.2208 |     38.848 |     1.8
   79 |   1.1393 |     36.438 |   1.2180 |     38.480 |     1.8
   80 |   1.1369 |     36.091 |   1.2127 |     38.603 |     1.8
   81 |   1.1299 |     35.885 |   1.2112 |     38.480 |     1.8
   82 |   1.1289 |     35.902 |   1.2098 |     38.511 |     1.9
   83 |   1.1266 |     35.707 |   1.2074 |     38.695 |     1.9
   84 |   1.1199 |     35.522 |   1.2015 |     38.419 |     1.9
   85 |   1.1133 |     35.376 |   1.2014 |     38.542 |     1.9
   86 |   1.1063 |     35.279 |   1.2001 |     38.388 |     2.0
   87 |   1.1028 |     35.262 |   1.1929 |     38.358 |     2.0
   88 |   1.0981 |     34.883 |   1.1900 |     38.082 |     2.0
   89 |   1.0917 |     34.731 |   1.1875 |     38.235 |     2.0
   90 |   1.0891 |     34.742 |   1.1866 |     38.174 |     2.0
   91 |   1.0802 |     34.563 |   1.1829 |     37.531 |     2.1
   92 |   1.0898 |     34.856 |   1.1761 |     37.684 |     2.1
   93 |   1.0742 |     34.173 |   1.1748 |     37.745 |     2.1
   94 |   1.0709 |     34.341 |   1.1754 |     38.143 |     2.1
   95 |   1.0638 |     34.157 |   1.1703 |     37.714 |     2.2
   96 |   1.0608 |     33.886 |   1.1651 |     37.653 |     2.2
   97 |   1.0563 |     33.496 |   1.1614 |     37.439 |     2.2
   98 |   1.0546 |     33.621 |   1.1645 |     37.623 |     2.2
   99 |   1.0452 |     33.328 |   1.1629 |     37.684 |     2.2
  100 |   1.0394 |     33.393 |   1.1578 |     37.500 |     2.3
  101 |   1.0385 |     33.382 |   1.1603 |     37.469 |     2.3
  102 |   1.0344 |     33.046 |   1.1543 |     36.857 |     2.3
  103 |   1.0269 |     32.786 |   1.1542 |     37.439 |     2.3
  104 |   1.0219 |     33.030 |   1.1499 |     37.194 |     2.4
  105 |   1.0203 |     33.052 |   1.1495 |     37.163 |     2.4
  106 |   1.0144 |     32.835 |   1.1437 |     36.979 |     2.4
  107 |   1.0133 |     32.526 |   1.1417 |     37.102 |     2.4
  108 |   1.0104 |     32.515 |   1.1406 |     37.010 |     2.4
  109 |   1.0043 |     32.277 |   1.1349 |     36.581 |     2.5
  110 |   1.0000 |     32.309 |   1.1360 |     37.255 |     2.5
  111 |   0.9976 |     32.087 |   1.1344 |     37.132 |     2.5
  112 |   0.9891 |     31.892 |   1.1271 |     36.826 |     2.5
  113 |   0.9844 |     31.735 |   1.1244 |     36.703 |     2.6
  114 |   0.9787 |     31.534 |   1.1228 |     36.275 |     2.6
  115 |   0.9735 |     31.383 |   1.1215 |     36.336 |     2.6
  116 |   0.9719 |     31.274 |   1.1209 |     36.213 |     2.6
  117 |   0.9721 |     31.681 |   1.1151 |     36.275 |     2.6
  118 |   0.9664 |     30.938 |   1.1197 |     36.428 |     2.7
  119 |   0.9630 |     30.689 |   1.1140 |     35.907 |     2.7
  120 |   0.9577 |     30.548 |   1.1133 |     36.029 |     2.7
  121 |   0.9534 |     30.353 |   1.1130 |     36.183 |     2.7
  122 |   0.9514 |     30.380 |   1.1058 |     35.478 |     2.8
  123 |   0.9475 |     29.963 |   1.1053 |     35.233 |     2.8
  124 |   0.9422 |     30.082 |   1.1025 |     35.417 |     2.8
  125 |   0.9409 |     29.611 |   1.0997 |     34.896 |     2.8
  126 |   0.9328 |     29.438 |   1.0968 |     34.957 |     2.8
  127 |   0.9340 |     29.606 |   1.0991 |     34.865 |     2.9
  128 |   0.9264 |     29.264 |   1.0924 |     34.773 |     2.9
  129 |   0.9205 |     28.744 |   1.0922 |     34.773 |     2.9
  130 |   0.9232 |     29.037 |   1.0912 |     34.344 |     2.9
  131 |   0.9141 |     28.668 |   1.0868 |     34.191 |     3.0
  132 |   0.9124 |     28.479 |   1.0832 |     34.007 |     3.0
  133 |   0.9082 |     28.083 |   1.0890 |     34.589 |     3.0
  134 |   0.9047 |     28.034 |   1.0801 |     34.161 |     3.0
  135 |   0.8990 |     27.985 |   1.0816 |     34.252 |     3.1
  136 |   0.8963 |     27.883 |   1.0768 |     34.467 |     3.1
  137 |   0.8899 |     27.574 |   1.0745 |     34.069 |     3.1
  138 |   0.8877 |     27.520 |   1.0728 |     34.069 |     3.1
  139 |   0.8856 |     27.454 |   1.0699 |     33.732 |     3.1
  140 |   0.8808 |     27.189 |   1.0677 |     33.609 |     3.2
  141 |   0.8786 |     27.216 |   1.0607 |     33.425 |     3.2
  142 |   0.8764 |     27.091 |   1.0692 |     33.578 |     3.2
  143 |   0.8753 |     27.189 |   1.0600 |     33.670 |     3.2
  144 |   0.8707 |     27.124 |   1.0641 |     33.364 |     3.3
  145 |   0.8669 |     26.723 |   1.0629 |     33.609 |     3.3
  146 |   0.8612 |     26.620 |   1.0603 |     34.069 |     3.3
  147 |   0.8573 |     26.625 |   1.0559 |     33.517 |     3.3
  148 |   0.8565 |     26.132 |   1.0611 |     33.609 |     3.3
  149 |   0.8485 |     25.997 |   1.0558 |     33.517 |     3.4
  150 |   0.8489 |     26.241 |   1.0539 |     33.578 |     3.4
  151 |   0.8462 |     26.165 |   1.0538 |     33.180 |     3.4
  152 |   0.8419 |     26.013 |   1.0455 |     33.211 |     3.4
  153 |   0.8398 |     26.073 |   1.0508 |     33.701 |     3.5
  154 |   0.8325 |     25.520 |   1.0438 |     33.180 |     3.5
  155 |   0.8333 |     25.715 |   1.0469 |     33.456 |     3.5
  156 |   0.8267 |     25.558 |   1.0453 |     33.211 |     3.5
  157 |   0.8273 |     25.829 |   1.0473 |     33.487 |     3.5
  158 |   0.8224 |     25.536 |   1.0386 |     32.843 |     3.6
  159 |   0.8280 |     25.818 |   1.0394 |     32.996 |     3.6
  160 |   0.8157 |     25.081 |   1.0385 |     32.935 |     3.6
  161 |   0.8152 |     25.195 |   1.0411 |     32.935 |     3.6
  162 |   0.8114 |     25.320 |   1.0365 |     33.119 |     3.7
  163 |   0.8097 |     25.217 |   1.0376 |     32.659 |     3.7
  164 |   0.8071 |     25.065 |   1.0388 |     33.088 |     3.7
  165 |   0.8015 |     24.724 |   1.0323 |     32.537 |     3.7
  166 |   0.7980 |     24.881 |   1.0327 |     32.506 |     3.7
  167 |   0.7954 |     24.447 |   1.0383 |     32.629 |     3.8
  168 |   0.7905 |     24.344 |   1.0382 |     32.506 |     3.8
  169 |   0.7918 |     24.659 |   1.0268 |     31.924 |     3.8
  170 |   0.7887 |     24.496 |   1.0266 |     32.169 |     3.8
  171 |   0.7834 |     24.187 |   1.0224 |     31.863 |     3.9
  172 |   0.7770 |     24.144 |   1.0304 |     32.261 |     3.9
  173 |   0.7802 |     24.204 |   1.0300 |     32.169 |     3.9
  174 |   0.7778 |     23.971 |   1.0219 |     31.955 |     3.9
  175 |   0.7725 |     24.030 |   1.0242 |     32.138 |     3.9
  176 |   0.7696 |     23.808 |   1.0287 |     31.955 |     4.0
  177 |   0.7699 |     23.732 |   1.0222 |     31.679 |     4.0
  178 |   0.7678 |     23.884 |   1.0243 |     31.679 |     4.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 974,306

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2655 |     62.554 |   1.6518 |     48.591 |     0.0
    2 |   1.4982 |     46.484 |   1.4263 |     45.466 |     0.0
    3 |   1.4084 |     46.045 |   1.3840 |     44.547 |     0.1
    4 |   1.3694 |     44.652 |   1.3463 |     44.210 |     0.1
    5 |   1.3350 |     44.435 |   1.3168 |     43.627 |     0.1
    6 |   1.3006 |     43.417 |   1.2771 |     42.004 |     0.1
    7 |   1.2526 |     42.127 |   1.2541 |     41.820 |     0.2
    8 |   1.2150 |     40.989 |   1.2202 |     40.319 |     0.2
    9 |   1.1849 |     40.101 |   1.1895 |     40.165 |     0.2
   10 |   1.1546 |     39.125 |   1.1684 |     39.154 |     0.2
   11 |   1.1168 |     37.933 |   1.1426 |     38.021 |     0.3
   12 |   1.0897 |     36.871 |   1.1381 |     38.511 |     0.3
   13 |   1.0558 |     35.658 |   1.1198 |     36.765 |     0.3
   14 |   1.0276 |     34.428 |   1.0724 |     35.172 |     0.3
   15 |   0.9796 |     32.775 |   1.0564 |     35.263 |     0.4
   16 |   0.9486 |     31.356 |   1.0370 |     33.241 |     0.4
   17 |   0.9108 |     30.478 |   1.0122 |     33.548 |     0.4
   18 |   0.8868 |     29.768 |   1.0028 |     33.119 |     0.4
   19 |   0.8573 |     28.424 |   0.9982 |     32.445 |     0.5
   20 |   0.8353 |     27.839 |   0.9976 |     32.567 |     0.5
   21 |   0.7981 |     26.479 |   0.9790 |     31.893 |     0.5
   22 |   0.7643 |     25.255 |   0.9643 |     31.434 |     0.5
   23 |   0.7381 |     24.323 |   0.9635 |     30.699 |     0.5
   24 |   0.7255 |     24.236 |   0.9628 |     31.311 |     0.6
   25 |   0.7100 |     23.618 |   0.9400 |     29.626 |     0.6
   26 |   0.6774 |     22.703 |   0.9349 |     29.718 |     0.6
   27 |   0.6503 |     21.381 |   0.9560 |     29.810 |     0.6
   28 |   0.6279 |     20.530 |   0.9288 |     29.473 |     0.7
   29 |   0.5949 |     20.015 |   0.9311 |     29.013 |     0.7
   30 |   0.5755 |     19.007 |   0.9396 |     30.147 |     0.7
   31 |   0.5521 |     18.476 |   0.9591 |     29.075 |     0.7
   32 |   0.5313 |     17.875 |   0.9735 |     29.259 |     0.8
   33 |   0.5521 |     18.747 |   0.9269 |     28.707 |     0.8
   34 |   0.4957 |     16.726 |   0.9768 |     28.033 |     0.8
   35 |   0.4903 |     16.174 |   0.9507 |     28.033 |     0.8
   36 |   0.4722 |     15.805 |   0.9515 |     28.554 |     0.9
   37 |   0.4421 |     14.808 |   0.9474 |     27.328 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 274,242

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6800 |     70.915 |   2.1027 |     58.824 |     0.0
    2 |   1.8502 |     52.352 |   1.6291 |     48.346 |     0.0
    3 |   1.5461 |     46.657 |   1.4705 |     45.466 |     0.0
    4 |   1.4554 |     46.256 |   1.4270 |     45.987 |     0.1
    5 |   1.4270 |     46.267 |   1.4107 |     45.466 |     0.1
    6 |   1.4130 |     46.196 |   1.4020 |     45.466 |     0.1
    7 |   1.4046 |     46.359 |   1.3986 |     45.803 |     0.1
    8 |   1.4012 |     46.310 |   1.3937 |     45.466 |     0.1
    9 |   1.3953 |     46.240 |   1.3875 |     45.466 |     0.1
   10 |   1.3842 |     46.234 |   1.3745 |     46.017 |     0.1
   11 |   1.3672 |     46.121 |   1.3492 |     44.669 |     0.2
   12 |   1.3474 |     45.768 |   1.3355 |     44.884 |     0.2
   13 |   1.3278 |     45.627 |   1.3186 |     44.179 |     0.2
   14 |   1.3113 |     45.329 |   1.3101 |     43.842 |     0.2
   15 |   1.3022 |     45.069 |   1.2965 |     44.056 |     0.2
   16 |   1.2850 |     44.354 |   1.2889 |     44.179 |     0.2
   17 |   1.2719 |     43.915 |   1.2734 |     43.076 |     0.2
   18 |   1.2563 |     43.195 |   1.2583 |     42.739 |     0.3
   19 |   1.2403 |     42.642 |   1.2547 |     43.444 |     0.3
   20 |   1.2246 |     42.171 |   1.2359 |     41.912 |     0.3
   21 |   1.2117 |     42.219 |   1.2282 |     42.004 |     0.3
   22 |   1.2007 |     41.439 |   1.2136 |     42.494 |     0.3
   23 |   1.1760 |     40.773 |   1.1842 |     40.564 |     0.3
   24 |   1.1580 |     39.510 |   1.1744 |     40.472 |     0.3
   25 |   1.1384 |     38.557 |   1.1559 |     39.461 |     0.3
   26 |   1.1160 |     38.226 |   1.1473 |     38.971 |     0.4
   27 |   1.0884 |     36.714 |   1.1309 |     37.960 |     0.4
   28 |   1.0809 |     36.758 |   1.1316 |     38.327 |     0.4
   29 |   1.0774 |     36.492 |   1.1050 |     37.194 |     0.4
   30 |   1.0451 |     35.176 |   1.1082 |     37.286 |     0.4
   31 |   1.0283 |     34.688 |   1.0930 |     36.029 |     0.4
   32 |   1.0133 |     34.103 |   1.0788 |     36.581 |     0.4
   33 |   0.9998 |     33.881 |   1.0836 |     36.213 |     0.5
   34 |   0.9861 |     33.344 |   1.0614 |     36.366 |     0.5
   35 |   0.9731 |     32.396 |   1.0491 |     35.325 |     0.5
   36 |   0.9559 |     32.082 |   1.0473 |     35.080 |     0.5
   37 |   0.9455 |     31.740 |   1.0453 |     34.926 |     0.5
   38 |   0.9250 |     31.128 |   1.0329 |     35.080 |     0.5
   39 |   0.9211 |     30.852 |   1.0206 |     34.252 |     0.5
   40 |   0.9117 |     30.489 |   1.0222 |     33.885 |     0.6
   41 |   0.8946 |     29.914 |   1.0189 |     33.150 |     0.6
   42 |   0.8958 |     29.920 |   1.0019 |     32.782 |     0.6
   43 |   0.8785 |     29.324 |   1.0079 |     32.721 |     0.6
   44 |   0.8626 |     29.026 |   1.0044 |     32.353 |     0.6
   45 |   0.8510 |     28.473 |   1.0001 |     31.955 |     0.6
   46 |   0.8472 |     28.473 |   0.9851 |     32.322 |     0.6
   47 |   0.8316 |     27.503 |   0.9954 |     31.556 |     0.6
   48 |   0.8291 |     27.725 |   0.9888 |     31.373 |     0.7
   49 |   0.8128 |     26.972 |   0.9836 |     31.801 |     0.7
   50 |   0.8050 |     27.070 |   0.9918 |     31.556 |     0.7
   51 |   0.7985 |     26.414 |   0.9614 |     29.810 |     0.7
   52 |   0.7792 |     26.002 |   0.9726 |     30.607 |     0.7
   53 |   0.7753 |     25.796 |   0.9665 |     30.178 |     0.7
   54 |   0.7714 |     25.331 |   0.9643 |     29.994 |     0.7
   55 |   0.7646 |     25.450 |   0.9633 |     29.810 |     0.8
   56 |   0.7611 |     25.303 |   0.9528 |     29.841 |     0.8
   57 |   0.7534 |     25.287 |   0.9568 |     29.412 |     0.8
   58 |   0.7458 |     25.054 |   0.9692 |     30.116 |     0.8
   59 |   0.7272 |     24.371 |   0.9750 |     30.392 |     0.8
   60 |   0.7231 |     24.019 |   0.9749 |     29.933 |     0.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 317,666

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4898 |     93.449 |   3.4457 |     93.995 |     0.0
    2 |   3.2580 |     85.690 |   3.0188 |     82.935 |     0.0
    3 |   2.8803 |     82.932 |   2.8028 |     82.935 |     0.1
    4 |   2.7393 |     81.746 |   2.7045 |     81.893 |     0.1
    5 |   2.6394 |     71.213 |   2.6017 |     67.341 |     0.1
    6 |   2.5315 |     65.388 |   2.4948 |     58.824 |     0.1
    7 |   2.4303 |     59.049 |   2.4063 |     58.824 |     0.1
    8 |   2.3519 |     59.049 |   2.3364 |     58.824 |     0.2
    9 |   2.2887 |     59.049 |   2.2799 |     58.824 |     0.2
   10 |   2.2407 |     59.049 |   2.2311 |     58.824 |     0.2
   11 |   2.1932 |     59.049 |   2.1880 |     58.824 |     0.2
   12 |   2.1527 |     59.049 |   2.1473 |     58.824 |     0.2
   13 |   2.1158 |     59.049 |   2.1078 |     58.824 |     0.3
   14 |   2.0780 |     59.049 |   2.0706 |     58.824 |     0.3
   15 |   2.0416 |     58.209 |   2.0340 |     54.167 |     0.3
   16 |   2.0063 |     56.404 |   1.9967 |     53.799 |     0.3
   17 |   1.9694 |     53.879 |   1.9575 |     53.768 |     0.4
   18 |   1.9327 |     53.414 |   1.9165 |     48.897 |     0.4
   19 |   1.8912 |     48.640 |   1.8792 |     48.346 |     0.4
   20 |   1.8561 |     48.607 |   1.8443 |     48.346 |     0.4
   21 |   1.8232 |     48.607 |   1.8110 |     48.346 |     0.4
   22 |   1.7919 |     48.597 |   1.7782 |     48.346 |     0.5
   23 |   1.7614 |     48.597 |   1.7464 |     48.346 |     0.5
   24 |   1.7318 |     48.602 |   1.7175 |     48.346 |     0.5
   25 |   1.7031 |     48.607 |   1.6920 |     48.346 |     0.5
   26 |   1.6800 |     47.784 |   1.6689 |     45.466 |     0.5
   27 |   1.6577 |     46.207 |   1.6465 |     45.466 |     0.6
   28 |   1.6383 |     46.223 |   1.6252 |     45.466 |     0.6
   29 |   1.6200 |     46.213 |   1.6067 |     45.466 |     0.6
   30 |   1.6007 |     46.213 |   1.5904 |     45.466 |     0.6
   31 |   1.5862 |     46.213 |   1.5760 |     45.466 |     0.6
   32 |   1.5723 |     46.213 |   1.5628 |     45.466 |     0.7
   33 |   1.5622 |     46.213 |   1.5502 |     45.466 |     0.7
   34 |   1.5485 |     46.213 |   1.5378 |     45.466 |     0.7
   35 |   1.5364 |     46.213 |   1.5258 |     45.466 |     0.7
   36 |   1.5261 |     46.218 |   1.5150 |     45.466 |     0.7
   37 |   1.5132 |     46.213 |   1.5047 |     45.466 |     0.8
   38 |   1.5041 |     46.218 |   1.4955 |     45.466 |     0.8
   39 |   1.4949 |     46.218 |   1.4870 |     45.466 |     0.8
   40 |   1.4881 |     46.213 |   1.4795 |     45.466 |     0.8
   41 |   1.4805 |     46.213 |   1.4729 |     45.466 |     0.8
   42 |   1.4738 |     46.213 |   1.4668 |     45.466 |     0.9
   43 |   1.4682 |     46.213 |   1.4610 |     45.466 |     0.9
   44 |   1.4620 |     46.213 |   1.4560 |     45.466 |     0.9
   45 |   1.4586 |     46.213 |   1.4515 |     45.466 |     0.9
   46 |   1.4511 |     46.213 |   1.4473 |     45.466 |     0.9
   47 |   1.4452 |     46.213 |   1.4433 |     45.466 |     1.0
   48 |   1.4451 |     46.213 |   1.4397 |     45.466 |     1.0
   49 |   1.4423 |     46.213 |   1.4365 |     45.466 |     1.0
   50 |   1.4371 |     46.213 |   1.4337 |     45.466 |     1.0
   51 |   1.4362 |     46.213 |   1.4304 |     45.466 |     1.0
   52 |   1.4307 |     46.213 |   1.4278 |     45.466 |     1.1
   53 |   1.4272 |     46.223 |   1.4249 |     45.466 |     1.1
   54 |   1.4251 |     46.218 |   1.4219 |     45.466 |     1.1
   55 |   1.4195 |     46.213 |   1.4190 |     45.466 |     1.1
   56 |   1.4175 |     46.223 |   1.4156 |     45.404 |     1.1
   57 |   1.4119 |     45.974 |   1.4131 |     45.190 |     1.2
   58 |   1.4100 |     45.747 |   1.4095 |     45.067 |     1.2
   59 |   1.4041 |     45.378 |   1.4064 |     45.006 |     1.2
   60 |   1.4026 |     45.210 |   1.4041 |     44.884 |     1.2
   61 |   1.3965 |     45.042 |   1.4001 |     44.853 |     1.2
   62 |   1.3893 |     44.918 |   1.3968 |     44.914 |     1.3
   63 |   1.3831 |     44.858 |   1.3950 |     44.914 |     1.3
   64 |   1.3794 |     44.798 |   1.3916 |     44.761 |     1.3
   65 |   1.3750 |     44.690 |   1.3882 |     44.914 |     1.3
   66 |   1.3703 |     44.560 |   1.3852 |     44.730 |     1.3
   67 |   1.3668 |     44.598 |   1.3832 |     44.792 |     1.4
   68 |   1.3611 |     44.311 |   1.3833 |     44.669 |     1.4
   69 |   1.3583 |     44.148 |   1.3807 |     44.547 |     1.4
   70 |   1.3529 |     44.089 |   1.3791 |     44.761 |     1.4
   71 |   1.3491 |     43.975 |   1.3774 |     44.669 |     1.4
   72 |   1.3446 |     43.975 |   1.3764 |     44.700 |     1.5
   73 |   1.3430 |     43.915 |   1.3718 |     44.455 |     1.5
   74 |   1.3391 |     43.877 |   1.3709 |     44.638 |     1.5
   75 |   1.3350 |     43.823 |   1.3676 |     44.485 |     1.5
   76 |   1.3337 |     43.834 |   1.3691 |     44.485 |     1.5
   77 |   1.3283 |     43.861 |   1.3657 |     44.547 |     1.6
   78 |   1.3241 |     43.926 |   1.3610 |     44.301 |     1.6
   79 |   1.3224 |     43.780 |   1.3651 |     44.455 |     1.6
   80 |   1.3188 |     43.693 |   1.3619 |     44.516 |     1.6
   81 |   1.3175 |     43.731 |   1.3600 |     44.485 |     1.6
   82 |   1.3155 |     43.585 |   1.3609 |     44.485 |     1.7
   83 |   1.3105 |     43.791 |   1.3592 |     44.608 |     1.7
   84 |   1.3078 |     43.661 |   1.3566 |     44.547 |     1.7
   85 |   1.3063 |     43.585 |   1.3595 |     44.516 |     1.7
   86 |   1.3012 |     43.623 |   1.3567 |     44.638 |     1.7
   87 |   1.2992 |     43.509 |   1.3552 |     44.455 |     1.8
   88 |   1.2999 |     43.699 |   1.3539 |     44.608 |     1.8
   89 |   1.2969 |     43.709 |   1.3520 |     44.240 |     1.8
   90 |   1.2920 |     43.552 |   1.3503 |     44.485 |     1.8
   91 |   1.2894 |     43.693 |   1.3518 |     44.792 |     1.8
   92 |   1.2863 |     43.411 |   1.3501 |     44.669 |     1.9
   93 |   1.2853 |     43.487 |   1.3493 |     44.577 |     1.9
   94 |   1.2814 |     43.503 |   1.3458 |     44.638 |     1.9
   95 |   1.2809 |     43.455 |   1.3443 |     44.516 |     1.9
   96 |   1.2794 |     43.390 |   1.3409 |     44.608 |     1.9
   97 |   1.2778 |     43.498 |   1.3431 |     44.547 |     2.0
   98 |   1.2702 |     43.579 |   1.3380 |     44.485 |     2.0
   99 |   1.2714 |     43.617 |   1.3367 |     44.455 |     2.0
  100 |   1.2680 |     43.406 |   1.3392 |     44.638 |     2.0
  101 |   1.2651 |     43.363 |   1.3410 |     44.577 |     2.0
  102 |   1.2633 |     43.411 |   1.3382 |     44.424 |     2.1
  103 |   1.2627 |     43.395 |   1.3349 |     44.424 |     2.1
  104 |   1.2623 |     43.281 |   1.3357 |     44.455 |     2.1
  105 |   1.2583 |     43.390 |   1.3337 |     44.332 |     2.1
  106 |   1.2570 |     43.108 |   1.3342 |     44.301 |     2.1
  107 |   1.2533 |     43.108 |   1.3326 |     44.118 |     2.2
  108 |   1.2488 |     43.227 |   1.3315 |     44.301 |     2.2
  109 |   1.2482 |     42.967 |   1.3316 |     44.363 |     2.2
  110 |   1.2465 |     42.962 |   1.3307 |     44.179 |     2.2
  111 |   1.2456 |     42.978 |   1.3307 |     43.597 |     2.2
  112 |   1.2431 |     43.070 |   1.3308 |     44.118 |     2.3
  113 |   1.2423 |     42.891 |   1.3286 |     43.995 |     2.3
  114 |   1.2395 |     43.054 |   1.3266 |     44.087 |     2.3
  115 |   1.2365 |     43.086 |   1.3250 |     44.118 |     2.3
  116 |   1.2332 |     42.875 |   1.3254 |     44.240 |     2.3
  117 |   1.2341 |     42.924 |   1.3221 |     43.811 |     2.4
  118 |   1.2346 |     42.848 |   1.3207 |     43.658 |     2.4
  119 |   1.2316 |     42.761 |   1.3206 |     44.087 |     2.4
  120 |   1.2259 |     42.642 |   1.3205 |     43.903 |     2.4
  121 |   1.2248 |     42.696 |   1.3216 |     43.658 |     2.4
  122 |   1.2232 |     42.507 |   1.3156 |     43.536 |     2.4
  123 |   1.2212 |     42.528 |   1.3214 |     43.750 |     2.5
  124 |   1.2207 |     42.490 |   1.3201 |     43.719 |     2.5
  125 |   1.2174 |     42.469 |   1.3165 |     43.842 |     2.5
  126 |   1.2184 |     42.539 |   1.3216 |     43.842 |     2.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 861,314

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3416 |     85.078 |   2.9501 |     82.016 |     0.0
    2 |   2.6697 |     70.004 |   2.4796 |     60.570 |     0.1
    3 |   2.3735 |     59.466 |   2.2812 |     58.425 |     0.1
    4 |   2.2224 |     58.350 |   2.1579 |     57.445 |     0.1
    5 |   2.1133 |     57.298 |   2.0547 |     57.384 |     0.2
    6 |   2.0191 |     54.839 |   1.9668 |     52.175 |     0.2
    7 |   1.9323 |     50.618 |   1.8774 |     48.744 |     0.2
    8 |   1.8522 |     49.469 |   1.7993 |     48.591 |     0.3
    9 |   1.7775 |     49.101 |   1.7285 |     48.100 |     0.3
   10 |   1.7146 |     48.868 |   1.6741 |     48.162 |     0.3
   11 |   1.6623 |     48.673 |   1.6287 |     48.162 |     0.4
   12 |   1.6196 |     48.602 |   1.5879 |     48.100 |     0.4
   13 |   1.5798 |     48.494 |   1.5571 |     48.039 |     0.4
   14 |   1.5453 |     48.456 |   1.5277 |     48.162 |     0.5
   15 |   1.5186 |     47.432 |   1.5027 |     45.251 |     0.5
   16 |   1.4942 |     46.131 |   1.4782 |     45.374 |     0.5
   17 |   1.4711 |     45.877 |   1.4578 |     45.067 |     0.6
   18 |   1.4478 |     45.541 |   1.4420 |     45.098 |     0.6
   19 |   1.4311 |     45.367 |   1.4221 |     44.945 |     0.6
   20 |   1.4116 |     45.243 |   1.4095 |     44.884 |     0.7
   21 |   1.3964 |     45.189 |   1.3951 |     44.914 |     0.7
   22 |   1.3807 |     44.956 |   1.3807 |     44.638 |     0.7
   23 |   1.3697 |     44.739 |   1.3686 |     44.516 |     0.8
   24 |   1.3586 |     44.500 |   1.3591 |     44.424 |     0.8
   25 |   1.3443 |     44.370 |   1.3471 |     44.056 |     0.8
   26 |   1.3304 |     43.617 |   1.3361 |     43.627 |     0.9
   27 |   1.3176 |     43.097 |   1.3273 |     43.229 |     0.9
   28 |   1.3074 |     42.458 |   1.3175 |     42.647 |     0.9
   29 |   1.2923 |     42.057 |   1.3123 |     42.586 |     1.0
   30 |   1.2825 |     41.385 |   1.3007 |     41.697 |     1.0
   31 |   1.2746 |     40.919 |   1.2927 |     41.238 |     1.0
   32 |   1.2604 |     40.529 |   1.2804 |     41.023 |     1.1
   33 |   1.2513 |     39.971 |   1.2710 |     40.564 |     1.1
   34 |   1.2375 |     39.429 |   1.2631 |     39.890 |     1.1
   35 |   1.2262 |     39.131 |   1.2569 |     40.012 |     1.2
   36 |   1.2212 |     38.746 |   1.2460 |     39.982 |     1.2
   37 |   1.2049 |     38.161 |   1.2408 |     39.154 |     1.2
   38 |   1.1951 |     38.009 |   1.2298 |     39.369 |     1.3
   39 |   1.1856 |     37.446 |   1.2249 |     39.461 |     1.3
   40 |   1.1725 |     37.224 |   1.2122 |     38.358 |     1.3
   41 |   1.1637 |     37.077 |   1.2098 |     38.480 |     1.4
   42 |   1.1509 |     36.590 |   1.1974 |     38.205 |     1.4
   43 |   1.1399 |     36.400 |   1.1901 |     38.266 |     1.4
   44 |   1.1331 |     36.194 |   1.1814 |     37.868 |     1.5
   45 |   1.1183 |     35.658 |   1.1752 |     37.561 |     1.5
   46 |   1.1116 |     35.474 |   1.1683 |     37.990 |     1.5
   47 |   1.1016 |     35.327 |   1.1621 |     37.623 |     1.6
   48 |   1.0898 |     34.899 |   1.1575 |     37.255 |     1.6
   49 |   1.0859 |     34.861 |   1.1497 |     37.224 |     1.6
   50 |   1.0743 |     34.482 |   1.1436 |     36.703 |     1.7
   51 |   1.0662 |     34.108 |   1.1386 |     36.520 |     1.7
   52 |   1.0542 |     33.956 |   1.1341 |     36.244 |     1.7
   53 |   1.0451 |     33.425 |   1.1243 |     35.784 |     1.8
   54 |   1.0320 |     32.916 |   1.1189 |     35.723 |     1.8
   55 |   1.0247 |     32.651 |   1.1156 |     35.999 |     1.8
   56 |   1.0162 |     32.461 |   1.1082 |     35.355 |     1.9
   57 |   1.0087 |     32.152 |   1.1062 |     35.202 |     1.9
   58 |   1.0020 |     31.865 |   1.1031 |     35.386 |     1.9
   59 |   0.9868 |     31.448 |   1.0970 |     35.080 |     2.0
   60 |   0.9840 |     31.090 |   1.0894 |     34.681 |     2.0
   61 |   0.9741 |     30.657 |   1.0901 |     34.467 |     2.0
   62 |   0.9655 |     30.754 |   1.0853 |     34.988 |     2.1
   63 |   0.9559 |     30.299 |   1.0809 |     34.498 |     2.1
   64 |   0.9490 |     30.039 |   1.0789 |     34.743 |     2.1
   65 |   0.9432 |     29.681 |   1.0675 |     34.283 |     2.2
   66 |   0.9314 |     29.681 |   1.0668 |     33.854 |     2.2
   67 |   0.9228 |     29.324 |   1.0624 |     33.732 |     2.2
   68 |   0.9170 |     28.993 |   1.0568 |     33.609 |     2.3
   69 |   0.9136 |     28.609 |   1.0528 |     33.578 |     2.3
   70 |   0.9017 |     28.289 |   1.0568 |     33.609 |     2.3
   71 |   0.8955 |     28.153 |   1.0520 |     33.364 |     2.4
   72 |   0.8866 |     27.942 |   1.0452 |     32.782 |     2.4
   73 |   0.8774 |     27.980 |   1.0392 |     33.303 |     2.4
   74 |   0.8727 |     27.200 |   1.0425 |     33.241 |     2.5
   75 |   0.8624 |     26.831 |   1.0379 |     33.609 |     2.5
   76 |   0.8554 |     26.588 |   1.0389 |     33.150 |     2.5
   77 |   0.8490 |     26.620 |   1.0252 |     32.629 |     2.6
   78 |   0.8393 |     26.306 |   1.0252 |     33.395 |     2.6
   79 |   0.8378 |     26.425 |   1.0200 |     32.996 |     2.6
   80 |   0.8303 |     25.921 |   1.0163 |     31.924 |     2.7
   81 |   0.8242 |     25.872 |   1.0099 |     31.955 |     2.7
   82 |   0.8141 |     25.385 |   1.0139 |     32.292 |     2.7
   83 |   0.8076 |     25.141 |   1.0093 |     31.955 |     2.8
   84 |   0.8007 |     24.669 |   1.0006 |     31.740 |     2.8
   85 |   0.7930 |     24.697 |   1.0040 |     31.679 |     2.8
   86 |   0.7902 |     24.740 |   0.9963 |     31.556 |     2.9
   87 |   0.7848 |     24.388 |   0.9969 |     31.556 |     2.9
   88 |   0.7744 |     24.225 |   0.9925 |     31.618 |     2.9
   89 |   0.7698 |     23.759 |   0.9955 |     31.587 |     3.0
   90 |   0.7644 |     23.727 |   0.9934 |     31.464 |     3.0
   91 |   0.7570 |     23.477 |   0.9894 |     31.434 |     3.0
   92 |   0.7590 |     23.494 |   0.9857 |     31.311 |     3.1
   93 |   0.7512 |     23.077 |   0.9838 |     30.392 |     3.1
   94 |   0.7421 |     23.060 |   0.9802 |     31.219 |     3.1
   95 |   0.7351 |     22.616 |   0.9767 |     31.127 |     3.2
   96 |   0.7307 |     22.919 |   0.9746 |     30.545 |     3.2
   97 |   0.7262 |     22.231 |   0.9767 |     30.423 |     3.2
   98 |   0.7237 |     22.258 |   0.9718 |     30.576 |     3.3
   99 |   0.7144 |     21.928 |   0.9734 |     30.208 |     3.3
  100 |   0.7111 |     21.559 |   0.9729 |     30.239 |     3.3
  101 |   0.7013 |     21.576 |   0.9742 |     30.239 |     3.4
  102 |   0.6974 |     21.408 |   0.9683 |     30.270 |     3.4
  103 |   0.6939 |     21.180 |   0.9594 |     29.902 |     3.4
  104 |   0.6913 |     21.175 |   0.9701 |     30.331 |     3.5
  105 |   0.6857 |     21.104 |   0.9635 |     29.902 |     3.5
  106 |   0.6828 |     21.245 |   0.9627 |     29.841 |     3.5
  107 |   0.6739 |     20.828 |   0.9611 |     29.657 |     3.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 2,246,434

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2343 |     60.148 |   1.6549 |     48.376 |     0.1
    2 |   1.5057 |     46.543 |   1.4264 |     45.987 |     0.1
    3 |   1.4159 |     46.240 |   1.4079 |     45.466 |     0.2
    4 |   1.4059 |     46.397 |   1.3872 |     45.374 |     0.3
    5 |   1.3909 |     46.158 |   1.3793 |     45.466 |     0.3
    6 |   1.3697 |     45.319 |   1.3457 |     44.240 |     0.4
    7 |   1.3437 |     44.809 |   1.3228 |     44.148 |     0.4
    8 |   1.3289 |     44.381 |   1.3151 |     43.352 |     0.5
    9 |   1.3121 |     44.045 |   1.2974 |     42.402 |     0.6
   10 |   1.2843 |     43.298 |   1.2699 |     42.341 |     0.6
   11 |   1.2559 |     42.349 |   1.2489 |     40.809 |     0.7
   12 |   1.2213 |     41.434 |   1.2271 |     40.227 |     0.8
   13 |   1.1992 |     40.914 |   1.2080 |     39.308 |     0.8
   14 |   1.1873 |     40.496 |   1.1830 |     38.756 |     0.9
   15 |   1.1616 |     39.938 |   1.1713 |     38.542 |     1.0
   16 |   1.1347 |     38.654 |   1.1526 |     38.419 |     1.0
   17 |   1.1143 |     38.047 |   1.1473 |     38.143 |     1.1
   18 |   1.0942 |     37.462 |   1.1207 |     37.163 |     1.2
   19 |   1.0717 |     36.899 |   1.1238 |     37.806 |     1.2
   20 |   1.0556 |     36.167 |   1.1040 |     38.235 |     1.3
   21 |   1.0328 |     35.566 |   1.0826 |     36.520 |     1.3
   22 |   1.0240 |     34.829 |   1.0716 |     35.509 |     1.4
   23 |   1.0031 |     34.607 |   1.0741 |     35.938 |     1.5
   24 |   0.9889 |     33.723 |   1.0487 |     34.804 |     1.5
   25 |   0.9655 |     33.241 |   1.0395 |     34.957 |     1.6
   26 |   0.9501 |     32.510 |   1.0343 |     35.294 |     1.7
   27 |   0.9329 |     31.925 |   1.0285 |     35.049 |     1.7
   28 |   0.9082 |     30.906 |   1.0251 |     34.804 |     1.8
   29 |   0.8972 |     30.391 |   0.9932 |     32.690 |     1.9
   30 |   0.8757 |     30.099 |   0.9858 |     33.211 |     1.9
   31 |   0.8597 |     29.178 |   0.9906 |     32.077 |     2.0
   32 |   0.8659 |     29.822 |   0.9817 |     32.874 |     2.1
   33 |   0.8387 |     28.749 |   0.9883 |     33.027 |     2.1
   34 |   0.8158 |     27.812 |   0.9818 |     31.955 |     2.2
   35 |   0.8111 |     27.601 |   0.9660 |     32.047 |     2.2
   36 |   0.7865 |     27.086 |   0.9438 |     31.127 |     2.3
   37 |   0.7813 |     26.631 |   0.9858 |     32.537 |     2.4
   38 |   0.7792 |     26.598 |   0.9470 |     30.821 |     2.4
   39 |   0.7565 |     25.921 |   0.9442 |     30.362 |     2.5
   40 |   0.7403 |     25.287 |   0.9487 |     30.913 |     2.6
   41 |   0.7231 |     24.269 |   0.9305 |     29.350 |     2.6
   42 |   0.7211 |     24.756 |   0.9565 |     30.607 |     2.7
   43 |   0.7252 |     24.832 |   0.9336 |     30.392 |     2.8
   44 |   0.7056 |     24.095 |   0.9135 |     29.473 |     2.8
   45 |   0.6804 |     23.152 |   0.9151 |     29.044 |     2.9
   46 |   0.6755 |     23.125 |   0.9136 |     28.830 |     2.9
   47 |   0.6461 |     21.993 |   0.9059 |     29.105 |     3.0
   48 |   0.6525 |     22.551 |   0.9145 |     28.339 |     3.1
   49 |   0.6389 |     21.944 |   0.9193 |     28.983 |     3.1
   50 |   0.6306 |     21.467 |   0.9091 |     28.125 |     3.2
   51 |   0.6260 |     21.754 |   0.9127 |     28.094 |     3.3
   52 |   0.6079 |     20.904 |   0.8965 |     28.125 |     3.3
   53 |   0.6031 |     20.806 |   0.9131 |     28.339 |     3.4
   54 |   0.5950 |     20.595 |   0.9150 |     28.738 |     3.5
   55 |   0.5807 |     19.874 |   0.9050 |     27.880 |     3.5
   56 |   0.5752 |     19.630 |   0.9104 |     27.849 |     3.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 425,250

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3464 |     86.758 |   2.9703 |     83.333 |     0.0
    2 |   2.6989 |     73.272 |   2.5480 |     66.667 |     0.0
    3 |   2.4275 |     59.915 |   2.3498 |     58.824 |     0.0
    4 |   2.2733 |     58.984 |   2.2298 |     58.762 |     0.1
    5 |   2.1699 |     58.415 |   2.1346 |     57.016 |     0.1
    6 |   2.0846 |     56.740 |   2.0505 |     55.392 |     0.1
    7 |   2.0070 |     55.613 |   1.9785 |     54.596 |     0.1
    8 |   1.9405 |     54.058 |   1.9094 |     52.267 |     0.1
    9 |   1.8722 |     49.236 |   1.8425 |     48.223 |     0.2
   10 |   1.8098 |     48.548 |   1.7813 |     48.560 |     0.2
   11 |   1.7532 |     48.602 |   1.7268 |     48.407 |     0.2
   12 |   1.7021 |     48.624 |   1.6783 |     48.376 |     0.2
   13 |   1.6572 |     48.613 |   1.6347 |     48.376 |     0.2
   14 |   1.6197 |     47.134 |   1.5975 |     45.496 |     0.2
   15 |   1.5820 |     46.223 |   1.5666 |     45.496 |     0.3
   16 |   1.5547 |     46.213 |   1.5397 |     45.496 |     0.3
   17 |   1.5305 |     46.207 |   1.5188 |     45.527 |     0.3
   18 |   1.5100 |     46.218 |   1.4986 |     45.496 |     0.3
   19 |   1.4909 |     46.218 |   1.4831 |     45.527 |     0.3
   20 |   1.4735 |     46.229 |   1.4667 |     45.466 |     0.3
   21 |   1.4563 |     46.234 |   1.4518 |     45.496 |     0.4
   22 |   1.4455 |     46.164 |   1.4397 |     45.558 |     0.4
   23 |   1.4297 |     46.213 |   1.4293 |     45.496 |     0.4
   24 |   1.4198 |     46.175 |   1.4170 |     45.558 |     0.4
   25 |   1.4091 |     46.028 |   1.4094 |     45.466 |     0.4
   26 |   1.3991 |     45.741 |   1.3997 |     45.435 |     0.4
   27 |   1.3898 |     45.443 |   1.3913 |     44.975 |     0.5
   28 |   1.3820 |     45.259 |   1.3858 |     44.761 |     0.5
   29 |   1.3705 |     44.750 |   1.3785 |     44.761 |     0.5
   30 |   1.3664 |     44.847 |   1.3726 |     45.037 |     0.5
   31 |   1.3562 |     44.295 |   1.3661 |     43.719 |     0.5
   32 |   1.3495 |     43.780 |   1.3611 |     43.934 |     0.5
   33 |   1.3380 |     43.363 |   1.3518 |     43.260 |     0.6
   34 |   1.3305 |     42.864 |   1.3444 |     43.015 |     0.6
   35 |   1.3223 |     42.821 |   1.3374 |     42.892 |     0.6
   36 |   1.3124 |     42.582 |   1.3329 |     42.708 |     0.6
   37 |   1.3067 |     42.198 |   1.3244 |     42.555 |     0.6
   38 |   1.2966 |     41.835 |   1.3167 |     42.402 |     0.6
   39 |   1.2895 |     41.580 |   1.3107 |     41.912 |     0.7
   40 |   1.2788 |     41.434 |   1.3038 |     41.636 |     0.7
   41 |   1.2693 |     41.060 |   1.3012 |     41.697 |     0.7
   42 |   1.2645 |     40.816 |   1.2950 |     41.789 |     0.7
   43 |   1.2554 |     40.708 |   1.2890 |     41.146 |     0.7
   44 |   1.2445 |     40.296 |   1.2811 |     41.299 |     0.7
   45 |   1.2371 |     40.144 |   1.2767 |     40.778 |     0.8
   46 |   1.2296 |     39.895 |   1.2709 |     40.931 |     0.8
   47 |   1.2232 |     39.754 |   1.2671 |     40.625 |     0.8
   48 |   1.2155 |     39.591 |   1.2600 |     40.656 |     0.8
   49 |   1.2039 |     39.266 |   1.2546 |     40.104 |     0.8
   50 |   1.1982 |     38.920 |   1.2473 |     39.859 |     0.8
   51 |   1.1924 |     38.654 |   1.2419 |     39.890 |     0.9
   52 |   1.1835 |     38.421 |   1.2393 |     39.828 |     0.9
   53 |   1.1757 |     38.058 |   1.2342 |     39.400 |     0.9
   54 |   1.1682 |     37.879 |   1.2343 |     39.828 |     0.9
   55 |   1.1617 |     37.614 |   1.2248 |     38.848 |     0.9
   56 |   1.1530 |     37.565 |   1.2192 |     39.062 |     0.9
   57 |   1.1448 |     37.110 |   1.2117 |     38.634 |     1.0
   58 |   1.1353 |     36.796 |   1.2077 |     38.327 |     1.0
   59 |   1.1306 |     36.552 |   1.2055 |     38.572 |     1.0
   60 |   1.1249 |     36.275 |   1.2006 |     38.572 |     1.0
   61 |   1.1148 |     36.194 |   1.1942 |     38.297 |     1.0
   62 |   1.1082 |     35.842 |   1.1908 |     38.205 |     1.0
   63 |   1.0984 |     35.642 |   1.1864 |     37.960 |     1.1
   64 |   1.0894 |     35.300 |   1.1880 |     38.113 |     1.1
   65 |   1.0859 |     35.148 |   1.1786 |     37.653 |     1.1
   66 |   1.0779 |     34.758 |   1.1707 |     37.224 |     1.1
   67 |   1.0717 |     34.460 |   1.1705 |     37.286 |     1.1
   68 |   1.0604 |     34.179 |   1.1653 |     37.224 |     1.1
   69 |   1.0551 |     33.902 |   1.1619 |     37.224 |     1.2
   70 |   1.0465 |     33.523 |   1.1582 |     36.734 |     1.2
   71 |   1.0395 |     33.230 |   1.1524 |     36.397 |     1.2
   72 |   1.0312 |     33.003 |   1.1529 |     36.397 |     1.2
   73 |   1.0245 |     32.900 |   1.1466 |     36.673 |     1.2
   74 |   1.0148 |     32.320 |   1.1415 |     36.213 |     1.2
   75 |   1.0131 |     32.136 |   1.1352 |     35.846 |     1.3
   76 |   1.0047 |     31.941 |   1.1332 |     36.244 |     1.3
   77 |   0.9940 |     31.426 |   1.1285 |     35.815 |     1.3
   78 |   0.9880 |     31.399 |   1.1273 |     35.938 |     1.3
   79 |   0.9804 |     31.058 |   1.1273 |     35.938 |     1.3
   80 |   0.9739 |     30.868 |   1.1252 |     35.754 |     1.3
   81 |   0.9669 |     30.624 |   1.1218 |     35.294 |     1.4
   82 |   0.9603 |     30.364 |   1.1169 |     34.743 |     1.4
   83 |   0.9523 |     30.104 |   1.1155 |     34.896 |     1.4
   84 |   0.9451 |     29.931 |   1.1108 |     34.804 |     1.4
   85 |   0.9396 |     29.649 |   1.1085 |     34.681 |     1.4
   86 |   0.9312 |     29.427 |   1.1040 |     35.202 |     1.4
   87 |   0.9235 |     29.031 |   1.1025 |     34.559 |     1.5
   88 |   0.9155 |     28.744 |   1.0970 |     34.712 |     1.5
   89 |   0.9091 |     28.527 |   1.0909 |     34.007 |     1.5
   90 |   0.9036 |     28.392 |   1.0948 |     34.436 |     1.5
   91 |   0.8991 |     28.181 |   1.0910 |     34.161 |     1.5
   92 |   0.8923 |     28.116 |   1.0885 |     33.609 |     1.5
   93 |   0.8826 |     27.595 |   1.0819 |     33.885 |     1.6
   94 |   0.8779 |     27.335 |   1.0823 |     33.732 |     1.6
   95 |   0.8709 |     26.940 |   1.0756 |     33.548 |     1.6
   96 |   0.8627 |     26.880 |   1.0783 |     33.487 |     1.6
   97 |   0.8569 |     26.338 |   1.0758 |     32.843 |     1.6
   98 |   0.8492 |     26.268 |   1.0710 |     33.364 |     1.6
   99 |   0.8425 |     26.160 |   1.0739 |     33.333 |     1.7
  100 |   0.8427 |     25.694 |   1.0666 |     32.629 |     1.7
  101 |   0.8316 |     25.607 |   1.0703 |     32.567 |     1.7
  102 |   0.8235 |     25.114 |   1.0663 |     32.843 |     1.7
  103 |   0.8197 |     25.103 |   1.0628 |     32.629 |     1.7
  104 |   0.8110 |     24.865 |   1.0602 |     32.935 |     1.7
  105 |   0.8068 |     24.512 |   1.0608 |     32.445 |     1.8
  106 |   0.7998 |     24.496 |   1.0625 |     32.629 |     1.8
  107 |   0.7952 |     24.133 |   1.0565 |     32.537 |     1.8
  108 |   0.7848 |     23.900 |   1.0585 |     32.659 |     1.8
  109 |   0.7810 |     23.602 |   1.0572 |     32.629 |     1.8
  110 |   0.7757 |     23.407 |   1.0640 |     32.966 |     1.8
  111 |   0.7693 |     23.077 |   1.0613 |     32.751 |     1.9
  112 |   0.7645 |     23.033 |   1.0557 |     32.659 |     1.9
  113 |   0.7569 |     22.741 |   1.0536 |     31.863 |     1.9
  114 |   0.7469 |     22.632 |   1.0560 |     32.537 |     1.9
  115 |   0.7441 |     22.464 |   1.0526 |     31.985 |     1.9
  116 |   0.7407 |     22.356 |   1.0497 |     32.200 |     1.9
  117 |   0.7345 |     22.069 |   1.0500 |     31.893 |     2.0
  118 |   0.7295 |     21.879 |   1.0525 |     31.801 |     2.0
  119 |   0.7229 |     21.624 |   1.0498 |     32.322 |     2.0
  120 |   0.7168 |     21.381 |   1.0509 |     32.384 |     2.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 624,930

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4496 |     79.037 |   3.4056 |     86.857 |     0.0
    2 |   3.2505 |     85.961 |   3.0380 |     85.417 |     0.1
    3 |   2.8763 |     85.371 |   2.7680 |     86.857 |     0.1
    4 |   2.6916 |     72.280 |   2.6449 |     59.252 |     0.1
    5 |   2.5875 |     59.298 |   2.5513 |     58.824 |     0.1
    6 |   2.4962 |     59.103 |   2.4640 |     58.824 |     0.2
    7 |   2.4187 |     59.043 |   2.3869 |     58.824 |     0.2
    8 |   2.3467 |     58.984 |   2.3205 |     58.824 |     0.2
    9 |   2.2877 |     58.843 |   2.2629 |     58.824 |     0.3
   10 |   2.2323 |     58.653 |   2.2123 |     58.824 |     0.3
   11 |   2.1859 |     58.263 |   2.1642 |     58.824 |     0.3
   12 |   2.1399 |     57.846 |   2.1184 |     58.824 |     0.3
   13 |   2.0945 |     57.353 |   2.0716 |     58.824 |     0.4
   14 |   2.0499 |     56.632 |   2.0279 |     57.567 |     0.4
   15 |   2.0103 |     56.215 |   1.9876 |     53.768 |     0.4
   16 |   1.9728 |     55.586 |   1.9501 |     53.768 |     0.5
   17 |   1.9328 |     53.218 |   1.9059 |     48.346 |     0.5
   18 |   1.8901 |     50.233 |   1.8587 |     48.346 |     0.5
   19 |   1.8450 |     49.344 |   1.8135 |     48.346 |     0.5
   20 |   1.8020 |     49.041 |   1.7742 |     48.346 |     0.6
   21 |   1.7649 |     48.895 |   1.7407 |     48.346 |     0.6
   22 |   1.7309 |     48.710 |   1.7102 |     48.346 |     0.6
   23 |   1.7046 |     48.662 |   1.6826 |     48.346 |     0.7
   24 |   1.6792 |     47.074 |   1.6584 |     45.466 |     0.7
   25 |   1.6596 |     46.299 |   1.6366 |     45.466 |     0.7
   26 |   1.6371 |     46.229 |   1.6175 |     45.466 |     0.7
   27 |   1.6164 |     46.229 |   1.5994 |     45.466 |     0.8
   28 |   1.6021 |     46.213 |   1.5835 |     45.466 |     0.8
   29 |   1.5839 |     46.218 |   1.5689 |     45.466 |     0.8
   30 |   1.5721 |     46.218 |   1.5559 |     45.466 |     0.8
   31 |   1.5579 |     46.218 |   1.5437 |     45.466 |     0.9
   32 |   1.5472 |     46.213 |   1.5315 |     45.466 |     0.9
   33 |   1.5343 |     46.213 |   1.5175 |     45.466 |     0.9
   34 |   1.5190 |     46.218 |   1.5056 |     45.466 |     1.0
   35 |   1.5110 |     46.213 |   1.4954 |     45.466 |     1.0
   36 |   1.4995 |     46.213 |   1.4860 |     45.466 |     1.0
   37 |   1.4889 |     46.191 |   1.4769 |     45.466 |     1.0
   38 |   1.4824 |     46.267 |   1.4688 |     45.466 |     1.1
   39 |   1.4741 |     46.218 |   1.4622 |     45.466 |     1.1
   40 |   1.4695 |     46.196 |   1.4565 |     45.466 |     1.1
   41 |   1.4633 |     46.353 |   1.4513 |     45.466 |     1.2
   42 |   1.4569 |     46.370 |   1.4468 |     45.466 |     1.2
   43 |   1.4510 |     46.272 |   1.4423 |     45.466 |     1.2
   44 |   1.4471 |     46.240 |   1.4383 |     45.466 |     1.2
   45 |   1.4434 |     46.191 |   1.4347 |     45.435 |     1.3
   46 |   1.4381 |     46.186 |   1.4305 |     45.466 |     1.3
   47 |   1.4337 |     46.364 |   1.4278 |     45.496 |     1.3
   48 |   1.4296 |     46.196 |   1.4241 |     45.435 |     1.4
   49 |   1.4272 |     46.229 |   1.4211 |     45.496 |     1.4
   50 |   1.4264 |     46.272 |   1.4189 |     45.619 |     1.4
   51 |   1.4188 |     46.364 |   1.4149 |     45.558 |     1.4
   52 |   1.4158 |     46.283 |   1.4124 |     45.496 |     1.5
   53 |   1.4130 |     46.234 |   1.4094 |     45.496 |     1.5
   54 |   1.4096 |     46.278 |   1.4069 |     45.619 |     1.5
   55 |   1.4068 |     46.175 |   1.4027 |     45.435 |     1.5
   56 |   1.4028 |     46.180 |   1.4006 |     45.558 |     1.6
   57 |   1.3988 |     46.283 |   1.3954 |     45.435 |     1.6
   58 |   1.3954 |     46.131 |   1.3930 |     45.558 |     1.6
   59 |   1.3880 |     45.953 |   1.3876 |     45.435 |     1.7
   60 |   1.3803 |     45.963 |   1.3816 |     45.251 |     1.7
   61 |   1.3771 |     45.866 |   1.3796 |     45.190 |     1.7
   62 |   1.3675 |     45.503 |   1.3725 |     45.435 |     1.7
   63 |   1.3643 |     45.454 |   1.3685 |     45.037 |     1.8
   64 |   1.3564 |     45.481 |   1.3635 |     45.098 |     1.8
   65 |   1.3497 |     45.286 |   1.3609 |     45.435 |     1.8
   66 |   1.3454 |     45.292 |   1.3572 |     44.975 |     1.9
   67 |   1.3391 |     45.335 |   1.3546 |     44.730 |     1.9
   68 |   1.3342 |     45.021 |   1.3510 |     44.884 |     1.9
   69 |   1.3288 |     44.950 |   1.3480 |     45.129 |     1.9
   70 |   1.3241 |     44.815 |   1.3443 |     44.884 |     2.0
   71 |   1.3201 |     44.798 |   1.3423 |     44.577 |     2.0
   72 |   1.3132 |     44.463 |   1.3400 |     44.914 |     2.0
   73 |   1.3080 |     44.289 |   1.3362 |     44.179 |     2.0
   74 |   1.3034 |     43.959 |   1.3357 |     44.148 |     2.1
   75 |   1.2980 |     43.693 |   1.3326 |     44.301 |     2.1
   76 |   1.2959 |     43.493 |   1.3306 |     43.995 |     2.1
   77 |   1.2910 |     43.401 |   1.3279 |     44.118 |     2.2
   78 |   1.2857 |     43.032 |   1.3242 |     44.179 |     2.2
   79 |   1.2795 |     43.124 |   1.3258 |     44.332 |     2.2
   80 |   1.2741 |     42.788 |   1.3165 |     43.903 |     2.2
   81 |   1.2702 |     42.707 |   1.3168 |     43.873 |     2.3
   82 |   1.2670 |     42.582 |   1.3142 |     43.811 |     2.3
   83 |   1.2644 |     42.507 |   1.3135 |     43.474 |     2.3
   84 |   1.2606 |     42.398 |   1.3122 |     43.750 |     2.4
   85 |   1.2584 |     42.187 |   1.3092 |     43.382 |     2.4
   86 |   1.2494 |     42.274 |   1.3066 |     43.444 |     2.4
   87 |   1.2465 |     41.726 |   1.3080 |     43.536 |     2.4
   88 |   1.2442 |     41.894 |   1.3038 |     43.321 |     2.5
   89 |   1.2370 |     41.407 |   1.3018 |     43.076 |     2.5
   90 |   1.2354 |     41.634 |   1.3043 |     43.107 |     2.5
   91 |   1.2350 |     41.190 |   1.2999 |     42.770 |     2.5
   92 |   1.2288 |     41.038 |   1.2961 |     43.199 |     2.6
   93 |   1.2241 |     40.811 |   1.2972 |     43.199 |     2.6
   94 |   1.2230 |     40.919 |   1.2919 |     42.892 |     2.6
   95 |   1.2200 |     40.550 |   1.2943 |     42.555 |     2.7
   96 |   1.2159 |     40.507 |   1.2937 |     42.862 |     2.7
   97 |   1.2114 |     40.453 |   1.2914 |     42.862 |     2.7
   98 |   1.2087 |     40.372 |   1.2906 |     42.953 |     2.7
   99 |   1.2043 |     40.236 |   1.2866 |     42.678 |     2.8
  100 |   1.2039 |     40.231 |   1.2864 |     42.279 |     2.8
  101 |   1.1991 |     39.922 |   1.2874 |     42.708 |     2.8
  102 |   1.1987 |     40.036 |   1.2859 |     43.015 |     2.9
  103 |   1.1945 |     39.819 |   1.2876 |     42.678 |     2.9
  104 |   1.1885 |     39.516 |   1.2805 |     42.862 |     2.9
  105 |   1.1860 |     39.386 |   1.2785 |     42.188 |     2.9
  106 |   1.1822 |     39.494 |   1.2808 |     42.310 |     3.0
  107 |   1.1837 |     39.358 |   1.2829 |     42.494 |     3.0
  108 |   1.1793 |     39.440 |   1.2807 |     42.402 |     3.0
  109 |   1.1759 |     39.358 |   1.2793 |     42.188 |     3.0
  110 |   1.1740 |     39.023 |   1.2760 |     42.555 |     3.1
  111 |   1.1714 |     38.974 |   1.2754 |     42.004 |     3.1
  112 |   1.1692 |     38.855 |   1.2764 |     42.371 |     3.1
  113 |   1.1634 |     39.185 |   1.2769 |     42.218 |     3.2
  114 |   1.1620 |     39.028 |   1.2807 |     42.402 |     3.2
  115 |   1.1639 |     38.768 |   1.2737 |     42.157 |     3.2
  116 |   1.1555 |     38.497 |   1.2693 |     41.820 |     3.2
  117 |   1.1532 |     38.594 |   1.2718 |     42.463 |     3.3
  118 |   1.1560 |     38.676 |   1.2703 |     42.371 |     3.3
  119 |   1.1525 |     38.632 |   1.2729 |     42.157 |     3.3
  120 |   1.1489 |     38.399 |   1.2681 |     42.096 |     3.4
  121 |   1.1444 |     38.470 |   1.2772 |     42.218 |     3.4
  122 |   1.1468 |     38.454 |   1.2738 |     42.433 |     3.4
  123 |   1.1386 |     38.302 |   1.2768 |     42.096 |     3.4
  124 |   1.1418 |     37.961 |   1.2694 |     41.942 |     3.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 425,858

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5939 |     68.579 |   2.0484 |     58.824 |     0.0
    2 |   1.8328 |     51.403 |   1.6215 |     48.346 |     0.0
    3 |   1.5387 |     46.381 |   1.4662 |     45.466 |     0.1
    4 |   1.4516 |     46.278 |   1.4290 |     45.466 |     0.1
    5 |   1.4238 |     46.256 |   1.4093 |     45.987 |     0.1
    6 |   1.4123 |     46.337 |   1.4000 |     45.466 |     0.1
    7 |   1.4030 |     46.218 |   1.3931 |     45.374 |     0.2
    8 |   1.3896 |     46.148 |   1.3680 |     45.466 |     0.2
    9 |   1.3643 |     45.958 |   1.3484 |     45.190 |     0.2
   10 |   1.3445 |     45.389 |   1.3291 |     44.822 |     0.2
   11 |   1.3275 |     45.205 |   1.3191 |     44.914 |     0.2
   12 |   1.3066 |     44.977 |   1.3061 |     44.332 |     0.3
   13 |   1.2912 |     44.674 |   1.2881 |     43.474 |     0.3
   14 |   1.2744 |     43.541 |   1.2591 |     42.616 |     0.3
   15 |   1.2488 |     42.577 |   1.2402 |     42.279 |     0.3
   16 |   1.2341 |     42.469 |   1.2200 |     41.085 |     0.4
   17 |   1.2114 |     41.770 |   1.2147 |     41.238 |     0.4
   18 |   1.1976 |     41.233 |   1.2072 |     41.115 |     0.4
   19 |   1.1908 |     40.778 |   1.2026 |     40.012 |     0.4
   20 |   1.1751 |     40.513 |   1.1789 |     40.165 |     0.4
   21 |   1.1633 |     39.971 |   1.1715 |     40.380 |     0.5
   22 |   1.1483 |     39.510 |   1.1702 |     39.951 |     0.5
   23 |   1.1318 |     38.730 |   1.1512 |     38.235 |     0.5
   24 |   1.1236 |     38.573 |   1.1315 |     38.664 |     0.5
   25 |   1.0998 |     37.717 |   1.1199 |     37.806 |     0.6
   26 |   1.0850 |     37.099 |   1.1191 |     37.592 |     0.6
   27 |   1.0672 |     36.698 |   1.1086 |     37.868 |     0.6
   28 |   1.0572 |     36.162 |   1.0972 |     37.255 |     0.6
   29 |   1.0540 |     36.281 |   1.1016 |     37.469 |     0.6
   30 |   1.0467 |     35.940 |   1.0906 |     37.623 |     0.7
   31 |   1.0394 |     36.113 |   1.0974 |     36.918 |     0.7
   32 |   1.0286 |     35.506 |   1.0914 |     37.163 |     0.7
   33 |   1.0115 |     34.829 |   1.0700 |     36.152 |     0.7
   34 |   1.0055 |     34.677 |   1.0651 |     36.305 |     0.8
   35 |   0.9974 |     34.439 |   1.0796 |     36.673 |     0.8
   36 |   0.9915 |     34.287 |   1.0684 |     36.152 |     0.8
   37 |   0.9768 |     33.913 |   1.0577 |     35.815 |     0.8
   38 |   0.9686 |     33.512 |   1.0636 |     35.815 |     0.8
   39 |   0.9563 |     32.959 |   1.0539 |     35.509 |     0.9
   40 |   0.9429 |     32.304 |   1.0452 |     34.957 |     0.9
   41 |   0.9371 |     31.963 |   1.0346 |     34.743 |     0.9
   42 |   0.9522 |     32.922 |   1.0316 |     34.007 |     0.9
   43 |   0.9279 |     31.762 |   1.0379 |     35.049 |     1.0
   44 |   0.9298 |     31.968 |   1.0478 |     34.957 |     1.0
   45 |   0.9081 |     31.036 |   1.0363 |     34.865 |     1.0
   46 |   0.9052 |     31.025 |   1.0139 |     33.578 |     1.0
   47 |   0.8929 |     30.863 |   1.0233 |     33.732 |     1.0
   48 |   0.8895 |     29.963 |   1.0052 |     33.272 |     1.1
   49 |   0.8839 |     30.061 |   1.0049 |     33.211 |     1.1
   50 |   0.8700 |     29.508 |   1.0093 |     33.517 |     1.1
   51 |   0.8553 |     29.291 |   1.0075 |     33.854 |     1.1
   52 |   0.8489 |     28.809 |   1.0036 |     33.058 |     1.2
   53 |   0.8411 |     28.744 |   1.0019 |     33.425 |     1.2
   54 |   0.8358 |     28.852 |   1.0068 |     33.487 |     1.2
   55 |   0.8287 |     28.164 |   1.0047 |     33.517 |     1.2
   56 |   0.8289 |     28.370 |   1.0083 |     33.456 |     1.2
   57 |   0.8247 |     27.953 |   0.9907 |     32.782 |     1.3
   58 |   0.8122 |     27.677 |   0.9822 |     32.475 |     1.3
   59 |   0.8015 |     27.238 |   0.9803 |     31.342 |     1.3
   60 |   0.7922 |     27.113 |   1.0004 |     32.537 |     1.3
   61 |   0.7938 |     27.135 |   0.9802 |     31.924 |     1.4
   62 |   0.7889 |     26.848 |   0.9856 |     32.261 |     1.4
   63 |   0.7840 |     26.615 |   0.9824 |     31.556 |     1.4
   64 |   0.7662 |     25.921 |   0.9804 |     31.311 |     1.4
   65 |   0.7636 |     26.143 |   0.9865 |     31.005 |     1.4
   66 |   0.7581 |     25.742 |   0.9787 |     30.821 |     1.5
   67 |   0.7557 |     25.796 |   0.9806 |     31.036 |     1.5
   68 |   0.7504 |     25.200 |   0.9843 |     31.771 |     1.5
   69 |   0.7485 |     25.455 |   0.9915 |     31.403 |     1.5
   70 |   0.7288 |     24.610 |   0.9589 |     30.913 |     1.6
   71 |   0.7291 |     24.355 |   0.9797 |     30.699 |     1.6
   72 |   0.7352 |     24.870 |   0.9800 |     30.699 |     1.6
   73 |   0.7317 |     24.399 |   0.9682 |     30.239 |     1.6
   74 |   0.7060 |     24.052 |   0.9681 |     30.208 |     1.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,114,018

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5758 |     68.422 |   2.0096 |     58.732 |     0.0
    2 |   1.7954 |     50.937 |   1.5945 |     45.496 |     0.1
    3 |   1.5260 |     46.316 |   1.4668 |     45.987 |     0.1
    4 |   1.4513 |     46.435 |   1.4270 |     45.711 |     0.2
    5 |   1.4249 |     46.261 |   1.4089 |     45.466 |     0.2
    6 |   1.4131 |     46.310 |   1.4015 |     45.466 |     0.2
    7 |   1.4062 |     46.229 |   1.3941 |     45.496 |     0.3
    8 |   1.3960 |     46.077 |   1.3824 |     45.404 |     0.3
    9 |   1.3851 |     45.969 |   1.3695 |     45.282 |     0.4
   10 |   1.3676 |     45.080 |   1.3488 |     44.056 |     0.4
   11 |   1.3498 |     44.663 |   1.3386 |     44.026 |     0.5
   12 |   1.3397 |     44.739 |   1.3315 |     44.424 |     0.5
   13 |   1.3301 |     44.468 |   1.3171 |     43.444 |     0.5
   14 |   1.3196 |     44.289 |   1.3156 |     43.658 |     0.6
   15 |   1.3157 |     44.127 |   1.3079 |     43.321 |     0.6
   16 |   1.3036 |     43.807 |   1.3023 |     42.953 |     0.7
   17 |   1.2905 |     43.487 |   1.2955 |     42.586 |     0.7
   18 |   1.2830 |     43.325 |   1.2874 |     42.310 |     0.7
   19 |   1.2759 |     42.620 |   1.2703 |     41.268 |     0.8
   20 |   1.2643 |     42.599 |   1.2709 |     41.942 |     0.8
   21 |   1.2618 |     42.322 |   1.2622 |     41.422 |     0.9
   22 |   1.2505 |     42.008 |   1.2662 |     41.575 |     0.9
   23 |   1.2419 |     41.618 |   1.2545 |     40.993 |     1.0
   24 |   1.2338 |     41.081 |   1.2496 |     40.043 |     1.0
   25 |   1.2308 |     41.222 |   1.2309 |     40.074 |     1.0
   26 |   1.2146 |     40.670 |   1.2212 |     39.216 |     1.1
   27 |   1.2011 |     40.713 |   1.2165 |     39.185 |     1.1
   28 |   1.1975 |     40.643 |   1.2025 |     39.430 |     1.2
   29 |   1.1903 |     40.112 |   1.2018 |     39.093 |     1.2
   30 |   1.1803 |     39.895 |   1.1926 |     39.093 |     1.2
   31 |   1.1722 |     39.846 |   1.2020 |     39.308 |     1.3
   32 |   1.1664 |     39.445 |   1.1796 |     38.450 |     1.3
   33 |   1.1543 |     39.245 |   1.1867 |     38.297 |     1.4
   34 |   1.1487 |     39.250 |   1.1814 |     38.297 |     1.4
   35 |   1.1435 |     38.540 |   1.1662 |     37.990 |     1.5
   36 |   1.1335 |     38.313 |   1.1674 |     38.021 |     1.5
   37 |   1.1308 |     38.015 |   1.1581 |     37.653 |     1.5
   38 |   1.1139 |     37.484 |   1.1606 |     38.695 |     1.6
   39 |   1.1112 |     37.625 |   1.1488 |     37.531 |     1.6
   40 |   1.1023 |     37.245 |   1.1566 |     37.408 |     1.7
   41 |   1.0944 |     36.666 |   1.1409 |     37.071 |     1.7
   42 |   1.0856 |     36.492 |   1.1325 |     36.918 |     1.7
   43 |   1.0751 |     36.151 |   1.1364 |     37.469 |     1.8
   44 |   1.0759 |     35.972 |   1.1286 |     37.010 |     1.8
   45 |   1.0642 |     36.053 |   1.1276 |     36.887 |     1.9
   46 |   1.0586 |     35.614 |   1.1311 |     36.612 |     1.9
   47 |   1.0511 |     35.219 |   1.1258 |     36.550 |     2.0
   48 |   1.0423 |     34.921 |   1.1189 |     37.040 |     2.0
   49 |   1.0307 |     34.682 |   1.1209 |     37.316 |     2.0
   50 |   1.0267 |     34.791 |   1.1131 |     36.581 |     2.1
   51 |   1.0245 |     34.151 |   1.1142 |     36.213 |     2.1
   52 |   1.0175 |     34.168 |   1.1110 |     35.662 |     2.2
   53 |   1.0109 |     34.092 |   1.0985 |     36.336 |     2.2
   54 |   0.9989 |     33.648 |   1.1012 |     36.121 |     2.2
   55 |   1.0025 |     33.843 |   1.1056 |     35.968 |     2.3
   56 |   0.9901 |     33.425 |   1.1131 |     36.091 |     2.3
   57 |   0.9774 |     32.754 |   1.1080 |     35.754 |     2.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 323,522

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5445 |     98.109 |   3.4930 |     97.886 |     0.0
    2 |   3.3137 |     83.577 |   3.0574 |     83.333 |     0.0
    3 |   2.8848 |     83.322 |   2.7835 |     83.333 |     0.1
    4 |   2.7082 |     81.296 |   2.6642 |     67.341 |     0.1
    5 |   2.5998 |     67.436 |   2.5618 |     66.912 |     0.1
    6 |   2.5006 |     64.142 |   2.4712 |     58.824 |     0.1
    7 |   2.4195 |     59.049 |   2.3924 |     58.824 |     0.1
    8 |   2.3479 |     59.049 |   2.3249 |     58.824 |     0.2
    9 |   2.2823 |     59.049 |   2.2653 |     58.824 |     0.2
   10 |   2.2269 |     59.049 |   2.2069 |     58.824 |     0.2
   11 |   2.1714 |     59.049 |   2.1544 |     58.824 |     0.2
   12 |   2.1221 |     59.043 |   2.1079 |     58.762 |     0.3
   13 |   2.0770 |     57.304 |   2.0633 |     57.077 |     0.3
   14 |   2.0334 |     54.085 |   2.0192 |     53.768 |     0.3
   15 |   1.9945 |     53.863 |   1.9773 |     53.768 |     0.3
   16 |   1.9558 |     53.863 |   1.9400 |     53.768 |     0.3
   17 |   1.9204 |     53.863 |   1.9054 |     53.768 |     0.4
   18 |   1.8868 |     51.257 |   1.8722 |     49.602 |     0.4
   19 |   1.8567 |     48.689 |   1.8401 |     48.376 |     0.4
   20 |   1.8254 |     48.597 |   1.8094 |     48.346 |     0.4
   21 |   1.7951 |     48.580 |   1.7789 |     48.346 |     0.4
   22 |   1.7665 |     48.580 |   1.7484 |     48.315 |     0.5
   23 |   1.7404 |     48.580 |   1.7171 |     48.346 |     0.5
   24 |   1.7054 |     48.597 |   1.6880 |     48.346 |     0.5
   25 |   1.6809 |     48.564 |   1.6615 |     48.284 |     0.5
   26 |   1.6552 |     48.044 |   1.6382 |     45.404 |     0.5
   27 |   1.6306 |     46.148 |   1.6175 |     45.466 |     0.6
   28 |   1.6144 |     46.158 |   1.5991 |     45.404 |     0.6
   29 |   1.5992 |     46.142 |   1.5829 |     45.466 |     0.6
   30 |   1.5844 |     46.158 |   1.5690 |     45.404 |     0.6
   31 |   1.5677 |     46.142 |   1.5561 |     45.466 |     0.6
   32 |   1.5566 |     46.158 |   1.5442 |     45.466 |     0.7
   33 |   1.5425 |     46.153 |   1.5337 |     45.404 |     0.7
   34 |   1.5351 |     46.153 |   1.5243 |     45.404 |     0.7
   35 |   1.5240 |     46.164 |   1.5149 |     45.466 |     0.7
   36 |   1.5150 |     46.158 |   1.5066 |     45.404 |     0.7
   37 |   1.5055 |     46.158 |   1.4991 |     45.466 |     0.8
   38 |   1.5033 |     46.158 |   1.4922 |     45.466 |     0.8
   39 |   1.4948 |     46.158 |   1.4859 |     45.466 |     0.8
   40 |   1.4874 |     46.164 |   1.4797 |     45.466 |     0.8
   41 |   1.4814 |     46.169 |   1.4741 |     45.466 |     0.8
   42 |   1.4776 |     46.158 |   1.4688 |     45.466 |     0.9
   43 |   1.4705 |     46.164 |   1.4638 |     45.466 |     0.9
   44 |   1.4651 |     46.175 |   1.4587 |     45.466 |     0.9
   45 |   1.4619 |     46.164 |   1.4543 |     45.466 |     0.9
   46 |   1.4559 |     46.175 |   1.4498 |     45.466 |     0.9
   47 |   1.4520 |     46.191 |   1.4451 |     45.466 |     1.0
   48 |   1.4482 |     46.180 |   1.4412 |     45.466 |     1.0
   49 |   1.4420 |     46.137 |   1.4350 |     45.435 |     1.0
   50 |   1.4344 |     46.153 |   1.4292 |     45.435 |     1.0
   51 |   1.4277 |     46.153 |   1.4241 |     45.404 |     1.0
   52 |   1.4231 |     46.045 |   1.4199 |     45.343 |     1.1
   53 |   1.4169 |     45.888 |   1.4150 |     45.374 |     1.1
   54 |   1.4082 |     45.692 |   1.4098 |     45.006 |     1.1
   55 |   1.4026 |     45.427 |   1.4052 |     44.822 |     1.1
   56 |   1.3954 |     45.302 |   1.3987 |     44.975 |     1.1
   57 |   1.3893 |     45.086 |   1.3946 |     44.945 |     1.2
   58 |   1.3832 |     44.901 |   1.3892 |     45.067 |     1.2
   59 |   1.3728 |     44.766 |   1.3822 |     44.975 |     1.2
   60 |   1.3659 |     44.690 |   1.3736 |     44.853 |     1.2
   61 |   1.3544 |     44.495 |   1.3677 |     44.914 |     1.2
   62 |   1.3445 |     44.072 |   1.3620 |     44.547 |     1.3
   63 |   1.3334 |     43.623 |   1.3577 |     44.271 |     1.3
   64 |   1.3250 |     43.308 |   1.3513 |     43.934 |     1.3
   65 |   1.3131 |     42.929 |   1.3453 |     43.811 |     1.3
   66 |   1.3094 |     42.756 |   1.3434 |     43.934 |     1.3
   67 |   1.2989 |     42.669 |   1.3393 |     44.026 |     1.4
   68 |   1.2899 |     42.523 |   1.3334 |     44.118 |     1.4
   69 |   1.2856 |     42.382 |   1.3274 |     43.719 |     1.4
   70 |   1.2796 |     42.084 |   1.3288 |     43.934 |     1.4
   71 |   1.2755 |     41.732 |   1.3248 |     44.087 |     1.4
   72 |   1.2667 |     41.629 |   1.3222 |     43.658 |     1.5
   73 |   1.2606 |     41.417 |   1.3184 |     43.597 |     1.5
   74 |   1.2564 |     41.222 |   1.3176 |     43.474 |     1.5
   75 |   1.2480 |     41.006 |   1.3124 |     43.719 |     1.5
   76 |   1.2450 |     40.919 |   1.3123 |     42.892 |     1.5
   77 |   1.2400 |     40.664 |   1.3105 |     43.199 |     1.6
   78 |   1.2341 |     40.561 |   1.3109 |     42.923 |     1.6
   79 |   1.2306 |     40.334 |   1.3100 |     42.831 |     1.6
   80 |   1.2228 |     40.139 |   1.3068 |     42.708 |     1.6
   81 |   1.2212 |     39.873 |   1.3084 |     42.892 |     1.6
   82 |   1.2155 |     39.808 |   1.3071 |     43.352 |     1.7
   83 |   1.2100 |     39.721 |   1.3093 |     43.597 |     1.7
   84 |   1.2078 |     39.521 |   1.3015 |     43.260 |     1.7
   85 |   1.2032 |     39.386 |   1.3017 |     42.892 |     1.7
   86 |   1.2004 |     39.201 |   1.2983 |     42.892 |     1.7
   87 |   1.1925 |     39.201 |   1.3009 |     43.076 |     1.8
   88 |   1.1904 |     39.153 |   1.3018 |     42.800 |     1.8
   89 |   1.1914 |     38.968 |   1.3000 |     42.770 |     1.8
   90 |   1.1832 |     38.643 |   1.3014 |     42.831 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,649,698

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7650 |     73.765 |   2.3349 |     58.824 |     0.1
    2 |   2.1216 |     57.965 |   1.9394 |     58.824 |     0.1
    3 |   1.7916 |     51.170 |   1.6567 |     45.496 |     0.2
    4 |   1.5914 |     46.348 |   1.5165 |     45.466 |     0.2
    5 |   1.4889 |     46.207 |   1.4579 |     46.017 |     0.3
    6 |   1.4472 |     46.326 |   1.4271 |     45.956 |     0.4
    7 |   1.4254 |     46.272 |   1.4126 |     45.466 |     0.4
    8 |   1.4163 |     46.207 |   1.4064 |     46.017 |     0.5
    9 |   1.4097 |     46.169 |   1.4003 |     45.466 |     0.6
   10 |   1.4044 |     46.272 |   1.3992 |     45.987 |     0.6
   11 |   1.4016 |     46.310 |   1.3945 |     46.048 |     0.7
   12 |   1.3980 |     46.196 |   1.3922 |     45.987 |     0.8
   13 |   1.3966 |     46.299 |   1.3883 |     45.466 |     0.8
   14 |   1.3915 |     46.299 |   1.3846 |     45.466 |     0.9
   15 |   1.3873 |     46.186 |   1.3774 |     45.404 |     0.9
   16 |   1.3781 |     45.671 |   1.3649 |     44.393 |     1.0
   17 |   1.3693 |     45.199 |   1.3543 |     44.179 |     1.1
   18 |   1.3593 |     44.912 |   1.3478 |     43.964 |     1.1
   19 |   1.3556 |     44.874 |   1.3461 |     44.271 |     1.2
   20 |   1.3468 |     44.880 |   1.3347 |     43.964 |     1.3
   21 |   1.3390 |     44.587 |   1.3333 |     43.995 |     1.3
   22 |   1.3343 |     44.528 |   1.3220 |     43.842 |     1.4
   23 |   1.3319 |     44.327 |   1.3301 |     43.352 |     1.4
   24 |   1.3341 |     44.192 |   1.3187 |     43.413 |     1.5
   25 |   1.3249 |     44.062 |   1.3171 |     43.260 |     1.6
   26 |   1.3183 |     43.872 |   1.3114 |     43.168 |     1.6
   27 |   1.3132 |     43.493 |   1.3053 |     43.015 |     1.7
   28 |   1.3074 |     43.336 |   1.3057 |     42.586 |     1.8
   29 |   1.3037 |     43.634 |   1.2951 |     42.433 |     1.8
   30 |   1.2888 |     43.032 |   1.2854 |     42.953 |     1.9
   31 |   1.2898 |     42.907 |   1.2891 |     42.433 |     2.0
   32 |   1.2819 |     42.756 |   1.2884 |     42.004 |     2.0
   33 |   1.2827 |     42.577 |   1.2787 |     41.575 |     2.1
   34 |   1.2758 |     42.674 |   1.2818 |     41.881 |     2.1
   35 |   1.2664 |     42.225 |   1.2655 |     41.176 |     2.2
   36 |   1.2605 |     42.116 |   1.2638 |     41.575 |     2.3
   37 |   1.2550 |     41.954 |   1.2694 |     41.759 |     2.3
   38 |   1.2563 |     42.523 |   1.2601 |     41.422 |     2.4
   39 |   1.2525 |     42.003 |   1.2531 |     41.452 |     2.5
   40 |   1.2453 |     41.959 |   1.2533 |     41.176 |     2.5
   41 |   1.2437 |     41.726 |   1.2557 |     41.268 |     2.6
   42 |   1.2401 |     41.732 |   1.2537 |     41.299 |     2.7
   43 |   1.2320 |     41.260 |   1.2460 |     41.238 |     2.7
   44 |   1.2283 |     41.304 |   1.2425 |     41.023 |     2.8
   45 |   1.2203 |     41.157 |   1.2352 |     40.472 |     2.8
   46 |   1.2180 |     41.081 |   1.2397 |     40.441 |     2.9
   47 |   1.2158 |     40.859 |   1.2389 |     40.564 |     3.0
   48 |   1.2123 |     40.811 |   1.2307 |     39.890 |     3.0
   49 |   1.2040 |     40.323 |   1.2357 |     40.472 |     3.1
   50 |   1.2064 |     40.420 |   1.2324 |     40.012 |     3.2
   51 |   1.2048 |     40.415 |   1.2290 |     39.767 |     3.2
   52 |   1.1986 |     40.312 |   1.2256 |     39.583 |     3.3
   53 |   1.1950 |     40.090 |   1.2355 |     40.502 |     3.4
   54 |   1.1950 |     40.122 |   1.2338 |     40.411 |     3.4
   55 |   1.1884 |     39.949 |   1.2288 |     40.165 |     3.5
   56 |   1.1817 |     39.971 |   1.2257 |     39.645 |     3.5
   57 |   1.1719 |     39.694 |   1.2131 |     40.012 |     3.6
   58 |   1.1696 |     39.575 |   1.2092 |     39.246 |     3.7
   59 |   1.1663 |     39.337 |   1.2107 |     39.583 |     3.7
   60 |   1.1646 |     39.077 |   1.1958 |     38.480 |     3.8
   61 |   1.1546 |     38.936 |   1.1995 |     39.400 |     3.9
   62 |   1.1466 |     38.405 |   1.1889 |     39.216 |     3.9
   63 |   1.1467 |     38.448 |   1.2013 |     39.920 |     4.0
   64 |   1.1446 |     38.741 |   1.1902 |     39.124 |     4.0
   65 |   1.1488 |     38.589 |   1.1922 |     38.940 |     4.1
   66 |   1.1422 |     38.118 |   1.1878 |     39.614 |     4.2
   67 |   1.1333 |     37.885 |   1.2024 |     39.461 |     4.2
   68 |   1.1368 |     38.096 |   1.1926 |     39.767 |     4.3
   69 |   1.1236 |     37.950 |   1.1865 |     39.001 |     4.4
   70 |   1.1234 |     37.998 |   1.1800 |     38.634 |     4.4
   71 |   1.1138 |     37.467 |   1.1786 |     38.113 |     4.5
   72 |   1.1091 |     37.115 |   1.1757 |     38.848 |     4.6
   73 |   1.1032 |     37.430 |   1.1705 |     38.848 |     4.6
   74 |   1.0930 |     36.839 |   1.1615 |     38.143 |     4.7
   75 |   1.0967 |     36.920 |   1.1607 |     38.603 |     4.7
   76 |   1.0856 |     36.926 |   1.1639 |     39.093 |     4.8
   77 |   1.0860 |     36.601 |   1.1575 |     37.898 |     4.9
   78 |   1.0792 |     36.416 |   1.1592 |     37.377 |     4.9
   79 |   1.0708 |     36.210 |   1.1602 |     37.469 |     5.0
   80 |   1.0671 |     36.059 |   1.1553 |     37.868 |     5.1
   81 |   1.0584 |     35.571 |   1.1633 |     37.714 |     5.1
   82 |   1.0741 |     36.005 |   1.1484 |     37.286 |     5.2
   83 |   1.0633 |     35.864 |   1.1357 |     37.194 |     5.3
   84 |   1.0487 |     35.300 |   1.1529 |     38.266 |     5.3
   85 |   1.0441 |     35.035 |   1.1388 |     36.612 |     5.4
   86 |   1.0416 |     35.322 |   1.1318 |     36.305 |     5.4
   87 |   1.0396 |     35.094 |   1.1369 |     37.592 |     5.5
   88 |   1.0404 |     35.008 |   1.1333 |     36.489 |     5.6
   89 |   1.0350 |     34.813 |   1.1235 |     36.397 |     5.6
   90 |   1.0272 |     34.254 |   1.1276 |     36.428 |     5.7
   91 |   1.0178 |     34.287 |   1.1302 |     37.040 |     5.8
   92 |   1.0171 |     34.151 |   1.1181 |     35.723 |     5.8
   93 |   1.0159 |     34.585 |   1.1223 |     36.458 |     5.9
   94 |   1.0139 |     34.108 |   1.1171 |     35.784 |     5.9
   95 |   1.0086 |     34.049 |   1.1042 |     36.581 |     6.0
   96 |   1.0096 |     33.902 |   1.1183 |     36.458 |     6.1
   97 |   0.9980 |     33.853 |   1.1119 |     35.815 |     6.1
   98 |   0.9948 |     33.458 |   1.1072 |     36.397 |     6.2
   99 |   0.9875 |     33.268 |   1.1124 |     36.091 |     6.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 710,050

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5631 |     68.666 |   2.0149 |     53.768 |     0.0
    2 |   1.7980 |     50.845 |   1.5946 |     45.466 |     0.1
    3 |   1.5184 |     46.218 |   1.4593 |     45.987 |     0.1
    4 |   1.4438 |     46.267 |   1.4195 |     45.466 |     0.1
    5 |   1.4210 |     46.353 |   1.4050 |     45.466 |     0.1
    6 |   1.4100 |     46.321 |   1.3997 |     45.987 |     0.2
    7 |   1.4034 |     46.321 |   1.3970 |     45.987 |     0.2
    8 |   1.4001 |     46.397 |   1.3907 |     45.466 |     0.2
    9 |   1.3974 |     46.337 |   1.3911 |     45.466 |     0.2
   10 |   1.3941 |     46.435 |   1.3894 |     45.987 |     0.3
   11 |   1.3935 |     46.158 |   1.3889 |     45.466 |     0.3
   12 |   1.3903 |     46.126 |   1.3819 |     45.343 |     0.3
   13 |   1.3898 |     45.877 |   1.3804 |     45.190 |     0.3
   14 |   1.3777 |     45.584 |   1.3632 |     45.221 |     0.4
   15 |   1.3616 |     45.416 |   1.3547 |     43.873 |     0.4
   16 |   1.3527 |     44.766 |   1.3349 |     43.719 |     0.4
   17 |   1.3406 |     44.641 |   1.3293 |     43.964 |     0.5
   18 |   1.3356 |     44.647 |   1.3259 |     43.934 |     0.5
   19 |   1.3296 |     44.338 |   1.3178 |     43.505 |     0.5
   20 |   1.3230 |     44.403 |   1.3146 |     43.352 |     0.5
   21 |   1.3126 |     44.094 |   1.3115 |     43.107 |     0.6
   22 |   1.3088 |     44.062 |   1.3097 |     43.382 |     0.6
   23 |   1.3023 |     43.845 |   1.3058 |     43.382 |     0.6
   24 |   1.2963 |     43.715 |   1.2948 |     42.371 |     0.7
   25 |   1.2936 |     43.547 |   1.2931 |     42.708 |     0.7
   26 |   1.2825 |     43.298 |   1.2919 |     42.586 |     0.7
   27 |   1.2826 |     43.113 |   1.2771 |     42.586 |     0.7
   28 |   1.2674 |     42.826 |   1.2875 |     42.739 |     0.8
   29 |   1.2628 |     42.658 |   1.2736 |     42.188 |     0.8
   30 |   1.2614 |     42.561 |   1.2648 |     42.065 |     0.8
   31 |   1.2465 |     42.409 |   1.2720 |     41.973 |     0.8
   32 |   1.2435 |     42.035 |   1.2488 |     41.575 |     0.9
   33 |   1.2340 |     41.905 |   1.2474 |     41.942 |     0.9
   34 |   1.2236 |     41.629 |   1.2429 |     41.483 |     0.9
   35 |   1.2246 |     41.678 |   1.2364 |     41.330 |     1.0
   36 |   1.2169 |     41.417 |   1.2466 |     41.636 |     1.0
   37 |   1.2069 |     41.428 |   1.2388 |     41.483 |     1.0
   38 |   1.2079 |     41.423 |   1.2279 |     41.085 |     1.0
   39 |   1.2044 |     41.239 |   1.2317 |     41.176 |     1.1
   40 |   1.2005 |     40.876 |   1.2328 |     40.931 |     1.1
   41 |   1.1877 |     40.718 |   1.2164 |     40.319 |     1.1
   42 |   1.1845 |     40.415 |   1.2114 |     40.809 |     1.1
   43 |   1.1734 |     40.014 |   1.2066 |     40.748 |     1.2
   44 |   1.1675 |     39.927 |   1.2055 |     40.625 |     1.2
   45 |   1.1609 |     39.635 |   1.1936 |     40.165 |     1.2
   46 |   1.1542 |     39.337 |   1.1837 |     39.583 |     1.3
   47 |   1.1366 |     39.115 |   1.1855 |     39.553 |     1.3
   48 |   1.1340 |     38.703 |   1.1877 |     40.012 |     1.3
   49 |   1.1283 |     38.529 |   1.1780 |     39.614 |     1.3
   50 |   1.1188 |     38.302 |   1.1841 |     39.430 |     1.4
   51 |   1.1189 |     37.966 |   1.1751 |     39.491 |     1.4
   52 |   1.1158 |     38.020 |   1.1481 |     38.879 |     1.4
   53 |   1.1095 |     37.874 |   1.1732 |     39.982 |     1.5
   54 |   1.1013 |     38.064 |   1.1559 |     38.113 |     1.5
   55 |   1.0913 |     37.467 |   1.1473 |     38.143 |     1.5
   56 |   1.0797 |     37.077 |   1.1528 |     38.756 |     1.5
   57 |   1.0736 |     36.758 |   1.1506 |     38.174 |     1.6
   58 |   1.0648 |     36.698 |   1.1480 |     38.542 |     1.6
   59 |   1.0611 |     36.660 |   1.1530 |     37.592 |     1.6
   60 |   1.0545 |     35.858 |   1.1279 |     37.071 |     1.6
   61 |   1.0506 |     36.070 |   1.1411 |     37.408 |     1.7
   62 |   1.0424 |     35.598 |   1.1281 |     36.703 |     1.7
   63 |   1.0383 |     35.501 |   1.1365 |     37.377 |     1.7
   64 |   1.0429 |     35.831 |   1.1188 |     37.194 |     1.7
   65 |   1.0328 |     35.560 |   1.1204 |     36.887 |     1.8
   66 |   1.0222 |     34.829 |   1.1293 |     37.316 |     1.8
   67 |   1.0208 |     35.024 |   1.1227 |     37.040 |     1.8
   68 |   1.0109 |     34.628 |   1.1323 |     37.132 |     1.8
   69 |   1.0042 |     34.195 |   1.1186 |     36.581 |     1.9
   70 |   0.9989 |     34.195 |   1.1098 |     36.673 |     1.9
   71 |   1.0070 |     34.818 |   1.1083 |     35.999 |     1.9
   72 |   0.9875 |     33.978 |   1.0965 |     35.447 |     2.0
   73 |   0.9852 |     33.658 |   1.1073 |     35.662 |     2.0
   74 |   0.9812 |     33.658 |   1.0962 |     35.600 |     2.0
   75 |   0.9936 |     33.805 |   1.0951 |     35.600 |     2.0
   76 |   0.9637 |     32.900 |   1.1029 |     35.570 |     2.1
   77 |   0.9615 |     32.851 |   1.0935 |     35.202 |     2.1
   78 |   0.9499 |     32.689 |   1.0976 |     35.662 |     2.1
   79 |   0.9464 |     32.564 |   1.0896 |     35.754 |     2.1
   80 |   0.9400 |     32.618 |   1.0935 |     35.754 |     2.2
   81 |   0.9331 |     32.434 |   1.0807 |     34.651 |     2.2
   82 |   0.9323 |     32.000 |   1.0827 |     34.835 |     2.2
   83 |   0.9270 |     32.038 |   1.0894 |     34.896 |     2.2
   84 |   0.9332 |     32.380 |   1.0780 |     35.018 |     2.3
   85 |   0.9187 |     31.350 |   1.1072 |     35.539 |     2.3
   86 |   0.9201 |     31.778 |   1.0812 |     34.773 |     2.3
   87 |   0.9070 |     30.760 |   1.0724 |     34.467 |     2.3
   88 |   0.9037 |     31.139 |   1.0850 |     35.202 |     2.4
   89 |   0.9009 |     31.182 |   1.0618 |     33.854 |     2.4
   90 |   0.8960 |     30.743 |   1.0695 |     34.559 |     2.4
   91 |   0.9081 |     31.063 |   1.0686 |     34.436 |     2.4
   92 |   0.8878 |     30.505 |   1.0562 |     34.651 |     2.5
   93 |   0.8838 |     30.180 |   1.0570 |     33.946 |     2.5
   94 |   0.8814 |     30.435 |   1.0666 |     33.946 |     2.5
   95 |   0.8677 |     30.185 |   1.0654 |     34.099 |     2.5
   96 |   0.8683 |     29.752 |   1.0697 |     34.252 |     2.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 367,586

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5654 |     68.032 |   2.0279 |     58.824 |     0.0
    2 |   1.7918 |     50.130 |   1.5890 |     45.466 |     0.0
    3 |   1.5091 |     46.104 |   1.4593 |     45.987 |     0.1
    4 |   1.4372 |     46.381 |   1.4186 |     45.466 |     0.1
    5 |   1.4008 |     46.332 |   1.3794 |     45.987 |     0.1
    6 |   1.3608 |     45.915 |   1.3448 |     44.761 |     0.1
    7 |   1.3272 |     45.161 |   1.3156 |     43.873 |     0.1
    8 |   1.2970 |     44.381 |   1.2988 |     43.750 |     0.1
    9 |   1.2768 |     43.942 |   1.2856 |     43.627 |     0.1
   10 |   1.2552 |     43.135 |   1.2661 |     42.831 |     0.2
   11 |   1.2331 |     42.631 |   1.2600 |     43.015 |     0.2
   12 |   1.2135 |     42.035 |   1.2441 |     41.636 |     0.2
   13 |   1.1972 |     41.184 |   1.2418 |     42.157 |     0.2
   14 |   1.1823 |     40.729 |   1.2206 |     41.452 |     0.2
   15 |   1.1632 |     40.220 |   1.2140 |     41.146 |     0.2
   16 |   1.1510 |     39.445 |   1.2053 |     40.870 |     0.3
   17 |   1.1283 |     39.044 |   1.1993 |     40.901 |     0.3
   18 |   1.1112 |     38.443 |   1.1803 |     40.196 |     0.3
   19 |   1.0936 |     37.679 |   1.1812 |     38.725 |     0.3
   20 |   1.0678 |     36.335 |   1.1676 |     39.001 |     0.3
   21 |   1.0504 |     35.636 |   1.1456 |     37.898 |     0.3
   22 |   1.0285 |     34.731 |   1.1269 |     37.439 |     0.4
   23 |   1.0149 |     34.244 |   1.1322 |     37.776 |     0.4
   24 |   0.9924 |     33.653 |   1.1164 |     36.612 |     0.4
   25 |   0.9647 |     32.472 |   1.0983 |     36.520 |     0.4
   26 |   0.9356 |     31.692 |   1.0930 |     37.010 |     0.4
   27 |   0.9295 |     31.155 |   1.0995 |     36.703 |     0.4
   28 |   0.9149 |     30.836 |   1.0946 |     36.213 |     0.5
   29 |   0.8895 |     29.947 |   1.0880 |     35.509 |     0.5
   30 |   0.8840 |     29.232 |   1.0735 |     34.957 |     0.5
   31 |   0.8538 |     28.544 |   1.0596 |     34.344 |     0.5
   32 |   0.8223 |     27.140 |   1.0810 |     34.559 |     0.5
   33 |   0.8061 |     26.485 |   1.0760 |     34.252 |     0.5
   34 |   0.7898 |     26.040 |   1.0851 |     33.885 |     0.6
   35 |   0.7851 |     26.073 |   1.0851 |     33.548 |     0.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 2,697,602

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2124 |     59.991 |   1.6642 |     48.376 |     0.1
    2 |   1.5104 |     46.137 |   1.4101 |     45.159 |     0.2
    3 |   1.3860 |     44.679 |   1.3478 |     43.566 |     0.3
    4 |   1.3311 |     43.498 |   1.2931 |     41.759 |     0.3
    5 |   1.2865 |     42.268 |   1.2534 |     39.920 |     0.4
    6 |   1.2464 |     40.762 |   1.2386 |     39.124 |     0.5
    7 |   1.2139 |     39.613 |   1.2010 |     38.358 |     0.6
    8 |   1.1900 |     39.055 |   1.1863 |     38.817 |     0.7
    9 |   1.1691 |     38.318 |   1.1740 |     38.113 |     0.8
   10 |   1.1420 |     37.527 |   1.1447 |     37.776 |     0.8
   11 |   1.1178 |     36.806 |   1.1180 |     36.305 |     0.9
   12 |   1.0915 |     36.042 |   1.1121 |     36.734 |     1.0
   13 |   1.0864 |     36.021 |   1.1213 |     37.102 |     1.1
   14 |   1.0562 |     34.802 |   1.1029 |     35.846 |     1.2
   15 |   1.0414 |     34.585 |   1.0849 |     35.447 |     1.3
   16 |   1.0212 |     34.363 |   1.1001 |     36.336 |     1.3
   17 |   1.0046 |     33.209 |   1.0740 |     34.589 |     1.4
   18 |   0.9817 |     32.792 |   1.0663 |     35.172 |     1.5
   19 |   0.9641 |     32.136 |   1.0854 |     35.938 |     1.6
   20 |   0.9473 |     31.865 |   1.0682 |     34.651 |     1.7
   21 |   0.9375 |     31.448 |   1.0518 |     34.620 |     1.8
   22 |   0.9131 |     30.890 |   1.0539 |     33.915 |     1.8
   23 |   0.8869 |     30.158 |   1.0203 |     33.303 |     1.9
   24 |   0.8826 |     29.958 |   1.0504 |     34.620 |     2.0
   25 |   0.8701 |     29.465 |   1.0139 |     33.272 |     2.1
   26 |   0.8447 |     28.484 |   1.0196 |     33.824 |     2.2
   27 |   0.8387 |     28.143 |   0.9833 |     32.506 |     2.3
   28 |   0.8162 |     27.595 |   0.9833 |     32.996 |     2.3
   29 |   0.7958 |     26.983 |   0.9788 |     32.721 |     2.4
   30 |   0.7951 |     27.010 |   0.9989 |     33.211 |     2.5
   31 |   0.7883 |     27.308 |   0.9943 |     32.812 |     2.6
   32 |   0.7671 |     26.349 |   0.9642 |     31.587 |     2.7
   33 |   0.7544 |     26.067 |   0.9790 |     31.648 |     2.8
   34 |   0.7348 |     25.054 |   0.9787 |     31.955 |     2.9
   35 |   0.7249 |     25.163 |   0.9648 |     31.281 |     2.9
   36 |   0.7418 |     25.141 |   0.9776 |     30.974 |     3.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 392,578

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5785 |     69.549 |   1.9986 |     54.075 |     0.0
    2 |   1.7831 |     50.146 |   1.6045 |     48.346 |     0.0
    3 |   1.5232 |     46.684 |   1.4611 |     45.466 |     0.1
    4 |   1.4435 |     46.337 |   1.4234 |     45.466 |     0.1
    5 |   1.4100 |     46.245 |   1.3918 |     45.987 |     0.1
    6 |   1.3882 |     46.213 |   1.3703 |     45.466 |     0.1
    7 |   1.3665 |     45.817 |   1.3572 |     44.608 |     0.1
    8 |   1.3458 |     44.896 |   1.3398 |     44.547 |     0.2
    9 |   1.3328 |     44.771 |   1.3267 |     43.750 |     0.2
   10 |   1.3165 |     44.235 |   1.3159 |     43.505 |     0.2
   11 |   1.3023 |     43.921 |   1.2878 |     42.525 |     0.2
   12 |   1.2747 |     42.712 |   1.2602 |     41.636 |     0.2
   13 |   1.2375 |     41.661 |   1.2482 |     41.575 |     0.3
   14 |   1.2180 |     41.282 |   1.2209 |     40.533 |     0.3
   15 |   1.1942 |     40.605 |   1.2124 |     40.778 |     0.3
   16 |   1.1768 |     40.247 |   1.2097 |     40.533 |     0.3
   17 |   1.1585 |     39.613 |   1.1919 |     39.828 |     0.3
   18 |   1.1488 |     39.391 |   1.1802 |     39.798 |     0.4
   19 |   1.1288 |     38.882 |   1.1662 |     39.522 |     0.4
   20 |   1.1017 |     37.776 |   1.1542 |     39.828 |     0.4
   21 |   1.0895 |     37.365 |   1.1537 |     39.828 |     0.4
   22 |   1.0710 |     36.601 |   1.1356 |     38.909 |     0.4
   23 |   1.0726 |     36.454 |   1.1459 |     38.725 |     0.5
   24 |   1.0506 |     35.501 |   1.1253 |     37.929 |     0.5
   25 |   1.0203 |     34.265 |   1.1055 |     37.286 |     0.5
   26 |   0.9931 |     33.545 |   1.0962 |     35.907 |     0.5
   27 |   0.9720 |     32.136 |   1.0850 |     36.520 |     0.5
   28 |   0.9515 |     31.529 |   1.0664 |     35.386 |     0.6
   29 |   0.9326 |     31.047 |   1.0689 |     34.896 |     0.6
   30 |   0.9077 |     29.974 |   1.0674 |     34.896 |     0.6
   31 |   0.8883 |     29.188 |   1.0452 |     33.670 |     0.6
   32 |   0.8685 |     28.587 |   1.0600 |     33.793 |     0.6
   33 |   0.8479 |     27.731 |   1.0232 |     32.935 |     0.7
   34 |   0.8271 |     27.352 |   1.0308 |     32.567 |     0.7
   35 |   0.8035 |     26.560 |   1.0090 |     32.047 |     0.7
   36 |   0.7766 |     25.000 |   1.0086 |     32.108 |     0.7
   37 |   0.7780 |     25.081 |   1.0181 |     31.526 |     0.7
   38 |   0.7482 |     24.014 |   1.0072 |     31.464 |     0.8
   39 |   0.7322 |     23.743 |   1.0166 |     31.526 |     0.8
   40 |   0.7149 |     23.077 |   0.9998 |     30.944 |     0.8
   41 |   0.6999 |     22.394 |   0.9841 |     30.331 |     0.8
   42 |   0.6775 |     21.733 |   0.9869 |     30.576 |     0.8
   43 |   0.6527 |     20.422 |   1.0054 |     31.342 |     0.9
   44 |   0.6496 |     20.969 |   0.9874 |     30.178 |     0.9
   45 |   0.6292 |     19.874 |   0.9893 |     30.392 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 730,498

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2700 |     84.953 |   2.7517 |     83.333 |     0.0
    2 |   2.4789 |     64.293 |   2.2412 |     58.824 |     0.0
    3 |   2.1035 |     56.757 |   2.0022 |     54.197 |     0.1
    4 |   1.9289 |     53.002 |   1.8592 |     48.346 |     0.1
    5 |   1.8112 |     48.900 |   1.7555 |     48.346 |     0.1
    6 |   1.7158 |     48.624 |   1.6651 |     48.346 |     0.1
    7 |   1.6349 |     48.553 |   1.5848 |     45.466 |     0.1
    8 |   1.5682 |     46.272 |   1.5245 |     45.466 |     0.2
    9 |   1.5131 |     46.202 |   1.4857 |     45.466 |     0.2
   10 |   1.4810 |     46.202 |   1.4605 |     45.466 |     0.2
   11 |   1.4619 |     46.261 |   1.4432 |     45.466 |     0.2
   12 |   1.4458 |     46.332 |   1.4333 |     45.895 |     0.2
   13 |   1.4364 |     46.251 |   1.4244 |     45.987 |     0.3
   14 |   1.4270 |     46.261 |   1.4178 |     45.466 |     0.3
   15 |   1.4222 |     46.343 |   1.4133 |     45.466 |     0.3
   16 |   1.4171 |     46.316 |   1.4096 |     45.466 |     0.3
   17 |   1.4134 |     46.419 |   1.4061 |     45.466 |     0.3
   18 |   1.4104 |     46.245 |   1.4032 |     45.466 |     0.4
   19 |   1.4066 |     46.202 |   1.4010 |     45.466 |     0.4
   20 |   1.4059 |     46.251 |   1.3983 |     45.466 |     0.4
   21 |   1.4034 |     46.261 |   1.3965 |     45.466 |     0.4
   22 |   1.4010 |     46.310 |   1.3935 |     45.466 |     0.4
   23 |   1.3965 |     46.419 |   1.3910 |     45.466 |     0.4
   24 |   1.3939 |     46.261 |   1.3872 |     45.466 |     0.5
   25 |   1.3877 |     46.294 |   1.3812 |     45.466 |     0.5
   26 |   1.3817 |     46.229 |   1.3816 |     45.864 |     0.5
   27 |   1.3738 |     46.299 |   1.3691 |     45.466 |     0.5
   28 |   1.3653 |     46.158 |   1.3615 |     45.404 |     0.5
   29 |   1.3565 |     45.974 |   1.3574 |     44.914 |     0.6
   30 |   1.3495 |     45.660 |   1.3516 |     44.118 |     0.6
   31 |   1.3457 |     45.438 |   1.3484 |     44.332 |     0.6
   32 |   1.3393 |     45.194 |   1.3422 |     44.271 |     0.6
   33 |   1.3344 |     45.254 |   1.3381 |     44.761 |     0.6
   34 |   1.3291 |     45.221 |   1.3358 |     44.577 |     0.7
   35 |   1.3276 |     45.107 |   1.3352 |     44.792 |     0.7
   36 |   1.3207 |     44.988 |   1.3289 |     44.087 |     0.7
   37 |   1.3188 |     45.010 |   1.3281 |     44.638 |     0.7
   38 |   1.3129 |     44.831 |   1.3259 |     44.087 |     0.7
   39 |   1.3109 |     44.826 |   1.3223 |     43.750 |     0.8
   40 |   1.3078 |     44.560 |   1.3245 |     44.271 |     0.8
   41 |   1.3001 |     44.560 |   1.3177 |     43.719 |     0.8
   42 |   1.2962 |     44.246 |   1.3124 |     43.505 |     0.8
   43 |   1.2940 |     44.235 |   1.3135 |     43.689 |     0.8
   44 |   1.2888 |     44.267 |   1.3097 |     43.260 |     0.9
   45 |   1.2847 |     44.024 |   1.3066 |     43.199 |     0.9
   46 |   1.2785 |     43.731 |   1.3012 |     43.199 |     0.9
   47 |   1.2734 |     43.373 |   1.2972 |     42.923 |     0.9
   48 |   1.2690 |     43.130 |   1.2957 |     42.892 |     0.9
   49 |   1.2635 |     42.875 |   1.2914 |     42.862 |     0.9
   50 |   1.2568 |     42.512 |   1.2827 |     42.586 |     1.0
   51 |   1.2502 |     42.078 |   1.2802 |     42.494 |     1.0
   52 |   1.2421 |     41.547 |   1.2725 |     41.544 |     1.0
   53 |   1.2339 |     41.342 |   1.2679 |     41.667 |     1.0
   54 |   1.2244 |     41.038 |   1.2588 |     41.238 |     1.0
   55 |   1.2165 |     40.480 |   1.2563 |     40.901 |     1.1
   56 |   1.2056 |     40.063 |   1.2440 |     40.502 |     1.1
   57 |   1.1968 |     39.797 |   1.2372 |     40.502 |     1.1
   58 |   1.1892 |     39.510 |   1.2272 |     40.104 |     1.1
   59 |   1.1785 |     39.321 |   1.2274 |     40.104 |     1.1
   60 |   1.1689 |     39.256 |   1.2122 |     40.227 |     1.2
   61 |   1.1605 |     38.990 |   1.2183 |     39.706 |     1.2
   62 |   1.1536 |     38.638 |   1.2032 |     39.828 |     1.2
   63 |   1.1436 |     38.529 |   1.1997 |     39.828 |     1.2
   64 |   1.1364 |     38.280 |   1.1951 |     39.706 |     1.2
   65 |   1.1262 |     37.955 |   1.1864 |     39.216 |     1.3
   66 |   1.1185 |     37.982 |   1.1855 |     39.430 |     1.3
   67 |   1.1132 |     37.489 |   1.1760 |     39.032 |     1.3
   68 |   1.1049 |     37.527 |   1.1733 |     39.246 |     1.3
   69 |   1.0958 |     37.148 |   1.1751 |     38.879 |     1.3
   70 |   1.0883 |     36.812 |   1.1693 |     38.603 |     1.3
   71 |   1.0842 |     36.758 |   1.1629 |     38.664 |     1.4
   72 |   1.0702 |     36.416 |   1.1578 |     38.695 |     1.4
   73 |   1.0660 |     36.254 |   1.1520 |     38.235 |     1.4
   74 |   1.0596 |     35.940 |   1.1518 |     38.235 |     1.4
   75 |   1.0517 |     35.696 |   1.1457 |     37.806 |     1.4
   76 |   1.0429 |     35.799 |   1.1414 |     37.592 |     1.5
   77 |   1.0410 |     35.539 |   1.1378 |     37.960 |     1.5
   78 |   1.0314 |     35.176 |   1.1292 |     37.714 |     1.5
   79 |   1.0198 |     35.024 |   1.1288 |     37.531 |     1.5
   80 |   1.0123 |     34.590 |   1.1283 |     37.439 |     1.5
   81 |   1.0082 |     34.401 |   1.1212 |     36.673 |     1.6
   82 |   1.0021 |     34.162 |   1.1223 |     37.040 |     1.6
   83 |   0.9891 |     33.496 |   1.1199 |     37.071 |     1.6
   84 |   0.9858 |     33.604 |   1.1208 |     36.979 |     1.6
   85 |   0.9785 |     33.117 |   1.1232 |     37.163 |     1.6
   86 |   0.9736 |     33.225 |   1.1090 |     36.703 |     1.7
   87 |   0.9643 |     32.483 |   1.1076 |     36.703 |     1.7
   88 |   0.9546 |     32.445 |   1.1063 |     36.275 |     1.7
   89 |   0.9503 |     32.304 |   1.1148 |     36.703 |     1.7
   90 |   0.9476 |     32.353 |   1.0982 |     36.152 |     1.7
   91 |   0.9410 |     32.201 |   1.0991 |     36.734 |     1.7
   92 |   0.9351 |     31.702 |   1.0990 |     36.428 |     1.8
   93 |   0.9349 |     31.822 |   1.0953 |     36.305 |     1.8
   94 |   0.9199 |     31.366 |   1.1001 |     36.734 |     1.8
   95 |   0.9163 |     31.209 |   1.0913 |     35.723 |     1.8
   96 |   0.9043 |     30.743 |   1.0895 |     35.754 |     1.8
   97 |   0.9083 |     30.462 |   1.0866 |     35.539 |     1.9
   98 |   0.8980 |     30.489 |   1.0906 |     35.999 |     1.9
   99 |   0.8881 |     30.207 |   1.0859 |     35.570 |     1.9
  100 |   0.8906 |     29.958 |   1.0875 |     35.355 |     1.9
  101 |   0.8804 |     29.757 |   1.0850 |     35.233 |     1.9
  102 |   0.8734 |     29.584 |   1.0860 |     35.325 |     2.0
  103 |   0.8656 |     29.221 |   1.0793 |     35.325 |     2.0
  104 |   0.8649 |     29.448 |   1.0829 |     35.202 |     2.0
  105 |   0.8550 |     28.939 |   1.0764 |     34.957 |     2.0
  106 |   0.8528 |     28.809 |   1.0792 |     34.681 |     2.0
  107 |   0.8413 |     28.419 |   1.0797 |     35.049 |     2.1
  108 |   0.8388 |     28.462 |   1.0795 |     34.589 |     2.1
  109 |   0.8381 |     28.278 |   1.0808 |     34.896 |     2.1
  110 |   0.8376 |     28.370 |   1.0742 |     34.804 |     2.1
  111 |   0.8312 |     28.050 |   1.0719 |     34.681 |     2.1
  112 |   0.8278 |     27.904 |   1.0780 |     35.049 |     2.1
  113 |   0.8151 |     27.785 |   1.0768 |     34.865 |     2.2
  114 |   0.8127 |     27.135 |   1.0735 |     34.283 |     2.2
  115 |   0.8076 |     27.384 |   1.0734 |     35.080 |     2.2
  116 |   0.8006 |     27.064 |   1.0686 |     35.141 |     2.2
  117 |   0.7922 |     26.588 |   1.0684 |     34.436 |     2.2
  118 |   0.7923 |     26.468 |   1.0775 |     34.620 |     2.3
  119 |   0.7806 |     26.436 |   1.0704 |     34.406 |     2.3
  120 |   0.7829 |     26.566 |   1.0689 |     34.007 |     2.3
  121 |   0.7793 |     25.964 |   1.0676 |     34.099 |     2.3
  122 |   0.7719 |     26.029 |   1.0752 |     33.885 |     2.3
  123 |   0.7682 |     25.791 |   1.0700 |     34.038 |     2.4
  124 |   0.7609 |     25.862 |   1.0810 |     33.793 |     2.4
  125 |   0.7601 |     25.796 |   1.0663 |     33.548 |     2.4
  126 |   0.7502 |     25.444 |   1.0685 |     33.487 |     2.4
  127 |   0.7574 |     25.515 |   1.0640 |     33.180 |     2.4
  128 |   0.7461 |     25.141 |   1.0824 |     33.824 |     2.5
  129 |   0.7453 |     25.244 |   1.0684 |     33.333 |     2.5
  130 |   0.7353 |     24.821 |   1.0835 |     33.395 |     2.5
  131 |   0.7335 |     24.447 |   1.0644 |     33.793 |     2.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 381,858

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7414 |     72.811 |   2.2490 |     57.874 |     0.0
    2 |   1.9803 |     54.058 |   1.7725 |     50.490 |     0.0
    3 |   1.6471 |     48.624 |   1.5524 |     46.048 |     0.0
    4 |   1.4902 |     46.110 |   1.4469 |     45.159 |     0.1
    5 |   1.4177 |     45.156 |   1.4008 |     45.037 |     0.1
    6 |   1.3845 |     45.015 |   1.3706 |     44.332 |     0.1
    7 |   1.3432 |     44.219 |   1.3346 |     44.056 |     0.1
    8 |   1.3104 |     43.503 |   1.3035 |     42.616 |     0.1
    9 |   1.2797 |     42.490 |   1.2808 |     41.054 |     0.1
   10 |   1.2485 |     41.282 |   1.2592 |     40.533 |     0.2
   11 |   1.2259 |     40.708 |   1.2393 |     40.227 |     0.2
   12 |   1.1983 |     39.651 |   1.2222 |     40.472 |     0.2
   13 |   1.1807 |     39.483 |   1.2073 |     39.491 |     0.2
   14 |   1.1579 |     38.687 |   1.1852 |     39.124 |     0.2
   15 |   1.1323 |     37.663 |   1.1798 |     39.154 |     0.2
   16 |   1.1061 |     36.850 |   1.1641 |     37.806 |     0.3
   17 |   1.0780 |     35.425 |   1.1501 |     37.990 |     0.3
   18 |   1.0590 |     34.699 |   1.1362 |     37.040 |     0.3
   19 |   1.0355 |     34.016 |   1.1130 |     36.060 |     0.3
   20 |   1.0092 |     32.580 |   1.1146 |     36.887 |     0.3
   21 |   0.9836 |     31.491 |   1.1045 |     35.999 |     0.3
   22 |   0.9681 |     31.193 |   1.0959 |     35.325 |     0.4
   23 |   0.9409 |     29.990 |   1.0855 |     34.988 |     0.4
   24 |   0.9249 |     29.703 |   1.0772 |     34.865 |     0.4
   25 |   0.8991 |     28.668 |   1.0615 |     34.467 |     0.4
   26 |   0.8864 |     28.262 |   1.0402 |     33.333 |     0.4
   27 |   0.8687 |     27.211 |   1.0487 |     33.517 |     0.4
   28 |   0.8513 |     27.043 |   1.0255 |     32.598 |     0.5
   29 |   0.8436 |     26.663 |   1.0168 |     32.567 |     0.5
   30 |   0.8136 |     25.531 |   1.0145 |     32.016 |     0.5
   31 |   0.7887 |     24.886 |   1.0084 |     32.966 |     0.5
   32 |   0.7780 |     24.296 |   1.0004 |     31.955 |     0.5
   33 |   0.7592 |     23.770 |   1.0082 |     31.587 |     0.5
   34 |   0.7543 |     23.906 |   0.9954 |     31.618 |     0.5
   35 |   0.7372 |     23.169 |   0.9970 |     32.016 |     0.6
   36 |   0.7227 |     22.724 |   0.9927 |     31.097 |     0.6
   37 |   0.7027 |     21.874 |   0.9862 |     31.679 |     0.6
   38 |   0.6990 |     21.814 |   0.9785 |     30.270 |     0.6
   39 |   0.6833 |     21.462 |   0.9792 |     29.749 |     0.6
   40 |   0.6689 |     20.676 |   1.0004 |     31.250 |     0.6
   41 |   0.6693 |     21.451 |   0.9746 |     30.086 |     0.7
   42 |   0.6421 |     19.950 |   0.9963 |     30.760 |     0.7
   43 |   0.6431 |     19.896 |   0.9794 |     29.626 |     0.7
   44 |   0.6247 |     19.446 |   0.9847 |     29.718 |     0.7
   45 |   0.6078 |     18.915 |   0.9789 |     29.105 |     0.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 3,047,138

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3044 |     62.473 |   1.6887 |     48.346 |     0.1
    2 |   1.5212 |     46.679 |   1.4336 |     45.987 |     0.2
    3 |   1.4198 |     46.375 |   1.4055 |     45.466 |     0.2
    4 |   1.4058 |     46.240 |   1.3939 |     45.466 |     0.3
    5 |   1.4025 |     46.283 |   1.3942 |     45.466 |     0.4
    6 |   1.4023 |     46.419 |   1.3919 |     45.466 |     0.5
    7 |   1.3969 |     46.278 |   1.3941 |     45.987 |     0.6
    8 |   1.3933 |     46.402 |   1.3913 |     46.906 |     0.6
    9 |   1.3906 |     46.321 |   1.3854 |     45.006 |     0.7
   10 |   1.3829 |     45.985 |   1.3690 |     45.619 |     0.8
   11 |   1.3705 |     45.378 |   1.3540 |     44.363 |     0.9
   12 |   1.3562 |     45.037 |   1.3405 |     44.271 |     1.0
   13 |   1.3422 |     44.766 |   1.3346 |     44.179 |     1.0
   14 |   1.3359 |     44.723 |   1.3227 |     44.118 |     1.1
   15 |   1.3264 |     44.425 |   1.3238 |     44.577 |     1.2
   16 |   1.3153 |     44.197 |   1.3237 |     43.750 |     1.3
   17 |   1.3132 |     44.067 |   1.3122 |     44.056 |     1.4
   18 |   1.3096 |     43.932 |   1.3043 |     43.964 |     1.4
   19 |   1.3043 |     43.774 |   1.2942 |     43.413 |     1.5
   20 |   1.2862 |     43.168 |   1.2871 |     42.494 |     1.6
   21 |   1.2831 |     42.983 |   1.2767 |     42.249 |     1.7
   22 |   1.2694 |     42.696 |   1.2702 |     41.728 |     1.8
   23 |   1.2714 |     42.902 |   1.2771 |     42.310 |     1.8
   24 |   1.2608 |     42.339 |   1.2609 |     41.759 |     1.9
   25 |   1.2472 |     42.301 |   1.2623 |     41.850 |     2.0
   26 |   1.2496 |     42.203 |   1.2558 |     41.881 |     2.1
   27 |   1.2445 |     42.203 |   1.2486 |     41.085 |     2.2
   28 |   1.2280 |     41.520 |   1.2476 |     41.422 |     2.2
   29 |   1.2238 |     41.575 |   1.2377 |     41.023 |     2.3
   30 |   1.2093 |     41.071 |   1.2332 |     41.023 |     2.4
   31 |   1.2134 |     40.962 |   1.2262 |     40.594 |     2.5
   32 |   1.2007 |     40.442 |   1.2337 |     40.809 |     2.6
   33 |   1.1944 |     40.659 |   1.2148 |     40.502 |     2.6
   34 |   1.1930 |     40.231 |   1.2196 |     40.625 |     2.7
   35 |   1.1889 |     40.485 |   1.2238 |     41.115 |     2.8
   36 |   1.1711 |     39.472 |   1.2293 |     40.074 |     2.9
   37 |   1.1768 |     40.166 |   1.2137 |     40.625 |     3.0
   38 |   1.1608 |     38.811 |   1.2067 |     39.859 |     3.0
   39 |   1.1478 |     38.849 |   1.1916 |     39.093 |     3.1
   40 |   1.1514 |     38.865 |   1.2075 |     39.522 |     3.2
   41 |   1.1419 |     38.535 |   1.1686 |     38.695 |     3.3
   42 |   1.1254 |     38.296 |   1.1744 |     39.124 |     3.4
   43 |   1.1290 |     38.166 |   1.1890 |     38.971 |     3.4
   44 |   1.1223 |     37.700 |   1.1643 |     38.327 |     3.5
   45 |   1.1026 |     37.321 |   1.1633 |     37.960 |     3.6
   46 |   1.0878 |     36.936 |   1.1587 |     37.990 |     3.7
   47 |   1.0940 |     36.687 |   1.1580 |     38.419 |     3.8
   48 |   1.1151 |     37.825 |   1.1594 |     38.082 |     3.8
   49 |   1.0792 |     36.471 |   1.1402 |     37.837 |     3.9
   50 |   1.0655 |     35.994 |   1.1339 |     37.316 |     4.0
   51 |   1.0554 |     35.479 |   1.1452 |     37.653 |     4.1
   52 |   1.0481 |     35.560 |   1.1070 |     35.784 |     4.2
   53 |   1.0475 |     35.078 |   1.1395 |     37.776 |     4.2
   54 |   1.0538 |     35.598 |   1.1319 |     36.734 |     4.3
   55 |   1.0404 |     35.213 |   1.1260 |     37.868 |     4.4
   56 |   1.0318 |     34.959 |   1.1202 |     36.520 |     4.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,322,594

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.0578 |     80.137 |   2.5004 |     61.673 |     0.0
    2 |   2.2685 |     58.593 |   2.1098 |     54.044 |     0.1
    3 |   1.9972 |     54.259 |   1.8921 |     48.591 |     0.1
    4 |   1.8152 |     49.133 |   1.7282 |     48.162 |     0.2
    5 |   1.6765 |     48.640 |   1.6124 |     48.223 |     0.2
    6 |   1.5760 |     48.022 |   1.5293 |     45.588 |     0.3
    7 |   1.5069 |     46.186 |   1.4722 |     45.251 |     0.3
    8 |   1.4546 |     46.034 |   1.4326 |     45.129 |     0.3
    9 |   1.4159 |     45.801 |   1.4024 |     44.730 |     0.4
   10 |   1.3863 |     45.259 |   1.3756 |     44.087 |     0.4
   11 |   1.3585 |     44.230 |   1.3513 |     43.474 |     0.5
   12 |   1.3348 |     43.200 |   1.3355 |     42.831 |     0.5
   13 |   1.3108 |     42.507 |   1.3172 |     41.575 |     0.6
   14 |   1.2943 |     41.602 |   1.3055 |     41.054 |     0.6
   15 |   1.2740 |     40.729 |   1.2880 |     40.625 |     0.6
   16 |   1.2563 |     40.540 |   1.2696 |     39.706 |     0.7
   17 |   1.2396 |     39.451 |   1.2568 |     39.338 |     0.7
   18 |   1.2206 |     39.055 |   1.2413 |     38.817 |     0.8
   19 |   1.2023 |     38.394 |   1.2272 |     38.174 |     0.8
   20 |   1.1837 |     38.047 |   1.2098 |     37.531 |     0.9
   21 |   1.1652 |     37.370 |   1.1987 |     37.806 |     0.9
   22 |   1.1496 |     36.882 |   1.1826 |     37.653 |     0.9
   23 |   1.1301 |     36.481 |   1.1728 |     37.408 |     1.0
   24 |   1.1171 |     35.891 |   1.1673 |     37.623 |     1.0
   25 |   1.1014 |     35.555 |   1.1527 |     36.642 |     1.1
   26 |   1.0837 |     34.791 |   1.1387 |     35.876 |     1.1
   27 |   1.0654 |     34.319 |   1.1326 |     36.091 |     1.1
   28 |   1.0473 |     33.588 |   1.1177 |     35.876 |     1.2
   29 |   1.0328 |     33.182 |   1.1051 |     35.417 |     1.2
   30 |   1.0218 |     32.564 |   1.1002 |     34.743 |     1.3
   31 |   1.0063 |     32.017 |   1.0925 |     34.743 |     1.3
   32 |   0.9864 |     31.497 |   1.0843 |     33.977 |     1.4
   33 |   0.9723 |     30.830 |   1.0680 |     33.915 |     1.4
   34 |   0.9595 |     30.234 |   1.0643 |     33.732 |     1.4
   35 |   0.9441 |     29.855 |   1.0580 |     33.272 |     1.5
   36 |   0.9317 |     29.481 |   1.0424 |     33.027 |     1.5
   37 |   0.9163 |     28.679 |   1.0447 |     32.629 |     1.6
   38 |   0.9049 |     28.511 |   1.0304 |     32.537 |     1.6
   39 |   0.8858 |     27.964 |   1.0265 |     32.200 |     1.7
   40 |   0.8766 |     27.389 |   1.0099 |     31.893 |     1.7
   41 |   0.8619 |     27.146 |   1.0150 |     32.292 |     1.7
   42 |   0.8508 |     26.257 |   1.0073 |     31.985 |     1.8
   43 |   0.8375 |     26.040 |   0.9958 |     31.342 |     1.8
   44 |   0.8241 |     25.943 |   0.9936 |     31.189 |     1.9
   45 |   0.8116 |     25.309 |   0.9881 |     31.373 |     1.9
   46 |   0.7992 |     24.751 |   0.9836 |     31.158 |     2.0
   47 |   0.7956 |     24.680 |   0.9870 |     31.219 |     2.0
   48 |   0.7791 |     23.992 |   0.9738 |     30.178 |     2.0
   49 |   0.7649 |     23.450 |   0.9639 |     30.270 |     2.1
   50 |   0.7517 |     22.979 |   0.9701 |     30.760 |     2.1
   51 |   0.7422 |     22.616 |   0.9569 |     29.718 |     2.2
   52 |   0.7309 |     22.340 |   0.9614 |     29.718 |     2.2
   53 |   0.7230 |     22.172 |   0.9501 |     29.473 |     2.3
   54 |   0.7118 |     21.744 |   0.9548 |     29.963 |     2.3
   55 |   0.7016 |     21.413 |   0.9560 |     28.922 |     2.3
   56 |   0.6903 |     21.028 |   0.9506 |     29.320 |     2.4
   57 |   0.6788 |     20.416 |   0.9507 |     29.136 |     2.4
   58 |   0.6710 |     20.476 |   0.9397 |     28.309 |     2.5
   59 |   0.6623 |     19.896 |   0.9365 |     28.585 |     2.5
   60 |   0.6522 |     19.723 |   0.9455 |     28.554 |     2.6
   61 |   0.6436 |     19.311 |   0.9254 |     28.064 |     2.6
   62 |   0.6350 |     18.942 |   0.9307 |     27.819 |     2.6
   63 |   0.6184 |     18.742 |   0.9350 |     28.646 |     2.7
   64 |   0.6098 |     18.574 |   0.9266 |     28.523 |     2.7
   65 |   0.6033 |     17.935 |   0.9296 |     27.880 |     2.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 377,282

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6103 |     70.264 |   2.0221 |     58.824 |     0.0
    2 |   1.7803 |     50.444 |   1.5673 |     45.466 |     0.0
    3 |   1.5034 |     46.294 |   1.4524 |     45.466 |     0.1
    4 |   1.4414 |     46.278 |   1.4191 |     45.466 |     0.1
    5 |   1.4140 |     46.213 |   1.3992 |     45.466 |     0.1
    6 |   1.3953 |     46.272 |   1.3829 |     45.343 |     0.1
    7 |   1.3731 |     45.844 |   1.3555 |     44.424 |     0.1
    8 |   1.3509 |     45.221 |   1.3439 |     44.638 |     0.2
    9 |   1.3369 |     45.124 |   1.3279 |     44.148 |     0.2
   10 |   1.3213 |     45.161 |   1.3220 |     44.700 |     0.2
   11 |   1.3109 |     44.815 |   1.3044 |     44.822 |     0.2
   12 |   1.2924 |     44.186 |   1.2966 |     43.015 |     0.2
   13 |   1.2667 |     42.566 |   1.2614 |     41.544 |     0.2
   14 |   1.2417 |     41.921 |   1.2443 |     41.115 |     0.3
   15 |   1.2241 |     41.591 |   1.2440 |     42.096 |     0.3
   16 |   1.2144 |     41.136 |   1.2341 |     41.728 |     0.3
   17 |   1.1961 |     40.431 |   1.2125 |     40.625 |     0.3
   18 |   1.1694 |     39.803 |   1.1891 |     40.472 |     0.3
   19 |   1.1479 |     39.012 |   1.1810 |     39.951 |     0.4
   20 |   1.1327 |     38.795 |   1.1764 |     39.614 |     0.4
   21 |   1.1181 |     38.405 |   1.1636 |     39.338 |     0.4
   22 |   1.1043 |     37.484 |   1.1414 |     38.572 |     0.4
   23 |   1.0828 |     36.985 |   1.1353 |     38.113 |     0.4
   24 |   1.0688 |     36.037 |   1.1400 |     37.776 |     0.4
   25 |   1.0488 |     35.501 |   1.1206 |     37.806 |     0.5
   26 |   1.0289 |     34.433 |   1.1047 |     36.979 |     0.5
   27 |   1.0045 |     33.978 |   1.0957 |     36.152 |     0.5
   28 |   0.9919 |     33.436 |   1.0979 |     36.428 |     0.5
   29 |   0.9699 |     32.672 |   1.0915 |     35.968 |     0.5
   30 |   0.9574 |     32.087 |   1.0834 |     35.386 |     0.6
   31 |   0.9391 |     31.659 |   1.0733 |     34.498 |     0.6
   32 |   0.9234 |     30.955 |   1.0732 |     33.824 |     0.6
   33 |   0.9119 |     30.608 |   1.0528 |     34.528 |     0.6
   34 |   0.9031 |     30.315 |   1.0476 |     34.344 |     0.6
   35 |   0.8902 |     29.513 |   1.0633 |     34.069 |     0.6
   36 |   0.8851 |     29.573 |   1.0450 |     33.732 |     0.7
   37 |   0.8521 |     28.625 |   1.0534 |     33.640 |     0.7
   38 |   0.8354 |     27.731 |   1.0366 |     33.150 |     0.7
   39 |   0.8310 |     27.958 |   1.0475 |     32.904 |     0.7
   40 |   0.8315 |     28.105 |   1.0446 |     32.966 |     0.7
   41 |   0.7986 |     26.756 |   1.0459 |     33.456 |     0.8
   42 |   0.7823 |     25.997 |   1.0465 |     32.782 |     0.8
   43 |   0.7735 |     25.937 |   1.0330 |     31.679 |     0.8
   44 |   0.7718 |     25.547 |   1.0309 |     31.924 |     0.8
   45 |   0.7696 |     25.520 |   1.0306 |     31.740 |     0.8
   46 |   0.7476 |     24.648 |   1.0073 |     31.587 |     0.8
   47 |   0.7282 |     23.716 |   1.0311 |     32.016 |     0.9
   48 |   0.7325 |     24.101 |   1.0330 |     31.189 |     0.9
   49 |   0.7214 |     23.813 |   1.0465 |     31.587 |     0.9
   50 |   0.7036 |     23.066 |   1.0255 |     30.944 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 274,658

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5828 |     68.704 |   2.0222 |     58.824 |     0.0
    2 |   1.8034 |     50.894 |   1.5972 |     45.466 |     0.0
    3 |   1.5244 |     46.278 |   1.4629 |     45.466 |     0.0
    4 |   1.4501 |     46.218 |   1.4240 |     45.987 |     0.1
    5 |   1.4226 |     46.272 |   1.4107 |     45.466 |     0.1
    6 |   1.4107 |     46.261 |   1.3998 |     45.466 |     0.1
    7 |   1.4040 |     46.316 |   1.3925 |     45.251 |     0.1
    8 |   1.3919 |     46.223 |   1.3823 |     45.466 |     0.1
    9 |   1.3774 |     46.288 |   1.3580 |     45.374 |     0.1
   10 |   1.3598 |     45.866 |   1.3462 |     44.884 |     0.1
   11 |   1.3485 |     45.730 |   1.3377 |     44.393 |     0.2
   12 |   1.3370 |     45.649 |   1.3254 |     44.301 |     0.2
   13 |   1.3237 |     45.346 |   1.3239 |     44.853 |     0.2
   14 |   1.3128 |     45.189 |   1.3096 |     44.087 |     0.2
   15 |   1.3045 |     44.836 |   1.3047 |     43.995 |     0.2
   16 |   1.2976 |     44.517 |   1.2944 |     43.750 |     0.2
   17 |   1.2837 |     44.224 |   1.2889 |     43.781 |     0.2
   18 |   1.2707 |     43.682 |   1.2850 |     42.892 |     0.3
   19 |   1.2585 |     43.075 |   1.2753 |     42.984 |     0.3
   20 |   1.2439 |     42.555 |   1.2530 |     41.360 |     0.3
   21 |   1.2256 |     41.715 |   1.2436 |     41.483 |     0.3
   22 |   1.2077 |     41.455 |   1.2258 |     40.717 |     0.3
   23 |   1.1948 |     40.946 |   1.2214 |     40.380 |     0.3
   24 |   1.1836 |     40.870 |   1.2114 |     40.288 |     0.3
   25 |   1.1672 |     39.911 |   1.2033 |     39.675 |     0.4
   26 |   1.1551 |     39.472 |   1.2041 |     39.369 |     0.4
   27 |   1.1340 |     38.643 |   1.1813 |     39.154 |     0.4
   28 |   1.1240 |     38.150 |   1.1669 |     39.093 |     0.4
   29 |   1.1087 |     37.858 |   1.1680 |     39.216 |     0.4
   30 |   1.0933 |     37.224 |   1.1570 |     38.634 |     0.4
   31 |   1.0816 |     36.801 |   1.1497 |     37.469 |     0.4
   32 |   1.0663 |     36.265 |   1.1290 |     37.010 |     0.4
   33 |   1.0476 |     35.327 |   1.1220 |     36.673 |     0.5
   34 |   1.0394 |     34.688 |   1.1083 |     35.325 |     0.5
   35 |   1.0295 |     34.439 |   1.0997 |     35.325 |     0.5
   36 |   1.0117 |     33.816 |   1.0995 |     35.600 |     0.5
   37 |   0.9980 |     33.176 |   1.0948 |     34.712 |     0.5
   38 |   0.9874 |     32.824 |   1.0884 |     34.651 |     0.5
   39 |   0.9714 |     32.586 |   1.0770 |     34.651 |     0.5
   40 |   0.9674 |     32.185 |   1.0795 |     33.946 |     0.6
   41 |   0.9586 |     31.730 |   1.0715 |     33.793 |     0.6
   42 |   0.9428 |     31.345 |   1.0729 |     34.283 |     0.6
   43 |   0.9287 |     30.884 |   1.0610 |     33.824 |     0.6
   44 |   0.9156 |     30.586 |   1.0441 |     33.732 |     0.6
   45 |   0.9027 |     30.294 |   1.0545 |     33.211 |     0.6
   46 |   0.8935 |     29.784 |   1.0488 |     33.241 |     0.6
   47 |   0.8889 |     29.405 |   1.0468 |     32.996 |     0.7
   48 |   0.8769 |     29.383 |   1.0474 |     32.751 |     0.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,728,610

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3789 |     88.031 |   3.2064 |     82.353 |     0.1
    2 |   3.0712 |     82.174 |   2.9349 |     83.333 |     0.1
    3 |   2.8316 |     74.794 |   2.7345 |     67.402 |     0.2
    4 |   2.6622 |     67.404 |   2.5901 |     66.912 |     0.3
    5 |   2.5335 |     67.084 |   2.4811 |     66.912 |     0.3
    6 |   2.4383 |     66.602 |   2.3942 |     65.165 |     0.4
    7 |   2.3581 |     63.757 |   2.3209 |     61.642 |     0.4
    8 |   2.2922 |     60.880 |   2.2590 |     59.038 |     0.5
    9 |   2.2338 |     59.574 |   2.2036 |     58.854 |     0.6
   10 |   2.1817 |     59.043 |   2.1546 |     58.824 |     0.6
   11 |   2.1354 |     58.843 |   2.1067 |     58.824 |     0.7
   12 |   2.0899 |     58.642 |   2.0593 |     58.732 |     0.8
   13 |   2.0427 |     57.548 |   2.0084 |     57.322 |     0.8
   14 |   1.9913 |     54.871 |   1.9556 |     52.941 |     0.9
   15 |   1.9370 |     52.585 |   1.8944 |     52.451 |     1.0
   16 |   1.8785 |     51.458 |   1.8408 |     49.908 |     1.0
   17 |   1.8290 |     50.780 |   1.7917 |     50.031 |     1.1
   18 |   1.7824 |     49.946 |   1.7503 |     48.775 |     1.1
   19 |   1.7442 |     49.545 |   1.7125 |     48.223 |     1.2
   20 |   1.7056 |     49.160 |   1.6774 |     48.162 |     1.3
   21 |   1.6707 |     48.878 |   1.6437 |     48.039 |     1.3
   22 |   1.6405 |     48.689 |   1.6146 |     47.978 |     1.4
   23 |   1.6122 |     48.304 |   1.5899 |     48.039 |     1.5
   24 |   1.5889 |     47.768 |   1.5688 |     47.273 |     1.5
   25 |   1.5673 |     47.025 |   1.5491 |     45.711 |     1.6
   26 |   1.5477 |     46.413 |   1.5325 |     45.159 |     1.7
   27 |   1.5308 |     45.790 |   1.5160 |     45.067 |     1.7
   28 |   1.5137 |     45.687 |   1.5030 |     45.067 |     1.8
   29 |   1.4988 |     45.497 |   1.4906 |     44.884 |     1.8
   30 |   1.4852 |     45.335 |   1.4773 |     44.669 |     1.9
   31 |   1.4739 |     45.259 |   1.4657 |     44.761 |     2.0
   32 |   1.4609 |     45.346 |   1.4553 |     44.608 |     2.0
   33 |   1.4478 |     45.140 |   1.4443 |     44.363 |     2.1
   34 |   1.4383 |     45.140 |   1.4364 |     44.638 |     2.2
   35 |   1.4257 |     45.010 |   1.4271 |     44.547 |     2.2
   36 |   1.4144 |     45.021 |   1.4179 |     44.271 |     2.3
   37 |   1.4063 |     44.804 |   1.4107 |     44.577 |     2.3
   38 |   1.3992 |     44.761 |   1.4047 |     44.179 |     2.4
   39 |   1.3881 |     44.750 |   1.3950 |     44.148 |     2.5
   40 |   1.3819 |     44.663 |   1.3874 |     44.332 |     2.5
   41 |   1.3713 |     44.522 |   1.3802 |     44.118 |     2.6
   42 |   1.3607 |     44.343 |   1.3732 |     44.210 |     2.7
   43 |   1.3513 |     44.132 |   1.3666 |     44.026 |     2.7
   44 |   1.3448 |     43.991 |   1.3590 |     44.056 |     2.8
   45 |   1.3355 |     43.720 |   1.3523 |     43.781 |     2.9
   46 |   1.3261 |     43.357 |   1.3462 |     43.842 |     2.9
   47 |   1.3211 |     43.379 |   1.3417 |     43.811 |     3.0
   48 |   1.3135 |     43.119 |   1.3347 |     43.290 |     3.0
   49 |   1.3013 |     43.037 |   1.3283 |     43.505 |     3.1
   50 |   1.2945 |     42.523 |   1.3218 |     43.505 |     3.2
   51 |   1.2874 |     42.550 |   1.3133 |     43.505 |     3.2
   52 |   1.2791 |     42.344 |   1.3100 |     43.045 |     3.3
   53 |   1.2734 |     42.035 |   1.3024 |     42.555 |     3.4
   54 |   1.2623 |     41.347 |   1.2955 |     42.188 |     3.4
   55 |   1.2559 |     41.103 |   1.2899 |     42.157 |     3.5
   56 |   1.2458 |     40.453 |   1.2874 |     41.759 |     3.5
   57 |   1.2390 |     40.448 |   1.2793 |     41.207 |     3.6
   58 |   1.2349 |     40.139 |   1.2767 |     41.544 |     3.7
   59 |   1.2297 |     39.960 |   1.2691 |     40.288 |     3.7
   60 |   1.2179 |     39.391 |   1.2641 |     40.472 |     3.8
   61 |   1.2126 |     39.228 |   1.2608 |     40.074 |     3.9
   62 |   1.2053 |     38.833 |   1.2553 |     39.491 |     3.9
   63 |   1.2027 |     38.660 |   1.2510 |     38.971 |     4.0
   64 |   1.1944 |     38.053 |   1.2442 |     39.675 |     4.1
   65 |   1.1867 |     37.966 |   1.2406 |     39.001 |     4.1
   66 |   1.1817 |     37.755 |   1.2396 |     38.725 |     4.2
   67 |   1.1770 |     37.159 |   1.2311 |     38.327 |     4.2
   68 |   1.1696 |     37.067 |   1.2257 |     37.929 |     4.3
   69 |   1.1626 |     36.855 |   1.2210 |     37.592 |     4.4
   70 |   1.1520 |     36.378 |   1.2189 |     38.051 |     4.4
   71 |   1.1492 |     36.427 |   1.2141 |     37.561 |     4.5
   72 |   1.1423 |     35.826 |   1.2114 |     37.684 |     4.6
   73 |   1.1333 |     35.940 |   1.2037 |     37.194 |     4.6
   74 |   1.1299 |     35.392 |   1.1992 |     37.469 |     4.7
   75 |   1.1216 |     35.127 |   1.1992 |     37.286 |     4.8
   76 |   1.1191 |     35.083 |   1.1917 |     36.765 |     4.8
   77 |   1.1173 |     34.856 |   1.1909 |     36.795 |     4.9
   78 |   1.1033 |     34.569 |   1.1861 |     37.347 |     4.9
   79 |   1.0978 |     34.374 |   1.1821 |     36.520 |     5.0
   80 |   1.0934 |     34.319 |   1.1791 |     36.703 |     5.1
   81 |   1.0847 |     33.967 |   1.1725 |     36.612 |     5.1
   82 |   1.0808 |     33.658 |   1.1742 |     36.550 |     5.2
   83 |   1.0706 |     33.593 |   1.1728 |     36.550 |     5.3
   84 |   1.0683 |     33.263 |   1.1628 |     35.907 |     5.3
   85 |   1.0637 |     33.030 |   1.1610 |     36.121 |     5.4
   86 |   1.0595 |     32.987 |   1.1626 |     36.213 |     5.4
   87 |   1.0549 |     32.748 |   1.1581 |     36.213 |     5.5
   88 |   1.0471 |     32.537 |   1.1555 |     35.570 |     5.6
   89 |   1.0382 |     32.120 |   1.1453 |     35.509 |     5.6
   90 |   1.0344 |     32.217 |   1.1447 |     35.846 |     5.7
   91 |   1.0316 |     31.865 |   1.1426 |     35.754 |     5.8
   92 |   1.0254 |     31.865 |   1.1414 |     35.509 |     5.8
   93 |   1.0232 |     31.659 |   1.1386 |     35.539 |     5.9
   94 |   1.0166 |     31.426 |   1.1353 |     35.570 |     6.0
   95 |   1.0076 |     31.497 |   1.1345 |     35.478 |     6.0
   96 |   1.0039 |     31.031 |   1.1311 |     35.478 |     6.1
   97 |   0.9969 |     30.895 |   1.1322 |     35.784 |     6.1
   98 |   0.9943 |     30.798 |   1.1341 |     35.692 |     6.2
   99 |   0.9922 |     30.955 |   1.1245 |     34.896 |     6.3
  100 |   0.9829 |     30.630 |   1.1209 |     34.865 |     6.3
  101 |   0.9807 |     30.743 |   1.1181 |     35.294 |     6.4
  102 |   0.9766 |     30.082 |   1.1153 |     35.110 |     6.5
  103 |   0.9698 |     30.272 |   1.1166 |     35.539 |     6.5
  104 |   0.9693 |     29.904 |   1.1132 |     34.896 |     6.6
  105 |   0.9631 |     30.093 |   1.1057 |     34.559 |     6.6
  106 |   0.9640 |     29.801 |   1.1116 |     35.141 |     6.7
  107 |   0.9541 |     29.486 |   1.1087 |     34.712 |     6.8
  108 |   0.9493 |     29.210 |   1.1038 |     34.344 |     6.8
  109 |   0.9461 |     29.313 |   1.1024 |     34.835 |     6.9
  110 |   0.9413 |     29.226 |   1.1039 |     34.988 |     7.0
  111 |   0.9365 |     29.091 |   1.0981 |     34.436 |     7.0
  112 |   0.9373 |     29.112 |   1.0936 |     34.222 |     7.1
  113 |   0.9307 |     29.080 |   1.0978 |     34.651 |     7.2
  114 |   0.9254 |     29.010 |   1.0920 |     34.406 |     7.2
  115 |   0.9163 |     28.343 |   1.0904 |     34.099 |     7.3
  116 |   0.9177 |     28.338 |   1.0869 |     34.375 |     7.3
  117 |   0.9083 |     28.197 |   1.0882 |     34.804 |     7.4
  118 |   0.9064 |     28.143 |   1.0828 |     34.406 |     7.5
  119 |   0.9000 |     28.246 |   1.0884 |     34.283 |     7.5
  120 |   0.9015 |     28.013 |   1.0859 |     34.743 |     7.6
  121 |   0.8999 |     27.996 |   1.0873 |     34.314 |     7.7
  122 |   0.8896 |     27.742 |   1.0894 |     34.375 |     7.7
  123 |   0.8887 |     27.725 |   1.0797 |     33.578 |     7.8
  124 |   0.8797 |     27.438 |   1.0827 |     34.007 |     7.8
  125 |   0.8868 |     27.563 |   1.0712 |     33.946 |     7.9
  126 |   0.8773 |     27.243 |   1.0785 |     33.977 |     8.0
  127 |   0.8748 |     27.238 |   1.0754 |     33.578 |     8.0
  128 |   0.8657 |     27.075 |   1.0811 |     34.222 |     8.1
  129 |   0.8637 |     27.021 |   1.0741 |     34.069 |     8.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 306,466

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5968 |     69.717 |   2.0087 |     58.824 |     0.0
    2 |   1.7572 |     49.735 |   1.5508 |     45.466 |     0.0
    3 |   1.4986 |     46.316 |   1.4513 |     45.987 |     0.1
    4 |   1.4359 |     46.305 |   1.4137 |     45.466 |     0.1
    5 |   1.4070 |     46.288 |   1.3898 |     46.017 |     0.1
    6 |   1.3840 |     46.299 |   1.3703 |     45.466 |     0.1
    7 |   1.3675 |     46.077 |   1.3624 |     45.312 |     0.1
    8 |   1.3512 |     45.763 |   1.3372 |     44.884 |     0.1
    9 |   1.3233 |     44.641 |   1.3079 |     43.137 |     0.2
   10 |   1.2920 |     43.736 |   1.2804 |     42.402 |     0.2
   11 |   1.2692 |     43.086 |   1.2641 |     42.525 |     0.2
   12 |   1.2470 |     42.382 |   1.2495 |     42.218 |     0.2
   13 |   1.2301 |     42.122 |   1.2460 |     42.371 |     0.2
   14 |   1.2142 |     41.829 |   1.2214 |     42.004 |     0.2
   15 |   1.1906 |     41.255 |   1.2101 |     40.839 |     0.3
   16 |   1.1820 |     40.783 |   1.1930 |     40.962 |     0.3
   17 |   1.1602 |     40.187 |   1.1807 |     40.257 |     0.3
   18 |   1.1393 |     39.423 |   1.1772 |     40.104 |     0.3
   19 |   1.1289 |     38.903 |   1.1473 |     38.756 |     0.3
   20 |   1.1057 |     37.993 |   1.1447 |     39.767 |     0.4
   21 |   1.0889 |     37.549 |   1.1382 |     39.246 |     0.4
   22 |   1.0681 |     36.508 |   1.1279 |     37.929 |     0.4
   23 |   1.0539 |     35.528 |   1.1225 |     37.868 |     0.4
   24 |   1.0295 |     34.742 |   1.1085 |     36.826 |     0.4
   25 |   1.0153 |     33.897 |   1.0907 |     35.570 |     0.4
   26 |   0.9956 |     32.748 |   1.0883 |     35.386 |     0.5
   27 |   0.9761 |     32.353 |   1.0790 |     35.938 |     0.5
   28 |   0.9604 |     31.827 |   1.0708 |     35.110 |     0.5
   29 |   0.9347 |     31.161 |   1.0625 |     34.498 |     0.5
   30 |   0.9217 |     30.668 |   1.0504 |     34.681 |     0.5
   31 |   0.9113 |     30.489 |   1.0423 |     33.854 |     0.5
   32 |   0.8968 |     29.784 |   1.0192 |     32.935 |     0.6
   33 |   0.8752 |     29.215 |   1.0200 |     33.732 |     0.6
   34 |   0.8588 |     28.251 |   1.0341 |     33.548 |     0.6
   35 |   0.8413 |     27.883 |   1.0110 |     32.843 |     0.6
   36 |   0.8316 |     27.585 |   1.0192 |     33.058 |     0.6
   37 |   0.8087 |     26.512 |   1.0007 |     31.832 |     0.7
   38 |   0.7918 |     26.024 |   0.9938 |     32.016 |     0.7
   39 |   0.7795 |     25.775 |   1.0021 |     32.475 |     0.7
   40 |   0.7739 |     25.656 |   0.9889 |     32.016 |     0.7
   41 |   0.7608 |     25.103 |   1.0070 |     32.261 |     0.7
   42 |   0.7488 |     24.637 |   0.9857 |     30.913 |     0.7
   43 |   0.7312 |     24.166 |   0.9857 |     31.189 |     0.8
   44 |   0.7249 |     23.640 |   0.9862 |     30.821 |     0.8
   45 |   0.7034 |     23.250 |   0.9949 |     31.005 |     0.8
   46 |   0.6976 |     22.589 |   0.9758 |     30.423 |     0.8
   47 |   0.6815 |     22.253 |   0.9826 |     29.994 |     0.8
   48 |   0.6678 |     22.036 |   0.9870 |     30.208 |     0.8
   49 |   0.6651 |     21.901 |   0.9883 |     29.565 |     0.9
   50 |   0.6851 |     22.556 |   0.9752 |     29.871 |     0.9
   51 |   0.6527 |     21.435 |   0.9870 |     29.565 |     0.9
   52 |   0.6270 |     20.801 |   0.9748 |     29.105 |     0.9
   53 |   0.6319 |     20.687 |   0.9702 |     29.136 |     0.9
   54 |   0.6125 |     20.335 |   0.9728 |     29.044 |     1.0
   55 |   0.6068 |     20.064 |   0.9836 |     29.473 |     1.0
   56 |   0.6035 |     19.918 |   0.9659 |     28.676 |     1.0
   57 |   0.6096 |     20.243 |   0.9873 |     28.431 |     1.0
   58 |   0.5953 |     19.771 |   0.9790 |     28.401 |     1.0
   59 |   0.5749 |     18.628 |   0.9827 |     29.320 |     1.0
   60 |   0.5718 |     18.910 |   0.9841 |     28.401 |     1.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 986,690

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7954 |     75.623 |   2.3845 |     59.436 |     0.0
    2 |   2.1739 |     58.279 |   1.9679 |     58.058 |     0.1
    3 |   1.8378 |     51.089 |   1.6967 |     48.346 |     0.1
    4 |   1.6355 |     47.676 |   1.5592 |     45.466 |     0.2
    5 |   1.5251 |     45.871 |   1.4753 |     45.619 |     0.2
    6 |   1.4590 |     45.774 |   1.4320 |     45.374 |     0.2
    7 |   1.4225 |     45.281 |   1.4039 |     44.853 |     0.3
    8 |   1.3981 |     44.771 |   1.3812 |     44.301 |     0.3
    9 |   1.3720 |     44.376 |   1.3583 |     44.210 |     0.4
   10 |   1.3480 |     43.736 |   1.3317 |     43.321 |     0.4
   11 |   1.3216 |     43.184 |   1.3088 |     42.953 |     0.5
   12 |   1.2947 |     42.566 |   1.2798 |     41.238 |     0.5
   13 |   1.2689 |     41.775 |   1.2601 |     41.054 |     0.5
   14 |   1.2467 |     41.044 |   1.2506 |     40.870 |     0.6
   15 |   1.2281 |     40.545 |   1.2421 |     40.319 |     0.6
   16 |   1.2271 |     40.697 |   1.2234 |     39.614 |     0.7
   17 |   1.2010 |     39.521 |   1.2177 |     39.093 |     0.7
   18 |   1.1836 |     39.082 |   1.1901 |     38.266 |     0.7
   19 |   1.1579 |     38.410 |   1.1713 |     38.021 |     0.8
   20 |   1.1427 |     38.074 |   1.1670 |     37.439 |     0.8
   21 |   1.1360 |     37.852 |   1.1554 |     37.714 |     0.9
   22 |   1.1202 |     37.240 |   1.1485 |     38.113 |     0.9
   23 |   1.1066 |     37.207 |   1.1412 |     37.377 |     0.9
   24 |   1.0985 |     36.720 |   1.1503 |     37.776 |     1.0
   25 |   1.0964 |     36.769 |   1.1442 |     36.520 |     1.0
   26 |   1.0777 |     36.205 |   1.1242 |     36.366 |     1.1
   27 |   1.0664 |     35.349 |   1.1190 |     35.968 |     1.1
   28 |   1.0539 |     35.067 |   1.1057 |     35.692 |     1.1
   29 |   1.0439 |     34.563 |   1.0972 |     36.366 |     1.2
   30 |   1.0321 |     34.092 |   1.0970 |     35.263 |     1.2
   31 |   1.0200 |     33.783 |   1.0896 |     35.447 |     1.3
   32 |   1.0119 |     33.393 |   1.0751 |     35.233 |     1.3
   33 |   1.0001 |     33.160 |   1.0741 |     35.049 |     1.4
   34 |   1.0005 |     33.360 |   1.0742 |     34.926 |     1.4
   35 |   0.9934 |     32.786 |   1.0755 |     34.835 |     1.4
   36 |   0.9787 |     31.925 |   1.0712 |     34.406 |     1.5
   37 |   0.9669 |     32.017 |   1.0442 |     33.670 |     1.5
   38 |   0.9489 |     31.464 |   1.0523 |     33.303 |     1.6
   39 |   0.9611 |     31.502 |   1.0529 |     33.088 |     1.6
   40 |   0.9346 |     30.456 |   1.0425 |     33.333 |     1.6
   41 |   0.9256 |     30.131 |   1.0276 |     32.659 |     1.7
   42 |   0.9152 |     29.963 |   1.0304 |     32.966 |     1.7
   43 |   0.9160 |     30.142 |   1.0290 |     32.414 |     1.8
   44 |   0.9111 |     29.757 |   1.1890 |     37.377 |     1.8
   45 |   0.9538 |     31.491 |   1.0409 |     32.966 |     1.8
   46 |   0.8927 |     29.085 |   1.0075 |     31.985 |     1.9
   47 |   0.9129 |     29.844 |   1.0352 |     33.027 |     1.9
   48 |   0.8994 |     29.524 |   1.0070 |     32.047 |     2.0
   49 |   0.8751 |     28.684 |   1.0199 |     32.200 |     2.0
   50 |   0.8635 |     28.045 |   1.0189 |     32.475 |     2.1
   51 |   0.8591 |     28.175 |   1.0239 |     32.016 |     2.1
   52 |   0.8529 |     27.601 |   1.0122 |     30.790 |     2.1
   53 |   0.8445 |     27.384 |   0.9913 |     30.300 |     2.2
   54 |   0.8426 |     27.156 |   1.0082 |     32.138 |     2.2
   55 |   0.8345 |     27.227 |   0.9931 |     31.127 |     2.3
   56 |   0.8242 |     26.793 |   0.9939 |     31.434 |     2.3
   57 |   0.8178 |     26.403 |   0.9992 |     31.893 |     2.3
   58 |   0.8141 |     26.512 |   0.9901 |     30.821 |     2.4
   59 |   0.8049 |     26.376 |   0.9913 |     31.097 |     2.4
   60 |   0.8093 |     26.430 |   0.9856 |     31.495 |     2.5
   61 |   0.8127 |     26.447 |   0.9920 |     31.403 |     2.5
   62 |   0.8105 |     26.669 |   1.0195 |     32.200 |     2.5
   63 |   0.7999 |     26.463 |   0.9894 |     30.882 |     2.6
   64 |   0.7959 |     26.360 |   0.9882 |     30.882 |     2.6
   65 |   0.7745 |     25.748 |   0.9845 |     30.974 |     2.7
   66 |   0.7668 |     24.995 |   0.9824 |     30.760 |     2.7
   67 |   0.7716 |     25.379 |   0.9759 |     30.668 |     2.8
   68 |   0.7650 |     25.228 |   0.9664 |     30.545 |     2.8
   69 |   0.7603 |     24.967 |   0.9745 |     30.699 |     2.8
   70 |   0.7489 |     24.474 |   0.9848 |     30.821 |     2.9
   71 |   0.7556 |     24.821 |   0.9660 |     29.626 |     2.9
   72 |   0.7469 |     24.691 |   0.9831 |     30.576 |     3.0
   73 |   0.7495 |     24.539 |   0.9740 |     30.178 |     3.0
   74 |   0.7393 |     24.496 |   0.9864 |     29.504 |     3.0
   75 |   0.7477 |     24.415 |   0.9857 |     30.178 |     3.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 682,882

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4719 |     91.266 |   3.2511 |     83.333 |     0.0
    2 |   2.8858 |     83.333 |   2.7040 |     83.333 |     0.0
    3 |   2.5650 |     68.243 |   2.4533 |     58.824 |     0.1
    4 |   2.3386 |     59.049 |   2.2619 |     58.824 |     0.1
    5 |   2.1864 |     58.984 |   2.1385 |     58.762 |     0.1
    6 |   2.0831 |     56.670 |   2.0426 |     53.768 |     0.1
    7 |   1.9943 |     54.042 |   1.9582 |     53.768 |     0.2
    8 |   1.9173 |     53.235 |   1.8847 |     48.468 |     0.2
    9 |   1.8503 |     48.629 |   1.8195 |     48.346 |     0.2
   10 |   1.7912 |     48.607 |   1.7607 |     48.346 |     0.2
   11 |   1.7358 |     48.602 |   1.7063 |     48.346 |     0.3
   12 |   1.6862 |     48.602 |   1.6577 |     48.346 |     0.3
   13 |   1.6424 |     47.583 |   1.6165 |     45.466 |     0.3
   14 |   1.6052 |     46.218 |   1.5823 |     45.466 |     0.3
   15 |   1.5756 |     46.213 |   1.5538 |     45.466 |     0.4
   16 |   1.5461 |     46.207 |   1.5300 |     45.466 |     0.4
   17 |   1.5236 |     46.213 |   1.5112 |     45.466 |     0.4
   18 |   1.5083 |     46.213 |   1.4962 |     45.466 |     0.4
   19 |   1.4950 |     46.207 |   1.4839 |     45.466 |     0.5
   20 |   1.4870 |     46.223 |   1.4736 |     45.466 |     0.5
   21 |   1.4732 |     46.207 |   1.4636 |     45.466 |     0.5
   22 |   1.4619 |     46.213 |   1.4554 |     45.466 |     0.5
   23 |   1.4547 |     46.213 |   1.4477 |     45.466 |     0.5
   24 |   1.4459 |     46.213 |   1.4418 |     45.466 |     0.6
   25 |   1.4413 |     46.213 |   1.4342 |     45.466 |     0.6
   26 |   1.4323 |     46.353 |   1.4279 |     45.404 |     0.6
   27 |   1.4230 |     46.099 |   1.4247 |     45.343 |     0.6
   28 |   1.4176 |     45.822 |   1.4163 |     45.159 |     0.7
   29 |   1.4121 |     45.617 |   1.4105 |     45.190 |     0.7
   30 |   1.4047 |     45.492 |   1.4067 |     45.159 |     0.7
   31 |   1.3999 |     45.617 |   1.4023 |     45.037 |     0.7
   32 |   1.3918 |     45.346 |   1.3999 |     45.098 |     0.8
   33 |   1.3892 |     45.275 |   1.3956 |     45.037 |     0.8
   34 |   1.3840 |     45.140 |   1.3942 |     44.945 |     0.8
   35 |   1.3789 |     45.151 |   1.3932 |     45.741 |     0.8
   36 |   1.3728 |     45.102 |   1.3887 |     45.098 |     0.9
   37 |   1.3673 |     44.739 |   1.3890 |     44.975 |     0.9
   38 |   1.3640 |     44.891 |   1.3882 |     45.221 |     0.9
   39 |   1.3578 |     44.598 |   1.3849 |     44.792 |     0.9
   40 |   1.3531 |     44.560 |   1.3814 |     44.822 |     1.0
   41 |   1.3496 |     44.262 |   1.3830 |     45.496 |     1.0
   42 |   1.3464 |     44.414 |   1.3765 |     45.221 |     1.0
   43 |   1.3412 |     44.246 |   1.3711 |     45.129 |     1.0
   44 |   1.3343 |     44.051 |   1.3718 |     45.067 |     1.0
   45 |   1.3298 |     43.969 |   1.3676 |     44.884 |     1.1
   46 |   1.3256 |     43.904 |   1.3662 |     45.098 |     1.1
   47 |   1.3196 |     44.034 |   1.3603 |     44.363 |     1.1
   48 |   1.3164 |     43.769 |   1.3605 |     44.730 |     1.1
   49 |   1.3092 |     43.617 |   1.3592 |     45.067 |     1.2
   50 |   1.3055 |     43.699 |   1.3576 |     44.455 |     1.2
   51 |   1.2975 |     43.677 |   1.3536 |     44.975 |     1.2
   52 |   1.2926 |     43.482 |   1.3548 |     45.190 |     1.2
   53 |   1.2890 |     43.563 |   1.3487 |     44.853 |     1.3
   54 |   1.2838 |     43.281 |   1.3571 |     44.730 |     1.3
   55 |   1.2800 |     43.113 |   1.3440 |     44.087 |     1.3
   56 |   1.2719 |     42.891 |   1.3407 |     44.485 |     1.3
   57 |   1.2669 |     42.799 |   1.3378 |     44.179 |     1.4
   58 |   1.2595 |     42.691 |   1.3396 |     44.332 |     1.4
   59 |   1.2549 |     42.431 |   1.3360 |     44.210 |     1.4
   60 |   1.2478 |     42.328 |   1.3387 |     44.485 |     1.4
   61 |   1.2422 |     42.008 |   1.3332 |     44.179 |     1.4
   62 |   1.2370 |     42.013 |   1.3325 |     44.056 |     1.5
   63 |   1.2339 |     41.699 |   1.3301 |     43.934 |     1.5
   64 |   1.2308 |     41.634 |   1.3341 |     43.781 |     1.5
   65 |   1.2226 |     41.396 |   1.3312 |     43.627 |     1.5
   66 |   1.2183 |     41.423 |   1.3247 |     43.719 |     1.6
   67 |   1.2121 |     41.293 |   1.3199 |     43.536 |     1.6
   68 |   1.2109 |     41.098 |   1.3220 |     43.811 |     1.6
   69 |   1.2026 |     40.870 |   1.3167 |     44.056 |     1.6
   70 |   1.1996 |     40.697 |   1.3212 |     43.750 |     1.7
   71 |   1.1953 |     40.496 |   1.3082 |     43.015 |     1.7
   72 |   1.1873 |     40.280 |   1.3157 |     43.107 |     1.7
   73 |   1.1877 |     39.846 |   1.3184 |     43.597 |     1.7
   74 |   1.1774 |     39.992 |   1.3133 |     42.984 |     1.8
   75 |   1.1761 |     40.003 |   1.3107 |     42.310 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 198,402

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4425 |     85.614 |   3.2438 |     78.309 |     0.0
    2 |   2.8669 |     81.979 |   2.6388 |     70.098 |     0.0
    3 |   2.4889 |     61.243 |   2.3837 |     59.007 |     0.0
    4 |   2.2739 |     58.604 |   2.1973 |     57.659 |     0.1
    5 |   2.1237 |     57.342 |   2.0777 |     55.607 |     0.1
    6 |   2.0249 |     55.532 |   1.9863 |     54.657 |     0.1
    7 |   1.9417 |     52.547 |   1.9031 |     49.540 |     0.1
    8 |   1.8644 |     48.602 |   1.8282 |     48.100 |     0.1
    9 |   1.7952 |     48.201 |   1.7634 |     48.039 |     0.1
   10 |   1.7338 |     48.190 |   1.7077 |     48.039 |     0.1
   11 |   1.6824 |     48.266 |   1.6581 |     48.070 |     0.1
   12 |   1.6393 |     48.380 |   1.6159 |     48.284 |     0.2
   13 |   1.5967 |     46.987 |   1.5781 |     45.343 |     0.2
   14 |   1.5627 |     46.072 |   1.5467 |     45.374 |     0.2
   15 |   1.5299 |     46.148 |   1.5182 |     45.251 |     0.2
   16 |   1.5050 |     46.104 |   1.4944 |     45.343 |     0.2
   17 |   1.4831 |     46.099 |   1.4743 |     45.435 |     0.2
   18 |   1.4621 |     46.164 |   1.4589 |     45.282 |     0.2
   19 |   1.4481 |     46.115 |   1.4425 |     45.466 |     0.2
   20 |   1.4333 |     45.942 |   1.4292 |     45.466 |     0.3
   21 |   1.4173 |     45.530 |   1.4154 |     45.404 |     0.3
   22 |   1.4062 |     45.790 |   1.4041 |     45.221 |     0.3
   23 |   1.3905 |     45.644 |   1.3926 |     44.822 |     0.3
   24 |   1.3765 |     45.151 |   1.3819 |     44.884 |     0.3
   25 |   1.3712 |     45.021 |   1.3735 |     44.761 |     0.3
   26 |   1.3589 |     44.609 |   1.3648 |     44.179 |     0.3
   27 |   1.3478 |     44.235 |   1.3577 |     43.444 |     0.3
   28 |   1.3403 |     43.888 |   1.3498 |     43.474 |     0.4
   29 |   1.3299 |     43.661 |   1.3426 |     42.923 |     0.4
   30 |   1.3195 |     43.352 |   1.3339 |     42.862 |     0.4
   31 |   1.3114 |     43.054 |   1.3265 |     42.862 |     0.4
   32 |   1.3036 |     42.620 |   1.3220 |     42.525 |     0.4
   33 |   1.2936 |     42.420 |   1.3128 |     42.096 |     0.4
   34 |   1.2871 |     42.284 |   1.3075 |     42.402 |     0.4
   35 |   1.2777 |     42.068 |   1.3001 |     42.494 |     0.5
   36 |   1.2707 |     41.916 |   1.2911 |     41.973 |     0.5
   37 |   1.2625 |     41.488 |   1.2872 |     41.391 |     0.5
   38 |   1.2528 |     40.989 |   1.2789 |     41.422 |     0.5
   39 |   1.2422 |     40.632 |   1.2751 |     41.360 |     0.5
   40 |   1.2352 |     40.366 |   1.2685 |     40.931 |     0.5
   41 |   1.2270 |     40.220 |   1.2605 |     40.564 |     0.5
   42 |   1.2173 |     39.792 |   1.2580 |     40.533 |     0.5
   43 |   1.2098 |     39.646 |   1.2495 |     40.533 |     0.6
   44 |   1.2020 |     39.543 |   1.2456 |     40.074 |     0.6
   45 |   1.1941 |     39.109 |   1.2431 |     40.319 |     0.6
   46 |   1.1873 |     39.060 |   1.2331 |     40.165 |     0.6
   47 |   1.1781 |     38.524 |   1.2286 |     39.798 |     0.6
   48 |   1.1719 |     38.307 |   1.2237 |     39.430 |     0.6
   49 |   1.1608 |     37.961 |   1.2189 |     39.369 |     0.6
   50 |   1.1545 |     37.793 |   1.2111 |     38.848 |     0.6
   51 |   1.1507 |     37.603 |   1.2080 |     39.001 |     0.7
   52 |   1.1431 |     37.337 |   1.2058 |     39.032 |     0.7
   53 |   1.1339 |     36.909 |   1.2002 |     38.388 |     0.7
   54 |   1.1268 |     36.601 |   1.1968 |     38.388 |     0.7
   55 |   1.1207 |     36.357 |   1.1938 |     38.419 |     0.7
   56 |   1.1141 |     36.032 |   1.1881 |     38.174 |     0.7
   57 |   1.1053 |     35.685 |   1.1814 |     38.143 |     0.7
   58 |   1.0979 |     35.452 |   1.1807 |     38.082 |     0.7
   59 |   1.0913 |     35.138 |   1.1776 |     37.806 |     0.8
   60 |   1.0879 |     34.948 |   1.1737 |     37.898 |     0.8
   61 |   1.0832 |     34.737 |   1.1654 |     37.316 |     0.8
   62 |   1.0743 |     34.357 |   1.1639 |     37.102 |     0.8
   63 |   1.0657 |     34.124 |   1.1603 |     37.439 |     0.8
   64 |   1.0621 |     33.826 |   1.1538 |     36.887 |     0.8
   65 |   1.0560 |     33.702 |   1.1519 |     37.163 |     0.8
   66 |   1.0480 |     33.626 |   1.1456 |     36.949 |     0.8
   67 |   1.0434 |     33.257 |   1.1434 |     36.612 |     0.9
   68 |   1.0367 |     33.090 |   1.1403 |     36.703 |     0.9
   69 |   1.0302 |     32.954 |   1.1367 |     36.366 |     0.9
   70 |   1.0237 |     32.781 |   1.1333 |     36.152 |     0.9
   71 |   1.0192 |     32.548 |   1.1295 |     36.428 |     0.9
   72 |   1.0123 |     32.331 |   1.1298 |     36.520 |     0.9
   73 |   1.0064 |     32.098 |   1.1186 |     36.121 |     0.9
   74 |   0.9980 |     32.006 |   1.1201 |     35.999 |     0.9
   75 |   0.9913 |     31.719 |   1.1148 |     35.968 |     1.0
   76 |   0.9882 |     31.480 |   1.1111 |     36.091 |     1.0
   77 |   0.9810 |     31.415 |   1.1109 |     35.600 |     1.0
   78 |   0.9760 |     30.917 |   1.1089 |     35.692 |     1.0
   79 |   0.9700 |     30.917 |   1.0995 |     35.846 |     1.0
   80 |   0.9622 |     30.624 |   1.1010 |     35.478 |     1.0
   81 |   0.9564 |     30.342 |   1.0977 |     35.815 |     1.0
   82 |   0.9498 |     30.326 |   1.0960 |     35.018 |     1.0
   83 |   0.9443 |     29.952 |   1.0932 |     34.773 |     1.1
   84 |   0.9406 |     29.746 |   1.0871 |     34.528 |     1.1
   85 |   0.9391 |     29.421 |   1.0866 |     34.559 |     1.1
   86 |   0.9312 |     29.329 |   1.0858 |     34.344 |     1.1
   87 |   0.9231 |     28.972 |   1.0846 |     34.957 |     1.1
   88 |   0.9194 |     28.901 |   1.0889 |     34.589 |     1.1
   89 |   0.9107 |     28.744 |   1.0768 |     33.946 |     1.1
   90 |   0.9097 |     28.728 |   1.0752 |     34.375 |     1.2
   91 |   0.9035 |     28.381 |   1.0669 |     33.824 |     1.2
   92 |   0.8973 |     28.034 |   1.0757 |     33.885 |     1.2
   93 |   0.8911 |     27.747 |   1.0677 |     33.701 |     1.2
   94 |   0.8858 |     27.769 |   1.0658 |     33.885 |     1.2
   95 |   0.8787 |     27.324 |   1.0669 |     34.038 |     1.2
   96 |   0.8776 |     27.281 |   1.0622 |     33.793 |     1.2
   97 |   0.8687 |     26.983 |   1.0610 |     33.609 |     1.2
   98 |   0.8648 |     26.940 |   1.0593 |     33.824 |     1.3
   99 |   0.8595 |     26.653 |   1.0624 |     33.548 |     1.3
  100 |   0.8532 |     26.653 |   1.0534 |     33.272 |     1.3
  101 |   0.8523 |     26.355 |   1.0561 |     33.732 |     1.3
  102 |   0.8444 |     26.338 |   1.0605 |     33.670 |     1.3
  103 |   0.8414 |     26.132 |   1.0599 |     33.915 |     1.3
  104 |   0.8319 |     25.927 |   1.0506 |     33.058 |     1.3
  105 |   0.8324 |     25.932 |   1.0499 |     33.303 |     1.3
  106 |   0.8245 |     25.661 |   1.0513 |     33.272 |     1.4
  107 |   0.8205 |     25.477 |   1.0442 |     32.537 |     1.4
  108 |   0.8126 |     25.287 |   1.0551 |     33.578 |     1.4
  109 |   0.8085 |     25.157 |   1.0457 |     33.272 |     1.4
  110 |   0.8063 |     25.108 |   1.0531 |     33.517 |     1.4
  111 |   0.8032 |     25.276 |   1.0447 |     33.027 |     1.4
  112 |   0.7984 |     24.902 |   1.0391 |     33.272 |     1.4
  113 |   0.7940 |     24.572 |   1.0432 |     32.996 |     1.4
  114 |   0.7901 |     24.675 |   1.0398 |     32.751 |     1.5
  115 |   0.7834 |     24.328 |   1.0456 |     33.119 |     1.5
  116 |   0.7784 |     24.155 |   1.0424 |     33.333 |     1.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 2,240,770

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2316 |     61.281 |   1.6343 |     48.346 |     0.1
    2 |   1.4946 |     46.695 |   1.4214 |     45.987 |     0.1
    3 |   1.4161 |     46.158 |   1.4004 |     45.466 |     0.2
    4 |   1.4051 |     46.310 |   1.4055 |     46.140 |     0.3
    5 |   1.3972 |     46.218 |   1.3885 |     45.466 |     0.3
    6 |   1.3912 |     46.169 |   1.3751 |     45.037 |     0.4
    7 |   1.3725 |     45.248 |   1.3550 |     44.179 |     0.4
    8 |   1.3540 |     44.912 |   1.3353 |     43.658 |     0.5
    9 |   1.3440 |     44.495 |   1.3231 |     43.474 |     0.6
   10 |   1.3270 |     44.316 |   1.3173 |     43.689 |     0.6
   11 |   1.3134 |     43.942 |   1.3065 |     43.627 |     0.7
   12 |   1.3019 |     43.650 |   1.2998 |     43.137 |     0.8
   13 |   1.2937 |     43.238 |   1.2878 |     42.555 |     0.8
   14 |   1.2811 |     42.647 |   1.2727 |     42.065 |     0.9
   15 |   1.2713 |     42.523 |   1.2646 |     41.605 |     1.0
   16 |   1.2554 |     42.176 |   1.2634 |     42.034 |     1.0
   17 |   1.2528 |     41.938 |   1.2480 |     41.115 |     1.1
   18 |   1.2505 |     41.515 |   1.2456 |     41.513 |     1.1
   19 |   1.2268 |     41.363 |   1.2370 |     41.207 |     1.2
   20 |   1.2188 |     40.713 |   1.2254 |     40.472 |     1.3
   21 |   1.2150 |     40.767 |   1.2218 |     40.870 |     1.3
   22 |   1.2003 |     40.399 |   1.2080 |     39.920 |     1.4
   23 |   1.1823 |     39.418 |   1.1955 |     39.491 |     1.5
   24 |   1.1741 |     39.023 |   1.1978 |     39.767 |     1.5
   25 |   1.1603 |     38.860 |   1.1817 |     38.695 |     1.6
   26 |   1.1412 |     37.885 |   1.1749 |     38.021 |     1.6
   27 |   1.1335 |     37.982 |   1.1532 |     38.235 |     1.7
   28 |   1.1231 |     37.489 |   1.1643 |     38.297 |     1.8
   29 |   1.1074 |     37.186 |   1.1485 |     37.224 |     1.8
   30 |   1.0917 |     36.525 |   1.1434 |     37.439 |     1.9
   31 |   1.0716 |     35.896 |   1.1207 |     36.336 |     2.0
   32 |   1.0670 |     35.977 |   1.1164 |     36.949 |     2.0
   33 |   1.0584 |     35.457 |   1.1179 |     36.826 |     2.1
   34 |   1.0507 |     35.235 |   1.1047 |     36.489 |     2.2
   35 |   1.0453 |     35.235 |   1.1236 |     36.673 |     2.2
   36 |   1.0228 |     34.677 |   1.1036 |     36.581 |     2.3
   37 |   1.0404 |     35.165 |   1.0903 |     35.876 |     2.3
   38 |   1.0116 |     34.249 |   1.1014 |     35.907 |     2.4
   39 |   0.9962 |     33.886 |   1.0911 |     35.233 |     2.5
   40 |   0.9745 |     32.624 |   1.0837 |     34.865 |     2.5
   41 |   0.9778 |     33.095 |   1.0968 |     35.999 |     2.6
   42 |   0.9659 |     33.214 |   1.0888 |     35.202 |     2.7
   43 |   0.9439 |     32.407 |   1.0661 |     34.252 |     2.7
   44 |   0.9374 |     31.870 |   1.0597 |     34.651 |     2.8
   45 |   0.9281 |     31.757 |   1.0830 |     34.926 |     2.9
   46 |   0.9094 |     31.068 |   1.0820 |     34.773 |     2.9
   47 |   0.9015 |     30.689 |   1.0692 |     34.375 |     3.0
   48 |   0.8989 |     30.668 |   1.0761 |     34.620 |     3.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 2,493,026

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1955 |     83.469 |   2.8684 |     83.333 |     0.1
    2 |   2.7104 |     74.837 |   2.5803 |     67.126 |     0.2
    3 |   2.4718 |     61.801 |   2.3688 |     58.272 |     0.3
    4 |   2.2996 |     58.290 |   2.2288 |     58.395 |     0.3
    5 |   2.1787 |     57.315 |   2.1157 |     54.596 |     0.4
    6 |   2.0813 |     56.394 |   2.0232 |     55.974 |     0.5
    7 |   1.9955 |     55.570 |   1.9501 |     54.105 |     0.6
    8 |   1.9281 |     53.787 |   1.8832 |     48.652 |     0.7
    9 |   1.8586 |     50.168 |   1.8151 |     48.192 |     0.8
   10 |   1.7972 |     48.895 |   1.7537 |     48.009 |     0.9
   11 |   1.7400 |     48.629 |   1.7027 |     48.131 |     0.9
   12 |   1.6913 |     48.548 |   1.6546 |     48.315 |     1.0
   13 |   1.6412 |     48.515 |   1.6100 |     48.100 |     1.1
   14 |   1.5992 |     48.488 |   1.5706 |     48.376 |     1.2
   15 |   1.5615 |     47.524 |   1.5368 |     45.803 |     1.3
   16 |   1.5316 |     46.180 |   1.5099 |     45.527 |     1.4
   17 |   1.5043 |     45.703 |   1.4823 |     45.435 |     1.5
   18 |   1.4785 |     45.378 |   1.4634 |     45.312 |     1.5
   19 |   1.4564 |     45.357 |   1.4390 |     45.129 |     1.6
   20 |   1.4343 |     45.053 |   1.4247 |     44.700 |     1.7
   21 |   1.4169 |     44.804 |   1.4093 |     44.179 |     1.8
   22 |   1.4031 |     44.528 |   1.3926 |     44.056 |     1.9
   23 |   1.3879 |     43.991 |   1.3839 |     43.321 |     2.0
   24 |   1.3744 |     43.319 |   1.3702 |     42.953 |     2.0
   25 |   1.3594 |     42.994 |   1.3589 |     42.371 |     2.1
   26 |   1.3493 |     42.398 |   1.3479 |     41.973 |     2.2
   27 |   1.3367 |     41.726 |   1.3378 |     41.667 |     2.3
   28 |   1.3240 |     41.450 |   1.3296 |     41.115 |     2.4
   29 |   1.3104 |     40.859 |   1.3206 |     40.993 |     2.5
   30 |   1.2960 |     40.599 |   1.3111 |     40.778 |     2.6
   31 |   1.2865 |     40.383 |   1.2997 |     40.227 |     2.6
   32 |   1.2749 |     39.792 |   1.2945 |     39.675 |     2.7
   33 |   1.2645 |     39.423 |   1.2861 |     39.828 |     2.8
   34 |   1.2505 |     39.256 |   1.2760 |     38.909 |     2.9
   35 |   1.2415 |     38.909 |   1.2687 |     39.154 |     3.0
   36 |   1.2293 |     38.605 |   1.2614 |     39.032 |     3.1
   37 |   1.2181 |     38.096 |   1.2550 |     38.756 |     3.1
   38 |   1.2085 |     37.998 |   1.2424 |     38.664 |     3.2
   39 |   1.1955 |     37.608 |   1.2404 |     38.358 |     3.3
   40 |   1.1877 |     37.430 |   1.2371 |     38.480 |     3.4
   41 |   1.1759 |     37.072 |   1.2271 |     38.511 |     3.5
   42 |   1.1656 |     36.866 |   1.2231 |     38.327 |     3.6
   43 |   1.1538 |     36.514 |   1.2125 |     37.806 |     3.7
   44 |   1.1464 |     35.983 |   1.2082 |     37.561 |     3.7
   45 |   1.1338 |     35.847 |   1.2029 |     37.071 |     3.8
   46 |   1.1260 |     35.365 |   1.1982 |     37.224 |     3.9
   47 |   1.1172 |     34.888 |   1.1927 |     37.132 |     4.0
   48 |   1.1060 |     34.352 |   1.1870 |     36.887 |     4.1
   49 |   1.0947 |     33.837 |   1.1779 |     36.458 |     4.2
   50 |   1.0869 |     33.821 |   1.1783 |     36.520 |     4.2
   51 |   1.0774 |     33.371 |   1.1682 |     36.397 |     4.3
   52 |   1.0690 |     33.236 |   1.1646 |     35.999 |     4.4
   53 |   1.0588 |     32.770 |   1.1582 |     35.938 |     4.5
   54 |   1.0464 |     32.564 |   1.1552 |     36.152 |     4.6
   55 |   1.0376 |     32.006 |   1.1469 |     35.325 |     4.7
   56 |   1.0336 |     32.028 |   1.1424 |     35.355 |     4.8
   57 |   1.0235 |     31.464 |   1.1411 |     35.784 |     4.8
   58 |   1.0173 |     31.226 |   1.1389 |     35.570 |     4.9
   59 |   1.0054 |     30.770 |   1.1283 |     34.865 |     5.0
   60 |   0.9936 |     30.722 |   1.1282 |     35.325 |     5.1
   61 |   0.9876 |     30.375 |   1.1231 |     35.294 |     5.2
   62 |   0.9797 |     30.332 |   1.1179 |     35.141 |     5.3
   63 |   0.9760 |     30.082 |   1.1131 |     34.436 |     5.3
   64 |   0.9670 |     29.860 |   1.1216 |     35.202 |     5.4
   65 |   0.9563 |     29.243 |   1.1143 |     35.080 |     5.5
   66 |   0.9506 |     29.042 |   1.1081 |     34.344 |     5.6
   67 |   0.9363 |     28.923 |   1.1064 |     34.099 |     5.7
   68 |   0.9351 |     28.560 |   1.0947 |     34.161 |     5.8
   69 |   0.9219 |     28.500 |   1.0966 |     34.528 |     5.9
   70 |   0.9137 |     27.823 |   1.0912 |     34.099 |     5.9
   71 |   0.9071 |     27.796 |   1.0858 |     33.854 |     6.0
   72 |   0.9067 |     27.807 |   1.0838 |     34.069 |     6.1
   73 |   0.8922 |     27.249 |   1.0918 |     34.283 |     6.2
   74 |   0.8845 |     27.129 |   1.0808 |     34.283 |     6.3
   75 |   0.8797 |     27.016 |   1.0775 |     34.007 |     6.4
   76 |   0.8742 |     26.821 |   1.0765 |     33.701 |     6.4
   77 |   0.8644 |     26.360 |   1.0709 |     33.885 |     6.5
   78 |   0.8587 |     26.268 |   1.0736 |     34.252 |     6.6
   79 |   0.8541 |     26.197 |   1.0665 |     33.548 |     6.7
   80 |   0.8464 |     25.845 |   1.0621 |     32.966 |     6.8
   81 |   0.8387 |     25.656 |   1.0638 |     33.180 |     6.9
   82 |   0.8359 |     25.423 |   1.0564 |     33.640 |     6.9
   83 |   0.8234 |     25.358 |   1.0601 |     33.241 |     7.0
   84 |   0.8242 |     25.070 |   1.0636 |     32.966 |     7.1
   85 |   0.8085 |     24.740 |   1.0583 |     33.395 |     7.2
   86 |   0.8083 |     24.772 |   1.0544 |     32.567 |     7.3
   87 |   0.8017 |     24.550 |   1.0485 |     32.966 |     7.4
   88 |   0.7945 |     24.187 |   1.0620 |     33.088 |     7.5
   89 |   0.7844 |     24.138 |   1.0519 |     32.782 |     7.5
   90 |   0.7828 |     23.797 |   1.0485 |     32.874 |     7.6
   91 |   0.7760 |     23.228 |   1.0468 |     32.751 |     7.7
   92 |   0.7750 |     24.025 |   1.0437 |     32.384 |     7.8
   93 |   0.7630 |     23.326 |   1.0414 |     32.200 |     7.9
   94 |   0.7591 |     23.104 |   1.0416 |     32.169 |     8.0
   95 |   0.7674 |     23.738 |   1.0397 |     31.955 |     8.0
   96 |   0.7532 |     22.844 |   1.0416 |     32.445 |     8.1
   97 |   0.7484 |     22.616 |   1.0372 |     31.342 |     8.2
   98 |   0.7427 |     22.199 |   1.0282 |     31.434 |     8.3
   99 |   0.7454 |     22.876 |   1.0358 |     31.526 |     8.4
  100 |   0.7288 |     22.112 |   1.0341 |     31.495 |     8.5
  101 |   0.7208 |     21.679 |   1.0324 |     31.863 |     8.6
  102 |   0.7146 |     21.581 |   1.0312 |     31.618 |     8.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 823,298

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3205 |     64.098 |   1.6620 |     48.346 |     0.0
    2 |   1.5023 |     46.467 |   1.4269 |     45.987 |     0.0
    3 |   1.4147 |     46.202 |   1.3849 |     45.343 |     0.1
    4 |   1.3816 |     46.169 |   1.3583 |     44.393 |     0.1
    5 |   1.3462 |     45.573 |   1.3249 |     44.087 |     0.1
    6 |   1.3130 |     45.644 |   1.3035 |     45.159 |     0.1
    7 |   1.2913 |     44.744 |   1.2918 |     43.597 |     0.2
    8 |   1.2647 |     43.314 |   1.2540 |     42.188 |     0.2
    9 |   1.2371 |     42.382 |   1.2463 |     41.636 |     0.2
   10 |   1.2155 |     41.482 |   1.2107 |     41.575 |     0.2
   11 |   1.1905 |     40.702 |   1.1983 |     40.165 |     0.3
   12 |   1.1759 |     39.971 |   1.2060 |     41.299 |     0.3
   13 |   1.1540 |     39.548 |   1.1699 |     39.062 |     0.3
   14 |   1.1273 |     38.719 |   1.1468 |     38.388 |     0.3
   15 |   1.1099 |     38.085 |   1.1419 |     39.093 |     0.3
   16 |   1.0974 |     37.896 |   1.1213 |     37.898 |     0.4
   17 |   1.0782 |     37.039 |   1.1120 |     37.163 |     0.4
   18 |   1.0501 |     36.151 |   1.1042 |     36.520 |     0.4
   19 |   1.0272 |     35.376 |   1.0946 |     37.102 |     0.4
   20 |   1.0162 |     34.970 |   1.0797 |     36.458 |     0.5
   21 |   0.9952 |     34.076 |   1.0700 |     35.141 |     0.5
   22 |   0.9779 |     33.328 |   1.0550 |     35.294 |     0.5
   23 |   0.9506 |     32.521 |   1.0395 |     34.773 |     0.5
   24 |   0.9325 |     31.735 |   1.0358 |     34.314 |     0.6
   25 |   0.9288 |     31.350 |   1.0485 |     34.620 |     0.6
   26 |   0.8933 |     30.147 |   1.0172 |     32.598 |     0.6
   27 |   0.8772 |     30.023 |   1.0221 |     33.119 |     0.6
   28 |   0.8679 |     29.438 |   1.0123 |     32.169 |     0.6
   29 |   0.8469 |     28.630 |   1.0159 |     32.353 |     0.7
   30 |   0.8415 |     28.045 |   0.9836 |     31.250 |     0.7
   31 |   0.8051 |     27.016 |   0.9942 |     32.016 |     0.7
   32 |   0.8024 |     27.059 |   0.9696 |     30.913 |     0.7
   33 |   0.7860 |     26.203 |   0.9755 |     31.127 |     0.8
   34 |   0.7672 |     25.531 |   0.9819 |     31.158 |     0.8
   35 |   0.7557 |     25.173 |   0.9604 |     29.841 |     0.8
   36 |   0.7288 |     24.496 |   0.9725 |     29.596 |     0.8
   37 |   0.7158 |     23.770 |   0.9443 |     29.688 |     0.8
   38 |   0.7016 |     23.158 |   0.9799 |     30.515 |     0.9
   39 |   0.6928 |     22.898 |   0.9538 |     28.922 |     0.9
   40 |   0.6878 |     23.028 |   0.9674 |     29.504 |     0.9
   41 |   0.6676 |     22.052 |   0.9603 |     28.615 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 616,866

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2139 |     85.880 |   2.7145 |     81.955 |     0.0
    2 |   2.4319 |     63.080 |   2.2016 |     58.670 |     0.0
    3 |   2.0715 |     56.188 |   1.9503 |     53.615 |     0.1
    4 |   1.8796 |     50.190 |   1.7908 |     48.254 |     0.1
    5 |   1.7454 |     48.467 |   1.6715 |     48.223 |     0.1
    6 |   1.6393 |     48.158 |   1.5834 |     46.446 |     0.1
    7 |   1.5645 |     46.429 |   1.5234 |     45.466 |     0.1
    8 |   1.5111 |     46.104 |   1.4789 |     45.374 |     0.1
    9 |   1.4703 |     46.034 |   1.4469 |     45.435 |     0.2
   10 |   1.4380 |     45.888 |   1.4201 |     44.914 |     0.2
   11 |   1.4128 |     45.308 |   1.3960 |     45.067 |     0.2
   12 |   1.3885 |     45.037 |   1.3753 |     45.037 |     0.2
   13 |   1.3674 |     44.397 |   1.3569 |     43.781 |     0.2
   14 |   1.3467 |     43.531 |   1.3389 |     42.800 |     0.3
   15 |   1.3266 |     42.886 |   1.3251 |     41.942 |     0.3
   16 |   1.3130 |     42.073 |   1.3084 |     42.096 |     0.3
   17 |   1.2934 |     41.434 |   1.3005 |     41.299 |     0.3
   18 |   1.2780 |     41.033 |   1.2830 |     41.176 |     0.3
   19 |   1.2620 |     40.204 |   1.2659 |     40.411 |     0.3
   20 |   1.2445 |     39.846 |   1.2531 |     39.706 |     0.4
   21 |   1.2328 |     39.413 |   1.2448 |     38.971 |     0.4
   22 |   1.2155 |     38.930 |   1.2296 |     39.246 |     0.4
   23 |   1.2044 |     38.470 |   1.2190 |     37.990 |     0.4
   24 |   1.1917 |     37.955 |   1.2136 |     38.511 |     0.4
   25 |   1.1781 |     37.663 |   1.2021 |     37.684 |     0.4
   26 |   1.1683 |     37.235 |   1.1952 |     37.561 |     0.5
   27 |   1.1562 |     36.964 |   1.1841 |     37.224 |     0.5
   28 |   1.1461 |     36.622 |   1.1766 |     37.224 |     0.5
   29 |   1.1312 |     36.346 |   1.1707 |     37.224 |     0.5
   30 |   1.1210 |     35.912 |   1.1620 |     37.071 |     0.5
   31 |   1.1084 |     35.669 |   1.1579 |     36.857 |     0.6
   32 |   1.0991 |     35.398 |   1.1505 |     36.121 |     0.6
   33 |   1.0915 |     34.986 |   1.1525 |     36.979 |     0.6
   34 |   1.0805 |     34.623 |   1.1372 |     36.275 |     0.6
   35 |   1.0677 |     34.482 |   1.1306 |     36.060 |     0.6
   36 |   1.0612 |     34.038 |   1.1247 |     36.060 |     0.6
   37 |   1.0477 |     33.485 |   1.1191 |     35.325 |     0.7
   38 |   1.0379 |     33.192 |   1.1150 |     35.509 |     0.7
   39 |   1.0297 |     33.030 |   1.1049 |     34.926 |     0.7
   40 |   1.0222 |     32.575 |   1.1038 |     34.835 |     0.7
   41 |   1.0122 |     32.315 |   1.0945 |     34.957 |     0.7
   42 |   1.0057 |     32.055 |   1.0971 |     35.478 |     0.8
   43 |   0.9971 |     31.957 |   1.0885 |     35.110 |     0.8
   44 |   0.9836 |     31.432 |   1.0761 |     34.344 |     0.8
   45 |   0.9783 |     30.852 |   1.0754 |     34.528 |     0.8
   46 |   0.9631 |     30.586 |   1.0763 |     34.620 |     0.8
   47 |   0.9526 |     30.164 |   1.0663 |     34.252 |     0.8
   48 |   0.9486 |     30.239 |   1.0642 |     33.824 |     0.9
   49 |   0.9377 |     29.844 |   1.0568 |     33.793 |     0.9
   50 |   0.9317 |     29.308 |   1.0527 |     33.517 |     0.9
   51 |   0.9238 |     29.497 |   1.0491 |     32.843 |     0.9
   52 |   0.9138 |     28.739 |   1.0474 |     33.977 |     0.9
   53 |   0.9043 |     28.592 |   1.0406 |     32.874 |     1.0
   54 |   0.8936 |     28.170 |   1.0354 |     32.629 |     1.0
   55 |   0.8901 |     28.332 |   1.0321 |     33.058 |     1.0
   56 |   0.8808 |     27.931 |   1.0276 |     32.475 |     1.0
   57 |   0.8701 |     27.303 |   1.0241 |     32.414 |     1.0
   58 |   0.8613 |     27.075 |   1.0275 |     32.751 |     1.0
   59 |   0.8561 |     26.761 |   1.0140 |     32.077 |     1.1
   60 |   0.8428 |     26.669 |   1.0107 |     32.169 |     1.1
   61 |   0.8377 |     26.403 |   1.0082 |     31.464 |     1.1
   62 |   0.8309 |     26.273 |   1.0072 |     31.924 |     1.1
   63 |   0.8217 |     25.780 |   0.9982 |     30.882 |     1.1
   64 |   0.8149 |     25.639 |   0.9987 |     31.556 |     1.2
   65 |   0.8064 |     25.325 |   0.9944 |     31.250 |     1.2
   66 |   0.8002 |     25.466 |   0.9896 |     30.515 |     1.2
   67 |   0.7938 |     24.940 |   0.9882 |     31.189 |     1.2
   68 |   0.7802 |     24.344 |   0.9827 |     30.699 |     1.2
   69 |   0.7795 |     24.675 |   0.9761 |     30.607 |     1.2
   70 |   0.7679 |     24.084 |   0.9785 |     30.362 |     1.3
   71 |   0.7613 |     24.003 |   0.9780 |     30.913 |     1.3
   72 |   0.7534 |     23.521 |   0.9902 |     30.362 |     1.3
   73 |   0.7463 |     23.510 |   0.9720 |     30.055 |     1.3
   74 |   0.7419 |     23.320 |   0.9733 |     30.086 |     1.3
   75 |   0.7354 |     23.033 |   0.9676 |     29.810 |     1.4
   76 |   0.7278 |     22.860 |   0.9691 |     29.963 |     1.4
   77 |   0.7169 |     22.085 |   0.9699 |     29.994 |     1.4
   78 |   0.7090 |     22.432 |   0.9660 |     29.779 |     1.4
   79 |   0.7060 |     22.096 |   0.9659 |     29.841 |     1.4
   80 |   0.7014 |     21.966 |   0.9573 |     29.933 |     1.4
   81 |   0.6908 |     21.516 |   0.9533 |     29.167 |     1.5
   82 |   0.6842 |     21.321 |   0.9572 |     29.841 |     1.5
   83 |   0.6804 |     21.223 |   0.9457 |     29.136 |     1.5
   84 |   0.6719 |     21.007 |   0.9522 |     29.412 |     1.5
   85 |   0.6671 |     20.703 |   0.9560 |     29.749 |     1.5
   86 |   0.6611 |     20.757 |   0.9436 |     28.830 |     1.6
   87 |   0.6508 |     20.275 |   0.9465 |     29.136 |     1.6
   88 |   0.6472 |     20.329 |   0.9472 |     28.523 |     1.6
   89 |   0.6418 |     20.026 |   0.9502 |     29.565 |     1.6
   90 |   0.6344 |     19.820 |   0.9472 |     28.830 |     1.6
   91 |   0.6282 |     19.327 |   0.9414 |     28.493 |     1.6
   92 |   0.6203 |     19.305 |   0.9429 |     28.768 |     1.7
   93 |   0.6189 |     19.159 |   0.9423 |     29.136 |     1.7
   94 |   0.6131 |     19.517 |   0.9402 |     28.952 |     1.7
   95 |   0.6062 |     19.045 |   0.9406 |     28.523 |     1.7
   96 |   0.5994 |     18.791 |   0.9389 |     28.615 |     1.7
   97 |   0.5933 |     18.514 |   0.9411 |     28.370 |     1.8
   98 |   0.5884 |     18.541 |   0.9394 |     28.094 |     1.8
   99 |   0.5846 |     18.590 |   0.9427 |     28.156 |     1.8
  100 |   0.5802 |     18.368 |   0.9306 |     27.849 |     1.8
  101 |   0.5738 |     17.799 |   0.9373 |     27.819 |     1.8
  102 |   0.5695 |     17.935 |   0.9487 |     27.972 |     1.8
  103 |   0.5620 |     17.674 |   0.9303 |     27.941 |     1.9
  104 |   0.5548 |     17.376 |   0.9311 |     27.819 |     1.9
  105 |   0.5527 |     17.447 |   0.9381 |     28.064 |     1.9
  106 |   0.5474 |     17.171 |   0.9371 |     27.788 |     1.9
  107 |   0.5550 |     17.604 |   0.9381 |     27.635 |     1.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 801,378

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2041 |     59.943 |   1.5769 |     45.772 |     0.0
    2 |   1.4671 |     46.272 |   1.4045 |     45.987 |     0.0
    3 |   1.3784 |     45.676 |   1.3457 |     43.964 |     0.1
    4 |   1.3391 |     45.042 |   1.3235 |     44.455 |     0.1
    5 |   1.3009 |     44.430 |   1.3002 |     43.352 |     0.1
    6 |   1.2587 |     42.815 |   1.2466 |     41.636 |     0.1
    7 |   1.2155 |     40.832 |   1.2230 |     39.982 |     0.1
    8 |   1.1780 |     39.337 |   1.1796 |     39.062 |     0.1
    9 |   1.1436 |     38.259 |   1.1418 |     37.714 |     0.2
   10 |   1.0924 |     36.806 |   1.1195 |     37.469 |     0.2
   11 |   1.0564 |     35.826 |   1.0908 |     36.213 |     0.2
   12 |   1.0022 |     33.566 |   1.0599 |     35.447 |     0.2
   13 |   0.9754 |     32.911 |   1.0332 |     33.946 |     0.2
   14 |   0.9237 |     30.657 |   1.0212 |     34.896 |     0.2
   15 |   0.8679 |     28.625 |   0.9818 |     32.904 |     0.3
   16 |   0.8207 |     26.793 |   0.9400 |     31.373 |     0.3
   17 |   0.7849 |     25.612 |   0.9507 |     31.250 |     0.3
   18 |   0.7448 |     24.166 |   0.9286 |     29.871 |     0.3
   19 |   0.6949 |     22.329 |   0.9433 |     30.515 |     0.3
   20 |   0.6644 |     21.375 |   0.9114 |     29.013 |     0.3
   21 |   0.6240 |     20.015 |   0.9039 |     27.512 |     0.4
   22 |   0.5905 |     18.791 |   0.9261 |     29.197 |     0.4
   23 |   0.5623 |     17.956 |   0.8951 |     27.451 |     0.4
   24 |   0.5363 |     17.003 |   0.9286 |     28.217 |     0.4
   25 |   0.5059 |     16.071 |   0.9339 |     26.716 |     0.4
   26 |   0.4867 |     15.588 |   0.9202 |     27.420 |     0.4
   27 |   0.4643 |     14.900 |   0.9330 |     27.941 |     0.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 509,026

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2729 |     83.453 |   2.7006 |     71.630 |     0.0
    2 |   2.3697 |     60.116 |   2.1269 |     56.464 |     0.0
    3 |   1.9824 |     53.370 |   1.8523 |     48.070 |     0.1
    4 |   1.7691 |     48.467 |   1.6822 |     48.100 |     0.1
    5 |   1.6351 |     48.293 |   1.5736 |     48.039 |     0.1
    6 |   1.5446 |     47.421 |   1.5011 |     45.159 |     0.1
    7 |   1.4825 |     45.785 |   1.4504 |     45.006 |     0.1
    8 |   1.4393 |     45.416 |   1.4130 |     44.975 |     0.1
    9 |   1.4025 |     45.091 |   1.3859 |     44.210 |     0.2
   10 |   1.3710 |     44.327 |   1.3602 |     44.485 |     0.2
   11 |   1.3474 |     44.121 |   1.3428 |     44.179 |     0.2
   12 |   1.3273 |     43.401 |   1.3225 |     43.199 |     0.2
   13 |   1.3072 |     42.680 |   1.3062 |     41.912 |     0.2
   14 |   1.2867 |     42.046 |   1.2931 |     41.238 |     0.3
   15 |   1.2701 |     41.195 |   1.2799 |     41.452 |     0.3
   16 |   1.2552 |     40.675 |   1.2609 |     41.115 |     0.3
   17 |   1.2378 |     40.252 |   1.2490 |     39.982 |     0.3
   18 |   1.2214 |     39.315 |   1.2320 |     40.196 |     0.3
   19 |   1.2035 |     39.006 |   1.2195 |     39.583 |     0.4
   20 |   1.1924 |     38.275 |   1.2070 |     38.480 |     0.4
   21 |   1.1758 |     37.749 |   1.1963 |     37.684 |     0.4
   22 |   1.1625 |     37.180 |   1.1826 |     38.082 |     0.4
   23 |   1.1477 |     37.007 |   1.1736 |     37.745 |     0.4
   24 |   1.1317 |     36.324 |   1.1601 |     36.979 |     0.4
   25 |   1.1181 |     35.896 |   1.1495 |     36.673 |     0.5
   26 |   1.1026 |     35.446 |   1.1438 |     36.458 |     0.5
   27 |   1.0931 |     35.170 |   1.1271 |     35.570 |     0.5
   28 |   1.0777 |     34.607 |   1.1181 |     35.325 |     0.5
   29 |   1.0698 |     34.347 |   1.1089 |     35.539 |     0.5
   30 |   1.0569 |     33.859 |   1.0992 |     35.355 |     0.6
   31 |   1.0413 |     33.371 |   1.0922 |     34.712 |     0.6
   32 |   1.0300 |     33.279 |   1.0785 |     34.559 |     0.6
   33 |   1.0183 |     32.661 |   1.0735 |     34.712 |     0.6
   34 |   1.0073 |     32.233 |   1.0697 |     34.559 |     0.6
   35 |   0.9996 |     31.784 |   1.0587 |     34.681 |     0.6
   36 |   0.9874 |     31.589 |   1.0513 |     33.977 |     0.7
   37 |   0.9771 |     31.209 |   1.0448 |     33.824 |     0.7
   38 |   0.9670 |     30.982 |   1.0342 |     33.885 |     0.7
   39 |   0.9560 |     30.635 |   1.0345 |     33.578 |     0.7
   40 |   0.9483 |     30.234 |   1.0216 |     33.517 |     0.7
   41 |   0.9334 |     30.158 |   1.0200 |     33.150 |     0.8
   42 |   0.9225 |     29.708 |   1.0132 |     32.966 |     0.8
   43 |   0.9140 |     29.264 |   1.0040 |     32.445 |     0.8
   44 |   0.9063 |     29.020 |   1.0013 |     31.985 |     0.8
   45 |   0.8941 |     28.695 |   0.9904 |     31.648 |     0.8
   46 |   0.8887 |     28.191 |   0.9859 |     31.495 |     0.8
   47 |   0.8792 |     28.029 |   0.9796 |     31.464 |     0.9
   48 |   0.8668 |     27.687 |   0.9747 |     31.740 |     0.9
   49 |   0.8606 |     27.525 |   0.9710 |     31.495 |     0.9
   50 |   0.8490 |     27.064 |   0.9676 |     31.311 |     0.9
   51 |   0.8451 |     27.124 |   0.9613 |     30.974 |     0.9
   52 |   0.8376 |     26.517 |   0.9580 |     30.790 |     1.0
   53 |   0.8249 |     26.067 |   0.9550 |     31.127 |     1.0
   54 |   0.8171 |     25.661 |   0.9476 |     30.944 |     1.0
   55 |   0.8052 |     25.856 |   0.9467 |     30.147 |     1.0
   56 |   0.7979 |     25.390 |   0.9394 |     30.055 |     1.0
   57 |   0.7896 |     24.989 |   0.9391 |     30.760 |     1.1
   58 |   0.7883 |     24.935 |   0.9300 |     30.025 |     1.1
   59 |   0.7723 |     24.252 |   0.9265 |     30.116 |     1.1
   60 |   0.7633 |     24.149 |   0.9204 |     29.688 |     1.1
   61 |   0.7583 |     24.084 |   0.9182 |     29.657 |     1.1
   62 |   0.7544 |     23.629 |   0.9126 |     29.197 |     1.1
   63 |   0.7454 |     23.618 |   0.9083 |     29.136 |     1.2
   64 |   0.7381 |     23.261 |   0.9118 |     29.105 |     1.2
   65 |   0.7334 |     22.936 |   0.9027 |     28.646 |     1.2
   66 |   0.7240 |     22.648 |   0.9009 |     28.646 |     1.2
   67 |   0.7194 |     22.697 |   0.8985 |     28.768 |     1.2
   68 |   0.7149 |     22.399 |   0.8953 |     28.064 |     1.3
   69 |   0.7022 |     22.009 |   0.8832 |     28.523 |     1.3
   70 |   0.6948 |     22.145 |   0.8886 |     28.033 |     1.3
   71 |   0.6857 |     21.397 |   0.8834 |     28.064 |     1.3
   72 |   0.6823 |     21.337 |   0.8812 |     27.911 |     1.3
   73 |   0.6742 |     21.337 |   0.8811 |     27.849 |     1.3
   74 |   0.6704 |     21.164 |   0.8848 |     28.064 |     1.4
   75 |   0.6651 |     20.747 |   0.8772 |     27.665 |     1.4
   76 |   0.6618 |     20.627 |   0.8741 |     27.543 |     1.4
   77 |   0.6544 |     20.552 |   0.8751 |     27.206 |     1.4
   78 |   0.6509 |     20.048 |   0.8773 |     27.267 |     1.4
   79 |   0.6405 |     19.712 |   0.8611 |     26.869 |     1.5
   80 |   0.6340 |     19.733 |   0.8691 |     26.930 |     1.5
   81 |   0.6318 |     19.473 |   0.8737 |     27.267 |     1.5
   82 |   0.6268 |     19.576 |   0.8599 |     26.654 |     1.5
   83 |   0.6170 |     19.224 |   0.8602 |     26.838 |     1.5
   84 |   0.6117 |     18.926 |   0.8556 |     26.471 |     1.6
   85 |   0.6102 |     18.904 |   0.8558 |     26.624 |     1.6
   86 |   0.6001 |     18.655 |   0.8550 |     26.317 |     1.6
   87 |   0.5963 |     18.623 |   0.8564 |     26.072 |     1.6
   88 |   0.5908 |     18.205 |   0.8518 |     26.164 |     1.6
   89 |   0.5854 |     18.227 |   0.8598 |     26.225 |     1.6
   90 |   0.5828 |     17.772 |   0.8451 |     26.256 |     1.7
   91 |   0.5769 |     17.815 |   0.8500 |     25.919 |     1.7
   92 |   0.5748 |     17.637 |   0.8449 |     25.919 |     1.7
   93 |   0.5707 |     17.604 |   0.8549 |     25.980 |     1.7
   94 |   0.5629 |     17.414 |   0.8512 |     25.797 |     1.7
   95 |   0.5603 |     17.317 |   0.8468 |     25.888 |     1.8
   96 |   0.5500 |     17.019 |   0.8477 |     25.306 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 2,500,322

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6128 |     70.552 |   2.0441 |     53.860 |     0.1
    2 |   1.7984 |     50.715 |   1.5829 |     45.466 |     0.2
    3 |   1.5172 |     46.294 |   1.4587 |     45.987 |     0.2
    4 |   1.4442 |     46.256 |   1.4246 |     45.987 |     0.3
    5 |   1.4210 |     46.288 |   1.4081 |     45.466 |     0.4
    6 |   1.4118 |     46.316 |   1.3995 |     45.466 |     0.5
    7 |   1.4050 |     46.321 |   1.3963 |     45.466 |     0.6
    8 |   1.4019 |     46.310 |   1.3947 |     45.987 |     0.7
    9 |   1.4011 |     46.348 |   1.3915 |     45.466 |     0.8
   10 |   1.3969 |     46.186 |   1.3891 |     45.987 |     0.8
   11 |   1.3947 |     46.278 |   1.3889 |     45.466 |     0.9
   12 |   1.3931 |     46.370 |   1.3865 |     45.466 |     1.0
   13 |   1.3928 |     46.272 |   1.3847 |     45.466 |     1.1
   14 |   1.3890 |     46.332 |   1.3885 |     45.466 |     1.2
   15 |   1.3871 |     46.343 |   1.3743 |     45.987 |     1.3
   16 |   1.3768 |     46.066 |   1.3619 |     45.037 |     1.3
   17 |   1.3637 |     45.281 |   1.3575 |     45.067 |     1.4
   18 |   1.3515 |     44.901 |   1.3408 |     44.026 |     1.5
   19 |   1.3480 |     44.690 |   1.3342 |     44.393 |     1.6
   20 |   1.3344 |     44.555 |   1.3373 |     44.516 |     1.7
   21 |   1.3360 |     44.630 |   1.3250 |     44.271 |     1.8
   22 |   1.3309 |     44.647 |   1.3210 |     44.608 |     1.8
   23 |   1.3290 |     44.576 |   1.3239 |     44.608 |     1.9
   24 |   1.3215 |     44.230 |   1.3110 |     43.811 |     2.0
   25 |   1.3170 |     43.969 |   1.3034 |     43.719 |     2.1
   26 |   1.3120 |     43.839 |   1.3081 |     43.444 |     2.2
   27 |   1.3087 |     43.791 |   1.3133 |     43.719 |     2.3
   28 |   1.3031 |     43.585 |   1.2868 |     42.279 |     2.3
   29 |   1.2918 |     43.113 |   1.2918 |     43.015 |     2.4
   30 |   1.2901 |     43.113 |   1.2926 |     42.371 |     2.5
   31 |   1.2876 |     43.037 |   1.2752 |     42.126 |     2.6
   32 |   1.2766 |     42.924 |   1.2840 |     42.188 |     2.7
   33 |   1.2760 |     42.745 |   1.2678 |     41.789 |     2.8
   34 |   1.2671 |     42.479 |   1.2765 |     42.096 |     2.8
   35 |   1.2639 |     42.425 |   1.2644 |     42.004 |     2.9
   36 |   1.2623 |     42.544 |   1.2711 |     42.157 |     3.0
   37 |   1.2635 |     42.572 |   1.2629 |     41.728 |     3.1
   38 |   1.2483 |     42.246 |   1.2523 |     41.268 |     3.2
   39 |   1.2519 |     42.344 |   1.2461 |     41.575 |     3.3
   40 |   1.2466 |     41.856 |   1.2535 |     41.942 |     3.3
   41 |   1.2457 |     42.100 |   1.2629 |     41.912 |     3.4
   42 |   1.2450 |     42.106 |   1.2550 |     41.299 |     3.5
   43 |   1.2513 |     41.986 |   1.2514 |     41.422 |     3.6
   44 |   1.2326 |     41.759 |   1.2447 |     41.268 |     3.7
   45 |   1.2271 |     41.434 |   1.2459 |     41.452 |     3.8
   46 |   1.2348 |     41.531 |   1.2397 |     40.931 |     3.8
   47 |   1.2222 |     41.065 |   1.2306 |     41.023 |     3.9
   48 |   1.2268 |     41.206 |   1.2403 |     41.023 |     4.0
   49 |   1.2129 |     41.006 |   1.2318 |     40.931 |     4.1
   50 |   1.2118 |     40.849 |   1.2269 |     40.502 |     4.2
   51 |   1.2103 |     40.740 |   1.2415 |     41.299 |     4.3
   52 |   1.2045 |     40.572 |   1.2195 |     40.502 |     4.3
   53 |   1.2065 |     40.827 |   1.2181 |     41.115 |     4.4
   54 |   1.1959 |     40.437 |   1.2219 |     40.594 |     4.5
   55 |   1.1936 |     40.708 |   1.2233 |     40.594 |     4.6
   56 |   1.2013 |     40.756 |   1.2338 |     40.625 |     4.7
   57 |   1.2052 |     40.648 |   1.2270 |     40.564 |     4.8
   58 |   1.2025 |     40.827 |   1.2116 |     40.135 |     4.8
   59 |   1.1832 |     40.350 |   1.2178 |     40.502 |     4.9
   60 |   1.1794 |     40.025 |   1.2141 |     40.257 |     5.0
   61 |   1.1829 |     40.139 |   1.2150 |     40.319 |     5.1
   62 |   1.1819 |     40.442 |   1.2169 |     40.778 |     5.2
   63 |   1.1816 |     40.041 |   1.1941 |     40.196 |     5.3
   64 |   1.1711 |     39.776 |   1.1895 |     40.165 |     5.4
   65 |   1.1660 |     39.337 |   1.1924 |     39.400 |     5.4
   66 |   1.1638 |     39.478 |   1.2026 |     39.951 |     5.5
   67 |   1.1596 |     39.136 |   1.1900 |     38.358 |     5.6
   68 |   1.1459 |     38.985 |   1.2054 |     39.675 |     5.7
   69 |   1.1464 |     38.990 |   1.1880 |     39.124 |     5.8
   70 |   1.1440 |     38.649 |   1.1803 |     37.868 |     5.9
   71 |   1.1493 |     38.817 |   1.1807 |     39.430 |     5.9
   72 |   1.1412 |     38.481 |   1.1687 |     38.358 |     6.0
   73 |   1.1391 |     38.535 |   1.1777 |     38.695 |     6.1
   74 |   1.1441 |     38.714 |   1.1698 |     39.001 |     6.2
   75 |   1.1311 |     38.356 |   1.1651 |     38.266 |     6.3
   76 |   1.1312 |     38.226 |   1.1847 |     39.369 |     6.4
   77 |   1.1260 |     38.215 |   1.1579 |     38.572 |     6.4
   78 |   1.1291 |     38.259 |   1.1644 |     38.511 |     6.5
   79 |   1.1344 |     38.454 |   1.1758 |     38.174 |     6.6
   80 |   1.1297 |     38.199 |   1.1703 |     38.051 |     6.7
   81 |   1.1217 |     38.112 |   1.1512 |     38.082 |     6.8
   82 |   1.1258 |     38.237 |   1.1509 |     36.918 |     6.9
   83 |   1.1072 |     37.673 |   1.1611 |     37.623 |     6.9
   84 |   1.1069 |     37.971 |   1.1696 |     38.235 |     7.0
   85 |   1.1050 |     37.700 |   1.1659 |     38.511 |     7.1
   86 |   1.1067 |     37.050 |   1.1553 |     38.082 |     7.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 2,045,794

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1657 |     59.189 |   1.5949 |     45.466 |     0.1
    2 |   1.4687 |     46.121 |   1.3977 |     45.496 |     0.1
    3 |   1.3667 |     45.021 |   1.3454 |     43.903 |     0.2
    4 |   1.3073 |     43.162 |   1.2762 |     41.115 |     0.3
    5 |   1.2435 |     41.201 |   1.2303 |     40.349 |     0.3
    6 |   1.1873 |     39.543 |   1.1752 |     38.603 |     0.4
    7 |   1.1446 |     38.210 |   1.1358 |     36.887 |     0.4
    8 |   1.0942 |     36.443 |   1.1018 |     36.275 |     0.5
    9 |   1.0539 |     35.062 |   1.0753 |     35.294 |     0.6
   10 |   1.0229 |     33.837 |   1.0393 |     34.773 |     0.6
   11 |   0.9994 |     33.252 |   1.0268 |     34.099 |     0.7
   12 |   0.9692 |     31.892 |   0.9990 |     33.333 |     0.8
   13 |   0.9194 |     30.559 |   0.9776 |     32.077 |     0.8
   14 |   0.8896 |     29.454 |   0.9645 |     31.648 |     0.9
   15 |   0.8612 |     28.516 |   0.9725 |     32.353 |     1.0
   16 |   0.8336 |     27.530 |   0.9407 |     31.189 |     1.0
   17 |   0.8177 |     26.994 |   0.9242 |     30.116 |     1.1
   18 |   0.7802 |     26.078 |   0.9239 |     30.729 |     1.1
   19 |   0.7570 |     25.103 |   0.9182 |     30.729 |     1.2
   20 |   0.7405 |     24.967 |   0.9047 |     29.810 |     1.3
   21 |   0.7184 |     24.117 |   0.8942 |     29.259 |     1.3
   22 |   0.7016 |     23.532 |   0.8708 |     28.493 |     1.4
   23 |   0.6618 |     22.020 |   0.8671 |     28.370 |     1.5
   24 |   0.6584 |     22.199 |   0.8744 |     28.248 |     1.5
   25 |   0.6452 |     21.727 |   0.8821 |     29.259 |     1.6
   26 |   0.6149 |     20.714 |   0.8571 |     27.574 |     1.7
   27 |   0.5988 |     20.048 |   0.8599 |     27.788 |     1.7
   28 |   0.5951 |     20.042 |   0.8732 |     27.911 |     1.8
   29 |   0.5675 |     18.888 |   0.8769 |     27.696 |     1.9
   30 |   0.5622 |     18.942 |   0.8569 |     27.175 |     1.9
   31 |   0.5472 |     18.709 |   0.8555 |     26.930 |     2.0
   32 |   0.5297 |     17.913 |   0.8628 |     26.654 |     2.0
   33 |   0.5131 |     17.241 |   0.8634 |     26.440 |     2.1
   34 |   0.5000 |     16.981 |   0.8786 |     27.482 |     2.2
   35 |   0.4859 |     16.488 |   0.8538 |     26.440 |     2.2
   36 |   0.4756 |     16.195 |   0.8416 |     26.103 |     2.3
   37 |   0.4656 |     15.876 |   0.8706 |     26.654 |     2.4
   38 |   0.4672 |     15.941 |   0.8711 |     25.735 |     2.4
   39 |   0.4614 |     15.702 |   0.8825 |     26.777 |     2.5
   40 |   0.4541 |     15.480 |   0.8542 |     26.134 |     2.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 1,243,682

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5690 |     68.921 |   1.9861 |     58.824 |     0.0
    2 |   1.7683 |     50.249 |   1.5697 |     45.466 |     0.1
    3 |   1.5040 |     46.213 |   1.4474 |     45.466 |     0.1
    4 |   1.4380 |     46.240 |   1.4178 |     45.496 |     0.2
    5 |   1.4154 |     46.142 |   1.4035 |     45.987 |     0.2
    6 |   1.4040 |     46.099 |   1.3934 |     45.987 |     0.3
    7 |   1.3943 |     46.299 |   1.3879 |     45.987 |     0.3
    8 |   1.3864 |     46.175 |   1.3745 |     45.496 |     0.3
    9 |   1.3758 |     46.018 |   1.3659 |     45.496 |     0.4
   10 |   1.3713 |     45.996 |   1.3588 |     44.975 |     0.4
   11 |   1.3524 |     45.069 |   1.3426 |     44.056 |     0.5
   12 |   1.3290 |     43.856 |   1.3202 |     42.831 |     0.5
   13 |   1.3125 |     44.072 |   1.2980 |     42.678 |     0.6
   14 |   1.2988 |     43.753 |   1.2916 |     42.647 |     0.6
   15 |   1.2848 |     43.411 |   1.2745 |     42.371 |     0.6
   16 |   1.2719 |     43.086 |   1.2753 |     42.678 |     0.7
   17 |   1.2595 |     42.680 |   1.2633 |     42.249 |     0.7
   18 |   1.2518 |     42.674 |   1.2524 |     42.126 |     0.8
   19 |   1.2431 |     42.322 |   1.2415 |     41.513 |     0.8
   20 |   1.2226 |     41.553 |   1.2303 |     40.257 |     0.9
   21 |   1.2135 |     41.206 |   1.2244 |     40.901 |     0.9
   22 |   1.2070 |     40.941 |   1.2090 |     40.012 |     0.9
   23 |   1.1993 |     40.540 |   1.2114 |     40.104 |     1.0
   24 |   1.1951 |     40.448 |   1.2119 |     41.115 |     1.0
   25 |   1.1813 |     40.247 |   1.1828 |     40.288 |     1.1
   26 |   1.1627 |     39.862 |   1.1935 |     40.380 |     1.1
   27 |   1.1570 |     39.559 |   1.1816 |     40.411 |     1.2
   28 |   1.1523 |     39.261 |   1.1758 |     39.798 |     1.2
   29 |   1.1374 |     38.941 |   1.1658 |     39.277 |     1.2
   30 |   1.1294 |     38.833 |   1.1742 |     39.951 |     1.3
   31 |   1.1231 |     38.670 |   1.1531 |     39.277 |     1.3
   32 |   1.1076 |     38.036 |   1.1513 |     39.154 |     1.4
   33 |   1.1040 |     37.722 |   1.1529 |     38.113 |     1.4
   34 |   1.0937 |     37.592 |   1.1430 |     38.419 |     1.5
   35 |   1.0855 |     36.969 |   1.1288 |     38.634 |     1.5
   36 |   1.0861 |     37.229 |   1.1156 |     37.377 |     1.6
   37 |   1.0640 |     36.368 |   1.1207 |     37.776 |     1.6
   38 |   1.0556 |     36.622 |   1.1189 |     38.511 |     1.6
   39 |   1.0484 |     35.809 |   1.1085 |     36.826 |     1.7
   40 |   1.0406 |     35.755 |   1.1083 |     37.439 |     1.7
   41 |   1.0299 |     35.441 |   1.1001 |     37.377 |     1.8
   42 |   1.0143 |     34.639 |   1.0882 |     37.040 |     1.8
   43 |   1.0057 |     34.325 |   1.0890 |     36.213 |     1.9
   44 |   0.9972 |     34.054 |   1.0775 |     36.152 |     1.9
   45 |   0.9872 |     33.550 |   1.0771 |     36.366 |     1.9
   46 |   0.9809 |     33.463 |   1.0740 |     35.846 |     2.0
   47 |   0.9762 |     33.236 |   1.0788 |     35.570 |     2.0
   48 |   0.9664 |     33.046 |   1.0702 |     35.141 |     2.1
   49 |   0.9682 |     32.992 |   1.0740 |     36.826 |     2.1
   50 |   0.9539 |     32.531 |   1.0458 |     34.896 |     2.1
   51 |   0.9408 |     32.022 |   1.0577 |     36.336 |     2.2
   52 |   0.9461 |     32.559 |   1.0514 |     34.620 |     2.2
   53 |   0.9312 |     31.708 |   1.0384 |     34.406 |     2.3
   54 |   0.9596 |     32.694 |   1.0328 |     34.589 |     2.3
   55 |   0.9277 |     31.832 |   1.0263 |     34.498 |     2.4
   56 |   0.9219 |     31.432 |   1.0276 |     33.977 |     2.4
   57 |   0.9142 |     31.339 |   1.0314 |     34.007 |     2.4
   58 |   0.8893 |     30.158 |   1.0210 |     33.517 |     2.5
   59 |   0.8809 |     29.822 |   1.0118 |     33.762 |     2.5
   60 |   0.8742 |     29.394 |   1.0161 |     33.364 |     2.6
   61 |   0.8773 |     29.535 |   1.0125 |     33.425 |     2.6
   62 |   0.8607 |     29.075 |   0.9961 |     32.782 |     2.7
   63 |   0.8456 |     28.376 |   1.0028 |     32.935 |     2.7
   64 |   0.8480 |     28.663 |   0.9995 |     32.812 |     2.7
   65 |   0.8252 |     28.013 |   0.9881 |     32.169 |     2.8
   66 |   0.8294 |     27.850 |   1.0048 |     32.812 |     2.8
   67 |   0.8290 |     27.904 |   0.9930 |     32.506 |     2.9
   68 |   0.8119 |     27.476 |   0.9958 |     32.261 |     2.9
   69 |   0.8059 |     27.162 |   0.9841 |     31.924 |     3.0
   70 |   0.8042 |     26.989 |   0.9989 |     33.211 |     3.0
   71 |   0.8015 |     26.945 |   0.9854 |     32.812 |     3.0
   72 |   0.7829 |     26.577 |   0.9702 |     31.924 |     3.1
   73 |   0.7805 |     26.365 |   0.9668 |     31.036 |     3.1
   74 |   0.7691 |     25.677 |   0.9796 |     31.740 |     3.2
   75 |   0.7642 |     25.607 |   0.9630 |     31.219 |     3.2
   76 |   0.7781 |     26.268 |   0.9596 |     30.852 |     3.3
   77 |   0.7538 |     25.520 |   0.9572 |     31.127 |     3.3
   78 |   0.7487 |     25.287 |   0.9612 |     30.300 |     3.3
   79 |   0.7426 |     25.179 |   0.9573 |     30.300 |     3.4
   80 |   0.7406 |     25.098 |   0.9402 |     30.790 |     3.4
   81 |   0.7313 |     24.642 |   0.9656 |     31.710 |     3.5
   82 |   0.7247 |     24.301 |   0.9755 |     31.403 |     3.5
   83 |   0.7143 |     24.106 |   0.9523 |     30.147 |     3.6
   84 |   0.7158 |     24.025 |   0.9595 |     30.760 |     3.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 1,106,402

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8615 |     75.098 |   2.3554 |     58.824 |     0.0
    2 |   2.1117 |     57.970 |   1.9107 |     58.824 |     0.1
    3 |   1.7707 |     49.886 |   1.6480 |     48.346 |     0.1
    4 |   1.5817 |     46.749 |   1.5210 |     45.466 |     0.2
    5 |   1.4972 |     46.196 |   1.4663 |     45.987 |     0.2
    6 |   1.4564 |     46.234 |   1.4367 |     45.466 |     0.3
    7 |   1.4366 |     46.202 |   1.4207 |     45.466 |     0.3
    8 |   1.4238 |     46.326 |   1.4127 |     45.496 |     0.3
    9 |   1.4137 |     46.261 |   1.4047 |     45.466 |     0.4
   10 |   1.4073 |     46.245 |   1.4007 |     45.466 |     0.4
   11 |   1.4028 |     46.305 |   1.3960 |     45.466 |     0.5
   12 |   1.4019 |     46.223 |   1.3942 |     45.466 |     0.5
   13 |   1.3984 |     46.294 |   1.3927 |     45.956 |     0.6
   14 |   1.3963 |     46.202 |   1.3898 |     45.987 |     0.6
   15 |   1.3923 |     46.332 |   1.3851 |     45.466 |     0.6
   16 |   1.3883 |     46.245 |   1.3799 |     45.466 |     0.7
   17 |   1.3845 |     46.462 |   1.3782 |     45.496 |     0.7
   18 |   1.3787 |     46.164 |   1.3749 |     45.956 |     0.8
   19 |   1.3754 |     46.337 |   1.3645 |     45.987 |     0.8
   20 |   1.3699 |     46.245 |   1.3621 |     44.945 |     0.9
   21 |   1.3651 |     45.860 |   1.3582 |     45.251 |     0.9
   22 |   1.3584 |     45.898 |   1.3477 |     44.792 |     0.9
   23 |   1.3475 |     45.443 |   1.3358 |     44.363 |     1.0
   24 |   1.3381 |     45.042 |   1.3281 |     44.271 |     1.0
   25 |   1.3311 |     44.972 |   1.3272 |     44.210 |     1.1
   26 |   1.3239 |     44.663 |   1.3192 |     44.118 |     1.1
   27 |   1.3223 |     44.679 |   1.3155 |     44.424 |     1.2
   28 |   1.3160 |     44.614 |   1.3117 |     44.026 |     1.2
   29 |   1.3073 |     44.533 |   1.3172 |     44.301 |     1.2
   30 |   1.3041 |     44.175 |   1.3079 |     43.627 |     1.3
   31 |   1.3003 |     44.051 |   1.3010 |     43.658 |     1.3
   32 |   1.3016 |     44.181 |   1.3055 |     43.689 |     1.4
   33 |   1.2920 |     43.785 |   1.2940 |     43.137 |     1.4
   34 |   1.2844 |     43.585 |   1.2987 |     43.413 |     1.5
   35 |   1.2798 |     43.363 |   1.2936 |     43.076 |     1.5
   36 |   1.2742 |     42.962 |   1.2802 |     42.433 |     1.5
   37 |   1.2669 |     42.783 |   1.2818 |     42.433 |     1.6
   38 |   1.2565 |     42.171 |   1.2689 |     41.759 |     1.6
   39 |   1.2489 |     41.813 |   1.2685 |     41.085 |     1.7
   40 |   1.2470 |     42.089 |   1.2662 |     41.299 |     1.7
   41 |   1.2441 |     41.585 |   1.2598 |     41.299 |     1.8
   42 |   1.2407 |     41.309 |   1.2483 |     40.625 |     1.8
   43 |   1.2307 |     40.924 |   1.2490 |     40.656 |     1.8
   44 |   1.2287 |     40.968 |   1.2454 |     39.798 |     1.9
   45 |   1.2241 |     41.071 |   1.2381 |     40.625 |     1.9
   46 |   1.2105 |     40.697 |   1.2336 |     40.564 |     2.0
   47 |   1.2073 |     40.372 |   1.2296 |     39.890 |     2.0
   48 |   1.2109 |     40.475 |   1.2256 |     40.380 |     2.1
   49 |   1.1995 |     40.220 |   1.2292 |     40.411 |     2.1
   50 |   1.1973 |     40.090 |   1.2229 |     39.951 |     2.1
   51 |   1.1916 |     39.911 |   1.2126 |     39.461 |     2.2
   52 |   1.1871 |     39.640 |   1.2154 |     39.828 |     2.2
   53 |   1.1833 |     39.879 |   1.2147 |     40.411 |     2.3
   54 |   1.1787 |     39.792 |   1.2078 |     39.675 |     2.3
   55 |   1.1683 |     39.711 |   1.2135 |     39.859 |     2.4
   56 |   1.1670 |     39.608 |   1.2077 |     39.767 |     2.4
   57 |   1.1661 |     39.299 |   1.2050 |     40.104 |     2.4
   58 |   1.1684 |     39.337 |   1.1959 |     39.737 |     2.5
   59 |   1.1580 |     39.071 |   1.1912 |     39.737 |     2.5
   60 |   1.1487 |     39.023 |   1.1917 |     39.737 |     2.6
   61 |   1.1463 |     39.147 |   1.1811 |     39.400 |     2.6
   62 |   1.1483 |     38.995 |   1.1888 |     39.369 |     2.7
   63 |   1.1444 |     38.681 |   1.1874 |     39.338 |     2.7
   64 |   1.1420 |     38.871 |   1.1784 |     39.185 |     2.7
   65 |   1.1310 |     38.421 |   1.1800 |     39.001 |     2.8
   66 |   1.1360 |     38.692 |   1.1646 |     38.634 |     2.8
   67 |   1.1262 |     38.253 |   1.1805 |     38.971 |     2.9
   68 |   1.1218 |     37.728 |   1.1735 |     39.062 |     2.9
   69 |   1.1122 |     37.733 |   1.1679 |     38.787 |     3.0
   70 |   1.1013 |     37.522 |   1.1570 |     38.542 |     3.0
   71 |   1.1086 |     37.592 |   1.1613 |     38.235 |     3.0
   72 |   1.0978 |     37.500 |   1.1452 |     37.745 |     3.1
   73 |   1.0935 |     37.121 |   1.1619 |     38.082 |     3.1
   74 |   1.0987 |     37.267 |   1.1525 |     38.511 |     3.2
   75 |   1.0854 |     37.094 |   1.1418 |     37.714 |     3.2
   76 |   1.0835 |     36.834 |   1.1430 |     37.714 |     3.3
   77 |   1.0898 |     37.148 |   1.1391 |     38.051 |     3.3
   78 |   1.0759 |     36.308 |   1.1333 |     38.174 |     3.3
   79 |   1.0770 |     36.568 |   1.1255 |     37.224 |     3.4
   80 |   1.0661 |     36.156 |   1.1324 |     37.194 |     3.4
   81 |   1.0646 |     36.048 |   1.1272 |     37.714 |     3.5
   82 |   1.0708 |     36.335 |   1.1249 |     37.286 |     3.5
   83 |   1.0615 |     35.994 |   1.1234 |     37.377 |     3.6
   84 |   1.0537 |     35.663 |   1.1219 |     37.040 |     3.6
   85 |   1.0563 |     35.793 |   1.1386 |     37.776 |     3.6
   86 |   1.0605 |     35.891 |   1.1293 |     37.623 |     3.7
   87 |   1.0601 |     35.961 |   1.1173 |     37.102 |     3.7
   88 |   1.0531 |     35.772 |   1.1147 |     37.347 |     3.8
   89 |   1.0563 |     35.891 |   1.1201 |     37.040 |     3.8
   90 |   1.0477 |     35.685 |   1.1070 |     37.255 |     3.9
   91 |   1.0353 |     35.143 |   1.1000 |     36.581 |     3.9
   92 |   1.0336 |     35.046 |   1.1110 |     36.857 |     3.9
   93 |   1.0372 |     35.181 |   1.1127 |     36.949 |     4.0
   94 |   1.0377 |     35.587 |   1.1036 |     37.500 |     4.0
   95 |   1.0274 |     34.921 |   1.1053 |     36.795 |     4.1
   96 |   1.0301 |     35.398 |   1.0990 |     36.489 |     4.1
   97 |   1.0220 |     35.051 |   1.0967 |     36.612 |     4.2
   98 |   1.0341 |     35.148 |   1.0990 |     36.826 |     4.2
   99 |   1.0246 |     35.105 |   1.0803 |     35.876 |     4.2
  100 |   1.0153 |     34.509 |   1.0908 |     36.336 |     4.3
  101 |   1.0155 |     34.926 |   1.0843 |     35.999 |     4.3
  102 |   1.0093 |     34.368 |   1.0882 |     36.121 |     4.4
  103 |   1.0150 |     34.487 |   1.0987 |     36.857 |     4.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 669,986

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3298 |     83.577 |   2.9321 |     83.241 |     0.0
    2 |   2.7163 |     70.514 |   2.5753 |     66.912 |     0.1
    3 |   2.4817 |     63.259 |   2.3982 |     58.946 |     0.1
    4 |   2.3367 |     59.363 |   2.2726 |     58.854 |     0.1
    5 |   2.2292 |     58.285 |   2.1853 |     58.395 |     0.1
    6 |   2.1438 |     57.905 |   2.0933 |     57.874 |     0.2
    7 |   2.0651 |     57.082 |   2.0127 |     57.169 |     0.2
    8 |   1.9889 |     55.987 |   1.9407 |     55.515 |     0.2
    9 |   1.9170 |     53.218 |   1.8669 |     48.683 |     0.3
   10 |   1.8469 |     50.087 |   1.7966 |     48.499 |     0.3
   11 |   1.7815 |     49.512 |   1.7339 |     48.315 |     0.3
   12 |   1.7220 |     49.095 |   1.6790 |     48.376 |     0.3
   13 |   1.6702 |     48.803 |   1.6317 |     48.039 |     0.4
   14 |   1.6270 |     47.150 |   1.5893 |     45.496 |     0.4
   15 |   1.5847 |     46.386 |   1.5546 |     45.496 |     0.4
   16 |   1.5545 |     46.283 |   1.5261 |     45.496 |     0.5
   17 |   1.5240 |     46.169 |   1.4998 |     45.466 |     0.5
   18 |   1.5025 |     46.099 |   1.4788 |     45.496 |     0.5
   19 |   1.4765 |     45.898 |   1.4600 |     45.312 |     0.5
   20 |   1.4585 |     45.741 |   1.4422 |     44.730 |     0.6
   21 |   1.4426 |     45.557 |   1.4267 |     44.853 |     0.6
   22 |   1.4284 |     45.210 |   1.4158 |     44.424 |     0.6
   23 |   1.4147 |     44.826 |   1.4031 |     43.382 |     0.7
   24 |   1.4021 |     44.403 |   1.3906 |     43.290 |     0.7
   25 |   1.3898 |     43.953 |   1.3811 |     42.984 |     0.7
   26 |   1.3778 |     43.850 |   1.3712 |     42.494 |     0.7
   27 |   1.3670 |     43.428 |   1.3600 |     42.157 |     0.8
   28 |   1.3587 |     43.227 |   1.3497 |     42.126 |     0.8
   29 |   1.3441 |     42.783 |   1.3383 |     42.310 |     0.8
   30 |   1.3323 |     42.696 |   1.3316 |     42.004 |     0.9
   31 |   1.3228 |     42.322 |   1.3231 |     41.789 |     0.9
   32 |   1.3152 |     42.290 |   1.3141 |     41.483 |     0.9
   33 |   1.3029 |     41.743 |   1.3026 |     40.717 |     0.9
   34 |   1.2923 |     41.087 |   1.2978 |     40.472 |     1.0
   35 |   1.2832 |     40.735 |   1.2855 |     40.227 |     1.0
   36 |   1.2703 |     40.659 |   1.2794 |     40.043 |     1.0
   37 |   1.2630 |     40.307 |   1.2693 |     39.706 |     1.1
   38 |   1.2523 |     40.090 |   1.2655 |     39.767 |     1.1
   39 |   1.2455 |     39.857 |   1.2550 |     39.185 |     1.1
   40 |   1.2349 |     39.770 |   1.2520 |     39.185 |     1.1
   41 |   1.2244 |     39.299 |   1.2429 |     39.308 |     1.2
   42 |   1.2177 |     39.055 |   1.2350 |     38.879 |     1.2
   43 |   1.2075 |     39.017 |   1.2299 |     38.879 |     1.2
   44 |   1.1988 |     38.746 |   1.2260 |     39.583 |     1.3
   45 |   1.1907 |     38.584 |   1.2209 |     38.909 |     1.3
   46 |   1.1817 |     38.389 |   1.2165 |     38.817 |     1.3
   47 |   1.1727 |     38.291 |   1.2076 |     38.388 |     1.3
   48 |   1.1684 |     38.107 |   1.2034 |     38.450 |     1.4
   49 |   1.1589 |     37.744 |   1.2008 |     38.419 |     1.4
   50 |   1.1539 |     37.467 |   1.1954 |     38.174 |     1.4
   51 |   1.1463 |     37.457 |   1.1873 |     38.174 |     1.5
   52 |   1.1367 |     36.931 |   1.1893 |     37.898 |     1.5
   53 |   1.1286 |     36.790 |   1.1831 |     37.837 |     1.5
   54 |   1.1199 |     36.492 |   1.1808 |     37.623 |     1.5
   55 |   1.1164 |     36.427 |   1.1751 |     37.990 |     1.6
   56 |   1.1062 |     36.183 |   1.1722 |     37.684 |     1.6
   57 |   1.0994 |     35.896 |   1.1684 |     37.469 |     1.6
   58 |   1.0935 |     35.723 |   1.1667 |     37.255 |     1.7
   59 |   1.0848 |     35.441 |   1.1584 |     37.286 |     1.7
   60 |   1.0787 |     35.300 |   1.1531 |     37.132 |     1.7
   61 |   1.0670 |     34.834 |   1.1544 |     37.010 |     1.7
   62 |   1.0628 |     34.856 |   1.1480 |     37.010 |     1.8
   63 |   1.0520 |     34.466 |   1.1499 |     36.979 |     1.8
   64 |   1.0514 |     34.282 |   1.1418 |     36.703 |     1.8
   65 |   1.0416 |     33.751 |   1.1386 |     36.550 |     1.9
   66 |   1.0303 |     33.675 |   1.1309 |     36.244 |     1.9
   67 |   1.0261 |     33.431 |   1.1289 |     35.968 |     1.9
   68 |   1.0235 |     33.030 |   1.1282 |     36.029 |     1.9
   69 |   1.0125 |     32.526 |   1.1259 |     35.938 |     2.0
   70 |   1.0085 |     32.466 |   1.1210 |     36.244 |     2.0
   71 |   1.0016 |     32.125 |   1.1155 |     35.631 |     2.0
   72 |   0.9960 |     31.919 |   1.1136 |     35.539 |     2.1
   73 |   0.9852 |     31.518 |   1.1126 |     35.172 |     2.1
   74 |   0.9803 |     31.497 |   1.1061 |     35.447 |     2.1
   75 |   0.9725 |     30.895 |   1.1020 |     35.263 |     2.1
   76 |   0.9630 |     30.711 |   1.1008 |     35.386 |     2.2
   77 |   0.9592 |     30.733 |   1.0950 |     35.509 |     2.2
   78 |   0.9481 |     30.028 |   1.0945 |     34.896 |     2.2
   79 |   0.9448 |     29.941 |   1.0976 |     35.080 |     2.3
   80 |   0.9456 |     29.855 |   1.0842 |     34.528 |     2.3
   81 |   0.9355 |     29.367 |   1.0815 |     34.161 |     2.3
   82 |   0.9253 |     29.053 |   1.0765 |     34.161 |     2.3
   83 |   0.9203 |     28.928 |   1.0754 |     34.436 |     2.4
   84 |   0.9111 |     28.663 |   1.0741 |     34.130 |     2.4
   85 |   0.9025 |     28.468 |   1.0736 |     34.375 |     2.4
   86 |   0.9015 |     27.948 |   1.0685 |     33.548 |     2.5
   87 |   0.8922 |     27.682 |   1.0682 |     34.038 |     2.5
   88 |   0.8872 |     27.617 |   1.0575 |     33.425 |     2.5
   89 |   0.8804 |     27.368 |   1.0560 |     33.670 |     2.5
   90 |   0.8713 |     27.287 |   1.0584 |     33.487 |     2.6
   91 |   0.8699 |     26.907 |   1.0520 |     33.272 |     2.6
   92 |   0.8627 |     26.972 |   1.0489 |     33.364 |     2.6
   93 |   0.8554 |     26.463 |   1.0440 |     32.812 |     2.7
   94 |   0.8510 |     26.387 |   1.0489 |     32.567 |     2.7
   95 |   0.8449 |     26.387 |   1.0399 |     32.751 |     2.7
   96 |   0.8465 |     26.241 |   1.0390 |     32.567 |     2.7
   97 |   0.8327 |     25.840 |   1.0347 |     32.659 |     2.8
   98 |   0.8269 |     25.466 |   1.0311 |     32.690 |     2.8
   99 |   0.8202 |     25.255 |   1.0290 |     32.230 |     2.8
  100 |   0.8159 |     25.249 |   1.0265 |     32.475 |     2.9
  101 |   0.8117 |     25.016 |   1.0263 |     32.261 |     2.9
  102 |   0.8025 |     25.011 |   1.0236 |     32.230 |     2.9
  103 |   0.7952 |     24.659 |   1.0197 |     32.537 |     2.9
  104 |   0.7902 |     24.355 |   1.0205 |     31.893 |     3.0
  105 |   0.7891 |     24.306 |   1.0136 |     31.924 |     3.0
  106 |   0.7816 |     24.187 |   1.0167 |     32.445 |     3.0
  107 |   0.7724 |     23.878 |   1.0123 |     32.016 |     3.1
  108 |   0.7683 |     23.700 |   1.0132 |     31.464 |     3.1
  109 |   0.7624 |     23.369 |   1.0095 |     31.955 |     3.1
  110 |   0.7627 |     23.418 |   1.0069 |     31.219 |     3.1
  111 |   0.7613 |     23.450 |   1.0060 |     31.710 |     3.2
  112 |   0.7522 |     22.963 |   1.0056 |     31.618 |     3.2
  113 |   0.7460 |     22.914 |   1.0039 |     31.526 |     3.2
  114 |   0.7398 |     22.974 |   0.9968 |     31.648 |     3.3
  115 |   0.7332 |     22.410 |   0.9982 |     31.311 |     3.3
  116 |   0.7294 |     22.567 |   0.9952 |     30.913 |     3.3
  117 |   0.7264 |     22.264 |   0.9978 |     31.373 |     3.3
  118 |   0.7184 |     22.080 |   0.9987 |     31.219 |     3.4
  119 |   0.7129 |     21.776 |   0.9919 |     31.066 |     3.4
  120 |   0.7085 |     21.679 |   0.9914 |     31.618 |     3.4
  121 |   0.7039 |     21.570 |   0.9935 |     31.066 |     3.5
  122 |   0.6968 |     21.700 |   0.9938 |     30.668 |     3.5
  123 |   0.6925 |     21.375 |   0.9947 |     30.913 |     3.5
  124 |   0.6920 |     21.283 |   0.9891 |     30.607 |     3.5
  125 |   0.6875 |     20.898 |   0.9875 |     31.311 |     3.6
  126 |   0.6875 |     21.023 |   0.9903 |     30.760 |     3.6
  127 |   0.6777 |     20.622 |   0.9832 |     30.576 |     3.6
  128 |   0.6738 |     20.806 |   0.9854 |     30.025 |     3.7
  129 |   0.6705 |     20.595 |   0.9819 |     29.933 |     3.7
  130 |   0.6653 |     20.351 |   0.9893 |     30.208 |     3.7
  131 |   0.6584 |     20.129 |   0.9807 |     30.607 |     3.7
  132 |   0.6596 |     20.167 |   0.9877 |     29.994 |     3.8
  133 |   0.6465 |     19.880 |   0.9780 |     30.086 |     3.8
  134 |   0.6429 |     19.706 |   0.9798 |     29.718 |     3.8
  135 |   0.6458 |     19.538 |   0.9865 |     29.657 |     3.9
  136 |   0.6386 |     19.533 |   0.9793 |     29.902 |     3.9
  137 |   0.6337 |     19.343 |   0.9829 |     30.055 |     3.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,210,402

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5087 |     67.274 |   1.9565 |     54.350 |     0.0
    2 |   1.7354 |     48.803 |   1.5551 |     45.466 |     0.1
    3 |   1.4937 |     46.353 |   1.4466 |     45.466 |     0.1
    4 |   1.4333 |     46.229 |   1.4247 |     45.558 |     0.2
    5 |   1.4169 |     46.164 |   1.4039 |     45.987 |     0.2
    6 |   1.4069 |     46.440 |   1.3984 |     45.435 |     0.2
    7 |   1.3969 |     46.072 |   1.3882 |     45.527 |     0.3
    8 |   1.3872 |     46.099 |   1.3789 |     45.987 |     0.3
    9 |   1.3717 |     45.990 |   1.3569 |     45.404 |     0.4
   10 |   1.3462 |     44.652 |   1.3288 |     43.352 |     0.4
   11 |   1.3170 |     43.861 |   1.3176 |     43.260 |     0.5
   12 |   1.2873 |     43.455 |   1.2756 |     42.831 |     0.5
   13 |   1.2585 |     42.447 |   1.2482 |     41.605 |     0.5
   14 |   1.2256 |     41.499 |   1.2401 |     41.759 |     0.6
   15 |   1.2092 |     40.930 |   1.2228 |     40.931 |     0.6
   16 |   1.1941 |     40.534 |   1.1959 |     40.441 |     0.7
   17 |   1.1645 |     39.917 |   1.1887 |     39.645 |     0.7
   18 |   1.1517 |     39.120 |   1.1677 |     39.491 |     0.7
   19 |   1.1274 |     38.464 |   1.1541 |     38.695 |     0.8
   20 |   1.1038 |     37.440 |   1.1389 |     38.235 |     0.8
   21 |   1.0879 |     37.072 |   1.1214 |     37.776 |     0.9
   22 |   1.0693 |     36.173 |   1.1160 |     37.561 |     0.9
   23 |   1.0459 |     35.338 |   1.1090 |     36.857 |     0.9
   24 |   1.0427 |     35.468 |   1.1141 |     37.592 |     1.0
   25 |   1.0283 |     34.607 |   1.0792 |     36.489 |     1.0
   26 |   0.9995 |     33.788 |   1.0619 |     35.876 |     1.1
   27 |   0.9791 |     32.846 |   1.0674 |     35.662 |     1.1
   28 |   0.9665 |     32.423 |   1.0643 |     35.233 |     1.1
   29 |   0.9515 |     31.724 |   1.0488 |     35.080 |     1.2
   30 |   0.9262 |     31.041 |   1.0281 |     34.375 |     1.2
   31 |   0.9066 |     30.158 |   1.0382 |     34.651 |     1.3
   32 |   0.8929 |     29.866 |   1.0185 |     33.915 |     1.3
   33 |   0.8708 |     29.172 |   1.0077 |     33.517 |     1.4
   34 |   0.8566 |     28.446 |   1.0031 |     32.751 |     1.4
   35 |   0.8544 |     28.370 |   0.9783 |     32.322 |     1.4
   36 |   0.8456 |     28.002 |   0.9895 |     32.353 |     1.5
   37 |   0.8222 |     27.184 |   0.9741 |     31.801 |     1.5
   38 |   0.8020 |     26.398 |   0.9714 |     32.261 |     1.6
   39 |   0.7858 |     25.818 |   0.9554 |     31.679 |     1.6
   40 |   0.7747 |     25.694 |   0.9757 |     32.322 |     1.6
   41 |   0.7587 |     25.114 |   0.9545 |     31.373 |     1.7
   42 |   0.7561 |     25.011 |   0.9727 |     32.047 |     1.7
   43 |   0.7427 |     24.637 |   0.9586 |     30.944 |     1.8
   44 |   0.7367 |     24.133 |   0.9785 |     31.403 |     1.8
   45 |   0.7258 |     23.889 |   0.9454 |     30.208 |     1.8
   46 |   0.6837 |     22.361 |   0.9435 |     29.994 |     1.9
   47 |   0.6889 |     22.692 |   0.9488 |     31.066 |     1.9
   48 |   0.6706 |     21.744 |   0.9437 |     30.055 |     2.0
   49 |   0.6552 |     21.191 |   0.9555 |     29.841 |     2.0
   50 |   0.6486 |     20.942 |   0.9433 |     29.534 |     2.0
   51 |   0.6487 |     21.251 |   0.9340 |     29.718 |     2.1
   52 |   0.6376 |     21.104 |   0.9285 |     29.197 |     2.1
   53 |   0.6154 |     19.907 |   0.9392 |     29.320 |     2.2
   54 |   0.6082 |     19.842 |   0.9352 |     28.952 |     2.2
   55 |   0.5962 |     19.262 |   0.9267 |     28.799 |     2.3
   56 |   0.5743 |     18.476 |   0.9509 |     29.565 |     2.3
   57 |   0.5716 |     18.639 |   0.9261 |     29.075 |     2.3
   58 |   0.5586 |     18.075 |   0.9259 |     28.370 |     2.4
   59 |   0.5584 |     18.254 |   0.9216 |     29.075 |     2.4
   60 |   0.5447 |     17.718 |   0.9456 |     28.830 |     2.5
   61 |   0.5551 |     18.390 |   0.9510 |     28.493 |     2.5
   62 |   0.5392 |     17.653 |   0.9307 |     28.585 |     2.5
   63 |   0.5295 |     17.214 |   0.9301 |     28.033 |     2.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,191,842

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5399 |     67.084 |   2.0007 |     58.548 |     0.0
    2 |   1.7551 |     49.415 |   1.5682 |     45.466 |     0.1
    3 |   1.5037 |     46.213 |   1.4499 |     45.466 |     0.1
    4 |   1.4412 |     46.213 |   1.4203 |     45.466 |     0.2
    5 |   1.4197 |     46.316 |   1.4117 |     45.374 |     0.2
    6 |   1.4085 |     46.229 |   1.4000 |     45.987 |     0.2
    7 |   1.4006 |     46.310 |   1.3968 |     45.987 |     0.3
    8 |   1.3970 |     46.359 |   1.3944 |     45.466 |     0.3
    9 |   1.3899 |     46.229 |   1.3796 |     45.343 |     0.4
   10 |   1.3773 |     45.627 |   1.3667 |     43.873 |     0.4
   11 |   1.3552 |     44.836 |   1.3383 |     44.118 |     0.5
   12 |   1.3396 |     44.257 |   1.3268 |     43.015 |     0.5
   13 |   1.3272 |     43.910 |   1.3149 |     43.107 |     0.5
   14 |   1.3154 |     43.877 |   1.3048 |     42.892 |     0.6
   15 |   1.3056 |     43.639 |   1.2901 |     43.137 |     0.6
   16 |   1.2906 |     43.238 |   1.2865 |     42.126 |     0.7
   17 |   1.2687 |     42.642 |   1.2710 |     41.023 |     0.7
   18 |   1.2468 |     41.645 |   1.2576 |     41.391 |     0.7
   19 |   1.2341 |     41.092 |   1.2393 |     40.012 |     0.8
   20 |   1.2129 |     40.632 |   1.2231 |     40.196 |     0.8
   21 |   1.1952 |     39.982 |   1.2037 |     39.645 |     0.9
   22 |   1.1721 |     38.963 |   1.1942 |     38.664 |     0.9
   23 |   1.1577 |     38.286 |   1.1786 |     38.603 |     1.0
   24 |   1.1337 |     37.711 |   1.1739 |     38.143 |     1.0
   25 |   1.1170 |     36.991 |   1.1564 |     37.377 |     1.0
   26 |   1.0968 |     36.779 |   1.1417 |     37.102 |     1.1
   27 |   1.0837 |     35.940 |   1.1349 |     37.224 |     1.1
   28 |   1.0706 |     35.571 |   1.1497 |     37.469 |     1.2
   29 |   1.0580 |     35.403 |   1.1274 |     36.949 |     1.2
   30 |   1.0446 |     34.878 |   1.1049 |     36.275 |     1.2
   31 |   1.0153 |     34.032 |   1.1008 |     35.570 |     1.3
   32 |   0.9999 |     33.604 |   1.0894 |     35.600 |     1.3
   33 |   0.9812 |     32.970 |   1.0877 |     35.600 |     1.4
   34 |   0.9625 |     32.412 |   1.0747 |     36.183 |     1.4
   35 |   0.9558 |     32.358 |   1.0565 |     35.999 |     1.4
   36 |   0.9421 |     32.049 |   1.0486 |     35.417 |     1.5
   37 |   0.9208 |     31.242 |   1.0433 |     34.467 |     1.5
   38 |   0.9055 |     30.743 |   1.0301 |     34.252 |     1.6
   39 |   0.8906 |     30.093 |   1.0399 |     34.804 |     1.6
   40 |   0.8786 |     29.627 |   1.0210 |     33.915 |     1.7
   41 |   0.8803 |     29.649 |   1.0421 |     34.130 |     1.7
   42 |   0.8583 |     28.749 |   1.0333 |     34.283 |     1.7
   43 |   0.8481 |     28.414 |   1.0146 |     33.640 |     1.8
   44 |   0.8319 |     28.159 |   1.0367 |     34.804 |     1.8
   45 |   0.8148 |     27.492 |   1.0149 |     34.314 |     1.9
   46 |   0.7925 |     26.528 |   1.0113 |     32.690 |     1.9
   47 |   0.7835 |     26.252 |   1.0109 |     33.854 |     1.9
   48 |   0.7843 |     26.067 |   1.0203 |     33.640 |     2.0
   49 |   0.7592 |     25.461 |   1.0088 |     33.119 |     2.0
   50 |   0.7534 |     25.309 |   1.0088 |     32.077 |     2.1
   51 |   0.7405 |     24.913 |   1.0057 |     32.598 |     2.1
   52 |   0.7351 |     24.810 |   0.9876 |     32.138 |     2.2
   53 |   0.7233 |     24.388 |   0.9872 |     32.384 |     2.2
   54 |   0.7126 |     23.786 |   1.0043 |     32.200 |     2.2
   55 |   0.7057 |     23.851 |   1.0099 |     32.966 |     2.3
   56 |   0.6853 |     23.196 |   0.9917 |     31.648 |     2.3
   57 |   0.6810 |     22.616 |   1.0037 |     31.495 |     2.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,073,954

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7832 |     75.033 |   2.4325 |     60.539 |     0.0
    2 |   2.2345 |     59.000 |   2.0379 |     58.824 |     0.1
    3 |   1.9203 |     53.907 |   1.7721 |     48.315 |     0.1
    4 |   1.6977 |     46.933 |   1.5999 |     45.466 |     0.2
    5 |   1.5647 |     46.261 |   1.5069 |     45.496 |     0.2
    6 |   1.4885 |     46.251 |   1.4573 |     45.466 |     0.3
    7 |   1.4561 |     46.272 |   1.4340 |     46.048 |     0.3
    8 |   1.4340 |     46.316 |   1.4188 |     45.558 |     0.3
    9 |   1.4203 |     45.915 |   1.4008 |     44.485 |     0.4
   10 |   1.4025 |     45.367 |   1.3893 |     44.792 |     0.4
   11 |   1.3865 |     44.934 |   1.3728 |     44.914 |     0.5
   12 |   1.3737 |     44.826 |   1.3570 |     44.271 |     0.5
   13 |   1.3647 |     44.723 |   1.3498 |     43.903 |     0.6
   14 |   1.3490 |     44.408 |   1.3367 |     43.934 |     0.6
   15 |   1.3380 |     44.154 |   1.3304 |     44.118 |     0.6
   16 |   1.3294 |     44.143 |   1.3286 |     44.179 |     0.7
   17 |   1.3174 |     43.563 |   1.3122 |     43.444 |     0.7
   18 |   1.3044 |     43.460 |   1.2968 |     42.953 |     0.8
   19 |   1.2924 |     42.842 |   1.2857 |     41.789 |     0.8
   20 |   1.2783 |     42.349 |   1.2807 |     41.850 |     0.9
   21 |   1.2676 |     42.203 |   1.2710 |     41.697 |     0.9
   22 |   1.2554 |     42.030 |   1.2611 |     41.575 |     0.9
   23 |   1.2509 |     42.187 |   1.2498 |     41.513 |     1.0
   24 |   1.2464 |     41.797 |   1.2463 |     40.809 |     1.0
   25 |   1.2280 |     41.049 |   1.2391 |     40.686 |     1.1
   26 |   1.2230 |     40.854 |   1.2344 |     40.625 |     1.1
   27 |   1.2072 |     40.545 |   1.2285 |     40.502 |     1.2
   28 |   1.1966 |     40.252 |   1.2135 |     40.288 |     1.2
   29 |   1.1885 |     40.009 |   1.2061 |     40.257 |     1.2
   30 |   1.1818 |     39.711 |   1.2069 |     40.349 |     1.3
   31 |   1.1700 |     39.754 |   1.2019 |     39.706 |     1.3
   32 |   1.1661 |     39.640 |   1.1983 |     39.767 |     1.4
   33 |   1.1600 |     38.947 |   1.1868 |     39.675 |     1.4
   34 |   1.1667 |     39.516 |   1.1898 |     39.675 |     1.4
   35 |   1.1484 |     39.098 |   1.1865 |     38.879 |     1.5
   36 |   1.1372 |     38.719 |   1.1720 |     38.542 |     1.5
   37 |   1.1327 |     38.345 |   1.1690 |     38.603 |     1.6
   38 |   1.1289 |     38.275 |   1.1563 |     38.143 |     1.6
   39 |   1.1244 |     38.183 |   1.1656 |     37.960 |     1.7
   40 |   1.1159 |     37.592 |   1.1517 |     37.960 |     1.7
   41 |   1.1052 |     37.267 |   1.1562 |     38.205 |     1.7
   42 |   1.0962 |     36.974 |   1.1498 |     38.082 |     1.8
   43 |   1.0907 |     36.584 |   1.1349 |     37.469 |     1.8
   44 |   1.0892 |     36.785 |   1.1355 |     37.806 |     1.9
   45 |   1.0831 |     36.552 |   1.1301 |     37.347 |     1.9
   46 |   1.0752 |     36.021 |   1.1311 |     37.286 |     2.0
   47 |   1.0727 |     36.129 |   1.1152 |     36.979 |     2.0
   48 |   1.0578 |     35.365 |   1.1131 |     36.612 |     2.0
   49 |   1.0462 |     34.980 |   1.1071 |     36.887 |     2.1
   50 |   1.0394 |     35.219 |   1.1003 |     36.550 |     2.1
   51 |   1.0390 |     34.872 |   1.0948 |     35.938 |     2.2
   52 |   1.0300 |     34.482 |   1.1017 |     36.366 |     2.2
   53 |   1.0317 |     34.894 |   1.1013 |     36.520 |     2.3
   54 |   1.0301 |     34.395 |   1.1040 |     36.213 |     2.3
   55 |   1.0162 |     34.265 |   1.0923 |     35.325 |     2.3
   56 |   1.0108 |     34.157 |   1.0973 |     36.244 |     2.4
   57 |   1.0065 |     33.664 |   1.0831 |     35.478 |     2.4
   58 |   0.9993 |     33.442 |   1.0774 |     35.417 |     2.5
   59 |   0.9882 |     33.192 |   1.0799 |     35.263 |     2.5
   60 |   0.9910 |     33.171 |   1.0876 |     35.999 |     2.6
   61 |   0.9861 |     33.106 |   1.0723 |     35.325 |     2.6
   62 |   0.9792 |     32.981 |   1.0735 |     35.049 |     2.6
   63 |   0.9771 |     32.835 |   1.0680 |     34.528 |     2.7
   64 |   0.9751 |     32.699 |   1.0693 |     35.325 |     2.7
   65 |   0.9702 |     32.564 |   1.0600 |     34.314 |     2.8
   66 |   0.9659 |     32.689 |   1.0496 |     35.233 |     2.8
   67 |   0.9633 |     32.309 |   1.0600 |     35.049 |     2.8
   68 |   0.9605 |     32.331 |   1.0527 |     35.141 |     2.9
   69 |   0.9558 |     32.217 |   1.0585 |     35.294 |     2.9
   70 |   0.9498 |     31.740 |   1.0487 |     34.773 |     3.0
   71 |   0.9410 |     31.957 |   1.0491 |     33.885 |     3.0
   72 |   0.9452 |     31.751 |   1.0671 |     35.141 |     3.1
   73 |   0.9553 |     32.223 |   1.0584 |     35.080 |     3.1
   74 |   0.9466 |     31.957 |   1.0493 |     34.743 |     3.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 2,400,482

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3221 |     83.301 |   3.1378 |     75.092 |     0.1
    2 |   2.9936 |     68.991 |   2.8688 |     67.096 |     0.2
    3 |   2.7716 |     67.181 |   2.6957 |     66.912 |     0.3
    4 |   2.6272 |     66.900 |   2.5758 |     65.104 |     0.3
    5 |   2.5190 |     63.242 |   2.4796 |     59.651 |     0.4
    6 |   2.4335 |     60.121 |   2.4013 |     59.069 |     0.5
    7 |   2.3610 |     59.076 |   2.3367 |     58.824 |     0.6
    8 |   2.2955 |     58.588 |   2.2674 |     58.885 |     0.7
    9 |   2.2291 |     58.176 |   2.2015 |     58.762 |     0.8
   10 |   2.1598 |     57.493 |   2.1279 |     57.077 |     0.9
   11 |   2.0951 |     54.275 |   2.0639 |     52.145 |     0.9
   12 |   2.0380 |     52.167 |   2.0058 |     51.225 |     1.0
   13 |   1.9825 |     51.019 |   1.9498 |     49.540 |     1.1
   14 |   1.9324 |     50.569 |   1.9035 |     49.142 |     1.2
   15 |   1.8874 |     50.190 |   1.8625 |     49.050 |     1.3
   16 |   1.8485 |     49.848 |   1.8247 |     49.357 |     1.4
   17 |   1.8136 |     49.599 |   1.7872 |     48.468 |     1.4
   18 |   1.7805 |     49.296 |   1.7551 |     48.836 |     1.5
   19 |   1.7448 |     49.101 |   1.7218 |     48.407 |     1.6
   20 |   1.7131 |     48.868 |   1.6909 |     48.438 |     1.7
   21 |   1.6848 |     48.759 |   1.6594 |     48.192 |     1.8
   22 |   1.6505 |     48.055 |   1.6299 |     47.181 |     1.9
   23 |   1.6212 |     47.063 |   1.6098 |     46.569 |     2.0
   24 |   1.5988 |     46.489 |   1.5803 |     45.374 |     2.0
   25 |   1.5752 |     46.267 |   1.5589 |     45.251 |     2.1
   26 |   1.5525 |     46.126 |   1.5406 |     45.282 |     2.2
   27 |   1.5328 |     45.969 |   1.5219 |     45.251 |     2.3
   28 |   1.5140 |     45.958 |   1.5060 |     45.221 |     2.4
   29 |   1.4987 |     45.774 |   1.4899 |     45.159 |     2.5
   30 |   1.4762 |     45.600 |   1.4737 |     45.067 |     2.5
   31 |   1.4616 |     45.243 |   1.4605 |     44.914 |     2.6
   32 |   1.4452 |     45.091 |   1.4495 |     44.577 |     2.7
   33 |   1.4303 |     44.777 |   1.4364 |     44.332 |     2.8
   34 |   1.4171 |     44.322 |   1.4216 |     44.301 |     2.9
   35 |   1.4020 |     43.975 |   1.4119 |     43.934 |     3.0
   36 |   1.3940 |     43.904 |   1.4015 |     43.873 |     3.1
   37 |   1.3763 |     43.628 |   1.3923 |     43.536 |     3.1
   38 |   1.3671 |     43.373 |   1.3852 |     43.719 |     3.2
   39 |   1.3579 |     43.205 |   1.3740 |     43.444 |     3.3
   40 |   1.3456 |     42.907 |   1.3647 |     43.352 |     3.4
   41 |   1.3310 |     42.685 |   1.3549 |     43.137 |     3.5
   42 |   1.3210 |     42.371 |   1.3513 |     42.984 |     3.6
   43 |   1.3101 |     41.878 |   1.3406 |     42.770 |     3.6
   44 |   1.3026 |     41.542 |   1.3322 |     42.188 |     3.7
   45 |   1.2886 |     41.103 |   1.3268 |     42.157 |     3.8
   46 |   1.2814 |     40.794 |   1.3174 |     41.544 |     3.9
   47 |   1.2731 |     40.567 |   1.3123 |     41.728 |     4.0
   48 |   1.2617 |     40.020 |   1.3050 |     40.993 |     4.1
   49 |   1.2529 |     39.624 |   1.2947 |     40.778 |     4.2
   50 |   1.2424 |     39.321 |   1.2902 |     40.901 |     4.2
   51 |   1.2353 |     39.082 |   1.2805 |     40.411 |     4.3
   52 |   1.2247 |     38.399 |   1.2779 |     40.227 |     4.4
   53 |   1.2203 |     38.264 |   1.2689 |     39.553 |     4.5
   54 |   1.2082 |     37.966 |   1.2680 |     39.920 |     4.6
   55 |   1.2005 |     37.554 |   1.2648 |     39.062 |     4.7
   56 |   1.1935 |     37.191 |   1.2513 |     38.909 |     4.7
   57 |   1.1838 |     36.736 |   1.2475 |     39.062 |     4.8
   58 |   1.1749 |     36.584 |   1.2396 |     38.695 |     4.9
   59 |   1.1683 |     36.308 |   1.2359 |     37.929 |     5.0
   60 |   1.1603 |     35.712 |   1.2297 |     37.898 |     5.1
   61 |   1.1540 |     35.587 |   1.2265 |     37.990 |     5.2
   62 |   1.1467 |     35.528 |   1.2226 |     37.561 |     5.3
   63 |   1.1364 |     35.132 |   1.2137 |     37.347 |     5.3
   64 |   1.1294 |     34.937 |   1.2147 |     37.500 |     5.4
   65 |   1.1221 |     35.073 |   1.2073 |     37.500 |     5.5
   66 |   1.1148 |     34.498 |   1.2006 |     37.469 |     5.6
   67 |   1.1090 |     34.395 |   1.1959 |     37.286 |     5.7
   68 |   1.1045 |     34.173 |   1.1958 |     37.010 |     5.8
   69 |   1.0959 |     34.108 |   1.1906 |     37.347 |     5.8
   70 |   1.0896 |     33.648 |   1.1842 |     37.040 |     5.9
   71 |   1.0821 |     33.425 |   1.1865 |     37.377 |     6.0
   72 |   1.0746 |     33.122 |   1.1778 |     37.040 |     6.1
   73 |   1.0675 |     33.252 |   1.1740 |     37.132 |     6.2
   74 |   1.0637 |     32.970 |   1.1691 |     36.826 |     6.3
   75 |   1.0562 |     32.699 |   1.1665 |     36.642 |     6.3
   76 |   1.0492 |     32.412 |   1.1627 |     36.366 |     6.4
   77 |   1.0400 |     32.239 |   1.1618 |     36.612 |     6.5
   78 |   1.0366 |     32.093 |   1.1593 |     36.336 |     6.6
   79 |   1.0293 |     32.028 |   1.1542 |     36.244 |     6.7
   80 |   1.0251 |     31.643 |   1.1494 |     35.999 |     6.8
   81 |   1.0213 |     31.605 |   1.1464 |     36.183 |     6.9
   82 |   1.0118 |     31.193 |   1.1372 |     35.968 |     6.9
   83 |   1.0053 |     31.144 |   1.1403 |     36.121 |     7.0
   84 |   1.0024 |     31.025 |   1.1316 |     35.815 |     7.1
   85 |   0.9931 |     30.673 |   1.1327 |     35.815 |     7.2
   86 |   0.9907 |     30.754 |   1.1289 |     35.723 |     7.3
   87 |   0.9832 |     30.500 |   1.1296 |     35.539 |     7.4
   88 |   0.9745 |     30.375 |   1.1216 |     35.325 |     7.4
   89 |   0.9696 |     29.969 |   1.1238 |     35.539 |     7.5
   90 |   0.9637 |     29.486 |   1.1193 |     35.018 |     7.6
   91 |   0.9655 |     29.952 |   1.1134 |     35.294 |     7.7
   92 |   0.9564 |     29.280 |   1.1103 |     35.080 |     7.8
   93 |   0.9487 |     29.221 |   1.1180 |     34.498 |     7.9
   94 |   0.9509 |     29.237 |   1.1115 |     35.202 |     7.9
   95 |   0.9382 |     28.869 |   1.1059 |     34.191 |     8.0
   96 |   0.9274 |     28.581 |   1.1071 |     34.589 |     8.1
   97 |   0.9252 |     28.679 |   1.0988 |     34.436 |     8.2
   98 |   0.9223 |     28.305 |   1.1003 |     34.436 |     8.3
   99 |   0.9151 |     28.294 |   1.0988 |     33.854 |     8.4
  100 |   0.9087 |     27.780 |   1.0958 |     34.161 |     8.5
  101 |   0.9062 |     27.698 |   1.0962 |     34.344 |     8.5
  102 |   0.9026 |     27.476 |   1.0928 |     33.885 |     8.6
  103 |   0.8983 |     27.292 |   1.0981 |     33.578 |     8.7
  104 |   0.8897 |     27.211 |   1.0892 |     33.793 |     8.8
  105 |   0.8851 |     26.907 |   1.0931 |     33.701 |     8.9
  106 |   0.8827 |     27.270 |   1.0880 |     33.824 |     9.0
  107 |   0.8762 |     26.777 |   1.0946 |     34.099 |     9.0
  108 |   0.8696 |     26.490 |   1.0893 |     33.762 |     9.1
  109 |   0.8688 |     26.523 |   1.0797 |     33.640 |     9.2
  110 |   0.8653 |     26.279 |   1.0834 |     33.027 |     9.3
  111 |   0.8582 |     26.013 |   1.0812 |     32.996 |     9.4
  112 |   0.8553 |     26.002 |   1.0758 |     33.150 |     9.5
  113 |   0.8461 |     25.406 |   1.0774 |     32.812 |     9.6
  114 |   0.8464 |     25.634 |   1.0767 |     32.812 |     9.6
  115 |   0.8404 |     25.417 |   1.0795 |     33.150 |     9.7
  116 |   0.8338 |     25.173 |   1.0766 |     32.904 |     9.8
  117 |   0.8293 |     25.038 |   1.0712 |     32.904 |     9.9
  118 |   0.8295 |     25.087 |   1.0676 |     32.200 |    10.0
  119 |   0.8213 |     24.794 |   1.0716 |     32.414 |    10.1
  120 |   0.8202 |     24.843 |   1.0748 |     32.935 |    10.1
  121 |   0.8170 |     24.350 |   1.0682 |     32.077 |    10.2
  122 |   0.8096 |     24.615 |   1.0703 |     32.322 |    10.3
  123 |   0.8058 |     24.241 |   1.0660 |     32.384 |    10.4
  124 |   0.8049 |     24.214 |   1.0623 |     31.495 |    10.5
  125 |   0.7965 |     23.965 |   1.0684 |     32.108 |    10.6
  126 |   0.7923 |     23.759 |   1.0654 |     32.047 |    10.6
  127 |   0.7856 |     23.624 |   1.0550 |     31.127 |    10.7
  128 |   0.7871 |     23.710 |   1.0639 |     31.342 |    10.8
  129 |   0.7833 |     23.597 |   1.0594 |     31.373 |    10.9
  130 |   0.7799 |     23.505 |   1.0640 |     31.464 |    11.0
  131 |   0.7747 |     23.293 |   1.0567 |     31.587 |    11.1
  132 |   0.7720 |     23.087 |   1.0510 |     31.158 |    11.2
  133 |   0.7691 |     22.974 |   1.0590 |     31.618 |    11.2
  134 |   0.7653 |     22.638 |   1.0591 |     31.005 |    11.3
  135 |   0.7611 |     22.914 |   1.0560 |     31.281 |    11.4
  136 |   0.7565 |     22.399 |   1.0641 |     31.710 |    11.5
  137 |   0.7563 |     22.800 |   1.0503 |     31.005 |    11.6
  138 |   0.7569 |     22.659 |   1.0504 |     30.760 |    11.7
  139 |   0.7542 |     22.746 |   1.0576 |     31.281 |    11.7
  140 |   0.7484 |     22.302 |   1.0617 |     31.219 |    11.8
  141 |   0.7470 |     22.421 |   1.0468 |     31.219 |    11.9
  142 |   0.7413 |     22.269 |   1.0561 |     31.066 |    12.0
  143 |   0.7349 |     22.123 |   1.0431 |     31.097 |    12.1
  144 |   0.7315 |     21.987 |   1.0518 |     31.342 |    12.2
  145 |   0.7326 |     22.063 |   1.0429 |     31.036 |    12.3
  146 |   0.7217 |     21.928 |   1.0536 |     30.821 |    12.3
  147 |   0.7243 |     21.841 |   1.0521 |     30.944 |    12.4
  148 |   0.7226 |     21.803 |   1.0460 |     30.331 |    12.5
  149 |   0.7231 |     21.651 |   1.0443 |     30.760 |    12.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 3,064,610

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2949 |     62.982 |   1.6603 |     45.527 |     0.1
    2 |   1.5027 |     46.218 |   1.4286 |     45.987 |     0.2
    3 |   1.4226 |     46.261 |   1.4074 |     45.466 |     0.3
    4 |   1.4045 |     46.223 |   1.4000 |     45.987 |     0.3
    5 |   1.3985 |     46.343 |   1.3886 |     45.987 |     0.4
    6 |   1.3834 |     45.963 |   1.3860 |     45.129 |     0.5
    7 |   1.3676 |     45.183 |   1.3492 |     44.301 |     0.6
    8 |   1.3474 |     44.777 |   1.3332 |     43.995 |     0.7
    9 |   1.3400 |     44.587 |   1.3325 |     44.056 |     0.8
   10 |   1.3317 |     44.370 |   1.3199 |     43.750 |     0.9
   11 |   1.3207 |     44.029 |   1.3197 |     43.719 |     0.9
   12 |   1.3091 |     43.709 |   1.3059 |     43.045 |     1.0
   13 |   1.2994 |     43.547 |   1.2893 |     42.126 |     1.1
   14 |   1.2892 |     43.422 |   1.2872 |     42.034 |     1.2
   15 |   1.2802 |     42.902 |   1.2867 |     42.647 |     1.3
   16 |   1.2769 |     42.886 |   1.2931 |     42.862 |     1.4
   17 |   1.2720 |     42.523 |   1.2519 |     41.023 |     1.5
   18 |   1.2590 |     42.084 |   1.2657 |     40.686 |     1.5
   19 |   1.2495 |     41.732 |   1.2527 |     40.901 |     1.6
   20 |   1.2428 |     41.401 |   1.2507 |     40.748 |     1.7
   21 |   1.2342 |     41.190 |   1.2408 |     41.115 |     1.8
   22 |   1.2240 |     41.098 |   1.2161 |     39.920 |     1.9
   23 |   1.2257 |     41.271 |   1.2289 |     40.012 |     2.0
   24 |   1.2166 |     40.984 |   1.2337 |     40.411 |     2.1
   25 |   1.2141 |     40.989 |   1.2288 |     40.411 |     2.1
   26 |   1.2004 |     40.708 |   1.2035 |     39.400 |     2.2
   27 |   1.1935 |     40.578 |   1.2167 |     39.798 |     2.3
   28 |   1.1830 |     40.307 |   1.2032 |     40.135 |     2.4
   29 |   1.1774 |     39.954 |   1.1905 |     39.982 |     2.5
   30 |   1.1588 |     39.407 |   1.1794 |     38.940 |     2.6
   31 |   1.1542 |     39.321 |   1.1620 |     38.480 |     2.7
   32 |   1.1400 |     38.687 |   1.1574 |     38.235 |     2.8
   33 |   1.1369 |     38.221 |   1.2072 |     39.920 |     2.8
   34 |   1.1228 |     38.042 |   1.1687 |     37.929 |     2.9
   35 |   1.1164 |     37.814 |   1.1602 |     38.450 |     3.0
   36 |   1.1114 |     37.191 |   1.1549 |     38.205 |     3.1
   37 |   1.1218 |     37.971 |   1.1476 |     37.898 |     3.2
   38 |   1.1004 |     37.381 |   1.1467 |     38.174 |     3.3
   39 |   1.1095 |     37.256 |   1.1336 |     38.051 |     3.4
   40 |   1.0993 |     36.958 |   1.1428 |     36.949 |     3.4
   41 |   1.0895 |     36.828 |   1.1398 |     37.776 |     3.5
   42 |   1.0757 |     36.200 |   1.1267 |     37.898 |     3.6
   43 |   1.0945 |     37.050 |   1.1332 |     38.143 |     3.7
   44 |   1.0707 |     36.205 |   1.1424 |     37.868 |     3.8
   45 |   1.0591 |     35.679 |   1.1299 |     37.868 |     3.9
   46 |   1.0585 |     35.755 |   1.1329 |     37.929 |     4.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,322,050

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2615 |     82.363 |   2.7384 |     81.893 |     0.0
    2 |   2.4815 |     66.683 |   2.2644 |     58.824 |     0.1
    3 |   2.1417 |     57.336 |   2.0286 |     58.824 |     0.1
    4 |   1.9606 |     54.622 |   1.8681 |     48.376 |     0.1
    5 |   1.8246 |     49.545 |   1.7476 |     48.346 |     0.2
    6 |   1.7109 |     48.916 |   1.6466 |     48.346 |     0.2
    7 |   1.6234 |     48.510 |   1.5726 |     45.466 |     0.2
    8 |   1.5610 |     46.402 |   1.5195 |     45.466 |     0.3
    9 |   1.5129 |     46.202 |   1.4841 |     45.466 |     0.3
   10 |   1.4815 |     46.207 |   1.4613 |     45.466 |     0.4
   11 |   1.4625 |     46.196 |   1.4446 |     45.466 |     0.4
   12 |   1.4494 |     46.234 |   1.4337 |     45.466 |     0.4
   13 |   1.4379 |     46.256 |   1.4247 |     45.466 |     0.5
   14 |   1.4292 |     46.099 |   1.4178 |     45.466 |     0.5
   15 |   1.4214 |     46.310 |   1.4098 |     45.466 |     0.5
   16 |   1.4137 |     46.234 |   1.4013 |     45.466 |     0.6
   17 |   1.4057 |     46.251 |   1.3956 |     45.466 |     0.6
   18 |   1.3953 |     46.245 |   1.3905 |     45.466 |     0.6
   19 |   1.3873 |     46.316 |   1.3797 |     45.435 |     0.7
   20 |   1.3764 |     46.001 |   1.3726 |     45.282 |     0.7
   21 |   1.3688 |     45.665 |   1.3658 |     44.210 |     0.7
   22 |   1.3599 |     45.367 |   1.3597 |     44.271 |     0.8
   23 |   1.3510 |     45.335 |   1.3543 |     44.577 |     0.8
   24 |   1.3457 |     45.189 |   1.3517 |     44.455 |     0.8
   25 |   1.3405 |     45.124 |   1.3457 |     44.210 |     0.9
   26 |   1.3326 |     45.004 |   1.3402 |     44.148 |     0.9
   27 |   1.3279 |     44.972 |   1.3375 |     44.608 |     0.9
   28 |   1.3217 |     45.151 |   1.3296 |     43.995 |     1.0
   29 |   1.3133 |     44.966 |   1.3228 |     44.240 |     1.0
   30 |   1.3064 |     44.853 |   1.3188 |     43.781 |     1.0
   31 |   1.2983 |     44.674 |   1.3137 |     43.566 |     1.1
   32 |   1.2917 |     44.224 |   1.3086 |     43.474 |     1.1
   33 |   1.2836 |     43.731 |   1.3039 |     43.474 |     1.2
   34 |   1.2775 |     43.270 |   1.3000 |     43.199 |     1.2
   35 |   1.2693 |     42.913 |   1.2974 |     42.984 |     1.2
   36 |   1.2635 |     42.669 |   1.2910 |     42.831 |     1.3
   37 |   1.2569 |     42.214 |   1.2886 |     42.371 |     1.3
   38 |   1.2490 |     41.900 |   1.2790 |     42.463 |     1.3
   39 |   1.2430 |     41.775 |   1.2809 |     42.647 |     1.4
   40 |   1.2389 |     41.358 |   1.2690 |     42.096 |     1.4
   41 |   1.2338 |     41.363 |   1.2672 |     41.912 |     1.4
   42 |   1.2271 |     41.374 |   1.2639 |     42.249 |     1.5
   43 |   1.2193 |     41.000 |   1.2604 |     41.759 |     1.5
   44 |   1.2132 |     41.081 |   1.2502 |     41.759 |     1.5
   45 |   1.2104 |     40.545 |   1.2535 |     41.728 |     1.6
   46 |   1.2009 |     40.426 |   1.2480 |     41.759 |     1.6
   47 |   1.1961 |     40.128 |   1.2399 |     41.238 |     1.6
   48 |   1.1873 |     40.057 |   1.2413 |     41.238 |     1.7
   49 |   1.1801 |     39.889 |   1.2276 |     41.023 |     1.7
   50 |   1.1743 |     39.331 |   1.2221 |     40.533 |     1.7
   51 |   1.1675 |     39.369 |   1.2226 |     40.564 |     1.8
   52 |   1.1629 |     38.893 |   1.2224 |     40.349 |     1.8
   53 |   1.1530 |     38.893 |   1.2135 |     39.828 |     1.8
   54 |   1.1484 |     38.139 |   1.2045 |     39.124 |     1.9
   55 |   1.1445 |     38.253 |   1.2025 |     39.430 |     1.9
   56 |   1.1334 |     37.765 |   1.1964 |     39.185 |     1.9
   57 |   1.1255 |     37.435 |   1.1976 |     39.216 |     2.0
   58 |   1.1232 |     37.305 |   1.1937 |     39.154 |     2.0
   59 |   1.1104 |     37.002 |   1.1845 |     39.308 |     2.0
   60 |   1.1038 |     36.666 |   1.1877 |     39.216 |     2.1
   61 |   1.1015 |     36.655 |   1.1851 |     39.185 |     2.1
   62 |   1.0940 |     36.400 |   1.1797 |     38.480 |     2.2
   63 |   1.0816 |     36.037 |   1.1783 |     38.909 |     2.2
   64 |   1.0762 |     36.075 |   1.1675 |     38.572 |     2.2
   65 |   1.0687 |     35.826 |   1.1604 |     38.725 |     2.3
   66 |   1.0616 |     35.452 |   1.1590 |     38.542 |     2.3
   67 |   1.0567 |     35.371 |   1.1557 |     38.143 |     2.3
   68 |   1.0503 |     35.246 |   1.1454 |     37.776 |     2.4
   69 |   1.0395 |     34.953 |   1.1561 |     38.205 |     2.4
   70 |   1.0340 |     34.807 |   1.1477 |     38.388 |     2.4
   71 |   1.0275 |     34.542 |   1.1392 |     37.776 |     2.5
   72 |   1.0212 |     34.292 |   1.1360 |     37.623 |     2.5
   73 |   1.0143 |     34.282 |   1.1324 |     37.408 |     2.5
   74 |   1.0053 |     33.886 |   1.1349 |     37.439 |     2.6
   75 |   0.9984 |     33.702 |   1.1367 |     37.653 |     2.6
   76 |   0.9958 |     33.615 |   1.1243 |     36.887 |     2.6
   77 |   0.9866 |     33.469 |   1.1238 |     37.377 |     2.7
   78 |   0.9855 |     33.225 |   1.1247 |     37.255 |     2.7
   79 |   0.9800 |     33.295 |   1.1292 |     37.040 |     2.7
   80 |   0.9744 |     32.840 |   1.1301 |     37.408 |     2.8
   81 |   0.9635 |     32.862 |   1.1184 |     37.102 |     2.8
   82 |   0.9631 |     32.315 |   1.1142 |     37.040 |     2.8
   83 |   0.9509 |     32.022 |   1.1112 |     36.642 |     2.9
   84 |   0.9540 |     32.223 |   1.1208 |     37.347 |     2.9
   85 |   0.9450 |     31.860 |   1.1130 |     37.040 |     2.9
   86 |   0.9412 |     31.784 |   1.1161 |     36.366 |     3.0
   87 |   0.9378 |     31.740 |   1.1099 |     36.612 |     3.0
   88 |   0.9366 |     31.307 |   1.1034 |     36.489 |     3.1
   89 |   0.9214 |     31.329 |   1.1150 |     36.979 |     3.1
   90 |   0.9221 |     31.296 |   1.1159 |     36.642 |     3.1
   91 |   0.9148 |     30.917 |   1.1095 |     36.244 |     3.2
   92 |   0.9103 |     30.440 |   1.1102 |     36.183 |     3.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 440,642

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3683 |     82.201 |   3.0890 |     83.333 |     0.0
    2 |   2.8039 |     79.936 |   2.6467 |     67.341 |     0.0
    3 |   2.5067 |     64.396 |   2.3968 |     58.854 |     0.1
    4 |   2.3122 |     59.319 |   2.2436 |     58.824 |     0.1
    5 |   2.1890 |     58.230 |   2.1293 |     58.824 |     0.1
    6 |   2.0923 |     57.748 |   2.0398 |     58.824 |     0.1
    7 |   2.0139 |     56.746 |   1.9668 |     55.147 |     0.1
    8 |   1.9410 |     53.749 |   1.8975 |     49.479 |     0.1
    9 |   1.8790 |     51.040 |   1.8350 |     48.560 |     0.2
   10 |   1.8186 |     50.211 |   1.7755 |     48.346 |     0.2
   11 |   1.7637 |     49.539 |   1.7210 |     48.346 |     0.2
   12 |   1.7092 |     49.160 |   1.6724 |     48.346 |     0.2
   13 |   1.6638 |     47.453 |   1.6304 |     45.466 |     0.2
   14 |   1.6244 |     46.408 |   1.5941 |     45.466 |     0.2
   15 |   1.5909 |     46.267 |   1.5638 |     45.466 |     0.3
   16 |   1.5598 |     46.223 |   1.5389 |     45.466 |     0.3
   17 |   1.5422 |     46.213 |   1.5180 |     45.466 |     0.3
   18 |   1.5193 |     46.207 |   1.5007 |     45.466 |     0.3
   19 |   1.5022 |     46.207 |   1.4850 |     45.466 |     0.3
   20 |   1.4882 |     46.213 |   1.4729 |     45.466 |     0.4
   21 |   1.4776 |     46.207 |   1.4623 |     45.466 |     0.4
   22 |   1.4642 |     46.213 |   1.4529 |     45.466 |     0.4
   23 |   1.4560 |     46.213 |   1.4444 |     45.466 |     0.4
   24 |   1.4455 |     46.202 |   1.4368 |     45.466 |     0.4
   25 |   1.4398 |     46.272 |   1.4289 |     45.466 |     0.4
   26 |   1.4302 |     46.278 |   1.4225 |     45.466 |     0.5
   27 |   1.4266 |     46.261 |   1.4148 |     45.466 |     0.5
   28 |   1.4157 |     46.251 |   1.4071 |     45.466 |     0.5
   29 |   1.4075 |     46.251 |   1.4000 |     45.466 |     0.5
   30 |   1.4031 |     46.267 |   1.3936 |     45.466 |     0.5
   31 |   1.3935 |     46.191 |   1.3876 |     45.466 |     0.5
   32 |   1.3862 |     46.223 |   1.3834 |     45.466 |     0.6
   33 |   1.3797 |     46.223 |   1.3781 |     45.466 |     0.6
   34 |   1.3743 |     46.050 |   1.3739 |     45.312 |     0.6
   35 |   1.3710 |     45.736 |   1.3687 |     45.435 |     0.6
   36 |   1.3651 |     45.584 |   1.3653 |     44.485 |     0.6
   37 |   1.3587 |     45.357 |   1.3587 |     44.914 |     0.6
   38 |   1.3541 |     45.308 |   1.3557 |     44.332 |     0.7
   39 |   1.3531 |     45.102 |   1.3500 |     44.332 |     0.7
   40 |   1.3464 |     45.270 |   1.3508 |     44.332 |     0.7
   41 |   1.3435 |     45.183 |   1.3432 |     44.638 |     0.7
   42 |   1.3374 |     45.167 |   1.3391 |     44.118 |     0.7
   43 |   1.3306 |     45.156 |   1.3366 |     44.179 |     0.7
   44 |   1.3256 |     44.983 |   1.3310 |     44.179 |     0.8
   45 |   1.3225 |     44.885 |   1.3281 |     44.118 |     0.8
   46 |   1.3196 |     44.842 |   1.3244 |     44.179 |     0.8
   47 |   1.3168 |     44.880 |   1.3212 |     44.271 |     0.8
   48 |   1.3108 |     44.755 |   1.3209 |     43.842 |     0.8
   49 |   1.3045 |     44.511 |   1.3145 |     43.811 |     0.9
   50 |   1.3009 |     44.446 |   1.3098 |     43.719 |     0.9
   51 |   1.2973 |     44.452 |   1.3070 |     43.995 |     0.9
   52 |   1.2933 |     44.430 |   1.3029 |     43.505 |     0.9
   53 |   1.2870 |     44.278 |   1.3008 |     43.536 |     0.9
   54 |   1.2816 |     44.284 |   1.2957 |     43.352 |     0.9
   55 |   1.2783 |     44.154 |   1.2923 |     43.413 |     1.0
   56 |   1.2739 |     44.002 |   1.2878 |     43.107 |     1.0
   57 |   1.2690 |     43.796 |   1.2840 |     43.505 |     1.0
   58 |   1.2627 |     43.682 |   1.2793 |     43.076 |     1.0
   59 |   1.2561 |     43.395 |   1.2784 |     43.290 |     1.0
   60 |   1.2512 |     43.081 |   1.2733 |     42.953 |     1.0
   61 |   1.2456 |     43.140 |   1.2711 |     43.076 |     1.1
   62 |   1.2430 |     42.669 |   1.2656 |     42.984 |     1.1
   63 |   1.2399 |     42.517 |   1.2629 |     42.892 |     1.1
   64 |   1.2327 |     42.143 |   1.2601 |     42.616 |     1.1
   65 |   1.2283 |     41.943 |   1.2580 |     42.708 |     1.1
   66 |   1.2240 |     41.835 |   1.2558 |     42.279 |     1.1
   67 |   1.2188 |     41.889 |   1.2493 |     41.850 |     1.2
   68 |   1.2159 |     41.417 |   1.2464 |     42.157 |     1.2
   69 |   1.2101 |     41.455 |   1.2429 |     41.973 |     1.2
   70 |   1.2094 |     40.897 |   1.2424 |     42.065 |     1.2
   71 |   1.2032 |     41.022 |   1.2373 |     41.513 |     1.2
   72 |   1.1976 |     40.865 |   1.2367 |     41.850 |     1.2
   73 |   1.1948 |     40.572 |   1.2314 |     41.942 |     1.3
   74 |   1.1918 |     40.464 |   1.2312 |     41.667 |     1.3
   75 |   1.1842 |     40.090 |   1.2261 |     42.004 |     1.3
   76 |   1.1813 |     40.312 |   1.2266 |     41.759 |     1.3
   77 |   1.1764 |     39.927 |   1.2222 |     41.330 |     1.3
   78 |   1.1725 |     39.819 |   1.2194 |     41.146 |     1.3
   79 |   1.1652 |     39.348 |   1.2192 |     41.238 |     1.4
   80 |   1.1660 |     39.597 |   1.2145 |     41.023 |     1.4
   81 |   1.1612 |     39.521 |   1.2136 |     41.023 |     1.4
   82 |   1.1577 |     39.185 |   1.2113 |     41.085 |     1.4
   83 |   1.1533 |     39.147 |   1.2049 |     40.686 |     1.4
   84 |   1.1514 |     39.125 |   1.2030 |     40.931 |     1.5
   85 |   1.1451 |     38.974 |   1.2132 |     41.085 |     1.5
   86 |   1.1418 |     38.865 |   1.1989 |     40.686 |     1.5
   87 |   1.1379 |     38.703 |   1.2024 |     40.656 |     1.5
   88 |   1.1349 |     38.318 |   1.1941 |     40.441 |     1.5
   89 |   1.1304 |     38.280 |   1.1923 |     40.380 |     1.5
   90 |   1.1246 |     38.199 |   1.1906 |     40.227 |     1.6
   91 |   1.1224 |     38.096 |   1.1910 |     40.135 |     1.6
   92 |   1.1186 |     37.760 |   1.1895 |     40.165 |     1.6
   93 |   1.1175 |     37.923 |   1.1833 |     39.737 |     1.6
   94 |   1.1165 |     37.874 |   1.1822 |     39.645 |     1.6
   95 |   1.1101 |     37.565 |   1.1854 |     39.767 |     1.6
   96 |   1.1076 |     37.278 |   1.1837 |     39.890 |     1.7
   97 |   1.1043 |     37.164 |   1.1769 |     39.737 |     1.7
   98 |   1.0976 |     36.985 |   1.1738 |     39.798 |     1.7
   99 |   1.0946 |     36.812 |   1.1751 |     40.074 |     1.7
  100 |   1.0923 |     36.850 |   1.1780 |     39.430 |     1.7
  101 |   1.0876 |     36.465 |   1.1722 |     39.982 |     1.7
  102 |   1.0846 |     36.579 |   1.1692 |     39.093 |     1.8
  103 |   1.0828 |     36.400 |   1.1699 |     39.400 |     1.8
  104 |   1.0808 |     36.454 |   1.1712 |     39.828 |     1.8
  105 |   1.0795 |     36.292 |   1.1610 |     39.522 |     1.8
  106 |   1.0734 |     36.286 |   1.1618 |     39.338 |     1.8
  107 |   1.0697 |     35.826 |   1.1635 |     39.185 |     1.8
  108 |   1.0650 |     35.799 |   1.1560 |     38.879 |     1.9
  109 |   1.0617 |     35.522 |   1.1572 |     39.093 |     1.9
  110 |   1.0542 |     35.425 |   1.1502 |     38.848 |     1.9
  111 |   1.0528 |     35.484 |   1.1534 |     38.848 |     1.9
  112 |   1.0499 |     35.132 |   1.1522 |     39.277 |     1.9
  113 |   1.0486 |     35.300 |   1.1539 |     38.756 |     1.9
  114 |   1.0438 |     34.997 |   1.1486 |     38.817 |     2.0
  115 |   1.0380 |     34.850 |   1.1484 |     38.879 |     2.0
  116 |   1.0385 |     34.807 |   1.1459 |     38.205 |     2.0
  117 |   1.0330 |     34.661 |   1.1446 |     38.450 |     2.0
  118 |   1.0308 |     34.471 |   1.1465 |     38.143 |     2.0
  119 |   1.0269 |     34.124 |   1.1386 |     37.990 |     2.1
  120 |   1.0251 |     34.260 |   1.1461 |     38.174 |     2.1
  121 |   1.0168 |     33.891 |   1.1360 |     37.623 |     2.1
  122 |   1.0184 |     33.669 |   1.1377 |     37.439 |     2.1
  123 |   1.0134 |     33.691 |   1.1380 |     37.960 |     2.1
  124 |   1.0133 |     33.490 |   1.1338 |     37.408 |     2.1
  125 |   1.0035 |     32.959 |   1.1309 |     37.500 |     2.2
  126 |   1.0023 |     33.279 |   1.1358 |     37.500 |     2.2
  127 |   0.9979 |     32.835 |   1.1258 |     36.826 |     2.2
  128 |   0.9954 |     32.743 |   1.1248 |     36.857 |     2.2
  129 |   0.9918 |     32.586 |   1.1230 |     36.979 |     2.2
  130 |   0.9864 |     32.418 |   1.1247 |     36.520 |     2.2
  131 |   0.9837 |     32.174 |   1.1189 |     36.336 |     2.3
  132 |   0.9797 |     32.195 |   1.1240 |     36.642 |     2.3
  133 |   0.9723 |     31.963 |   1.1194 |     36.183 |     2.3
  134 |   0.9708 |     31.925 |   1.1154 |     36.029 |     2.3
  135 |   0.9683 |     31.356 |   1.1166 |     35.907 |     2.3
  136 |   0.9679 |     31.730 |   1.1112 |     35.968 |     2.3
  137 |   0.9616 |     31.469 |   1.1130 |     35.938 |     2.4
  138 |   0.9598 |     31.513 |   1.1111 |     36.060 |     2.4
  139 |   0.9528 |     30.944 |   1.1098 |     35.509 |     2.4
  140 |   0.9493 |     30.949 |   1.1087 |     35.723 |     2.4
  141 |   0.9462 |     30.976 |   1.1038 |     35.784 |     2.4
  142 |   0.9424 |     30.814 |   1.1096 |     36.244 |     2.4
  143 |   0.9402 |     30.982 |   1.1109 |     35.846 |     2.5
  144 |   0.9358 |     30.765 |   1.0990 |     35.784 |     2.5
  145 |   0.9337 |     30.727 |   1.0979 |     35.447 |     2.5
  146 |   0.9354 |     30.462 |   1.0927 |     35.417 |     2.5
  147 |   0.9202 |     29.941 |   1.0923 |     35.447 |     2.5
  148 |   0.9191 |     30.256 |   1.0905 |     35.233 |     2.5
  149 |   0.9178 |     29.974 |   1.0910 |     35.325 |     2.6
  150 |   0.9191 |     30.055 |   1.0909 |     35.355 |     2.6
  151 |   0.9147 |     30.066 |   1.0868 |     35.049 |     2.6
  152 |   0.9059 |     29.703 |   1.0872 |     35.263 |     2.6
  153 |   0.9000 |     29.438 |   1.0873 |     34.896 |     2.6
  154 |   0.8973 |     29.627 |   1.0931 |     35.141 |     2.6
  155 |   0.9002 |     29.692 |   1.0836 |     35.018 |     2.7
  156 |   0.8896 |     29.308 |   1.0786 |     35.141 |     2.7
  157 |   0.8847 |     29.237 |   1.0744 |     35.172 |     2.7
  158 |   0.8863 |     29.080 |   1.0772 |     34.743 |     2.7
  159 |   0.8857 |     28.966 |   1.0815 |     34.773 |     2.7
  160 |   0.8810 |     29.004 |   1.0758 |     34.651 |     2.8
  161 |   0.8823 |     29.058 |   1.0794 |     34.988 |     2.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 876,738

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1194 |     56.594 |   1.5222 |     45.221 |     0.0
    2 |   1.4142 |     45.102 |   1.3392 |     43.505 |     0.0
    3 |   1.2953 |     42.425 |   1.2519 |     41.054 |     0.1
    4 |   1.2057 |     39.830 |   1.1781 |     38.174 |     0.1
    5 |   1.1307 |     37.495 |   1.1363 |     37.040 |     0.1
    6 |   1.0630 |     34.558 |   1.0776 |     34.620 |     0.1
    7 |   0.9929 |     32.217 |   1.0272 |     32.874 |     0.2
    8 |   0.9279 |     30.272 |   0.9707 |     31.832 |     0.2
    9 |   0.8601 |     27.720 |   0.9322 |     30.270 |     0.2
   10 |   0.8056 |     25.845 |   0.9091 |     28.952 |     0.2
   11 |   0.7565 |     24.258 |   0.9084 |     29.565 |     0.3
   12 |   0.7143 |     22.892 |   0.8828 |     29.197 |     0.3
   13 |   0.6651 |     21.283 |   0.8654 |     27.512 |     0.3
   14 |   0.6217 |     19.766 |   0.8523 |     27.849 |     0.3
   15 |   0.5835 |     18.780 |   0.8427 |     26.961 |     0.4
   16 |   0.5419 |     17.284 |   0.8403 |     25.980 |     0.4
   17 |   0.5067 |     16.217 |   0.8281 |     26.379 |     0.4
   18 |   0.4893 |     15.865 |   0.8338 |     26.011 |     0.4
   19 |   0.4644 |     14.873 |   0.8363 |     26.287 |     0.5
   20 |   0.4326 |     13.779 |   0.8349 |     25.705 |     0.5
   21 |   0.4144 |     13.481 |   0.8400 |     24.816 |     0.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 934,306

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3117 |     85.723 |   2.8163 |     83.333 |     0.0
    2 |   2.5650 |     70.655 |   2.3222 |     58.824 |     0.0
    3 |   2.1762 |     57.548 |   2.0583 |     58.824 |     0.1
    4 |   1.9884 |     55.554 |   1.9012 |     48.591 |     0.1
    5 |   1.8504 |     50.650 |   1.7769 |     48.346 |     0.1
    6 |   1.7405 |     48.916 |   1.6782 |     48.346 |     0.1
    7 |   1.6519 |     48.613 |   1.5957 |     48.346 |     0.1
    8 |   1.5756 |     46.792 |   1.5364 |     45.466 |     0.2
    9 |   1.5273 |     46.213 |   1.4968 |     45.466 |     0.2
   10 |   1.4931 |     46.213 |   1.4713 |     45.466 |     0.2
   11 |   1.4727 |     46.207 |   1.4542 |     45.466 |     0.2
   12 |   1.4545 |     46.142 |   1.4421 |     45.466 |     0.3
   13 |   1.4462 |     46.207 |   1.4330 |     45.466 |     0.3
   14 |   1.4369 |     46.164 |   1.4257 |     45.466 |     0.3
   15 |   1.4292 |     46.294 |   1.4204 |     45.466 |     0.3
   16 |   1.4260 |     46.245 |   1.4156 |     45.466 |     0.3
   17 |   1.4203 |     46.256 |   1.4122 |     45.466 |     0.4
   18 |   1.4168 |     46.326 |   1.4098 |     45.466 |     0.4
   19 |   1.4133 |     46.353 |   1.4047 |     45.466 |     0.4
   20 |   1.4087 |     46.261 |   1.3998 |     45.466 |     0.4
   21 |   1.3984 |     46.419 |   1.3876 |     45.466 |     0.4
   22 |   1.3829 |     46.234 |   1.3718 |     45.466 |     0.5
   23 |   1.3664 |     46.093 |   1.3584 |     44.730 |     0.5
   24 |   1.3540 |     45.481 |   1.3469 |     44.393 |     0.5
   25 |   1.3409 |     45.189 |   1.3403 |     44.087 |     0.5
   26 |   1.3296 |     45.362 |   1.3277 |     43.964 |     0.5
   27 |   1.3222 |     44.717 |   1.3221 |     44.210 |     0.6
   28 |   1.3090 |     44.533 |   1.3151 |     43.597 |     0.6
   29 |   1.3033 |     44.262 |   1.3198 |     43.536 |     0.6
   30 |   1.2973 |     44.127 |   1.3030 |     43.505 |     0.6
   31 |   1.2865 |     43.812 |   1.3082 |     43.382 |     0.6
   32 |   1.2801 |     43.384 |   1.2936 |     42.892 |     0.7
   33 |   1.2731 |     42.832 |   1.2940 |     42.739 |     0.7
   34 |   1.2718 |     43.162 |   1.2853 |     42.463 |     0.7
   35 |   1.2623 |     42.750 |   1.2793 |     42.923 |     0.7
   36 |   1.2548 |     42.447 |   1.2780 |     42.892 |     0.8
   37 |   1.2505 |     42.284 |   1.2742 |     42.616 |     0.8
   38 |   1.2431 |     42.160 |   1.2716 |     42.310 |     0.8
   39 |   1.2386 |     42.160 |   1.2637 |     42.218 |     0.8
   40 |   1.2330 |     42.003 |   1.2658 |     42.157 |     0.8
   41 |   1.2318 |     41.688 |   1.2570 |     42.188 |     0.9
   42 |   1.2228 |     41.678 |   1.2544 |     41.697 |     0.9
   43 |   1.2224 |     41.385 |   1.2517 |     41.667 |     0.9
   44 |   1.2126 |     41.309 |   1.2533 |     41.850 |     0.9
   45 |   1.2061 |     41.157 |   1.2491 |     41.636 |     0.9
   46 |   1.2038 |     41.109 |   1.2419 |     41.513 |     1.0
   47 |   1.2000 |     41.038 |   1.2391 |     41.391 |     1.0
   48 |   1.1931 |     40.751 |   1.2379 |     41.452 |     1.0
   49 |   1.1880 |     40.475 |   1.2364 |     41.452 |     1.0
   50 |   1.1862 |     40.496 |   1.2347 |     41.422 |     1.0
   51 |   1.1803 |     40.372 |   1.2317 |     41.544 |     1.1
   52 |   1.1738 |     40.052 |   1.2291 |     41.422 |     1.1
   53 |   1.1722 |     39.803 |   1.2265 |     40.809 |     1.1
   54 |   1.1680 |     39.711 |   1.2205 |     40.931 |     1.1
   55 |   1.1618 |     39.456 |   1.2243 |     40.901 |     1.1
   56 |   1.1590 |     39.440 |   1.2158 |     40.993 |     1.2
   57 |   1.1518 |     39.153 |   1.2172 |     41.146 |     1.2
   58 |   1.1505 |     39.044 |   1.2144 |     40.564 |     1.2
   59 |   1.1474 |     39.088 |   1.2134 |     41.054 |     1.2
   60 |   1.1424 |     38.806 |   1.2115 |     40.901 |     1.2
   61 |   1.1357 |     38.687 |   1.2076 |     41.054 |     1.3
   62 |   1.1309 |     38.611 |   1.2049 |     40.778 |     1.3
   63 |   1.1268 |     38.188 |   1.2054 |     40.135 |     1.3
   64 |   1.1272 |     38.210 |   1.2022 |     40.349 |     1.3
   65 |   1.1224 |     38.269 |   1.2045 |     40.502 |     1.4
   66 |   1.1192 |     37.982 |   1.2012 |     40.533 |     1.4
   67 |   1.1146 |     38.031 |   1.1984 |     40.380 |     1.4
   68 |   1.1112 |     37.966 |   1.1961 |     40.196 |     1.4
   69 |   1.1049 |     37.543 |   1.1922 |     39.982 |     1.4
   70 |   1.1000 |     37.679 |   1.1950 |     40.135 |     1.5
   71 |   1.0949 |     37.467 |   1.1916 |     39.767 |     1.5
   72 |   1.0940 |     37.142 |   1.2016 |     40.074 |     1.5
   73 |   1.0897 |     37.132 |   1.1890 |     39.767 |     1.5
   74 |   1.0847 |     37.061 |   1.1955 |     39.920 |     1.5
   75 |   1.0823 |     37.153 |   1.1873 |     40.043 |     1.6
   76 |   1.0756 |     36.709 |   1.1868 |     39.737 |     1.6
   77 |   1.0746 |     36.579 |   1.1895 |     40.441 |     1.6
   78 |   1.0690 |     36.617 |   1.1817 |     39.951 |     1.6
   79 |   1.0648 |     36.416 |   1.1764 |     39.798 |     1.6
   80 |   1.0559 |     36.070 |   1.1775 |     39.614 |     1.7
   81 |   1.0559 |     35.712 |   1.1787 |     39.246 |     1.7
   82 |   1.0519 |     35.772 |   1.1817 |     39.185 |     1.7
   83 |   1.0471 |     35.571 |   1.1762 |     39.216 |     1.7
   84 |   1.0449 |     35.528 |   1.1697 |     39.154 |     1.7
   85 |   1.0391 |     35.192 |   1.1754 |     39.185 |     1.8
   86 |   1.0332 |     35.132 |   1.1686 |     39.124 |     1.8
   87 |   1.0300 |     34.704 |   1.1636 |     38.787 |     1.8
   88 |   1.0271 |     34.650 |   1.1684 |     38.725 |     1.8
   89 |   1.0233 |     34.428 |   1.1673 |     38.787 |     1.8
   90 |   1.0164 |     34.498 |   1.1586 |     38.327 |     1.9
   91 |   1.0136 |     34.168 |   1.1671 |     38.664 |     1.9
   92 |   1.0063 |     33.919 |   1.1612 |     38.511 |     1.9
   93 |   1.0042 |     33.832 |   1.1609 |     38.756 |     1.9
   94 |   0.9972 |     33.826 |   1.1584 |     37.990 |     2.0
   95 |   0.9968 |     33.528 |   1.1557 |     38.051 |     2.0
   96 |   0.9914 |     33.138 |   1.1558 |     37.653 |     2.0
   97 |   0.9888 |     33.355 |   1.1492 |     37.408 |     2.0
   98 |   0.9871 |     33.111 |   1.1466 |     37.745 |     2.0
   99 |   0.9847 |     33.041 |   1.1578 |     37.960 |     2.1
  100 |   0.9740 |     32.667 |   1.1454 |     37.806 |     2.1
  101 |   0.9736 |     32.716 |   1.1490 |     37.561 |     2.1
  102 |   0.9728 |     32.613 |   1.1488 |     37.561 |     2.1
  103 |   0.9641 |     32.244 |   1.1393 |     37.469 |     2.1
  104 |   0.9613 |     32.277 |   1.1409 |     37.163 |     2.2
  105 |   0.9582 |     32.363 |   1.1377 |     36.703 |     2.2
  106 |   0.9534 |     31.778 |   1.1403 |     37.163 |     2.2
  107 |   0.9521 |     31.892 |   1.1380 |     36.734 |     2.2
  108 |   0.9446 |     31.594 |   1.1327 |     37.255 |     2.2
  109 |   0.9415 |     31.730 |   1.1379 |     36.887 |     2.3
  110 |   0.9358 |     31.274 |   1.1342 |     37.194 |     2.3
  111 |   0.9332 |     31.388 |   1.1340 |     37.010 |     2.3
  112 |   0.9321 |     31.301 |   1.1307 |     36.949 |     2.3
  113 |   0.9283 |     30.955 |   1.1293 |     36.673 |     2.3
  114 |   0.9239 |     30.884 |   1.1346 |     36.520 |     2.4
  115 |   0.9269 |     30.895 |   1.1238 |     36.581 |     2.4
  116 |   0.9230 |     30.879 |   1.1300 |     36.489 |     2.4
  117 |   0.9161 |     30.657 |   1.1231 |     36.213 |     2.4
  118 |   0.9098 |     30.359 |   1.1359 |     36.244 |     2.4
  119 |   0.9056 |     30.077 |   1.1209 |     36.060 |     2.5
  120 |   0.9023 |     29.898 |   1.1287 |     37.010 |     2.5
  121 |   0.8974 |     30.028 |   1.1298 |     36.612 |     2.5
  122 |   0.8938 |     29.616 |   1.1232 |     36.305 |     2.5
  123 |   0.8886 |     29.362 |   1.1329 |     36.489 |     2.6
  124 |   0.8874 |     29.264 |   1.1150 |     35.907 |     2.6
  125 |   0.8879 |     29.205 |   1.1122 |     35.815 |     2.6
  126 |   0.8823 |     29.291 |   1.1160 |     36.428 |     2.6
  127 |   0.8790 |     29.183 |   1.1196 |     35.907 |     2.6
  128 |   0.8721 |     28.939 |   1.1129 |     35.662 |     2.7
  129 |   0.8735 |     28.842 |   1.1127 |     35.723 |     2.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 1,745,506

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5378 |     93.422 |   3.4804 |     82.445 |     0.1
    2 |   3.3046 |     82.071 |   3.0767 |     82.935 |     0.1
    3 |   2.9209 |     83.209 |   2.8233 |     83.333 |     0.2
    4 |   2.7521 |     83.328 |   2.7085 |     83.333 |     0.3
    5 |   2.6481 |     74.209 |   2.6093 |     67.341 |     0.3
    6 |   2.5558 |     65.534 |   2.5228 |     66.912 |     0.4
    7 |   2.4765 |     61.606 |   2.4495 |     58.824 |     0.5
    8 |   2.4131 |     60.170 |   2.3871 |     58.824 |     0.5
    9 |   2.3554 |     59.374 |   2.3325 |     58.824 |     0.6
   10 |   2.3044 |     58.929 |   2.2845 |     58.824 |     0.6
   11 |   2.2620 |     58.577 |   2.2406 |     58.824 |     0.7
   12 |   2.2223 |     58.420 |   2.2010 |     58.824 |     0.8
   13 |   2.1867 |     58.122 |   2.1640 |     58.824 |     0.8
   14 |   2.1491 |     57.949 |   2.1295 |     58.824 |     0.9
   15 |   2.1158 |     57.293 |   2.0978 |     58.824 |     1.0
   16 |   2.0852 |     57.120 |   2.0655 |     58.824 |     1.0
   17 |   2.0569 |     56.746 |   2.0348 |     57.874 |     1.1
   18 |   2.0284 |     56.453 |   2.0051 |     58.303 |     1.2
   19 |   1.9982 |     56.144 |   1.9748 |     53.768 |     1.2
   20 |   1.9678 |     55.841 |   1.9459 |     53.768 |     1.3
   21 |   1.9400 |     55.174 |   1.9171 |     53.799 |     1.4
   22 |   1.9121 |     54.535 |   1.8891 |     51.777 |     1.4
   23 |   1.8848 |     52.514 |   1.8628 |     48.346 |     1.5
   24 |   1.8582 |     50.629 |   1.8373 |     48.346 |     1.5
   25 |   1.8349 |     49.729 |   1.8136 |     48.346 |     1.6
   26 |   1.8107 |     49.361 |   1.7910 |     48.346 |     1.7
   27 |   1.7891 |     49.193 |   1.7687 |     48.346 |     1.7
   28 |   1.7662 |     49.025 |   1.7471 |     48.284 |     1.8
   29 |   1.7451 |     48.857 |   1.7253 |     48.284 |     1.9
   30 |   1.7261 |     48.526 |   1.7048 |     48.254 |     1.9
   31 |   1.7044 |     47.692 |   1.6849 |     45.404 |     2.0
   32 |   1.6844 |     46.760 |   1.6667 |     45.404 |     2.1
   33 |   1.6718 |     46.527 |   1.6503 |     45.404 |     2.1
   34 |   1.6517 |     46.435 |   1.6332 |     45.435 |     2.2
   35 |   1.6351 |     46.294 |   1.6179 |     45.404 |     2.2
   36 |   1.6191 |     46.213 |   1.6041 |     45.435 |     2.3
   37 |   1.6088 |     46.261 |   1.5913 |     45.466 |     2.4
   38 |   1.5954 |     46.164 |   1.5805 |     45.404 |     2.4
   39 |   1.5826 |     46.131 |   1.5697 |     45.404 |     2.5
   40 |   1.5742 |     46.153 |   1.5601 |     45.435 |     2.6
   41 |   1.5629 |     46.110 |   1.5505 |     45.343 |     2.6
   42 |   1.5540 |     46.072 |   1.5430 |     45.374 |     2.7
   43 |   1.5432 |     46.083 |   1.5344 |     45.343 |     2.8
   44 |   1.5355 |     46.072 |   1.5256 |     45.343 |     2.8
   45 |   1.5281 |     46.066 |   1.5173 |     45.312 |     2.9
   46 |   1.5212 |     46.088 |   1.5097 |     45.312 |     2.9
   47 |   1.5102 |     46.061 |   1.5027 |     45.312 |     3.0
   48 |   1.5037 |     46.045 |   1.4964 |     45.343 |     3.1
   49 |   1.4987 |     46.039 |   1.4906 |     45.282 |     3.1
   50 |   1.4950 |     46.039 |   1.4845 |     45.435 |     3.2
   51 |   1.4872 |     45.996 |   1.4798 |     45.588 |     3.3
   52 |   1.4799 |     45.996 |   1.4743 |     45.496 |     3.3
   53 |   1.4760 |     46.001 |   1.4686 |     45.466 |     3.4
   54 |   1.4703 |     45.850 |   1.4642 |     45.619 |     3.5
   55 |   1.4684 |     45.871 |   1.4598 |     45.588 |     3.5
   56 |   1.4608 |     45.844 |   1.4552 |     45.650 |     3.6
   57 |   1.4526 |     45.785 |   1.4495 |     45.588 |     3.6
   58 |   1.4468 |     45.747 |   1.4448 |     45.711 |     3.7
   59 |   1.4417 |     45.747 |   1.4384 |     45.374 |     3.8
   60 |   1.4375 |     45.714 |   1.4334 |     45.588 |     3.8
   61 |   1.4281 |     45.600 |   1.4285 |     45.496 |     3.9
   62 |   1.4237 |     45.660 |   1.4244 |     45.374 |     4.0
   63 |   1.4159 |     45.557 |   1.4199 |     45.404 |     4.0
   64 |   1.4092 |     45.562 |   1.4126 |     45.435 |     4.1
   65 |   1.4010 |     45.622 |   1.4034 |     45.404 |     4.2
   66 |   1.3916 |     45.454 |   1.3967 |     45.312 |     4.2
   67 |   1.3843 |     45.270 |   1.3921 |     44.975 |     4.3
   68 |   1.3746 |     45.102 |   1.3826 |     44.914 |     4.4
   69 |   1.3629 |     44.950 |   1.3760 |     44.884 |     4.4
   70 |   1.3569 |     44.408 |   1.3699 |     44.700 |     4.5
   71 |   1.3486 |     43.997 |   1.3662 |     43.474 |     4.5
   72 |   1.3391 |     43.498 |   1.3589 |     43.842 |     4.6
   73 |   1.3335 |     43.281 |   1.3551 |     43.290 |     4.7
   74 |   1.3249 |     42.940 |   1.3519 |     42.953 |     4.7
   75 |   1.3164 |     42.593 |   1.3445 |     43.290 |     4.8
   76 |   1.3096 |     42.496 |   1.3401 |     42.953 |     4.9
   77 |   1.3068 |     42.366 |   1.3359 |     43.045 |     4.9
   78 |   1.2968 |     41.954 |   1.3343 |     42.800 |     5.0
   79 |   1.2904 |     41.997 |   1.3280 |     42.678 |     5.1
   80 |   1.2834 |     41.640 |   1.3256 |     42.708 |     5.1
   81 |   1.2785 |     41.428 |   1.3225 |     42.402 |     5.2
   82 |   1.2717 |     41.482 |   1.3168 |     42.831 |     5.2
   83 |   1.2688 |     41.255 |   1.3135 |     42.525 |     5.3
   84 |   1.2608 |     41.000 |   1.3095 |     42.647 |     5.4
   85 |   1.2560 |     40.876 |   1.3094 |     42.279 |     5.4
   86 |   1.2498 |     40.789 |   1.3079 |     42.525 |     5.5
   87 |   1.2439 |     40.469 |   1.3036 |     42.402 |     5.6
   88 |   1.2406 |     40.513 |   1.3011 |     42.310 |     5.6
   89 |   1.2309 |     40.274 |   1.3038 |     42.371 |     5.7
   90 |   1.2310 |     40.318 |   1.2971 |     42.065 |     5.8
   91 |   1.2224 |     39.911 |   1.2947 |     42.188 |     5.8
   92 |   1.2182 |     39.944 |   1.2953 |     41.973 |     5.9
   93 |   1.2149 |     39.824 |   1.2958 |     42.065 |     5.9
   94 |   1.2098 |     39.489 |   1.2904 |     41.850 |     6.0
   95 |   1.2023 |     39.591 |   1.2858 |     41.483 |     6.1
   96 |   1.1983 |     39.039 |   1.2900 |     41.605 |     6.1
   97 |   1.1970 |     38.995 |   1.2855 |     41.820 |     6.2
   98 |   1.1904 |     39.050 |   1.2838 |     41.942 |     6.3
   99 |   1.1829 |     38.762 |   1.2840 |     41.850 |     6.3
  100 |   1.1818 |     38.790 |   1.2841 |     41.728 |     6.4
  101 |   1.1803 |     38.529 |   1.2849 |     41.483 |     6.5
  102 |   1.1711 |     38.307 |   1.2834 |     41.912 |     6.5
  103 |   1.1704 |     38.529 |   1.2794 |     41.820 |     6.6
  104 |   1.1635 |     38.399 |   1.2755 |     41.513 |     6.6
  105 |   1.1618 |     38.166 |   1.2812 |     42.157 |     6.7
  106 |   1.1581 |     37.798 |   1.2727 |     41.942 |     6.8
  107 |   1.1552 |     37.982 |   1.2757 |     41.513 |     6.8
  108 |   1.1522 |     37.798 |   1.2765 |     42.279 |     6.9
  109 |   1.1505 |     37.798 |   1.2721 |     42.188 |     7.0
  110 |   1.1428 |     37.814 |   1.2722 |     42.004 |     7.0
  111 |   1.1370 |     37.554 |   1.2752 |     42.310 |     7.1
  112 |   1.1406 |     37.522 |   1.2744 |     42.586 |     7.2
  113 |   1.1354 |     37.375 |   1.2705 |     41.881 |     7.2
  114 |   1.1302 |     37.104 |   1.2721 |     41.881 |     7.3
  115 |   1.1240 |     37.153 |   1.2716 |     42.249 |     7.4
  116 |   1.1247 |     37.262 |   1.2677 |     42.402 |     7.4
  117 |   1.1172 |     36.915 |   1.2660 |     41.912 |     7.5
  118 |   1.1177 |     36.676 |   1.2647 |     41.391 |     7.5
  119 |   1.1184 |     37.067 |   1.2643 |     42.096 |     7.6
  120 |   1.1125 |     36.725 |   1.2695 |     41.483 |     7.7
  121 |   1.1033 |     36.573 |   1.2648 |     41.697 |     7.7
  122 |   1.0985 |     36.259 |   1.2655 |     41.667 |     7.8
  123 |   1.0997 |     36.313 |   1.2578 |     41.575 |     7.9
  124 |   1.0933 |     35.956 |   1.2608 |     41.544 |     7.9
  125 |   1.0914 |     36.113 |   1.2642 |     41.483 |     8.0
  126 |   1.0899 |     36.135 |   1.2563 |     41.238 |     8.1
  127 |   1.0894 |     35.799 |   1.2613 |     41.636 |     8.1
  128 |   1.0855 |     35.902 |   1.2528 |     41.176 |     8.2
  129 |   1.0844 |     35.772 |   1.2550 |     41.268 |     8.2
  130 |   1.0794 |     35.799 |   1.2576 |     41.483 |     8.3
  131 |   1.0762 |     35.647 |   1.2634 |     41.146 |     8.4
  132 |   1.0675 |     35.203 |   1.2565 |     41.054 |     8.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 833,890

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8720 |     78.002 |   2.4547 |     58.824 |     0.0
    2 |   2.2267 |     59.016 |   2.0339 |     58.824 |     0.1
    3 |   1.8946 |     52.866 |   1.7441 |     48.346 |     0.1
    4 |   1.6603 |     47.681 |   1.5746 |     45.466 |     0.1
    5 |   1.5415 |     46.245 |   1.5001 |     45.466 |     0.2
    6 |   1.4868 |     46.218 |   1.4609 |     45.466 |     0.2
    7 |   1.4602 |     46.305 |   1.4420 |     45.925 |     0.3
    8 |   1.4444 |     46.223 |   1.4279 |     45.987 |     0.3
    9 |   1.4297 |     46.332 |   1.4185 |     45.466 |     0.3
   10 |   1.4238 |     46.326 |   1.4111 |     45.466 |     0.4
   11 |   1.4158 |     46.175 |   1.4046 |     45.466 |     0.4
   12 |   1.4081 |     46.429 |   1.3982 |     45.466 |     0.4
   13 |   1.3997 |     46.234 |   1.3929 |     45.466 |     0.5
   14 |   1.3924 |     46.294 |   1.3885 |     45.466 |     0.5
   15 |   1.3873 |     46.077 |   1.3837 |     46.017 |     0.5
   16 |   1.3841 |     46.055 |   1.3784 |     45.466 |     0.6
   17 |   1.3796 |     46.294 |   1.3746 |     45.466 |     0.6
   18 |   1.3753 |     46.213 |   1.3719 |     45.159 |     0.7
   19 |   1.3702 |     46.240 |   1.3707 |     45.343 |     0.7
   20 |   1.3626 |     46.012 |   1.3595 |     44.700 |     0.7
   21 |   1.3558 |     45.703 |   1.3541 |     44.761 |     0.8
   22 |   1.3532 |     46.001 |   1.3472 |     44.945 |     0.8
   23 |   1.3483 |     45.790 |   1.3381 |     44.975 |     0.8
   24 |   1.3457 |     45.611 |   1.3342 |     44.516 |     0.9
   25 |   1.3394 |     45.595 |   1.3320 |     44.240 |     0.9
   26 |   1.3308 |     45.416 |   1.3307 |     44.240 |     0.9
   27 |   1.3294 |     45.514 |   1.3462 |     44.332 |     1.0
   28 |   1.3240 |     45.416 |   1.3261 |     44.087 |     1.0
   29 |   1.3231 |     45.465 |   1.3196 |     44.730 |     1.1
   30 |   1.3186 |     45.432 |   1.3111 |     44.118 |     1.1
   31 |   1.3154 |     45.194 |   1.3118 |     45.190 |     1.1
   32 |   1.3118 |     45.248 |   1.3096 |     44.485 |     1.2
   33 |   1.3074 |     45.232 |   1.3055 |     43.903 |     1.2
   34 |   1.3082 |     45.210 |   1.3037 |     44.363 |     1.2
   35 |   1.3041 |     45.069 |   1.2989 |     44.118 |     1.3
   36 |   1.3012 |     45.167 |   1.2960 |     43.842 |     1.3
   37 |   1.2945 |     44.853 |   1.2936 |     44.485 |     1.4
   38 |   1.2904 |     44.728 |   1.2867 |     44.608 |     1.4
   39 |   1.2858 |     44.858 |   1.2851 |     44.363 |     1.4
   40 |   1.2861 |     44.836 |   1.2925 |     44.271 |     1.5
   41 |   1.2884 |     44.706 |   1.2798 |     43.873 |     1.5
   42 |   1.2763 |     44.582 |   1.2736 |     42.953 |     1.5
   43 |   1.2701 |     44.278 |   1.2699 |     44.026 |     1.6
   44 |   1.2668 |     44.305 |   1.2763 |     43.321 |     1.6
   45 |   1.2654 |     44.078 |   1.2644 |     42.065 |     1.6
   46 |   1.2559 |     43.476 |   1.2595 |     42.739 |     1.7
   47 |   1.2517 |     43.043 |   1.2687 |     42.984 |     1.7
   48 |   1.2416 |     43.243 |   1.2446 |     42.126 |     1.8
   49 |   1.2404 |     43.054 |   1.2457 |     42.433 |     1.8
   50 |   1.2325 |     42.935 |   1.2385 |     41.452 |     1.8
   51 |   1.2311 |     42.832 |   1.2309 |     41.759 |     1.9
   52 |   1.2190 |     42.208 |   1.2293 |     42.494 |     1.9
   53 |   1.2238 |     42.208 |   1.2257 |     42.096 |     1.9
   54 |   1.2139 |     42.057 |   1.2137 |     41.850 |     2.0
   55 |   1.2063 |     41.732 |   1.2150 |     41.850 |     2.0
   56 |   1.2036 |     41.369 |   1.2088 |     41.697 |     2.1
   57 |   1.2007 |     41.553 |   1.2065 |     41.636 |     2.1
   58 |   1.2004 |     41.650 |   1.2055 |     41.176 |     2.1
   59 |   1.1959 |     41.260 |   1.2044 |     40.625 |     2.2
   60 |   1.1929 |     40.979 |   1.1888 |     40.319 |     2.2
   61 |   1.1872 |     41.049 |   1.1990 |     40.993 |     2.2
   62 |   1.1842 |     40.789 |   1.2008 |     41.238 |     2.3
   63 |   1.1734 |     40.821 |   1.1916 |     40.564 |     2.3
   64 |   1.1704 |     40.350 |   1.1914 |     40.686 |     2.3
   65 |   1.1663 |     40.361 |   1.1887 |     40.472 |     2.4
   66 |   1.1682 |     40.366 |   1.1862 |     40.411 |     2.4
   67 |   1.1629 |     40.036 |   1.1751 |     40.411 |     2.5
   68 |   1.1636 |     40.014 |   1.1921 |     40.380 |     2.5
   69 |   1.1714 |     40.290 |   1.1838 |     40.472 |     2.5
   70 |   1.1599 |     39.835 |   1.1635 |     38.971 |     2.6
   71 |   1.1532 |     39.689 |   1.1624 |     39.124 |     2.6
   72 |   1.1418 |     39.472 |   1.1586 |     38.848 |     2.6
   73 |   1.1391 |     39.364 |   1.1672 |     39.277 |     2.7
   74 |   1.1424 |     39.348 |   1.1645 |     39.430 |     2.7
   75 |   1.1469 |     39.581 |   1.1600 |     38.327 |     2.7
   76 |   1.1369 |     39.174 |   1.1598 |     39.890 |     2.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,189,954

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2012 |     84.542 |   2.7139 |     81.893 |     0.0
    2 |   2.4636 |     62.852 |   2.2566 |     58.824 |     0.1
    3 |   2.1335 |     57.033 |   2.0261 |     56.955 |     0.1
    4 |   1.9600 |     54.102 |   1.8762 |     48.529 |     0.1
    5 |   1.8322 |     49.707 |   1.7638 |     48.346 |     0.2
    6 |   1.7340 |     48.835 |   1.6679 |     48.346 |     0.2
    7 |   1.6403 |     48.391 |   1.5900 |     45.466 |     0.2
    8 |   1.5736 |     46.364 |   1.5330 |     45.466 |     0.3
    9 |   1.5229 |     46.207 |   1.4947 |     45.466 |     0.3
   10 |   1.4926 |     46.213 |   1.4698 |     45.466 |     0.4
   11 |   1.4700 |     46.207 |   1.4509 |     45.466 |     0.4
   12 |   1.4506 |     46.213 |   1.4368 |     45.466 |     0.4
   13 |   1.4394 |     46.305 |   1.4241 |     45.466 |     0.5
   14 |   1.4262 |     46.251 |   1.4122 |     45.527 |     0.5
   15 |   1.4113 |     46.283 |   1.4030 |     45.895 |     0.5
   16 |   1.4033 |     46.451 |   1.3960 |     45.466 |     0.6
   17 |   1.3936 |     46.245 |   1.3870 |     45.404 |     0.6
   18 |   1.3835 |     46.207 |   1.3771 |     45.312 |     0.6
   19 |   1.3733 |     46.034 |   1.3702 |     45.098 |     0.7
   20 |   1.3645 |     45.590 |   1.3650 |     44.516 |     0.7
   21 |   1.3549 |     45.194 |   1.3570 |     44.853 |     0.7
   22 |   1.3485 |     45.156 |   1.3512 |     44.363 |     0.8
   23 |   1.3379 |     44.766 |   1.3466 |     43.842 |     0.8
   24 |   1.3303 |     44.273 |   1.3397 |     43.750 |     0.8
   25 |   1.3227 |     43.655 |   1.3313 |     43.045 |     0.9
   26 |   1.3154 |     43.124 |   1.3287 |     43.321 |     0.9
   27 |   1.3041 |     43.005 |   1.3172 |     42.647 |     0.9
   28 |   1.2936 |     42.761 |   1.3151 |     43.045 |     1.0
   29 |   1.2848 |     42.344 |   1.3009 |     42.555 |     1.0
   30 |   1.2775 |     42.246 |   1.3008 |     42.341 |     1.0
   31 |   1.2657 |     42.230 |   1.2892 |     42.188 |     1.1
   32 |   1.2566 |     41.569 |   1.2806 |     41.912 |     1.1
   33 |   1.2477 |     41.558 |   1.2776 |     41.575 |     1.1
   34 |   1.2396 |     41.385 |   1.2727 |     42.004 |     1.2
   35 |   1.2284 |     40.946 |   1.2618 |     41.575 |     1.2
   36 |   1.2216 |     40.691 |   1.2600 |     41.299 |     1.2
   37 |   1.2145 |     40.605 |   1.2575 |     41.391 |     1.3
   38 |   1.2046 |     40.236 |   1.2476 |     41.115 |     1.3
   39 |   1.1944 |     39.819 |   1.2436 |     41.146 |     1.3
   40 |   1.1849 |     39.721 |   1.2410 |     40.870 |     1.4
   41 |   1.1760 |     39.256 |   1.2292 |     40.717 |     1.4
   42 |   1.1681 |     38.768 |   1.2308 |     40.717 |     1.4
   43 |   1.1589 |     38.676 |   1.2185 |     40.533 |     1.5
   44 |   1.1496 |     38.427 |   1.2165 |     39.859 |     1.5
   45 |   1.1430 |     38.031 |   1.2063 |     39.645 |     1.6
   46 |   1.1283 |     37.489 |   1.2035 |     39.859 |     1.6
   47 |   1.1206 |     37.413 |   1.1958 |     39.338 |     1.6
   48 |   1.1143 |     37.023 |   1.1907 |     38.971 |     1.7
   49 |   1.1023 |     36.508 |   1.1830 |     38.174 |     1.7
   50 |   1.0979 |     36.205 |   1.1840 |     38.235 |     1.7
   51 |   1.0835 |     36.086 |   1.1770 |     37.837 |     1.8
   52 |   1.0734 |     35.333 |   1.1635 |     37.592 |     1.8
   53 |   1.0677 |     35.024 |   1.1623 |     37.623 |     1.8
   54 |   1.0586 |     34.796 |   1.1636 |     37.316 |     1.9
   55 |   1.0495 |     34.260 |   1.1501 |     36.520 |     1.9
   56 |   1.0409 |     34.173 |   1.1514 |     36.305 |     1.9
   57 |   1.0316 |     33.669 |   1.1456 |     36.765 |     2.0
   58 |   1.0201 |     33.312 |   1.1414 |     36.305 |     2.0
   59 |   1.0151 |     33.263 |   1.1255 |     36.275 |     2.0
   60 |   1.0033 |     32.548 |   1.1243 |     36.183 |     2.1
   61 |   0.9899 |     32.342 |   1.1298 |     36.029 |     2.1
   62 |   0.9845 |     32.212 |   1.1238 |     35.417 |     2.1
   63 |   0.9787 |     31.735 |   1.1180 |     35.447 |     2.2
   64 |   0.9677 |     31.627 |   1.1127 |     35.478 |     2.2
   65 |   0.9584 |     31.150 |   1.1156 |     35.417 |     2.2
   66 |   0.9531 |     30.966 |   1.1045 |     34.620 |     2.3
   67 |   0.9456 |     30.581 |   1.1000 |     34.988 |     2.3
   68 |   0.9349 |     30.462 |   1.1003 |     35.110 |     2.3
   69 |   0.9330 |     30.630 |   1.0951 |     35.172 |     2.4
   70 |   0.9199 |     29.898 |   1.0877 |     34.988 |     2.4
   71 |   0.9088 |     29.432 |   1.0873 |     34.926 |     2.4
   72 |   0.8990 |     29.134 |   1.0882 |     34.865 |     2.5
   73 |   0.8968 |     29.075 |   1.0712 |     34.589 |     2.5
   74 |   0.8922 |     28.804 |   1.0748 |     33.732 |     2.5
   75 |   0.8809 |     28.235 |   1.0716 |     33.977 |     2.6
   76 |   0.8685 |     28.349 |   1.0730 |     34.375 |     2.6
   77 |   0.8606 |     27.991 |   1.0737 |     34.130 |     2.6
   78 |   0.8528 |     27.682 |   1.0644 |     33.640 |     2.7
   79 |   0.8452 |     27.487 |   1.0627 |     33.640 |     2.7
   80 |   0.8368 |     27.016 |   1.0677 |     33.180 |     2.7
   81 |   0.8360 |     26.875 |   1.0688 |     33.824 |     2.8
   82 |   0.8315 |     26.750 |   1.0713 |     33.670 |     2.8
   83 |   0.8203 |     26.371 |   1.0531 |     33.333 |     2.8
   84 |   0.8133 |     26.029 |   1.0616 |     33.180 |     2.9
   85 |   0.8063 |     25.964 |   1.0489 |     33.058 |     2.9
   86 |   0.8000 |     25.520 |   1.0518 |     32.721 |     2.9
   87 |   0.7950 |     25.374 |   1.0430 |     32.659 |     3.0
   88 |   0.7868 |     25.368 |   1.0521 |     32.690 |     3.0
   89 |   0.7794 |     25.081 |   1.0527 |     32.996 |     3.1
   90 |   0.7817 |     24.756 |   1.0412 |     32.261 |     3.1
   91 |   0.7667 |     24.491 |   1.0479 |     32.966 |     3.1
   92 |   0.7616 |     24.334 |   1.0452 |     32.475 |     3.2
   93 |   0.7579 |     24.274 |   1.0465 |     31.893 |     3.2
   94 |   0.7467 |     23.434 |   1.0464 |     32.567 |     3.2
   95 |   0.7479 |     24.036 |   1.0386 |     31.740 |     3.3
   96 |   0.7345 |     23.331 |   1.0429 |     31.434 |     3.3
   97 |   0.7283 |     23.304 |   1.0399 |     31.771 |     3.3
   98 |   0.7287 |     23.244 |   1.0488 |     31.219 |     3.4
   99 |   0.7221 |     22.784 |   1.0380 |     31.066 |     3.4
  100 |   0.7181 |     22.822 |   1.0324 |     31.434 |     3.4
  101 |   0.7084 |     22.562 |   1.0305 |     30.974 |     3.5
  102 |   0.7024 |     22.388 |   1.0268 |     30.760 |     3.5
  103 |   0.6932 |     21.917 |   1.0351 |     30.576 |     3.5
  104 |   0.6936 |     22.031 |   1.0310 |     31.036 |     3.6
  105 |   0.6844 |     21.711 |   1.0378 |     31.097 |     3.6
  106 |   0.6829 |     21.814 |   1.0431 |     30.699 |     3.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 415,714

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8488 |     75.829 |   2.4359 |     66.912 |     0.0
    2 |   2.2138 |     60.322 |   2.0148 |     58.824 |     0.0
    3 |   1.8758 |     53.533 |   1.7187 |     48.621 |     0.1
    4 |   1.6445 |     47.155 |   1.5552 |     45.987 |     0.1
    5 |   1.5264 |     46.272 |   1.4838 |     45.987 |     0.1
    6 |   1.4735 |     46.191 |   1.4488 |     45.466 |     0.1
    7 |   1.4451 |     46.267 |   1.4254 |     45.466 |     0.1
    8 |   1.4240 |     46.240 |   1.4061 |     45.466 |     0.2
    9 |   1.4064 |     46.153 |   1.3906 |     45.466 |     0.2
   10 |   1.3939 |     46.229 |   1.3864 |     45.435 |     0.2
   11 |   1.3825 |     46.039 |   1.3708 |     45.987 |     0.2
   12 |   1.3694 |     45.850 |   1.3555 |     44.608 |     0.2
   13 |   1.3567 |     45.454 |   1.3486 |     43.842 |     0.3
   14 |   1.3487 |     45.329 |   1.3405 |     44.608 |     0.3
   15 |   1.3408 |     45.416 |   1.3360 |     44.485 |     0.3
   16 |   1.3333 |     45.286 |   1.3423 |     44.669 |     0.3
   17 |   1.3308 |     45.384 |   1.3237 |     44.638 |     0.3
   18 |   1.3198 |     44.750 |   1.3194 |     44.240 |     0.4
   19 |   1.3119 |     44.950 |   1.3185 |     43.934 |     0.4
   20 |   1.3125 |     45.107 |   1.3151 |     44.118 |     0.4
   21 |   1.3052 |     44.880 |   1.3092 |     44.608 |     0.4
   22 |   1.2960 |     44.663 |   1.3024 |     43.597 |     0.4
   23 |   1.2948 |     44.343 |   1.2948 |     43.413 |     0.5
   24 |   1.2852 |     43.921 |   1.2917 |     42.586 |     0.5
   25 |   1.2772 |     43.839 |   1.2821 |     43.413 |     0.5
   26 |   1.2720 |     43.644 |   1.2790 |     43.536 |     0.5
   27 |   1.2671 |     43.466 |   1.2720 |     42.892 |     0.5
   28 |   1.2550 |     43.113 |   1.2640 |     42.770 |     0.6
   29 |   1.2542 |     43.195 |   1.2616 |     42.525 |     0.6
   30 |   1.2465 |     42.832 |   1.2669 |     42.984 |     0.6
   31 |   1.2409 |     42.572 |   1.2554 |     42.279 |     0.6
   32 |   1.2335 |     42.523 |   1.2455 |     42.371 |     0.6
   33 |   1.2187 |     41.515 |   1.2337 |     42.249 |     0.7
   34 |   1.2147 |     41.477 |   1.2332 |     41.513 |     0.7
   35 |   1.2107 |     41.065 |   1.2199 |     41.360 |     0.7
   36 |   1.1937 |     40.225 |   1.2170 |     41.085 |     0.7
   37 |   1.1906 |     40.448 |   1.2136 |     40.564 |     0.7
   38 |   1.1778 |     39.873 |   1.2110 |     41.360 |     0.8
   39 |   1.1699 |     39.283 |   1.2071 |     40.441 |     0.8
   40 |   1.1616 |     38.898 |   1.1813 |     39.277 |     0.8
   41 |   1.1493 |     38.627 |   1.1851 |     40.012 |     0.8
   42 |   1.1419 |     38.464 |   1.1715 |     39.369 |     0.8
   43 |   1.1297 |     38.161 |   1.1678 |     38.297 |     0.9
   44 |   1.1261 |     37.955 |   1.1656 |     38.695 |     0.9
   45 |   1.1214 |     37.695 |   1.1562 |     38.695 |     0.9
   46 |   1.1117 |     37.164 |   1.1475 |     38.450 |     0.9
   47 |   1.1004 |     36.622 |   1.1473 |     37.561 |     0.9
   48 |   1.0929 |     36.357 |   1.1458 |     37.837 |     1.0
   49 |   1.0864 |     36.129 |   1.1431 |     37.255 |     1.0
   50 |   1.0802 |     36.216 |   1.1320 |     37.347 |     1.0
   51 |   1.0806 |     36.194 |   1.1232 |     36.673 |     1.0
   52 |   1.0773 |     36.270 |   1.1239 |     36.458 |     1.0
   53 |   1.0615 |     35.338 |   1.1093 |     36.275 |     1.1
   54 |   1.0553 |     35.138 |   1.1032 |     36.121 |     1.1
   55 |   1.0462 |     34.943 |   1.1003 |     36.213 |     1.1
   56 |   1.0385 |     34.287 |   1.0909 |     35.539 |     1.1
   57 |   1.0329 |     34.374 |   1.0873 |     35.447 |     1.1
   58 |   1.0220 |     33.913 |   1.0943 |     36.121 |     1.1
   59 |   1.0239 |     33.891 |   1.0751 |     35.049 |     1.2
   60 |   1.0127 |     33.474 |   1.0758 |     35.600 |     1.2
   61 |   1.0157 |     33.431 |   1.0694 |     35.325 |     1.2
   62 |   1.0029 |     33.306 |   1.0697 |     34.957 |     1.2
   63 |   0.9960 |     33.165 |   1.0633 |     34.926 |     1.2
   64 |   0.9885 |     32.770 |   1.0561 |     34.528 |     1.3
   65 |   0.9935 |     33.024 |   1.0569 |     34.344 |     1.3
   66 |   0.9864 |     32.504 |   1.0547 |     34.222 |     1.3
   67 |   0.9714 |     31.935 |   1.0647 |     34.375 |     1.3
   68 |   0.9725 |     32.022 |   1.0625 |     34.926 |     1.3
   69 |   0.9655 |     31.811 |   1.0472 |     33.824 |     1.4
   70 |   0.9607 |     31.215 |   1.0619 |     34.804 |     1.4
   71 |   0.9534 |     31.540 |   1.0452 |     34.130 |     1.4
   72 |   0.9413 |     31.036 |   1.0412 |     33.977 |     1.4
   73 |   0.9405 |     30.770 |   1.0335 |     33.885 |     1.4
   74 |   0.9350 |     30.895 |   1.0357 |     33.885 |     1.5
   75 |   0.9355 |     31.253 |   1.0337 |     33.701 |     1.5
   76 |   0.9310 |     30.798 |   1.0336 |     33.915 |     1.5
   77 |   0.9257 |     30.749 |   1.0353 |     33.272 |     1.5
   78 |   0.9148 |     30.445 |   1.0286 |     33.517 |     1.5
   79 |   0.9071 |     29.985 |   1.0357 |     33.211 |     1.6
   80 |   0.9062 |     30.039 |   1.0295 |     33.180 |     1.6
   81 |   0.9026 |     29.795 |   1.0355 |     33.395 |     1.6
   82 |   0.8994 |     29.671 |   1.0265 |     32.812 |     1.6
   83 |   0.8935 |     29.741 |   1.0245 |     33.732 |     1.6
   84 |   0.8982 |     29.952 |   1.0280 |     33.364 |     1.7
   85 |   0.8904 |     29.687 |   1.0257 |     33.364 |     1.7
   86 |   0.8819 |     29.275 |   1.0099 |     32.475 |     1.7
   87 |   0.8709 |     28.506 |   1.0174 |     32.935 |     1.7
   88 |   0.8825 |     29.129 |   1.0099 |     33.058 |     1.7
   89 |   0.8673 |     28.777 |   1.0151 |     32.629 |     1.8
   90 |   0.8686 |     28.663 |   1.0099 |     32.384 |     1.8
   91 |   0.8680 |     28.804 |   1.0145 |     32.414 |     1.8
   92 |   0.8605 |     28.706 |   1.0040 |     32.874 |     1.8
   93 |   0.8566 |     28.181 |   1.0047 |     32.261 |     1.8
   94 |   0.8490 |     28.116 |   1.0199 |     32.384 |     1.9
   95 |   0.8543 |     28.468 |   1.0082 |     32.966 |     1.9
   96 |   0.8493 |     28.278 |   1.0271 |     33.395 |     1.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 2,338,466

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8872 |     77.276 |   2.3958 |     58.824 |     0.1
    2 |   2.2060 |     58.588 |   2.0165 |     55.607 |     0.2
    3 |   1.9131 |     54.481 |   1.7836 |     48.346 |     0.3
    4 |   1.6951 |     47.919 |   1.5869 |     45.466 |     0.3
    5 |   1.5482 |     46.251 |   1.5027 |     45.496 |     0.4
    6 |   1.4833 |     46.218 |   1.4570 |     45.864 |     0.5
    7 |   1.4568 |     46.240 |   1.4357 |     45.987 |     0.6
    8 |   1.4362 |     46.240 |   1.4239 |     45.466 |     0.7
    9 |   1.4262 |     46.283 |   1.4157 |     45.435 |     0.8
   10 |   1.4205 |     46.148 |   1.4087 |     45.987 |     0.8
   11 |   1.4130 |     46.256 |   1.4033 |     45.987 |     0.9
   12 |   1.4081 |     46.223 |   1.4023 |     45.987 |     1.0
   13 |   1.4053 |     46.348 |   1.3976 |     45.772 |     1.1
   14 |   1.4009 |     46.261 |   1.3961 |     45.711 |     1.2
   15 |   1.3998 |     46.321 |   1.3932 |     45.466 |     1.3
   16 |   1.3986 |     46.288 |   1.3928 |     45.558 |     1.3
   17 |   1.3972 |     46.364 |   1.3916 |     45.466 |     1.4
   18 |   1.3961 |     46.278 |   1.3882 |     45.466 |     1.5
   19 |   1.3927 |     46.186 |   1.3889 |     45.466 |     1.6
   20 |   1.3949 |     46.353 |   1.3850 |     45.466 |     1.7
   21 |   1.3897 |     46.321 |   1.3855 |     44.975 |     1.8
   22 |   1.3872 |     46.429 |   1.3814 |     45.466 |     1.8
   23 |   1.3845 |     46.316 |   1.3761 |     45.680 |     1.9
   24 |   1.3777 |     46.294 |   1.3737 |     45.466 |     2.0
   25 |   1.3795 |     46.278 |   1.3714 |     45.466 |     2.1
   26 |   1.3722 |     46.326 |   1.3634 |     45.466 |     2.2
   27 |   1.3691 |     46.028 |   1.3640 |     45.680 |     2.3
   28 |   1.3620 |     45.871 |   1.3636 |     45.037 |     2.3
   29 |   1.3655 |     45.519 |   1.3659 |     44.945 |     2.4
   30 |   1.3595 |     45.264 |   1.3476 |     44.669 |     2.5
   31 |   1.3539 |     45.107 |   1.3434 |     44.271 |     2.6
   32 |   1.3481 |     45.102 |   1.3393 |     44.271 |     2.7
   33 |   1.3430 |     44.912 |   1.3321 |     44.118 |     2.8
   34 |   1.3361 |     44.863 |   1.3316 |     44.148 |     2.8
   35 |   1.3352 |     44.853 |   1.3379 |     44.669 |     2.9
   36 |   1.3353 |     45.031 |   1.3296 |     44.210 |     3.0
   37 |   1.3332 |     44.706 |   1.3267 |     43.842 |     3.1
   38 |   1.3280 |     44.863 |   1.3246 |     43.842 |     3.2
   39 |   1.3278 |     44.571 |   1.3189 |     43.505 |     3.3
   40 |   1.3269 |     44.674 |   1.3244 |     44.118 |     3.3
   41 |   1.3213 |     44.820 |   1.3211 |     43.566 |     3.4
   42 |   1.3225 |     44.717 |   1.3206 |     44.271 |     3.5
   43 |   1.3202 |     44.576 |   1.3092 |     44.118 |     3.6
   44 |   1.3176 |     44.446 |   1.3182 |     44.700 |     3.7
   45 |   1.3156 |     44.582 |   1.3166 |     44.179 |     3.8
   46 |   1.3203 |     44.739 |   1.3163 |     44.761 |     3.8
   47 |   1.3147 |     44.517 |   1.3120 |     44.148 |     3.9
   48 |   1.3106 |     44.593 |   1.3047 |     44.026 |     4.0
   49 |   1.3162 |     44.479 |   1.3101 |     43.781 |     4.1
   50 |   1.3147 |     44.587 |   1.2995 |     44.148 |     4.2
   51 |   1.3085 |     44.544 |   1.3157 |     44.761 |     4.3
   52 |   1.3102 |     44.495 |   1.3091 |     44.210 |     4.4
   53 |   1.3094 |     44.137 |   1.3017 |     43.842 |     4.4
   54 |   1.3099 |     44.538 |   1.3020 |     43.995 |     4.5
   55 |   1.3048 |     44.295 |   1.2941 |     43.903 |     4.6
   56 |   1.3008 |     44.349 |   1.3027 |     44.240 |     4.7
   57 |   1.3004 |     44.148 |   1.2956 |     43.597 |     4.8
   58 |   1.2961 |     43.937 |   1.2933 |     43.536 |     4.9
   59 |   1.2910 |     43.682 |   1.2943 |     43.627 |     4.9
   60 |   1.2917 |     43.736 |   1.2936 |     43.719 |     5.0
   61 |   1.2960 |     43.628 |   1.2943 |     43.199 |     5.1
   62 |   1.2922 |     43.541 |   1.3049 |     43.444 |     5.2
   63 |   1.2889 |     43.449 |   1.2824 |     42.678 |     5.3
   64 |   1.2867 |     43.520 |   1.2969 |     42.831 |     5.4
   65 |   1.2810 |     43.357 |   1.2912 |     42.800 |     5.4
   66 |   1.2830 |     43.428 |   1.2971 |     42.862 |     5.5
   67 |   1.2852 |     43.466 |   1.2947 |     43.076 |     5.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 270,050

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4378 |     80.808 |   3.2614 |     80.852 |     0.0
    2 |   2.8583 |     79.459 |   2.6256 |     67.341 |     0.0
    3 |   2.4729 |     62.944 |   2.3655 |     58.824 |     0.0
    4 |   2.2695 |     58.664 |   2.2008 |     58.824 |     0.1
    5 |   2.1433 |     57.559 |   2.0928 |     58.425 |     0.1
    6 |   2.0486 |     56.469 |   2.0077 |     55.699 |     0.1
    7 |   1.9717 |     55.581 |   1.9337 |     54.075 |     0.1
    8 |   1.9007 |     53.067 |   1.8627 |     48.162 |     0.1
    9 |   1.8307 |     49.393 |   1.7937 |     48.070 |     0.1
   10 |   1.7669 |     48.748 |   1.7325 |     48.346 |     0.1
   11 |   1.7113 |     48.607 |   1.6781 |     48.346 |     0.2
   12 |   1.6603 |     48.591 |   1.6307 |     48.346 |     0.2
   13 |   1.6184 |     47.432 |   1.5919 |     45.466 |     0.2
   14 |   1.5829 |     46.256 |   1.5608 |     45.466 |     0.2
   15 |   1.5566 |     46.207 |   1.5350 |     45.466 |     0.2
   16 |   1.5349 |     46.207 |   1.5148 |     45.466 |     0.2
   17 |   1.5149 |     46.223 |   1.4979 |     45.466 |     0.2
   18 |   1.4985 |     46.202 |   1.4835 |     45.466 |     0.3
   19 |   1.4834 |     46.196 |   1.4712 |     45.466 |     0.3
   20 |   1.4695 |     46.055 |   1.4606 |     45.466 |     0.3
   21 |   1.4603 |     46.137 |   1.4511 |     45.466 |     0.3
   22 |   1.4524 |     46.191 |   1.4427 |     45.343 |     0.3
   23 |   1.4455 |     46.158 |   1.4345 |     45.435 |     0.3
   24 |   1.4375 |     46.083 |   1.4295 |     45.435 |     0.3
   25 |   1.4307 |     45.985 |   1.4213 |     45.251 |     0.4
   26 |   1.4220 |     45.947 |   1.4160 |     45.404 |     0.4
   27 |   1.4152 |     46.131 |   1.4103 |     45.159 |     0.4
   28 |   1.4100 |     45.817 |   1.4061 |     45.129 |     0.4
   29 |   1.4032 |     45.822 |   1.4005 |     44.945 |     0.4
   30 |   1.3997 |     45.795 |   1.3951 |     44.822 |     0.4
   31 |   1.3924 |     45.633 |   1.3916 |     45.312 |     0.4
   32 |   1.3894 |     45.487 |   1.3881 |     44.822 |     0.5
   33 |   1.3818 |     45.259 |   1.3840 |     44.393 |     0.5
   34 |   1.3775 |     45.069 |   1.3779 |     44.271 |     0.5
   35 |   1.3734 |     44.993 |   1.3732 |     44.485 |     0.5
   36 |   1.3665 |     44.923 |   1.3674 |     44.424 |     0.5
   37 |   1.3613 |     44.999 |   1.3616 |     44.393 |     0.5
   38 |   1.3534 |     44.858 |   1.3561 |     44.332 |     0.5
   39 |   1.3465 |     44.896 |   1.3503 |     44.485 |     0.5
   40 |   1.3404 |     44.863 |   1.3457 |     44.424 |     0.6
   41 |   1.3294 |     44.863 |   1.3333 |     44.945 |     0.6
   42 |   1.3223 |     44.636 |   1.3252 |     45.067 |     0.6
   43 |   1.3137 |     44.728 |   1.3199 |     44.210 |     0.6
   44 |   1.3057 |     44.343 |   1.3149 |     44.332 |     0.6
   45 |   1.2979 |     43.975 |   1.3038 |     43.597 |     0.6
   46 |   1.2895 |     43.688 |   1.2982 |     43.873 |     0.6
   47 |   1.2818 |     43.352 |   1.2928 |     42.862 |     0.7
   48 |   1.2731 |     42.929 |   1.2852 |     43.107 |     0.7
   49 |   1.2646 |     42.723 |   1.2811 |     43.413 |     0.7
   50 |   1.2606 |     42.550 |   1.2732 |     42.708 |     0.7
   51 |   1.2490 |     42.241 |   1.2689 |     42.647 |     0.7
   52 |   1.2417 |     42.225 |   1.2636 |     41.942 |     0.7
   53 |   1.2376 |     41.927 |   1.2573 |     42.463 |     0.7
   54 |   1.2263 |     41.602 |   1.2509 |     42.157 |     0.8
   55 |   1.2198 |     41.504 |   1.2447 |     42.402 |     0.8
   56 |   1.2141 |     41.596 |   1.2381 |     42.126 |     0.8
   57 |   1.2067 |     41.119 |   1.2349 |     42.402 |     0.8
   58 |   1.2005 |     41.141 |   1.2307 |     41.544 |     0.8
   59 |   1.1922 |     40.897 |   1.2300 |     41.544 |     0.8
   60 |   1.1884 |     40.756 |   1.2225 |     42.096 |     0.8
   61 |   1.1791 |     40.659 |   1.2194 |     41.697 |     0.8
   62 |   1.1742 |     40.366 |   1.2128 |     40.962 |     0.9
   63 |   1.1723 |     40.155 |   1.2120 |     41.973 |     0.9
   64 |   1.1628 |     39.927 |   1.2076 |     41.973 |     0.9
   65 |   1.1555 |     39.694 |   1.2034 |     41.115 |     0.9
   66 |   1.1534 |     39.613 |   1.1985 |     40.962 |     0.9
   67 |   1.1476 |     39.380 |   1.1980 |     40.901 |     0.9
   68 |   1.1436 |     39.093 |   1.1920 |     40.931 |     0.9
   69 |   1.1362 |     38.811 |   1.1909 |     40.257 |     1.0
   70 |   1.1312 |     38.784 |   1.1863 |     40.227 |     1.0
   71 |   1.1270 |     38.746 |   1.1883 |     40.533 |     1.0
   72 |   1.1241 |     38.416 |   1.1793 |     40.135 |     1.0
   73 |   1.1152 |     37.809 |   1.1783 |     40.074 |     1.0
   74 |   1.1128 |     38.026 |   1.1755 |     38.940 |     1.0
   75 |   1.1068 |     37.744 |   1.1714 |     39.553 |     1.0
   76 |   1.1007 |     37.576 |   1.1663 |     39.185 |     1.1
   77 |   1.0952 |     37.533 |   1.1716 |     39.154 |     1.1
   78 |   1.0919 |     36.980 |   1.1631 |     39.124 |     1.1
   79 |   1.0872 |     36.953 |   1.1693 |     39.951 |     1.1
   80 |   1.0839 |     36.834 |   1.1575 |     39.308 |     1.1
   81 |   1.0770 |     36.758 |   1.1566 |     38.756 |     1.1
   82 |   1.0758 |     36.378 |   1.1545 |     39.062 |     1.1
   83 |   1.0679 |     35.983 |   1.1496 |     39.154 |     1.1
   84 |   1.0689 |     35.902 |   1.1465 |     38.143 |     1.2
   85 |   1.0582 |     35.631 |   1.1458 |     38.266 |     1.2
   86 |   1.0494 |     35.311 |   1.1415 |     38.664 |     1.2
   87 |   1.0509 |     35.257 |   1.1375 |     37.990 |     1.2
   88 |   1.0451 |     35.100 |   1.1350 |     37.469 |     1.2
   89 |   1.0378 |     34.802 |   1.1297 |     37.714 |     1.2
   90 |   1.0346 |     34.650 |   1.1324 |     36.949 |     1.2
   91 |   1.0276 |     34.032 |   1.1318 |     37.040 |     1.3
   92 |   1.0252 |     33.832 |   1.1265 |     36.918 |     1.3
   93 |   1.0195 |     34.124 |   1.1238 |     36.826 |     1.3
   94 |   1.0156 |     33.853 |   1.1232 |     36.857 |     1.3
   95 |   1.0102 |     33.653 |   1.1168 |     36.703 |     1.3
   96 |   1.0058 |     33.355 |   1.1165 |     37.102 |     1.3
   97 |   0.9984 |     33.138 |   1.1079 |     36.795 |     1.3
   98 |   0.9979 |     33.263 |   1.1074 |     36.673 |     1.4
   99 |   0.9896 |     32.873 |   1.1051 |     36.581 |     1.4
  100 |   0.9895 |     32.705 |   1.1020 |     36.550 |     1.4
  101 |   0.9840 |     32.363 |   1.1051 |     36.734 |     1.4
  102 |   0.9795 |     32.493 |   1.1028 |     36.428 |     1.4
  103 |   0.9746 |     32.141 |   1.0980 |     36.581 |     1.4
  104 |   0.9685 |     32.174 |   1.0958 |     36.428 |     1.4
  105 |   0.9642 |     31.914 |   1.0955 |     36.121 |     1.4
  106 |   0.9599 |     32.163 |   1.0906 |     36.489 |     1.5
  107 |   0.9547 |     31.887 |   1.0899 |     36.581 |     1.5
  108 |   0.9508 |     31.540 |   1.0897 |     35.846 |     1.5
  109 |   0.9426 |     31.551 |   1.0859 |     36.458 |     1.5
  110 |   0.9388 |     31.193 |   1.0794 |     36.305 |     1.5
  111 |   0.9388 |     31.264 |   1.0840 |     36.091 |     1.5
  112 |   0.9324 |     30.695 |   1.0774 |     35.907 |     1.5
  113 |   0.9297 |     30.798 |   1.0794 |     35.846 |     1.6
  114 |   0.9272 |     30.836 |   1.0718 |     35.754 |     1.6
  115 |   0.9243 |     30.819 |   1.0809 |     36.152 |     1.6
  116 |   0.9196 |     30.592 |   1.0712 |     35.968 |     1.6
  117 |   0.9132 |     30.229 |   1.0638 |     35.784 |     1.6
  118 |   0.9107 |     30.239 |   1.0707 |     35.600 |     1.6
  119 |   0.9064 |     30.229 |   1.0684 |     36.060 |     1.6
  120 |   0.9034 |     30.109 |   1.0609 |     35.600 |     1.7
  121 |   0.9026 |     29.730 |   1.0559 |     35.233 |     1.7
  122 |   0.8982 |     29.752 |   1.0619 |     35.478 |     1.7
  123 |   0.8946 |     29.784 |   1.0625 |     35.386 |     1.7
  124 |   0.8921 |     29.546 |   1.0570 |     35.049 |     1.7
  125 |   0.8826 |     29.335 |   1.0540 |     34.743 |     1.7
  126 |   0.8813 |     29.145 |   1.0619 |     35.141 |     1.7
  127 |   0.8818 |     29.215 |   1.0460 |     34.589 |     1.7
  128 |   0.8766 |     29.042 |   1.0474 |     34.620 |     1.8
  129 |   0.8745 |     29.004 |   1.0408 |     34.528 |     1.8
  130 |   0.8681 |     28.831 |   1.0472 |     34.498 |     1.8
  131 |   0.8643 |     28.267 |   1.0416 |     34.406 |     1.8
  132 |   0.8648 |     28.338 |   1.0400 |     34.865 |     1.8
  133 |   0.8593 |     28.506 |   1.0368 |     34.804 |     1.8
  134 |   0.8542 |     28.240 |   1.0441 |     34.896 |     1.8
  135 |   0.8512 |     28.088 |   1.0412 |     34.436 |     1.9
  136 |   0.8497 |     28.218 |   1.0496 |     34.436 |     1.9
  137 |   0.8480 |     28.061 |   1.0361 |     34.283 |     1.9
  138 |   0.8469 |     27.877 |   1.0377 |     34.099 |     1.9
  139 |   0.8436 |     27.774 |   1.0302 |     34.069 |     1.9
  140 |   0.8374 |     27.536 |   1.0363 |     33.977 |     1.9
  141 |   0.8390 |     27.628 |   1.0273 |     33.732 |     1.9
  142 |   0.8386 |     27.606 |   1.0333 |     33.517 |     2.0
  143 |   0.8327 |     27.449 |   1.0353 |     33.946 |     2.0
  144 |   0.8288 |     27.324 |   1.0293 |     33.946 |     2.0
  145 |   0.8268 |     27.509 |   1.0293 |     33.548 |     2.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,357,538

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.0749 |     80.044 |   2.4859 |     60.600 |     0.0
    2 |   2.2553 |     58.772 |   2.0765 |     57.843 |     0.1
    3 |   1.9829 |     53.186 |   1.8803 |     48.192 |     0.1
    4 |   1.8151 |     48.824 |   1.7376 |     48.438 |     0.2
    5 |   1.6904 |     48.412 |   1.6238 |     48.039 |     0.2
    6 |   1.5947 |     48.039 |   1.5407 |     45.527 |     0.3
    7 |   1.5246 |     46.093 |   1.4863 |     45.129 |     0.3
    8 |   1.4753 |     45.714 |   1.4495 |     45.037 |     0.3
    9 |   1.4402 |     45.503 |   1.4224 |     45.374 |     0.4
   10 |   1.4095 |     45.107 |   1.3950 |     44.393 |     0.4
   11 |   1.3888 |     44.614 |   1.3824 |     44.485 |     0.5
   12 |   1.3666 |     44.576 |   1.3565 |     43.995 |     0.5
   13 |   1.3439 |     43.872 |   1.3434 |     43.627 |     0.5
   14 |   1.3218 |     43.466 |   1.3262 |     43.505 |     0.6
   15 |   1.3120 |     42.962 |   1.3121 |     42.279 |     0.6
   16 |   1.2935 |     42.550 |   1.3013 |     42.004 |     0.7
   17 |   1.2789 |     42.003 |   1.2850 |     42.126 |     0.7
   18 |   1.2610 |     41.423 |   1.2744 |     41.544 |     0.8
   19 |   1.2463 |     40.783 |   1.2633 |     41.728 |     0.8
   20 |   1.2301 |     40.269 |   1.2549 |     40.931 |     0.8
   21 |   1.2133 |     39.619 |   1.2342 |     39.614 |     0.9
   22 |   1.2020 |     38.795 |   1.2204 |     38.971 |     0.9
   23 |   1.1813 |     37.912 |   1.2063 |     39.032 |     1.0
   24 |   1.1589 |     37.137 |   1.1943 |     38.235 |     1.0
   25 |   1.1409 |     36.492 |   1.1773 |     38.113 |     1.1
   26 |   1.1242 |     35.620 |   1.1625 |     37.347 |     1.1
   27 |   1.1042 |     35.073 |   1.1483 |     36.979 |     1.1
   28 |   1.0830 |     34.330 |   1.1376 |     36.366 |     1.2
   29 |   1.0703 |     33.805 |   1.1218 |     36.581 |     1.2
   30 |   1.0516 |     33.100 |   1.1134 |     36.121 |     1.3
   31 |   1.0338 |     32.732 |   1.0977 |     35.662 |     1.3
   32 |   1.0212 |     32.309 |   1.0872 |     34.681 |     1.4
   33 |   1.0036 |     31.486 |   1.0777 |     34.896 |     1.4
   34 |   0.9868 |     31.161 |   1.0677 |     34.222 |     1.4
   35 |   0.9709 |     30.662 |   1.0554 |     34.069 |     1.5
   36 |   0.9576 |     30.120 |   1.0462 |     34.069 |     1.5
   37 |   0.9408 |     29.378 |   1.0359 |     33.180 |     1.6
   38 |   0.9251 |     28.955 |   1.0291 |     33.027 |     1.6
   39 |   0.9078 |     28.538 |   1.0153 |     31.679 |     1.6
   40 |   0.8932 |     28.067 |   1.0110 |     32.567 |     1.7
   41 |   0.8824 |     27.736 |   0.9978 |     32.016 |     1.7
   42 |   0.8682 |     26.961 |   0.9934 |     31.127 |     1.8
   43 |   0.8546 |     26.430 |   0.9836 |     31.097 |     1.8
   44 |   0.8433 |     26.149 |   0.9796 |     30.668 |     1.9
   45 |   0.8348 |     25.710 |   0.9670 |     30.392 |     1.9
   46 |   0.8160 |     25.271 |   0.9618 |     29.902 |     1.9
   47 |   0.8034 |     24.762 |   0.9582 |     30.515 |     2.0
   48 |   0.7909 |     24.572 |   0.9497 |     29.871 |     2.0
   49 |   0.7827 |     24.052 |   0.9476 |     29.871 |     2.1
   50 |   0.7662 |     23.564 |   0.9343 |     29.013 |     2.1
   51 |   0.7560 |     23.190 |   0.9330 |     28.707 |     2.2
   52 |   0.7446 |     22.779 |   0.9248 |     28.738 |     2.2
   53 |   0.7339 |     22.345 |   0.9189 |     28.248 |     2.2
   54 |   0.7192 |     21.939 |   0.9215 |     28.309 |     2.3
   55 |   0.7120 |     21.484 |   0.9144 |     28.278 |     2.3
   56 |   0.7000 |     21.099 |   0.9113 |     27.911 |     2.4
   57 |   0.6884 |     20.898 |   0.9051 |     28.248 |     2.4
   58 |   0.6790 |     20.362 |   0.9055 |     27.880 |     2.5
   59 |   0.6770 |     20.443 |   0.8964 |     27.482 |     2.5
   60 |   0.6621 |     20.048 |   0.8941 |     27.512 |     2.5
   61 |   0.6519 |     19.484 |   0.8892 |     27.574 |     2.6
   62 |   0.6430 |     19.457 |   0.8972 |     27.175 |     2.6
   63 |   0.6374 |     19.267 |   0.8830 |     27.237 |     2.7
   64 |   0.6241 |     19.029 |   0.8826 |     27.053 |     2.7
   65 |   0.6093 |     18.243 |   0.8751 |     26.532 |     2.8
   66 |   0.6009 |     18.314 |   0.8775 |     27.022 |     2.8
   67 |   0.5930 |     17.739 |   0.8820 |     26.777 |     2.8
   68 |   0.5890 |     18.048 |   0.8798 |     26.409 |     2.9
   69 |   0.5740 |     17.366 |   0.8760 |     26.593 |     2.9
   70 |   0.5699 |     17.089 |   0.8742 |     26.072 |     3.0
   71 |   0.5683 |     16.959 |   0.8705 |     26.593 |     3.0
   72 |   0.5603 |     16.835 |   0.8651 |     26.287 |     3.0
   73 |   0.5474 |     16.515 |   0.8612 |     26.287 |     3.1
   74 |   0.5378 |     16.298 |   0.8755 |     26.501 |     3.1
   75 |   0.5315 |     16.212 |   0.8687 |     26.072 |     3.2
   76 |   0.5264 |     16.022 |   0.8646 |     26.195 |     3.2
   77 |   0.5124 |     15.616 |   0.8593 |     25.919 |     3.3
   78 |   0.5114 |     15.410 |   0.8744 |     26.195 |     3.3
   79 |   0.5035 |     15.420 |   0.8637 |     26.562 |     3.3
   80 |   0.4954 |     15.068 |   0.8590 |     25.827 |     3.4
   81 |   0.4905 |     14.743 |   0.8613 |     25.613 |     3.4
   82 |   0.4861 |     14.581 |   0.8591 |     25.460 |     3.5
   83 |   0.4813 |     14.451 |   0.8611 |     25.490 |     3.5
   84 |   0.4716 |     14.234 |   0.8677 |     25.643 |     3.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,128,994

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1965 |     59.547 |   1.6255 |     48.346 |     0.0
    2 |   1.4954 |     45.855 |   1.4144 |     46.078 |     0.1
    3 |   1.3785 |     44.782 |   1.3357 |     43.536 |     0.1
    4 |   1.3138 |     43.075 |   1.2891 |     41.176 |     0.1
    5 |   1.2609 |     41.358 |   1.2373 |     40.165 |     0.2
    6 |   1.2035 |     39.093 |   1.1989 |     38.143 |     0.2
    7 |   1.1502 |     37.635 |   1.1550 |     36.336 |     0.3
    8 |   1.1022 |     36.210 |   1.1211 |     35.876 |     0.3
    9 |   1.0583 |     34.748 |   1.0940 |     35.754 |     0.3
   10 |   1.0114 |     33.555 |   1.0656 |     34.161 |     0.4
   11 |   0.9818 |     32.190 |   1.0574 |     34.651 |     0.4
   12 |   0.9484 |     31.513 |   1.0052 |     32.506 |     0.4
   13 |   0.9002 |     29.849 |   0.9877 |     32.475 |     0.5
   14 |   0.8638 |     29.031 |   0.9752 |     31.740 |     0.5
   15 |   0.8318 |     27.731 |   0.9586 |     31.618 |     0.6
   16 |   0.7918 |     26.062 |   0.9531 |     31.189 |     0.6
   17 |   0.7600 |     25.108 |   0.9595 |     31.832 |     0.6
   18 |   0.7348 |     24.431 |   0.9402 |     30.208 |     0.7
   19 |   0.7019 |     23.282 |   0.9188 |     29.167 |     0.7
   20 |   0.6774 |     22.502 |   0.9068 |     29.381 |     0.7
   21 |   0.6556 |     21.895 |   0.9035 |     28.799 |     0.8
   22 |   0.6274 |     20.969 |   0.9071 |     29.075 |     0.8
   23 |   0.6044 |     20.465 |   0.9112 |     29.657 |     0.8
   24 |   0.5887 |     19.836 |   0.9036 |     27.696 |     0.9
   25 |   0.5734 |     19.116 |   0.8959 |     28.094 |     0.9
   26 |   0.5478 |     18.298 |   0.8997 |     27.604 |     1.0
   27 |   0.5261 |     17.886 |   0.8961 |     27.451 |     1.0
   28 |   0.5124 |     17.317 |   0.9117 |     27.053 |     1.0
   29 |   0.4932 |     16.640 |   0.9016 |     27.145 |     1.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 420,834

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4057 |     84.184 |   3.1407 |     83.333 |     0.0
    2 |   2.8282 |     78.636 |   2.6155 |     66.912 |     0.0
    3 |   2.4651 |     62.554 |   2.3703 |     58.824 |     0.1
    4 |   2.2775 |     59.005 |   2.2196 |     58.272 |     0.1
    5 |   2.1479 |     58.106 |   2.0978 |     57.812 |     0.1
    6 |   2.0383 |     56.150 |   1.9966 |     54.841 |     0.1
    7 |   1.9483 |     52.330 |   1.9110 |     48.284 |     0.2
    8 |   1.8721 |     48.440 |   1.8367 |     48.254 |     0.2
    9 |   1.8025 |     48.586 |   1.7731 |     48.315 |     0.2
   10 |   1.7464 |     48.618 |   1.7155 |     48.315 |     0.2
   11 |   1.6949 |     48.607 |   1.6641 |     48.315 |     0.2
   12 |   1.6474 |     48.607 |   1.6178 |     48.315 |     0.3
   13 |   1.6045 |     46.624 |   1.5780 |     45.466 |     0.3
   14 |   1.5670 |     46.213 |   1.5454 |     45.466 |     0.3
   15 |   1.5399 |     46.213 |   1.5192 |     45.435 |     0.3
   16 |   1.5146 |     46.213 |   1.4979 |     45.466 |     0.4
   17 |   1.4912 |     46.213 |   1.4799 |     45.435 |     0.4
   18 |   1.4729 |     46.213 |   1.4666 |     45.435 |     0.4
   19 |   1.4576 |     46.207 |   1.4508 |     45.435 |     0.4
   20 |   1.4445 |     46.218 |   1.4391 |     45.435 |     0.4
   21 |   1.4335 |     46.207 |   1.4277 |     45.435 |     0.5
   22 |   1.4191 |     46.196 |   1.4198 |     45.435 |     0.5
   23 |   1.4092 |     46.196 |   1.4106 |     45.435 |     0.5
   24 |   1.4016 |     46.153 |   1.4062 |     45.190 |     0.5
   25 |   1.3918 |     45.963 |   1.3978 |     45.129 |     0.5
   26 |   1.3830 |     46.023 |   1.3876 |     44.945 |     0.6
   27 |   1.3756 |     45.709 |   1.3798 |     45.037 |     0.6
   28 |   1.3662 |     45.600 |   1.3742 |     44.945 |     0.6
   29 |   1.3560 |     45.308 |   1.3657 |     44.730 |     0.6
   30 |   1.3492 |     45.346 |   1.3611 |     45.221 |     0.7
   31 |   1.3393 |     45.161 |   1.3531 |     45.098 |     0.7
   32 |   1.3330 |     45.161 |   1.3460 |     45.251 |     0.7
   33 |   1.3243 |     44.750 |   1.3445 |     45.221 |     0.7
   34 |   1.3157 |     44.490 |   1.3367 |     44.853 |     0.7
   35 |   1.3066 |     44.246 |   1.3358 |     45.006 |     0.8
   36 |   1.2997 |     44.327 |   1.3278 |     44.455 |     0.8
   37 |   1.2898 |     44.034 |   1.3217 |     44.516 |     0.8
   38 |   1.2849 |     43.753 |   1.3176 |     44.363 |     0.8
   39 |   1.2770 |     43.634 |   1.3127 |     44.332 |     0.8
   40 |   1.2680 |     43.390 |   1.3072 |     43.903 |     0.9
   41 |   1.2619 |     43.146 |   1.3036 |     43.566 |     0.9
   42 |   1.2539 |     42.913 |   1.3036 |     43.811 |     0.9
   43 |   1.2451 |     42.832 |   1.2994 |     43.934 |     0.9
   44 |   1.2379 |     42.696 |   1.3015 |     44.179 |     1.0
   45 |   1.2313 |     42.458 |   1.2918 |     43.811 |     1.0
   46 |   1.2253 |     42.431 |   1.2880 |     44.148 |     1.0
   47 |   1.2191 |     42.290 |   1.2853 |     43.811 |     1.0
   48 |   1.2105 |     42.068 |   1.2839 |     43.566 |     1.0
   49 |   1.2034 |     41.894 |   1.2770 |     43.536 |     1.1
   50 |   1.1968 |     41.650 |   1.2748 |     43.107 |     1.1
   51 |   1.1901 |     41.369 |   1.2699 |     42.892 |     1.1
   52 |   1.1860 |     41.000 |   1.2739 |     43.107 |     1.1
   53 |   1.1766 |     40.648 |   1.2744 |     43.076 |     1.1
   54 |   1.1690 |     40.621 |   1.2682 |     43.137 |     1.2
   55 |   1.1651 |     40.323 |   1.2667 |     42.678 |     1.2
   56 |   1.1589 |     40.144 |   1.2605 |     42.647 |     1.2
   57 |   1.1489 |     39.754 |   1.2619 |     42.678 |     1.2
   58 |   1.1461 |     39.537 |   1.2617 |     42.157 |     1.3
   59 |   1.1383 |     39.174 |   1.2575 |     42.279 |     1.3
   60 |   1.1310 |     39.098 |   1.2503 |     41.759 |     1.3
   61 |   1.1214 |     38.752 |   1.2552 |     42.188 |     1.3
   62 |   1.1180 |     38.584 |   1.2457 |     42.004 |     1.3
   63 |   1.1104 |     38.389 |   1.2465 |     41.973 |     1.4
   64 |   1.1047 |     37.831 |   1.2406 |     42.034 |     1.4
   65 |   1.0955 |     37.939 |   1.2414 |     41.728 |     1.4
   66 |   1.0891 |     37.500 |   1.2500 |     41.912 |     1.4
   67 |   1.0819 |     36.936 |   1.2386 |     41.207 |     1.4
   68 |   1.0774 |     36.812 |   1.2396 |     40.993 |     1.5
   69 |   1.0705 |     36.145 |   1.2425 |     40.962 |     1.5
   70 |   1.0649 |     36.216 |   1.2343 |     41.268 |     1.5
   71 |   1.0548 |     35.647 |   1.2368 |     40.564 |     1.5
   72 |   1.0482 |     35.344 |   1.2405 |     41.391 |     1.5
   73 |   1.0435 |     35.208 |   1.2310 |     40.717 |     1.6
   74 |   1.0320 |     34.878 |   1.2317 |     40.962 |     1.6
   75 |   1.0236 |     34.612 |   1.2245 |     40.349 |     1.6
   76 |   1.0202 |     33.989 |   1.2265 |     40.288 |     1.6
   77 |   1.0139 |     34.141 |   1.2357 |     40.809 |     1.7
   78 |   1.0090 |     34.135 |   1.2238 |     39.767 |     1.7
   79 |   1.0068 |     33.523 |   1.2250 |     39.798 |     1.7
   80 |   1.0019 |     33.767 |   1.2192 |     38.940 |     1.7
   81 |   0.9898 |     32.754 |   1.2205 |     38.817 |     1.7
   82 |   0.9837 |     32.542 |   1.2160 |     39.093 |     1.8
   83 |   0.9753 |     32.564 |   1.2283 |     40.074 |     1.8
   84 |   0.9691 |     32.076 |   1.2201 |     39.491 |     1.8
   85 |   0.9666 |     32.331 |   1.2162 |     38.787 |     1.8
   86 |   0.9589 |     31.811 |   1.2223 |     39.001 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 185,186

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8775 |     77.790 |   2.3684 |     58.824 |     0.0
    2 |   2.1614 |     58.035 |   1.9906 |     58.793 |     0.0
    3 |   1.8557 |     52.346 |   1.7230 |     48.346 |     0.0
    4 |   1.6470 |     47.838 |   1.5668 |     45.466 |     0.1
    5 |   1.5327 |     46.223 |   1.4905 |     45.466 |     0.1
    6 |   1.4748 |     46.283 |   1.4518 |     45.466 |     0.1
    7 |   1.4491 |     46.332 |   1.4337 |     45.496 |     0.1
    8 |   1.4266 |     46.278 |   1.4146 |     45.466 |     0.1
    9 |   1.4134 |     46.229 |   1.4033 |     45.435 |     0.1
   10 |   1.4063 |     45.996 |   1.3992 |     45.190 |     0.1
   11 |   1.3923 |     45.947 |   1.3851 |     44.945 |     0.2
   12 |   1.3811 |     45.714 |   1.3722 |     44.975 |     0.2
   13 |   1.3682 |     45.454 |   1.3618 |     45.067 |     0.2
   14 |   1.3566 |     45.411 |   1.3506 |     45.006 |     0.2
   15 |   1.3401 |     45.340 |   1.3420 |     44.608 |     0.2
   16 |   1.3318 |     45.113 |   1.3302 |     44.455 |     0.2
   17 |   1.3144 |     44.750 |   1.3216 |     44.853 |     0.2
   18 |   1.3008 |     44.701 |   1.3057 |     44.271 |     0.2
   19 |   1.2866 |     44.121 |   1.2862 |     43.444 |     0.3
   20 |   1.2675 |     43.411 |   1.2722 |     43.658 |     0.3
   21 |   1.2507 |     43.043 |   1.2707 |     43.413 |     0.3
   22 |   1.2372 |     42.414 |   1.2509 |     42.218 |     0.3
   23 |   1.2282 |     42.246 |   1.2397 |     42.126 |     0.3
   24 |   1.2170 |     41.992 |   1.2306 |     42.096 |     0.3
   25 |   1.2020 |     41.369 |   1.2206 |     41.330 |     0.3
   26 |   1.1918 |     40.865 |   1.2217 |     41.605 |     0.4
   27 |   1.1792 |     40.041 |   1.2060 |     40.104 |     0.4
   28 |   1.1809 |     39.938 |   1.2015 |     40.288 |     0.4
   29 |   1.1652 |     39.526 |   1.1972 |     40.165 |     0.4
   30 |   1.1432 |     38.660 |   1.1888 |     39.706 |     0.4
   31 |   1.1275 |     38.253 |   1.1719 |     38.511 |     0.4
   32 |   1.1161 |     37.668 |   1.1654 |     38.358 |     0.4
   33 |   1.1006 |     37.213 |   1.1613 |     38.082 |     0.4
   34 |   1.0945 |     36.882 |   1.1541 |     38.695 |     0.5
   35 |   1.0856 |     36.774 |   1.1403 |     37.806 |     0.5
   36 |   1.0699 |     36.032 |   1.1342 |     38.082 |     0.5
   37 |   1.0613 |     35.988 |   1.1247 |     38.051 |     0.5
   38 |   1.0489 |     35.750 |   1.1197 |     37.806 |     0.5
   39 |   1.0346 |     35.176 |   1.1162 |     37.347 |     0.5
   40 |   1.0226 |     34.390 |   1.1052 |     36.949 |     0.5
   41 |   1.0235 |     34.330 |   1.1095 |     37.163 |     0.6
   42 |   1.0090 |     33.891 |   1.0997 |     36.458 |     0.6
   43 |   0.9949 |     33.268 |   1.0971 |     36.673 |     0.6
   44 |   0.9875 |     33.236 |   1.0802 |     36.703 |     0.6
   45 |   0.9727 |     32.586 |   1.0771 |     36.121 |     0.6
   46 |   0.9647 |     32.114 |   1.0716 |     35.662 |     0.6
   47 |   0.9576 |     31.795 |   1.0657 |     35.447 |     0.6
   48 |   0.9425 |     31.681 |   1.0745 |     35.294 |     0.6
   49 |   0.9412 |     31.453 |   1.0611 |     35.080 |     0.7
   50 |   0.9274 |     31.003 |   1.0441 |     34.559 |     0.7
   51 |   0.9147 |     30.516 |   1.0508 |     35.447 |     0.7
   52 |   0.9123 |     30.586 |   1.0486 |     34.743 |     0.7
   53 |   0.8975 |     30.185 |   1.0392 |     33.885 |     0.7
   54 |   0.8843 |     29.503 |   1.0303 |     33.548 |     0.7
   55 |   0.8825 |     29.546 |   1.0397 |     33.854 |     0.7
   56 |   0.8748 |     29.156 |   1.0237 |     32.721 |     0.8
   57 |   0.8666 |     28.842 |   1.0329 |     32.843 |     0.8
   58 |   0.8613 |     28.630 |   1.0204 |     33.027 |     0.8
   59 |   0.8559 |     28.381 |   1.0246 |     32.996 |     0.8
   60 |   0.8441 |     27.910 |   1.0236 |     33.425 |     0.8
   61 |   0.8445 |     28.343 |   1.0221 |     33.150 |     0.8
   62 |   0.8367 |     27.725 |   1.0188 |     32.904 |     0.8
   63 |   0.8250 |     27.476 |   1.0223 |     33.609 |     0.8
   64 |   0.8219 |     27.460 |   1.0212 |     32.751 |     0.9
   65 |   0.8067 |     26.452 |   1.0269 |     33.272 |     0.9
   66 |   0.8276 |     27.303 |   1.0190 |     32.384 |     0.9
   67 |   0.7949 |     26.111 |   1.0090 |     32.138 |     0.9
   68 |   0.7903 |     26.474 |   1.0080 |     31.801 |     0.9
   69 |   0.7823 |     25.910 |   1.0164 |     32.384 |     0.9
   70 |   0.7765 |     25.553 |   1.0120 |     32.077 |     0.9
   71 |   0.7651 |     25.564 |   1.0071 |     32.077 |     1.0
   72 |   0.7609 |     25.000 |   1.0167 |     32.230 |     1.0
   73 |   0.7600 |     25.049 |   1.0232 |     31.985 |     1.0
   74 |   0.7599 |     25.309 |   1.0041 |     32.077 |     1.0
   75 |   0.7484 |     24.680 |   1.0008 |     31.066 |     1.0
   76 |   0.7458 |     24.523 |   1.0047 |     31.526 |     1.0
   77 |   0.7370 |     24.653 |   1.0044 |     31.679 |     1.0
   78 |   0.7299 |     23.911 |   1.0090 |     31.771 |     1.1
   79 |   0.7206 |     23.607 |   1.0100 |     31.648 |     1.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 554,370

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4255 |     87.841 |   3.2482 |     83.058 |     0.0
    2 |   3.0351 |     83.268 |   2.8659 |     83.333 |     0.0
    3 |   2.7610 |     80.099 |   2.6869 |     67.953 |     0.1
    4 |   2.6183 |     67.561 |   2.5727 |     66.483 |     0.1
    5 |   2.5213 |     65.653 |   2.4908 |     65.839 |     0.1
    6 |   2.4485 |     63.594 |   2.4253 |     63.113 |     0.2
    7 |   2.3889 |     61.871 |   2.3678 |     60.539 |     0.2
    8 |   2.3352 |     60.409 |   2.3161 |     59.344 |     0.2
    9 |   2.2867 |     59.688 |   2.2666 |     58.915 |     0.2
   10 |   2.2338 |     59.086 |   2.2024 |     58.732 |     0.3
   11 |   2.1679 |     58.783 |   2.1443 |     58.640 |     0.3
   12 |   2.1206 |     58.604 |   2.0980 |     58.548 |     0.3
   13 |   2.0751 |     58.453 |   2.0543 |     58.609 |     0.3
   14 |   2.0346 |     56.865 |   2.0121 |     55.576 |     0.4
   15 |   1.9903 |     53.809 |   1.9702 |     52.298 |     0.4
   16 |   1.9518 |     52.574 |   1.9273 |     52.451 |     0.4
   17 |   1.9075 |     52.016 |   1.8823 |     51.746 |     0.4
   18 |   1.8660 |     51.604 |   1.8411 |     51.562 |     0.5
   19 |   1.8256 |     51.094 |   1.8032 |     51.287 |     0.5
   20 |   1.7879 |     50.851 |   1.7666 |     50.613 |     0.5
   21 |   1.7525 |     50.515 |   1.7311 |     50.551 |     0.5
   22 |   1.7205 |     49.957 |   1.7000 |     49.540 |     0.6
   23 |   1.6876 |     49.371 |   1.6734 |     48.192 |     0.6
   24 |   1.6632 |     48.542 |   1.6446 |     47.549 |     0.6
   25 |   1.6360 |     47.648 |   1.6218 |     46.844 |     0.6
   26 |   1.6141 |     47.015 |   1.6027 |     45.864 |     0.7
   27 |   1.5907 |     46.679 |   1.5788 |     46.017 |     0.7
   28 |   1.5724 |     46.223 |   1.5610 |     45.741 |     0.7
   29 |   1.5529 |     45.969 |   1.5450 |     45.374 |     0.7
   30 |   1.5321 |     45.530 |   1.5284 |     44.669 |     0.8
   31 |   1.5151 |     45.015 |   1.5121 |     44.577 |     0.8
   32 |   1.5004 |     44.966 |   1.4965 |     44.424 |     0.8
   33 |   1.4825 |     44.587 |   1.4830 |     44.179 |     0.8
   34 |   1.4716 |     44.365 |   1.4726 |     43.934 |     0.9
   35 |   1.4592 |     44.376 |   1.4581 |     43.873 |     0.9
   36 |   1.4459 |     43.856 |   1.4489 |     43.781 |     0.9
   37 |   1.4320 |     43.780 |   1.4396 |     44.179 |     0.9
   38 |   1.4269 |     43.812 |   1.4299 |     43.842 |     1.0
   39 |   1.4106 |     43.753 |   1.4189 |     43.811 |     1.0
   40 |   1.4013 |     43.406 |   1.4096 |     43.750 |     1.0
   41 |   1.3909 |     43.168 |   1.4002 |     43.873 |     1.0
   42 |   1.3791 |     43.151 |   1.3899 |     43.566 |     1.1
   43 |   1.3694 |     42.864 |   1.3815 |     43.352 |     1.1
   44 |   1.3586 |     42.528 |   1.3795 |     42.923 |     1.1
   45 |   1.3510 |     42.306 |   1.3676 |     42.433 |     1.1
   46 |   1.3435 |     41.986 |   1.3601 |     41.759 |     1.2
   47 |   1.3336 |     41.840 |   1.3535 |     42.341 |     1.2
   48 |   1.3245 |     41.531 |   1.3463 |     42.371 |     1.2
   49 |   1.3108 |     41.336 |   1.3393 |     42.096 |     1.2
   50 |   1.3032 |     41.374 |   1.3329 |     42.034 |     1.3
   51 |   1.2982 |     41.255 |   1.3269 |     41.575 |     1.3
   52 |   1.2905 |     41.136 |   1.3216 |     41.820 |     1.3
   53 |   1.2799 |     41.054 |   1.3177 |     42.126 |     1.3
   54 |   1.2848 |     41.363 |   1.3093 |     41.912 |     1.4
   55 |   1.2745 |     40.946 |   1.3021 |     41.789 |     1.4
   56 |   1.2608 |     40.616 |   1.2976 |     41.728 |     1.4
   57 |   1.2524 |     40.588 |   1.2895 |     41.912 |     1.4
   58 |   1.2472 |     40.215 |   1.2894 |     41.575 |     1.5
   59 |   1.2391 |     39.954 |   1.2766 |     40.778 |     1.5
   60 |   1.2303 |     39.602 |   1.2737 |     40.717 |     1.5
   61 |   1.2223 |     39.721 |   1.2691 |     40.748 |     1.5
   62 |   1.2164 |     39.510 |   1.2676 |     41.054 |     1.6
   63 |   1.2137 |     39.364 |   1.2638 |     40.472 |     1.6
   64 |   1.2096 |     39.391 |   1.2588 |     40.319 |     1.6
   65 |   1.2037 |     38.958 |   1.2506 |     39.767 |     1.6
   66 |   1.1966 |     38.903 |   1.2554 |     40.564 |     1.7
   67 |   1.1865 |     39.012 |   1.2447 |     40.012 |     1.7
   68 |   1.1813 |     38.546 |   1.2379 |     40.043 |     1.7
   69 |   1.1760 |     38.502 |   1.2384 |     40.165 |     1.7
   70 |   1.1680 |     38.166 |   1.2417 |     39.982 |     1.8
   71 |   1.1644 |     37.923 |   1.2320 |     39.859 |     1.8
   72 |   1.1582 |     37.988 |   1.2263 |     39.308 |     1.8
   73 |   1.1524 |     37.722 |   1.2286 |     39.951 |     1.8
   74 |   1.1487 |     37.820 |   1.2225 |     39.461 |     1.9
   75 |   1.1427 |     37.305 |   1.2169 |     39.461 |     1.9
   76 |   1.1352 |     37.202 |   1.2158 |     39.216 |     1.9
   77 |   1.1309 |     36.936 |   1.2126 |     39.185 |     1.9
   78 |   1.1221 |     36.628 |   1.2089 |     39.093 |     2.0
   79 |   1.1204 |     36.611 |   1.2059 |     39.154 |     2.0
   80 |   1.1172 |     36.238 |   1.2096 |     39.246 |     2.0
   81 |   1.1086 |     36.433 |   1.2057 |     39.032 |     2.0
   82 |   1.1082 |     36.118 |   1.2054 |     39.246 |     2.1
   83 |   1.0985 |     35.782 |   1.1958 |     38.603 |     2.1
   84 |   1.0907 |     35.967 |   1.1934 |     39.001 |     2.1
   85 |   1.0867 |     35.528 |   1.1960 |     38.695 |     2.1
   86 |   1.0819 |     35.642 |   1.1891 |     38.848 |     2.2
   87 |   1.0784 |     35.533 |   1.1907 |     39.032 |     2.2
   88 |   1.0757 |     35.170 |   1.1892 |     38.848 |     2.2
   89 |   1.0687 |     35.024 |   1.1826 |     38.542 |     2.2
   90 |   1.0639 |     34.861 |   1.1854 |     38.297 |     2.3
   91 |   1.0607 |     34.731 |   1.1828 |     38.511 |     2.3
   92 |   1.0571 |     34.910 |   1.1741 |     38.327 |     2.3
   93 |   1.0488 |     34.520 |   1.1745 |     38.327 |     2.3
   94 |   1.0440 |     34.227 |   1.1781 |     38.082 |     2.4
   95 |   1.0371 |     34.086 |   1.1742 |     38.113 |     2.4
   96 |   1.0357 |     33.810 |   1.1712 |     38.419 |     2.4
   97 |   1.0293 |     33.593 |   1.1664 |     37.868 |     2.4
   98 |   1.0231 |     33.583 |   1.1679 |     38.205 |     2.5
   99 |   1.0195 |     33.344 |   1.1650 |     37.776 |     2.5
  100 |   1.0178 |     33.236 |   1.1616 |     37.684 |     2.5
  101 |   1.0109 |     32.911 |   1.1580 |     37.561 |     2.5
  102 |   1.0082 |     32.878 |   1.1614 |     37.714 |     2.6
  103 |   1.0042 |     32.694 |   1.1626 |     37.408 |     2.6
  104 |   0.9968 |     32.298 |   1.1589 |     37.500 |     2.6
  105 |   0.9925 |     32.114 |   1.1585 |     37.132 |     2.6
  106 |   0.9872 |     32.244 |   1.1570 |     37.071 |     2.7
  107 |   0.9894 |     32.223 |   1.1511 |     36.918 |     2.7
  108 |   0.9834 |     32.017 |   1.1455 |     36.642 |     2.7
  109 |   0.9753 |     31.838 |   1.1461 |     37.224 |     2.7
  110 |   0.9770 |     31.892 |   1.1509 |     36.887 |     2.8
  111 |   0.9695 |     31.616 |   1.1435 |     36.765 |     2.8
  112 |   0.9633 |     31.404 |   1.1442 |     36.795 |     2.8
  113 |   0.9626 |     31.247 |   1.1500 |     36.397 |     2.8
  114 |   0.9543 |     31.134 |   1.1442 |     36.703 |     2.9
  115 |   0.9499 |     30.754 |   1.1477 |     36.826 |     2.9
  116 |   0.9532 |     31.112 |   1.1340 |     36.213 |     2.9
  117 |   0.9461 |     30.857 |   1.1360 |     36.060 |     2.9
  118 |   0.9408 |     30.413 |   1.1378 |     36.091 |     3.0
  119 |   0.9412 |     30.472 |   1.1386 |     36.397 |     3.0
  120 |   0.9368 |     30.402 |   1.1347 |     35.815 |     3.0
  121 |   0.9298 |     29.979 |   1.1304 |     36.029 |     3.0
  122 |   0.9276 |     30.147 |   1.1315 |     35.754 |     3.1
  123 |   0.9239 |     29.752 |   1.1262 |     35.876 |     3.1
  124 |   0.9181 |     29.898 |   1.1288 |     36.305 |     3.1
  125 |   0.9131 |     29.578 |   1.1238 |     35.325 |     3.1
  126 |   0.9099 |     29.643 |   1.1219 |     35.938 |     3.2
  127 |   0.9075 |     29.373 |   1.1238 |     35.202 |     3.2
  128 |   0.9053 |     29.156 |   1.1127 |     35.386 |     3.2
  129 |   0.9029 |     28.977 |   1.1241 |     35.509 |     3.2
  130 |   0.8948 |     28.647 |   1.1219 |     34.957 |     3.3
  131 |   0.8941 |     28.760 |   1.1163 |     35.018 |     3.3
  132 |   0.8893 |     28.527 |   1.1172 |     34.865 |     3.3
  133 |   0.8869 |     28.435 |   1.1088 |     35.141 |     3.3
  134 |   0.8822 |     28.153 |   1.1170 |     35.233 |     3.4
  135 |   0.8791 |     28.153 |   1.1113 |     34.252 |     3.4
  136 |   0.8726 |     28.056 |   1.1158 |     34.773 |     3.4
  137 |   0.8719 |     27.731 |   1.1125 |     34.528 |     3.4
  138 |   0.8683 |     27.590 |   1.1070 |     34.498 |     3.5
  139 |   0.8662 |     27.769 |   1.1074 |     34.222 |     3.5
  140 |   0.8583 |     27.254 |   1.1091 |     34.681 |     3.5
  141 |   0.8581 |     27.503 |   1.1036 |     34.559 |     3.5
  142 |   0.8503 |     27.270 |   1.1076 |     34.559 |     3.6
  143 |   0.8499 |     26.858 |   1.1080 |     34.681 |     3.6
  144 |   0.8539 |     27.297 |   1.1067 |     34.069 |     3.6
  145 |   0.8450 |     26.880 |   1.1006 |     34.252 |     3.6
  146 |   0.8398 |     26.999 |   1.0986 |     34.069 |     3.7
  147 |   0.8374 |     26.783 |   1.1020 |     34.161 |     3.7
  148 |   0.8370 |     26.430 |   1.0979 |     33.609 |     3.7
  149 |   0.8362 |     26.620 |   1.0999 |     34.099 |     3.7
  150 |   0.8279 |     26.252 |   1.0989 |     34.222 |     3.8
  151 |   0.8285 |     26.094 |   1.0878 |     33.180 |     3.8
  152 |   0.8237 |     26.002 |   1.0940 |     33.762 |     3.8
  153 |   0.8131 |     25.731 |   1.0957 |     33.701 |     3.8
  154 |   0.8224 |     25.780 |   1.0998 |     33.425 |     3.9
  155 |   0.8106 |     25.661 |   1.1006 |     33.395 |     3.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 551,330

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1928 |     81.735 |   2.6892 |     81.036 |     0.0
    2 |   2.4335 |     62.858 |   2.2312 |     58.762 |     0.0
    3 |   2.0962 |     57.244 |   1.9850 |     53.156 |     0.1
    4 |   1.8969 |     50.347 |   1.8053 |     48.560 |     0.1
    5 |   1.7432 |     49.014 |   1.6725 |     48.346 |     0.1
    6 |   1.6311 |     48.651 |   1.5792 |     46.783 |     0.1
    7 |   1.5545 |     46.391 |   1.5176 |     45.496 |     0.2
    8 |   1.4986 |     46.207 |   1.4728 |     45.496 |     0.2
    9 |   1.4596 |     45.963 |   1.4410 |     45.404 |     0.2
   10 |   1.4264 |     45.741 |   1.4147 |     45.037 |     0.2
   11 |   1.4025 |     45.324 |   1.3953 |     44.393 |     0.3
   12 |   1.3803 |     45.004 |   1.3750 |     44.210 |     0.3
   13 |   1.3633 |     44.072 |   1.3576 |     42.831 |     0.3
   14 |   1.3426 |     42.978 |   1.3402 |     42.341 |     0.3
   15 |   1.3241 |     42.490 |   1.3285 |     41.850 |     0.4
   16 |   1.3092 |     41.753 |   1.3129 |     41.636 |     0.4
   17 |   1.2900 |     40.924 |   1.3003 |     40.564 |     0.4
   18 |   1.2775 |     40.540 |   1.2862 |     40.472 |     0.4
   19 |   1.2617 |     40.155 |   1.2735 |     39.491 |     0.4
   20 |   1.2453 |     39.721 |   1.2630 |     39.277 |     0.5
   21 |   1.2291 |     39.163 |   1.2476 |     39.430 |     0.5
   22 |   1.2172 |     38.762 |   1.2345 |     38.940 |     0.5
   23 |   1.2018 |     38.362 |   1.2279 |     38.695 |     0.5
   24 |   1.1882 |     38.118 |   1.2157 |     38.143 |     0.6
   25 |   1.1732 |     37.646 |   1.2064 |     37.960 |     0.6
   26 |   1.1628 |     37.278 |   1.2033 |     38.297 |     0.6
   27 |   1.1509 |     37.316 |   1.1879 |     37.714 |     0.6
   28 |   1.1380 |     36.568 |   1.1808 |     37.561 |     0.7
   29 |   1.1247 |     36.335 |   1.1730 |     37.010 |     0.7
   30 |   1.1140 |     36.243 |   1.1677 |     37.163 |     0.7
   31 |   1.1019 |     35.598 |   1.1551 |     37.040 |     0.7
   32 |   1.0905 |     35.279 |   1.1528 |     37.132 |     0.8
   33 |   1.0817 |     34.943 |   1.1442 |     36.642 |     0.8
   34 |   1.0718 |     34.623 |   1.1433 |     36.765 |     0.8
   35 |   1.0573 |     33.951 |   1.1349 |     36.244 |     0.8
   36 |   1.0473 |     33.783 |   1.1220 |     35.754 |     0.8
   37 |   1.0362 |     33.236 |   1.1136 |     35.907 |     0.9
   38 |   1.0271 |     32.992 |   1.1080 |     35.233 |     0.9
   39 |   1.0142 |     32.531 |   1.0974 |     35.018 |     0.9
   40 |   1.0032 |     32.271 |   1.0934 |     34.835 |     0.9
   41 |   0.9933 |     31.784 |   1.0890 |     34.926 |     1.0
   42 |   0.9873 |     31.675 |   1.0786 |     34.406 |     1.0
   43 |   0.9756 |     31.274 |   1.0703 |     34.773 |     1.0
   44 |   0.9609 |     30.597 |   1.0764 |     35.080 |     1.0
   45 |   0.9544 |     30.505 |   1.0665 |     34.344 |     1.1
   46 |   0.9440 |     30.142 |   1.0647 |     34.681 |     1.1
   47 |   0.9305 |     29.774 |   1.0549 |     33.946 |     1.1
   48 |   0.9198 |     29.383 |   1.0577 |     33.824 |     1.1
   49 |   0.9114 |     29.069 |   1.0487 |     33.456 |     1.1
   50 |   0.8999 |     28.484 |   1.0419 |     33.395 |     1.2
   51 |   0.8909 |     28.581 |   1.0337 |     33.150 |     1.2
   52 |   0.8832 |     28.137 |   1.0337 |     32.874 |     1.2
   53 |   0.8708 |     27.736 |   1.0306 |     32.537 |     1.2
   54 |   0.8655 |     27.650 |   1.0211 |     32.659 |     1.3
   55 |   0.8535 |     27.081 |   1.0174 |     32.782 |     1.3
   56 |   0.8443 |     26.864 |   1.0184 |     32.230 |     1.3
   57 |   0.8394 |     26.474 |   1.0104 |     31.893 |     1.3
   58 |   0.8253 |     26.024 |   1.0064 |     32.138 |     1.4
   59 |   0.8190 |     25.927 |   1.0122 |     31.985 |     1.4
   60 |   0.8156 |     25.824 |   1.0085 |     31.679 |     1.4
   61 |   0.8014 |     25.298 |   0.9974 |     31.097 |     1.4
   62 |   0.7949 |     24.859 |   0.9988 |     31.771 |     1.5
   63 |   0.7873 |     24.285 |   0.9907 |     31.311 |     1.5
   64 |   0.7729 |     23.884 |   0.9900 |     31.219 |     1.5
   65 |   0.7718 |     23.840 |   0.9848 |     31.097 |     1.5
   66 |   0.7598 |     23.645 |   0.9783 |     31.005 |     1.5
   67 |   0.7535 |     23.515 |   0.9709 |     30.607 |     1.6
   68 |   0.7433 |     22.800 |   0.9788 |     30.790 |     1.6
   69 |   0.7398 |     23.039 |   0.9783 |     30.545 |     1.6
   70 |   0.7322 |     22.280 |   0.9690 |     30.362 |     1.6
   71 |   0.7267 |     22.199 |   0.9698 |     30.331 |     1.7
   72 |   0.7145 |     21.711 |   0.9673 |     30.607 |     1.7
   73 |   0.7075 |     21.733 |   0.9639 |     30.362 |     1.7
   74 |   0.6960 |     21.408 |   0.9618 |     30.025 |     1.7
   75 |   0.6903 |     21.110 |   0.9592 |     29.933 |     1.8
   76 |   0.6860 |     21.332 |   0.9588 |     30.147 |     1.8
   77 |   0.6763 |     20.844 |   0.9541 |     29.596 |     1.8
   78 |   0.6707 |     20.459 |   0.9520 |     29.718 |     1.8
   79 |   0.6654 |     20.129 |   0.9590 |     29.657 |     1.8
   80 |   0.6553 |     20.031 |   0.9568 |     29.871 |     1.9
   81 |   0.6508 |     19.674 |   0.9518 |     29.412 |     1.9
   82 |   0.6415 |     19.332 |   0.9477 |     29.105 |     1.9
   83 |   0.6381 |     19.311 |   0.9491 |     29.504 |     1.9
   84 |   0.6315 |     19.295 |   0.9415 |     28.952 |     2.0
   85 |   0.6255 |     18.861 |   0.9451 |     29.136 |     2.0
   86 |   0.6159 |     18.807 |   0.9400 |     29.167 |     2.0
   87 |   0.6137 |     18.742 |   0.9423 |     29.075 |     2.0
   88 |   0.6075 |     18.617 |   0.9470 |     29.044 |     2.1
   89 |   0.5971 |     18.287 |   0.9367 |     28.799 |     2.1
   90 |   0.5938 |     18.173 |   0.9380 |     28.615 |     2.1
   91 |   0.5874 |     17.929 |   0.9407 |     28.646 |     2.1
   92 |   0.5780 |     17.658 |   0.9441 |     29.044 |     2.2
   93 |   0.5779 |     17.696 |   0.9394 |     28.217 |     2.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 333,218

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5609 |     98.234 |   3.5127 |     97.089 |     0.0
    2 |   3.3436 |     85.988 |   3.1004 |     83.333 |     0.0
    3 |   2.9392 |     83.322 |   2.8258 |     83.333 |     0.1
    4 |   2.7385 |     77.449 |   2.6689 |     67.341 |     0.1
    5 |   2.6011 |     67.171 |   2.5449 |     66.912 |     0.1
    6 |   2.4953 |     63.958 |   2.4498 |     66.912 |     0.1
    7 |   2.4122 |     61.091 |   2.3743 |     58.824 |     0.2
    8 |   2.3513 |     59.303 |   2.3135 |     58.824 |     0.2
    9 |   2.2948 |     58.940 |   2.2608 |     58.824 |     0.2
   10 |   2.2445 |     58.398 |   2.2151 |     58.824 |     0.2
   11 |   2.2017 |     58.263 |   2.1721 |     58.824 |     0.3
   12 |   2.1608 |     58.003 |   2.1312 |     58.824 |     0.3
   13 |   2.1242 |     57.878 |   2.0941 |     58.824 |     0.3
   14 |   2.0890 |     57.434 |   2.0569 |     58.824 |     0.3
   15 |   2.0542 |     57.266 |   2.0200 |     58.824 |     0.3
   16 |   2.0169 |     56.897 |   1.9798 |     58.824 |     0.4
   17 |   1.9779 |     56.274 |   1.9376 |     58.517 |     0.4
   18 |   1.9384 |     54.719 |   1.8993 |     48.376 |     0.4
   19 |   1.9011 |     52.845 |   1.8647 |     48.346 |     0.4
   20 |   1.8657 |     51.273 |   1.8329 |     48.376 |     0.5
   21 |   1.8370 |     50.601 |   1.8025 |     48.346 |     0.5
   22 |   1.8046 |     50.417 |   1.7726 |     48.376 |     0.5
   23 |   1.7770 |     49.935 |   1.7449 |     48.346 |     0.5
   24 |   1.7491 |     49.729 |   1.7187 |     48.346 |     0.5
   25 |   1.7246 |     49.599 |   1.6949 |     48.346 |     0.6
   26 |   1.6979 |     48.987 |   1.6726 |     48.346 |     0.6
   27 |   1.6789 |     47.703 |   1.6507 |     45.466 |     0.6
   28 |   1.6560 |     46.717 |   1.6306 |     45.466 |     0.6
   29 |   1.6361 |     46.408 |   1.6120 |     45.466 |     0.7
   30 |   1.6207 |     46.337 |   1.5952 |     45.466 |     0.7
   31 |   1.6017 |     46.294 |   1.5798 |     45.466 |     0.7
   32 |   1.5876 |     46.256 |   1.5656 |     45.466 |     0.7
   33 |   1.5719 |     46.229 |   1.5530 |     45.466 |     0.7
   34 |   1.5623 |     46.223 |   1.5414 |     45.466 |     0.8
   35 |   1.5470 |     46.234 |   1.5311 |     45.466 |     0.8
   36 |   1.5373 |     46.213 |   1.5214 |     45.466 |     0.8
   37 |   1.5338 |     46.245 |   1.5124 |     45.466 |     0.8
   38 |   1.5225 |     46.202 |   1.5034 |     45.466 |     0.9
   39 |   1.5098 |     46.218 |   1.4940 |     45.466 |     0.9
   40 |   1.4973 |     46.218 |   1.4822 |     45.466 |     0.9
   41 |   1.4872 |     46.240 |   1.4707 |     45.466 |     0.9
   42 |   1.4752 |     46.207 |   1.4579 |     45.466 |     0.9
   43 |   1.4616 |     46.207 |   1.4464 |     45.466 |     1.0
   44 |   1.4508 |     46.213 |   1.4362 |     45.466 |     1.0
   45 |   1.4376 |     46.148 |   1.4274 |     45.466 |     1.0
   46 |   1.4300 |     46.278 |   1.4213 |     45.466 |     1.0
   47 |   1.4200 |     46.229 |   1.4117 |     45.466 |     1.1
   48 |   1.4124 |     46.272 |   1.4054 |     45.466 |     1.1
   49 |   1.4040 |     46.186 |   1.3994 |     45.466 |     1.1
   50 |   1.3948 |     45.703 |   1.3908 |     44.608 |     1.1
   51 |   1.3895 |     45.281 |   1.3842 |     44.179 |     1.1
   52 |   1.3787 |     45.048 |   1.3779 |     44.026 |     1.2
   53 |   1.3701 |     44.842 |   1.3700 |     44.056 |     1.2
   54 |   1.3651 |     44.728 |   1.3664 |     44.026 |     1.2
   55 |   1.3580 |     44.463 |   1.3637 |     43.995 |     1.2
   56 |   1.3546 |     44.251 |   1.3572 |     43.811 |     1.3
   57 |   1.3464 |     44.148 |   1.3527 |     43.689 |     1.3
   58 |   1.3416 |     44.224 |   1.3485 |     43.689 |     1.3
   59 |   1.3363 |     44.164 |   1.3438 |     43.566 |     1.3
   60 |   1.3297 |     43.731 |   1.3388 |     43.566 |     1.3
   61 |   1.3254 |     43.666 |   1.3373 |     43.597 |     1.4
   62 |   1.3193 |     43.617 |   1.3339 |     43.444 |     1.4
   63 |   1.3181 |     43.173 |   1.3292 |     43.505 |     1.4
   64 |   1.3109 |     43.368 |   1.3290 |     42.555 |     1.4
   65 |   1.3105 |     43.027 |   1.3252 |     42.494 |     1.5
   66 |   1.3082 |     43.005 |   1.3220 |     43.413 |     1.5
   67 |   1.3015 |     42.669 |   1.3174 |     43.413 |     1.5
   68 |   1.2982 |     42.761 |   1.3167 |     42.800 |     1.5
   69 |   1.2936 |     42.512 |   1.3129 |     43.137 |     1.5
   70 |   1.2897 |     42.414 |   1.3128 |     42.984 |     1.6
   71 |   1.2824 |     42.328 |   1.3101 |     42.831 |     1.6
   72 |   1.2819 |     42.192 |   1.3092 |     42.953 |     1.6
   73 |   1.2765 |     42.208 |   1.3072 |     43.076 |     1.6
   74 |   1.2726 |     42.051 |   1.3049 |     42.984 |     1.7
   75 |   1.2705 |     41.851 |   1.3026 |     42.831 |     1.7
   76 |   1.2691 |     41.976 |   1.2998 |     42.892 |     1.7
   77 |   1.2643 |     41.856 |   1.2967 |     42.862 |     1.7
   78 |   1.2640 |     41.694 |   1.2945 |     42.096 |     1.7
   79 |   1.2608 |     41.726 |   1.2953 |     42.831 |     1.8
   80 |   1.2569 |     41.775 |   1.2946 |     43.199 |     1.8
   81 |   1.2538 |     41.477 |   1.2917 |     43.045 |     1.8
   82 |   1.2477 |     41.482 |   1.2865 |     42.096 |     1.8
   83 |   1.2502 |     41.304 |   1.2907 |     43.107 |     1.9
   84 |   1.2426 |     41.190 |   1.2893 |     43.137 |     1.9
   85 |   1.2430 |     41.136 |   1.2889 |     42.862 |     1.9
   86 |   1.2368 |     41.136 |   1.2862 |     42.800 |     1.9
   87 |   1.2399 |     41.130 |   1.2825 |     42.739 |     1.9
   88 |   1.2311 |     41.054 |   1.2789 |     42.953 |     2.0
   89 |   1.2278 |     40.962 |   1.2818 |     42.892 |     2.0
   90 |   1.2271 |     40.610 |   1.2772 |     42.831 |     2.0
   91 |   1.2240 |     40.556 |   1.2754 |     42.525 |     2.0
   92 |   1.2237 |     40.556 |   1.2765 |     42.647 |     2.1
   93 |   1.2186 |     40.258 |   1.2729 |     42.433 |     2.1
   94 |   1.2159 |     40.410 |   1.2700 |     42.525 |     2.1
   95 |   1.2140 |     40.290 |   1.2702 |     42.402 |     2.1
   96 |   1.2105 |     40.236 |   1.2647 |     42.402 |     2.1
   97 |   1.2057 |     40.128 |   1.2629 |     42.096 |     2.2
   98 |   1.2063 |     40.155 |   1.2640 |     42.249 |     2.2
   99 |   1.2048 |     39.824 |   1.2614 |     41.667 |     2.2
  100 |   1.2017 |     39.987 |   1.2615 |     42.310 |     2.2
  101 |   1.1978 |     39.954 |   1.2626 |     42.525 |     2.3
  102 |   1.1974 |     40.036 |   1.2595 |     42.555 |     2.3
  103 |   1.1922 |     39.711 |   1.2627 |     42.616 |     2.3
  104 |   1.1899 |     39.797 |   1.2609 |     41.575 |     2.3
  105 |   1.1882 |     39.819 |   1.2589 |     42.616 |     2.3
  106 |   1.1841 |     39.738 |   1.2588 |     41.697 |     2.4
  107 |   1.1833 |     39.754 |   1.2575 |     42.555 |     2.4
  108 |   1.1814 |     39.483 |   1.2589 |     41.330 |     2.4
  109 |   1.1817 |     39.516 |   1.2543 |     42.249 |     2.4
  110 |   1.1792 |     39.548 |   1.2489 |     42.034 |     2.4
  111 |   1.1755 |     39.732 |   1.2520 |     42.279 |     2.5
  112 |   1.1726 |     39.429 |   1.2490 |     42.371 |     2.5
  113 |   1.1710 |     39.337 |   1.2506 |     42.341 |     2.5
  114 |   1.1682 |     39.272 |   1.2495 |     42.034 |     2.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 189,794

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8734 |     77.129 |   2.3375 |     58.824 |     0.0
    2 |   2.1154 |     58.425 |   1.9502 |     53.799 |     0.0
    3 |   1.8114 |     49.816 |   1.6845 |     45.650 |     0.0
    4 |   1.6078 |     46.213 |   1.5410 |     45.466 |     0.1
    5 |   1.5097 |     46.148 |   1.4737 |     45.987 |     0.1
    6 |   1.4621 |     46.245 |   1.4397 |     45.466 |     0.1
    7 |   1.4319 |     46.391 |   1.4166 |     45.251 |     0.1
    8 |   1.4099 |     46.034 |   1.3936 |     44.975 |     0.1
    9 |   1.3843 |     45.736 |   1.3708 |     44.577 |     0.1
   10 |   1.3623 |     45.161 |   1.3606 |     44.945 |     0.1
   11 |   1.3418 |     44.701 |   1.3398 |     44.118 |     0.1
   12 |   1.3252 |     44.311 |   1.3278 |     43.964 |     0.2
   13 |   1.3117 |     44.040 |   1.3127 |     44.516 |     0.2
   14 |   1.2964 |     44.110 |   1.2996 |     44.332 |     0.2
   15 |   1.2804 |     43.709 |   1.2940 |     44.393 |     0.2
   16 |   1.2646 |     43.558 |   1.2899 |     43.719 |     0.2
   17 |   1.2476 |     42.848 |   1.2690 |     43.352 |     0.2
   18 |   1.2377 |     42.414 |   1.2649 |     42.004 |     0.2
   19 |   1.2229 |     42.057 |   1.2461 |     42.188 |     0.3
   20 |   1.2130 |     41.705 |   1.2404 |     42.310 |     0.3
   21 |   1.1974 |     41.206 |   1.2370 |     42.923 |     0.3
   22 |   1.1798 |     40.659 |   1.2211 |     41.728 |     0.3
   23 |   1.1659 |     40.112 |   1.2221 |     42.310 |     0.3
   24 |   1.1552 |     39.570 |   1.1997 |     40.931 |     0.3
   25 |   1.1451 |     39.288 |   1.1926 |     40.839 |     0.3
   26 |   1.1211 |     38.015 |   1.1860 |     40.502 |     0.3
   27 |   1.1141 |     37.663 |   1.1705 |     39.767 |     0.4
   28 |   1.0872 |     36.790 |   1.1822 |     39.859 |     0.4
   29 |   1.0737 |     36.053 |   1.1873 |     39.798 |     0.4
   30 |   1.0639 |     35.820 |   1.1695 |     39.859 |     0.4
   31 |   1.0507 |     35.165 |   1.1577 |     39.093 |     0.4
   32 |   1.0328 |     34.693 |   1.1437 |     37.806 |     0.4
   33 |   1.0174 |     34.011 |   1.1412 |     38.235 |     0.4
   34 |   1.0011 |     32.922 |   1.1395 |     38.358 |     0.4
   35 |   0.9850 |     32.293 |   1.1393 |     37.714 |     0.5
   36 |   0.9701 |     31.740 |   1.1231 |     36.857 |     0.5
   37 |   0.9590 |     31.491 |   1.1219 |     37.010 |     0.5
   38 |   0.9368 |     30.668 |   1.1154 |     36.489 |     0.5
   39 |   0.9291 |     30.668 |   1.1377 |     37.071 |     0.5
   40 |   0.9171 |     29.828 |   1.1085 |     35.692 |     0.5
   41 |   0.8975 |     29.280 |   1.1023 |     35.846 |     0.5
   42 |   0.8928 |     29.308 |   1.0901 |     36.183 |     0.6
   43 |   0.8706 |     28.240 |   1.0982 |     35.907 |     0.6
   44 |   0.8751 |     28.370 |   1.1162 |     35.784 |     0.6
   45 |   0.8483 |     27.866 |   1.1004 |     35.631 |     0.6
   46 |   0.8339 |     27.368 |   1.0901 |     35.294 |     0.6
   47 |   0.8251 |     26.582 |   1.0727 |     34.191 |     0.6
   48 |   0.8119 |     26.533 |   1.1018 |     34.835 |     0.6
   49 |   0.8030 |     26.436 |   1.0863 |     34.743 |     0.6
   50 |   0.7950 |     25.656 |   1.0880 |     34.252 |     0.7
   51 |   0.7807 |     25.623 |   1.0986 |     33.793 |     0.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,423,490

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3484 |     89.917 |   2.9943 |     83.333 |     0.1
    2 |   2.7569 |     80.305 |   2.6136 |     59.467 |     0.2
    3 |   2.4990 |     59.807 |   2.4102 |     58.824 |     0.3
    4 |   2.3190 |     58.480 |   2.2497 |     58.824 |     0.3
    5 |   2.1882 |     57.569 |   2.1350 |     58.517 |     0.4
    6 |   2.0926 |     56.540 |   2.0469 |     58.119 |     0.5
    7 |   2.0101 |     55.781 |   1.9679 |     53.768 |     0.6
    8 |   1.9393 |     54.985 |   1.8940 |     53.768 |     0.7
    9 |   1.8665 |     52.547 |   1.8246 |     48.346 |     0.8
   10 |   1.8020 |     49.388 |   1.7612 |     48.346 |     0.8
   11 |   1.7426 |     48.824 |   1.7063 |     48.346 |     0.9
   12 |   1.6939 |     48.689 |   1.6604 |     48.346 |     1.0
   13 |   1.6498 |     47.968 |   1.6219 |     45.466 |     1.1
   14 |   1.6147 |     46.359 |   1.5895 |     45.466 |     1.2
   15 |   1.5843 |     46.213 |   1.5625 |     45.466 |     1.3
   16 |   1.5600 |     46.218 |   1.5400 |     45.466 |     1.4
   17 |   1.5383 |     46.245 |   1.5209 |     45.466 |     1.4
   18 |   1.5207 |     46.218 |   1.5050 |     45.466 |     1.5
   19 |   1.5036 |     46.251 |   1.4903 |     45.466 |     1.6
   20 |   1.4909 |     46.413 |   1.4763 |     45.466 |     1.7
   21 |   1.4755 |     46.240 |   1.4648 |     45.650 |     1.8
   22 |   1.4675 |     46.321 |   1.4547 |     45.956 |     1.9
   23 |   1.4553 |     46.348 |   1.4459 |     45.956 |     2.0
   24 |   1.4510 |     46.283 |   1.4392 |     46.048 |     2.0
   25 |   1.4407 |     46.213 |   1.4336 |     45.833 |     2.1
   26 |   1.4336 |     46.261 |   1.4273 |     46.017 |     2.2
   27 |   1.4272 |     46.093 |   1.4205 |     46.048 |     2.3
   28 |   1.4225 |     46.245 |   1.4141 |     46.017 |     2.4
   29 |   1.4165 |     46.115 |   1.4087 |     45.864 |     2.5
   30 |   1.4073 |     45.806 |   1.4027 |     45.037 |     2.5
   31 |   1.4054 |     45.709 |   1.3984 |     44.761 |     2.6
   32 |   1.3958 |     45.535 |   1.3934 |     44.945 |     2.7
   33 |   1.3913 |     45.579 |   1.3897 |     44.638 |     2.8
   34 |   1.3850 |     45.189 |   1.3852 |     44.485 |     2.9
   35 |   1.3825 |     45.357 |   1.3823 |     44.577 |     3.0
   36 |   1.3722 |     44.901 |   1.3752 |     44.485 |     3.0
   37 |   1.3682 |     44.918 |   1.3747 |     43.873 |     3.1
   38 |   1.3629 |     44.641 |   1.3677 |     44.026 |     3.2
   39 |   1.3521 |     44.506 |   1.3655 |     44.822 |     3.3
   40 |   1.3457 |     44.419 |   1.3619 |     43.995 |     3.4
   41 |   1.3377 |     43.764 |   1.3578 |     42.739 |     3.5
   42 |   1.3326 |     43.276 |   1.3515 |     43.107 |     3.6
   43 |   1.3249 |     43.379 |   1.3511 |     43.015 |     3.6
   44 |   1.3198 |     43.021 |   1.3455 |     42.463 |     3.7
   45 |   1.3122 |     42.566 |   1.3452 |     42.831 |     3.8
   46 |   1.3050 |     42.512 |   1.3434 |     42.892 |     3.9
   47 |   1.2995 |     42.366 |   1.3401 |     42.586 |     4.0
   48 |   1.2899 |     41.927 |   1.3393 |     42.004 |     4.1
   49 |   1.2810 |     41.407 |   1.3325 |     42.586 |     4.1
   50 |   1.2760 |     41.336 |   1.3314 |     41.973 |     4.2
   51 |   1.2676 |     40.957 |   1.3240 |     41.575 |     4.3
   52 |   1.2614 |     40.756 |   1.3245 |     41.912 |     4.4
   53 |   1.2517 |     40.410 |   1.3189 |     41.789 |     4.5
   54 |   1.2454 |     40.258 |   1.3210 |     41.697 |     4.6
   55 |   1.2387 |     40.068 |   1.3183 |     42.065 |     4.7
   56 |   1.2342 |     40.063 |   1.3119 |     41.544 |     4.7
   57 |   1.2246 |     39.602 |   1.3112 |     41.636 |     4.8
   58 |   1.2178 |     39.629 |   1.3065 |     41.299 |     4.9
   59 |   1.2179 |     39.526 |   1.3022 |     41.299 |     5.0
   60 |   1.2060 |     38.817 |   1.3023 |     41.513 |     5.1
   61 |   1.1984 |     38.833 |   1.2942 |     40.901 |     5.2
   62 |   1.1931 |     38.454 |   1.2969 |     41.146 |     5.2
   63 |   1.1895 |     38.562 |   1.2978 |     40.717 |     5.3
   64 |   1.1800 |     38.069 |   1.2917 |     40.564 |     5.4
   65 |   1.1770 |     38.215 |   1.2951 |     40.931 |     5.5
   66 |   1.1687 |     37.890 |   1.2866 |     40.380 |     5.6
   67 |   1.1595 |     37.668 |   1.2843 |     40.748 |     5.7
   68 |   1.1504 |     37.565 |   1.2872 |     40.686 |     5.7
   69 |   1.1473 |     37.256 |   1.2797 |     40.319 |     5.8
   70 |   1.1386 |     37.207 |   1.2743 |     40.288 |     5.9
   71 |   1.1360 |     37.083 |   1.2768 |     40.594 |     6.0
   72 |   1.1305 |     36.736 |   1.2722 |     40.227 |     6.1
   73 |   1.1233 |     36.752 |   1.2661 |     39.951 |     6.2
   74 |   1.1152 |     36.308 |   1.2685 |     40.043 |     6.3
   75 |   1.1102 |     36.254 |   1.2568 |     40.043 |     6.3
   76 |   1.1046 |     36.167 |   1.2614 |     39.522 |     6.4
   77 |   1.0950 |     36.026 |   1.2605 |     40.135 |     6.5
   78 |   1.0904 |     35.577 |   1.2571 |     40.165 |     6.6
   79 |   1.0826 |     35.371 |   1.2543 |     39.645 |     6.7
   80 |   1.0812 |     35.203 |   1.2463 |     39.430 |     6.8
   81 |   1.0748 |     35.268 |   1.2440 |     39.400 |     6.8
   82 |   1.0657 |     35.197 |   1.2447 |     39.522 |     6.9
   83 |   1.0640 |     34.861 |   1.2418 |     39.124 |     7.0
   84 |   1.0578 |     34.986 |   1.2456 |     39.859 |     7.1
   85 |   1.0516 |     34.720 |   1.2372 |     39.185 |     7.2
   86 |   1.0410 |     34.477 |   1.2332 |     38.634 |     7.3
   87 |   1.0355 |     34.357 |   1.2330 |     39.093 |     7.4
   88 |   1.0359 |     34.108 |   1.2310 |     38.971 |     7.4
   89 |   1.0281 |     34.233 |   1.2264 |     38.787 |     7.5
   90 |   1.0211 |     34.011 |   1.2287 |     38.971 |     7.6
   91 |   1.0184 |     33.848 |   1.2292 |     38.725 |     7.7
   92 |   1.0121 |     33.534 |   1.2254 |     39.308 |     7.8
   93 |   1.0075 |     33.220 |   1.2259 |     38.756 |     7.9
   94 |   1.0006 |     33.236 |   1.2267 |     38.480 |     7.9
   95 |   0.9933 |     33.138 |   1.2169 |     37.960 |     8.0
   96 |   0.9908 |     32.819 |   1.2204 |     38.419 |     8.1
   97 |   0.9840 |     32.699 |   1.2265 |     39.216 |     8.2
   98 |   0.9772 |     32.553 |   1.2178 |     38.388 |     8.3
   99 |   0.9743 |     32.564 |   1.2141 |     38.266 |     8.4
  100 |   0.9727 |     32.228 |   1.2080 |     37.960 |     8.5
  101 |   0.9594 |     32.103 |   1.2181 |     38.266 |     8.5
  102 |   0.9591 |     31.892 |   1.2041 |     37.623 |     8.6
  103 |   0.9558 |     31.832 |   1.2082 |     37.500 |     8.7
  104 |   0.9510 |     31.708 |   1.2070 |     37.561 |     8.8
  105 |   0.9466 |     31.209 |   1.2083 |     37.745 |     8.9
  106 |   0.9400 |     31.025 |   1.2070 |     37.684 |     9.0
  107 |   0.9370 |     31.063 |   1.2026 |     37.898 |     9.0
  108 |   0.9301 |     31.150 |   1.2056 |     37.531 |     9.1
  109 |   0.9250 |     30.472 |   1.2050 |     37.132 |     9.2
  110 |   0.9175 |     30.559 |   1.2010 |     36.979 |     9.3
  111 |   0.9191 |     30.223 |   1.2027 |     37.561 |     9.4
  112 |   0.9135 |     30.050 |   1.2077 |     36.979 |     9.5
  113 |   0.9052 |     30.066 |   1.2049 |     36.979 |     9.6
  114 |   0.9000 |     29.801 |   1.2048 |     37.194 |     9.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,485,602

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3680 |     88.318 |   3.0386 |     82.935 |     0.1
    2 |   2.8015 |     82.586 |   2.6597 |     70.037 |     0.2
    3 |   2.5402 |     63.047 |   2.4381 |     58.824 |     0.3
    4 |   2.3524 |     58.436 |   2.2820 |     58.824 |     0.3
    5 |   2.2210 |     57.493 |   2.1626 |     58.793 |     0.4
    6 |   2.1202 |     56.805 |   2.0719 |     56.464 |     0.5
    7 |   2.0396 |     56.128 |   1.9964 |     54.013 |     0.6
    8 |   1.9650 |     54.936 |   1.9283 |     50.153 |     0.7
    9 |   1.8999 |     50.271 |   1.8663 |     48.315 |     0.8
   10 |   1.8414 |     48.998 |   1.8090 |     48.284 |     0.8
   11 |   1.7858 |     48.759 |   1.7544 |     48.346 |     0.9
   12 |   1.7364 |     48.651 |   1.7047 |     48.376 |     1.0
   13 |   1.6883 |     48.635 |   1.6597 |     48.376 |     1.1
   14 |   1.6463 |     48.591 |   1.6200 |     48.346 |     1.2
   15 |   1.6066 |     47.833 |   1.5850 |     45.527 |     1.3
   16 |   1.5721 |     46.272 |   1.5513 |     45.404 |     1.4
   17 |   1.5421 |     46.088 |   1.5223 |     45.251 |     1.4
   18 |   1.5140 |     46.039 |   1.4988 |     45.221 |     1.5
   19 |   1.4939 |     46.034 |   1.4795 |     45.251 |     1.6
   20 |   1.4723 |     45.844 |   1.4645 |     45.190 |     1.7
   21 |   1.4535 |     45.763 |   1.4471 |     44.853 |     1.8
   22 |   1.4355 |     45.552 |   1.4352 |     44.332 |     1.9
   23 |   1.4201 |     44.977 |   1.4201 |     44.792 |     1.9
   24 |   1.4059 |     44.863 |   1.4098 |     44.240 |     2.0
   25 |   1.3892 |     44.609 |   1.3946 |     44.761 |     2.1
   26 |   1.3780 |     44.750 |   1.3843 |     45.098 |     2.2
   27 |   1.3652 |     44.343 |   1.3741 |     44.118 |     2.3
   28 |   1.3521 |     44.045 |   1.3639 |     44.179 |     2.4
   29 |   1.3355 |     43.861 |   1.3539 |     44.485 |     2.4
   30 |   1.3236 |     43.731 |   1.3388 |     43.995 |     2.5
   31 |   1.3061 |     43.498 |   1.3308 |     44.148 |     2.6
   32 |   1.2924 |     43.222 |   1.3192 |     43.934 |     2.7
   33 |   1.2804 |     42.620 |   1.3108 |     43.597 |     2.8
   34 |   1.2662 |     42.084 |   1.3001 |     42.371 |     2.9
   35 |   1.2572 |     41.710 |   1.2926 |     42.279 |     3.0
   36 |   1.2460 |     41.531 |   1.2815 |     42.678 |     3.0
   37 |   1.2320 |     40.957 |   1.2776 |     42.279 |     3.1
   38 |   1.2201 |     40.496 |   1.2735 |     42.678 |     3.2
   39 |   1.2093 |     40.339 |   1.2596 |     41.789 |     3.3
   40 |   1.1998 |     39.911 |   1.2573 |     42.188 |     3.4
   41 |   1.1908 |     39.711 |   1.2544 |     42.126 |     3.5
   42 |   1.1815 |     39.407 |   1.2502 |     41.575 |     3.5
   43 |   1.1696 |     39.082 |   1.2430 |     41.575 |     3.6
   44 |   1.1643 |     39.044 |   1.2322 |     40.931 |     3.7
   45 |   1.1529 |     38.584 |   1.2285 |     41.054 |     3.8
   46 |   1.1463 |     38.367 |   1.2261 |     41.238 |     3.9
   47 |   1.1355 |     37.868 |   1.2253 |     40.870 |     4.0
   48 |   1.1298 |     37.776 |   1.2214 |     40.778 |     4.0
   49 |   1.1242 |     37.408 |   1.2228 |     40.380 |     4.1
   50 |   1.1130 |     37.012 |   1.2152 |     40.104 |     4.2
   51 |   1.1107 |     37.023 |   1.2111 |     39.982 |     4.3
   52 |   1.0998 |     36.720 |   1.2116 |     40.104 |     4.4
   53 |   1.0969 |     36.281 |   1.2081 |     39.767 |     4.5
   54 |   1.0880 |     35.842 |   1.2071 |     40.043 |     4.6
   55 |   1.0774 |     35.809 |   1.2026 |     39.583 |     4.6
   56 |   1.0768 |     35.750 |   1.2022 |     39.645 |     4.7
   57 |   1.0654 |     35.300 |   1.2027 |     39.706 |     4.8
   58 |   1.0570 |     35.192 |   1.1966 |     39.308 |     4.9
   59 |   1.0545 |     34.726 |   1.1999 |     39.216 |     5.0
   60 |   1.0440 |     34.547 |   1.1910 |     38.756 |     5.1
   61 |   1.0403 |     34.298 |   1.1945 |     39.185 |     5.1
   62 |   1.0333 |     33.962 |   1.1892 |     39.308 |     5.2
   63 |   1.0279 |     33.572 |   1.1875 |     38.971 |     5.3
   64 |   1.0220 |     33.604 |   1.1829 |     38.879 |     5.4
   65 |   1.0146 |     33.182 |   1.1786 |     38.297 |     5.5
   66 |   1.0117 |     33.339 |   1.1804 |     38.051 |     5.6
   67 |   1.0027 |     32.970 |   1.1758 |     38.143 |     5.6
   68 |   0.9968 |     32.694 |   1.1724 |     38.051 |     5.7
   69 |   0.9901 |     32.531 |   1.1744 |     39.124 |     5.8
   70 |   0.9845 |     32.125 |   1.1725 |     37.684 |     5.9
   71 |   0.9806 |     32.152 |   1.1717 |     37.806 |     6.0
   72 |   0.9790 |     32.049 |   1.1621 |     37.714 |     6.1
   73 |   0.9653 |     31.567 |   1.1696 |     38.113 |     6.2
   74 |   0.9624 |     31.654 |   1.1649 |     37.469 |     6.2
   75 |   0.9536 |     31.312 |   1.1631 |     37.194 |     6.3
   76 |   0.9479 |     30.955 |   1.1617 |     37.469 |     6.4
   77 |   0.9495 |     31.052 |   1.1658 |     37.347 |     6.5
   78 |   0.9407 |     30.435 |   1.1580 |     37.745 |     6.6
   79 |   0.9342 |     30.397 |   1.1624 |     37.255 |     6.7
   80 |   0.9307 |     30.575 |   1.1624 |     36.979 |     6.7
   81 |   0.9235 |     30.272 |   1.1509 |     37.377 |     6.8
   82 |   0.9233 |     30.277 |   1.1460 |     36.305 |     6.9
   83 |   0.9150 |     29.730 |   1.1418 |     36.520 |     7.0
   84 |   0.9172 |     29.676 |   1.1480 |     36.489 |     7.1
   85 |   0.9045 |     29.221 |   1.1498 |     36.458 |     7.2
   86 |   0.8980 |     28.939 |   1.1422 |     36.060 |     7.2
   87 |   0.8968 |     28.825 |   1.1517 |     35.999 |     7.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 1,159,650

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2396 |     84.639 |   2.7521 |     83.333 |     0.0
    2 |   2.4965 |     66.634 |   2.2794 |     58.824 |     0.1
    3 |   2.1431 |     57.233 |   2.0343 |     53.768 |     0.1
    4 |   1.9579 |     54.416 |   1.8780 |     48.376 |     0.1
    5 |   1.8220 |     49.247 |   1.7571 |     48.346 |     0.1
    6 |   1.7146 |     48.689 |   1.6583 |     48.346 |     0.2
    7 |   1.6270 |     48.109 |   1.5815 |     45.466 |     0.2
    8 |   1.5630 |     46.218 |   1.5293 |     45.466 |     0.2
    9 |   1.5187 |     46.213 |   1.4944 |     45.466 |     0.3
   10 |   1.4944 |     46.207 |   1.4720 |     45.466 |     0.3
   11 |   1.4708 |     46.218 |   1.4552 |     45.466 |     0.3
   12 |   1.4545 |     46.213 |   1.4432 |     45.466 |     0.4
   13 |   1.4469 |     46.223 |   1.4336 |     45.466 |     0.4
   14 |   1.4364 |     46.256 |   1.4266 |     45.466 |     0.4
   15 |   1.4272 |     46.283 |   1.4198 |     45.466 |     0.4
   16 |   1.4242 |     46.234 |   1.4156 |     45.987 |     0.5
   17 |   1.4199 |     46.213 |   1.4121 |     45.987 |     0.5
   18 |   1.4139 |     46.267 |   1.4067 |     45.466 |     0.5
   19 |   1.4100 |     46.207 |   1.4037 |     45.466 |     0.6
   20 |   1.4080 |     46.316 |   1.4003 |     45.466 |     0.6
   21 |   1.4035 |     46.234 |   1.3971 |     45.466 |     0.6
   22 |   1.3978 |     46.288 |   1.3911 |     45.466 |     0.7
   23 |   1.3896 |     46.169 |   1.3852 |     45.466 |     0.7
   24 |   1.3823 |     46.337 |   1.3777 |     45.466 |     0.7
   25 |   1.3749 |     46.169 |   1.3696 |     45.282 |     0.7
   26 |   1.3651 |     46.001 |   1.3646 |     44.822 |     0.8
   27 |   1.3547 |     45.335 |   1.3549 |     44.271 |     0.8
   28 |   1.3478 |     44.934 |   1.3495 |     44.056 |     0.8
   29 |   1.3399 |     45.010 |   1.3468 |     44.761 |     0.9
   30 |   1.3343 |     44.836 |   1.3358 |     44.240 |     0.9
   31 |   1.3204 |     44.311 |   1.3282 |     43.719 |     0.9
   32 |   1.3111 |     43.883 |   1.3229 |     43.781 |     0.9
   33 |   1.3006 |     43.801 |   1.3134 |     43.658 |     1.0
   34 |   1.2876 |     43.200 |   1.3039 |     42.433 |     1.0
   35 |   1.2791 |     42.609 |   1.2954 |     42.126 |     1.0
   36 |   1.2627 |     42.003 |   1.2823 |     41.636 |     1.1
   37 |   1.2485 |     41.678 |   1.2734 |     41.115 |     1.1
   38 |   1.2365 |     40.854 |   1.2665 |     41.238 |     1.1
   39 |   1.2212 |     40.588 |   1.2590 |     40.502 |     1.2
   40 |   1.2070 |     40.177 |   1.2507 |     40.931 |     1.2
   41 |   1.1963 |     39.743 |   1.2378 |     40.625 |     1.2
   42 |   1.1846 |     39.272 |   1.2322 |     40.594 |     1.2
   43 |   1.1770 |     39.266 |   1.2311 |     40.380 |     1.3
   44 |   1.1645 |     38.893 |   1.2275 |     40.502 |     1.3
   45 |   1.1547 |     38.605 |   1.2226 |     40.380 |     1.3
   46 |   1.1434 |     38.421 |   1.2133 |     40.257 |     1.4
   47 |   1.1344 |     38.204 |   1.2173 |     40.012 |     1.4
   48 |   1.1239 |     37.917 |   1.2042 |     39.920 |     1.4
   49 |   1.1172 |     37.852 |   1.2060 |     39.890 |     1.4
   50 |   1.1087 |     37.392 |   1.1974 |     39.859 |     1.5
   51 |   1.0986 |     37.213 |   1.1920 |     39.308 |     1.5
   52 |   1.0919 |     37.126 |   1.1840 |     38.787 |     1.5
   53 |   1.0798 |     36.465 |   1.1866 |     38.879 |     1.6
   54 |   1.0747 |     36.243 |   1.1782 |     38.725 |     1.6
   55 |   1.0617 |     35.994 |   1.1781 |     38.419 |     1.6
   56 |   1.0580 |     35.815 |   1.1703 |     38.388 |     1.6
   57 |   1.0496 |     35.490 |   1.1678 |     38.297 |     1.7
   58 |   1.0412 |     35.295 |   1.1701 |     38.603 |     1.7
   59 |   1.0341 |     34.910 |   1.1633 |     38.511 |     1.7
   60 |   1.0256 |     34.726 |   1.1575 |     37.960 |     1.8
   61 |   1.0169 |     34.531 |   1.1556 |     38.235 |     1.8
   62 |   1.0112 |     34.319 |   1.1487 |     37.592 |     1.8
   63 |   1.0057 |     34.032 |   1.1464 |     37.377 |     1.9
   64 |   0.9956 |     33.729 |   1.1457 |     37.653 |     1.9
   65 |   0.9857 |     33.480 |   1.1376 |     36.949 |     1.9
   66 |   0.9747 |     33.035 |   1.1379 |     37.194 |     1.9
   67 |   0.9711 |     32.970 |   1.1293 |     36.244 |     2.0
   68 |   0.9654 |     32.515 |   1.1297 |     36.734 |     2.0
   69 |   0.9576 |     32.261 |   1.1269 |     35.662 |     2.0
   70 |   0.9501 |     31.816 |   1.1192 |     35.692 |     2.1
   71 |   0.9494 |     31.686 |   1.1225 |     36.183 |     2.1
   72 |   0.9368 |     31.112 |   1.1162 |     35.784 |     2.1
   73 |   0.9346 |     30.792 |   1.1152 |     35.447 |     2.1
   74 |   0.9221 |     30.565 |   1.1106 |     35.172 |     2.2
   75 |   0.9124 |     30.088 |   1.1107 |     34.835 |     2.2
   76 |   0.9017 |     29.719 |   1.1110 |     35.447 |     2.2
   77 |   0.8980 |     29.351 |   1.1057 |     35.509 |     2.3
   78 |   0.8954 |     29.286 |   1.0995 |     34.743 |     2.3
   79 |   0.8838 |     28.901 |   1.1052 |     35.202 |     2.3
   80 |   0.8768 |     28.386 |   1.1002 |     34.743 |     2.3
   81 |   0.8660 |     28.045 |   1.1005 |     34.589 |     2.4
   82 |   0.8640 |     28.034 |   1.1087 |     34.620 |     2.4
   83 |   0.8565 |     27.666 |   1.0957 |     34.804 |     2.4
   84 |   0.8484 |     27.368 |   1.0960 |     34.038 |     2.5
   85 |   0.8467 |     27.563 |   1.0942 |     34.375 |     2.5
   86 |   0.8356 |     27.108 |   1.0985 |     33.946 |     2.5
   87 |   0.8301 |     26.696 |   1.0911 |     34.406 |     2.6
   88 |   0.8191 |     26.674 |   1.0954 |     33.732 |     2.6
   89 |   0.8179 |     26.393 |   1.0845 |     34.161 |     2.6
   90 |   0.8136 |     26.084 |   1.0949 |     33.701 |     2.6
   91 |   0.8024 |     25.829 |   1.0861 |     33.333 |     2.7
   92 |   0.7968 |     25.650 |   1.0934 |     34.007 |     2.7
   93 |   0.7870 |     25.439 |   1.0905 |     33.395 |     2.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,296,034

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1742 |     83.052 |   2.6534 |     80.821 |     0.0
    2 |   2.3478 |     61.118 |   2.1480 |     57.996 |     0.1
    3 |   2.0198 |     54.871 |   1.9371 |     51.685 |     0.1
    4 |   1.8592 |     48.889 |   1.7917 |     48.346 |     0.1
    5 |   1.7344 |     48.618 |   1.6820 |     48.376 |     0.2
    6 |   1.6421 |     48.597 |   1.6004 |     48.346 |     0.2
    7 |   1.5743 |     47.551 |   1.5408 |     45.466 |     0.2
    8 |   1.5247 |     46.207 |   1.4983 |     45.466 |     0.3
    9 |   1.4871 |     46.213 |   1.4670 |     45.466 |     0.3
   10 |   1.4578 |     46.213 |   1.4424 |     45.466 |     0.3
   11 |   1.4319 |     46.213 |   1.4200 |     45.466 |     0.4
   12 |   1.4101 |     46.196 |   1.4017 |     45.312 |     0.4
   13 |   1.3930 |     46.018 |   1.3854 |     45.404 |     0.5
   14 |   1.3741 |     45.785 |   1.3690 |     45.067 |     0.5
   15 |   1.3575 |     45.432 |   1.3569 |     45.067 |     0.5
   16 |   1.3435 |     44.842 |   1.3457 |     44.056 |     0.6
   17 |   1.3303 |     44.251 |   1.3337 |     43.750 |     0.6
   18 |   1.3181 |     43.839 |   1.3270 |     43.168 |     0.6
   19 |   1.3066 |     43.650 |   1.3198 |     43.015 |     0.7
   20 |   1.2991 |     43.292 |   1.3132 |     43.107 |     0.7
   21 |   1.2836 |     42.902 |   1.3052 |     42.647 |     0.7
   22 |   1.2757 |     42.496 |   1.2966 |     42.555 |     0.8
   23 |   1.2658 |     42.441 |   1.2900 |     42.157 |     0.8
   24 |   1.2551 |     42.073 |   1.2843 |     41.881 |     0.8
   25 |   1.2409 |     41.466 |   1.2783 |     41.422 |     0.9
   26 |   1.2315 |     41.174 |   1.2776 |     41.391 |     0.9
   27 |   1.2194 |     40.718 |   1.2716 |     41.238 |     0.9
   28 |   1.2085 |     40.187 |   1.2663 |     41.360 |     1.0
   29 |   1.1944 |     39.927 |   1.2578 |     41.115 |     1.0
   30 |   1.1817 |     39.423 |   1.2451 |     40.870 |     1.0
   31 |   1.1662 |     39.358 |   1.2412 |     40.717 |     1.1
   32 |   1.1560 |     38.741 |   1.2313 |     40.502 |     1.1
   33 |   1.1391 |     38.210 |   1.2242 |     40.349 |     1.1
   34 |   1.1282 |     37.852 |   1.2176 |     39.308 |     1.2
   35 |   1.1105 |     37.056 |   1.2074 |     38.971 |     1.2
   36 |   1.0988 |     36.601 |   1.2015 |     38.848 |     1.2
   37 |   1.0792 |     35.360 |   1.1985 |     38.143 |     1.3
   38 |   1.0658 |     34.655 |   1.1806 |     37.714 |     1.3
   39 |   1.0526 |     33.821 |   1.1793 |     37.347 |     1.3
   40 |   1.0352 |     33.317 |   1.1689 |     37.347 |     1.4
   41 |   1.0182 |     32.759 |   1.1560 |     36.734 |     1.4
   42 |   1.0014 |     31.719 |   1.1436 |     35.938 |     1.4
   43 |   0.9861 |     31.301 |   1.1393 |     35.723 |     1.5
   44 |   0.9715 |     30.543 |   1.1374 |     35.662 |     1.5
   45 |   0.9544 |     30.250 |   1.1299 |     35.355 |     1.6
   46 |   0.9403 |     29.725 |   1.1151 |     35.478 |     1.6
   47 |   0.9228 |     29.091 |   1.1109 |     34.835 |     1.6
   48 |   0.9149 |     28.760 |   1.1095 |     34.743 |     1.7
   49 |   0.8964 |     28.050 |   1.1017 |     34.252 |     1.7
   50 |   0.8827 |     27.492 |   1.0851 |     34.161 |     1.7
   51 |   0.8709 |     27.048 |   1.0850 |     33.854 |     1.8
   52 |   0.8537 |     26.528 |   1.0809 |     33.946 |     1.8
   53 |   0.8425 |     26.002 |   1.0762 |     33.456 |     1.8
   54 |   0.8285 |     25.764 |   1.0758 |     33.333 |     1.9
   55 |   0.8182 |     25.417 |   1.0657 |     32.475 |     1.9
   56 |   0.8070 |     24.848 |   1.0652 |     32.567 |     1.9
   57 |   0.7966 |     24.480 |   1.0589 |     32.414 |     2.0
   58 |   0.7849 |     24.231 |   1.0686 |     32.996 |     2.0
   59 |   0.7696 |     23.586 |   1.0618 |     32.230 |     2.0
   60 |   0.7622 |     23.488 |   1.0463 |     32.077 |     2.1
   61 |   0.7525 |     22.936 |   1.0502 |     31.587 |     2.1
   62 |   0.7413 |     22.773 |   1.0528 |     32.077 |     2.1
   63 |   0.7294 |     22.269 |   1.0517 |     32.108 |     2.2
   64 |   0.7216 |     22.096 |   1.0413 |     30.913 |     2.2
   65 |   0.7082 |     21.819 |   1.0538 |     31.587 |     2.2
   66 |   0.6985 |     21.353 |   1.0544 |     31.771 |     2.3
   67 |   0.6927 |     21.213 |   1.0534 |     31.342 |     2.3
   68 |   0.6782 |     20.817 |   1.0489 |     31.832 |     2.3
   69 |   0.6682 |     20.535 |   1.0396 |     30.974 |     2.4
   70 |   0.6671 |     20.357 |   1.0408 |     30.852 |     2.4
   71 |   0.6509 |     19.918 |   1.0457 |     30.576 |     2.4
   72 |   0.6435 |     19.668 |   1.0334 |     30.607 |     2.5
   73 |   0.6339 |     19.397 |   1.0353 |     30.362 |     2.5
   74 |   0.6280 |     19.116 |   1.0432 |     30.270 |     2.5
   75 |   0.6180 |     18.926 |   1.0260 |     30.270 |     2.6
   76 |   0.6137 |     18.801 |   1.0462 |     29.626 |     2.6
   77 |   0.6061 |     18.601 |   1.0448 |     29.933 |     2.6
   78 |   0.5995 |     18.314 |   1.0506 |     30.208 |     2.7
   79 |   0.5921 |     18.108 |   1.0580 |     29.994 |     2.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,552,514

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3053 |     62.738 |   1.6947 |     48.346 |     0.0
    2 |   1.5263 |     46.809 |   1.4348 |     45.496 |     0.1
    3 |   1.4260 |     46.207 |   1.4089 |     45.466 |     0.1
    4 |   1.4076 |     46.240 |   1.3982 |     45.987 |     0.2
    5 |   1.3989 |     46.207 |   1.3896 |     45.987 |     0.2
    6 |   1.3794 |     45.812 |   1.3609 |     43.873 |     0.3
    7 |   1.3562 |     45.015 |   1.3421 |     44.087 |     0.3
    8 |   1.3391 |     44.793 |   1.3283 |     44.026 |     0.3
    9 |   1.3268 |     44.744 |   1.3186 |     44.455 |     0.4
   10 |   1.3137 |     44.089 |   1.3067 |     43.505 |     0.4
   11 |   1.3026 |     43.829 |   1.2870 |     42.739 |     0.5
   12 |   1.2841 |     43.281 |   1.2933 |     42.188 |     0.5
   13 |   1.2720 |     42.637 |   1.2710 |     42.096 |     0.6
   14 |   1.2646 |     42.864 |   1.2673 |     41.544 |     0.6
   15 |   1.2459 |     42.068 |   1.2497 |     41.238 |     0.6
   16 |   1.2332 |     41.721 |   1.2413 |     41.513 |     0.7
   17 |   1.2255 |     41.547 |   1.2377 |     40.778 |     0.7
   18 |   1.2079 |     40.892 |   1.2291 |     40.962 |     0.8
   19 |   1.1981 |     40.383 |   1.2252 |     39.890 |     0.8
   20 |   1.1834 |     40.079 |   1.2152 |     40.564 |     0.9
   21 |   1.1677 |     39.364 |   1.2067 |     39.920 |     0.9
   22 |   1.1546 |     39.044 |   1.1980 |     38.756 |     0.9
   23 |   1.1486 |     38.481 |   1.1929 |     38.695 |     1.0
   24 |   1.1282 |     37.625 |   1.2037 |     39.614 |     1.0
   25 |   1.1224 |     37.950 |   1.1829 |     38.511 |     1.1
   26 |   1.0982 |     37.012 |   1.1802 |     38.327 |     1.1
   27 |   1.0918 |     36.368 |   1.1668 |     38.143 |     1.2
   28 |   1.0823 |     36.210 |   1.1667 |     38.143 |     1.2
   29 |   1.0940 |     36.433 |   1.1596 |     37.776 |     1.2
   30 |   1.0603 |     35.560 |   1.1465 |     37.653 |     1.3
   31 |   1.0551 |     35.311 |   1.1285 |     37.132 |     1.3
   32 |   1.0362 |     35.002 |   1.1280 |     35.938 |     1.4
   33 |   1.0326 |     34.905 |   1.1032 |     35.784 |     1.4
   34 |   1.0178 |     34.211 |   1.1035 |     35.049 |     1.4
   35 |   1.0131 |     34.005 |   1.0931 |     35.478 |     1.5
   36 |   0.9855 |     33.214 |   1.0799 |     35.049 |     1.5
   37 |   0.9745 |     32.884 |   1.0904 |     35.447 |     1.6
   38 |   0.9693 |     32.483 |   1.0666 |     34.007 |     1.6
   39 |   0.9501 |     32.223 |   1.0896 |     34.528 |     1.7
   40 |   0.9431 |     32.076 |   1.0507 |     34.161 |     1.7
   41 |   0.9287 |     31.448 |   1.0583 |     33.793 |     1.7
   42 |   0.9208 |     31.031 |   1.0649 |     34.252 |     1.8
   43 |   0.9171 |     30.960 |   1.0882 |     36.060 |     1.8
   44 |   0.9174 |     31.215 |   1.0643 |     33.732 |     1.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 589,154

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5000 |     89.949 |   3.4418 |     83.578 |     0.0
    2 |   3.2462 |     71.998 |   3.0234 |     67.341 |     0.1
    3 |   2.8896 |     78.961 |   2.8048 |     83.333 |     0.1
    4 |   2.7397 |     81.675 |   2.7009 |     83.333 |     0.1
    5 |   2.6455 |     67.588 |   2.6103 |     59.252 |     0.1
    6 |   2.5557 |     60.734 |   2.5239 |     59.252 |     0.2
    7 |   2.4762 |     59.384 |   2.4434 |     58.824 |     0.2
    8 |   2.4028 |     59.043 |   2.3715 |     58.824 |     0.2
    9 |   2.3356 |     58.826 |   2.3079 |     58.824 |     0.2
   10 |   2.2775 |     58.631 |   2.2504 |     58.824 |     0.3
   11 |   2.2196 |     58.388 |   2.1847 |     58.824 |     0.3
   12 |   2.1577 |     57.797 |   2.1278 |     58.824 |     0.3
   13 |   2.1075 |     56.968 |   2.0764 |     58.824 |     0.3
   14 |   2.0611 |     56.307 |   2.0337 |     54.565 |     0.4
   15 |   2.0208 |     55.711 |   1.9928 |     53.768 |     0.4
   16 |   1.9821 |     54.649 |   1.9553 |     50.735 |     0.4
   17 |   1.9450 |     52.281 |   1.9183 |     48.376 |     0.4
   18 |   1.9133 |     49.967 |   1.8843 |     48.346 |     0.5
   19 |   1.8791 |     49.274 |   1.8528 |     48.346 |     0.5
   20 |   1.8512 |     48.998 |   1.8226 |     48.346 |     0.5
   21 |   1.8174 |     48.851 |   1.7931 |     48.346 |     0.6
   22 |   1.7883 |     48.700 |   1.7641 |     48.346 |     0.6
   23 |   1.7599 |     48.651 |   1.7358 |     48.346 |     0.6
   24 |   1.7339 |     48.624 |   1.7096 |     48.346 |     0.6
   25 |   1.7075 |     48.640 |   1.6844 |     48.346 |     0.7
   26 |   1.6837 |     48.618 |   1.6609 |     48.346 |     0.7
   27 |   1.6613 |     48.602 |   1.6391 |     48.346 |     0.7
   28 |   1.6422 |     48.597 |   1.6191 |     48.346 |     0.7
   29 |   1.6234 |     48.375 |   1.6016 |     48.346 |     0.8
   30 |   1.6048 |     46.939 |   1.5853 |     45.466 |     0.8
   31 |   1.5917 |     46.278 |   1.5712 |     45.466 |     0.8
   32 |   1.5733 |     46.175 |   1.5579 |     45.404 |     0.8
   33 |   1.5629 |     46.186 |   1.5452 |     45.466 |     0.9
   34 |   1.5490 |     46.164 |   1.5344 |     45.466 |     0.9
   35 |   1.5379 |     46.158 |   1.5244 |     45.404 |     0.9
   36 |   1.5307 |     46.121 |   1.5155 |     45.343 |     0.9
   37 |   1.5182 |     46.158 |   1.5076 |     45.343 |     1.0
   38 |   1.5107 |     46.142 |   1.5002 |     45.404 |     1.0
   39 |   1.5048 |     46.148 |   1.4932 |     45.404 |     1.0
   40 |   1.4969 |     46.148 |   1.4866 |     45.466 |     1.0
   41 |   1.4914 |     46.169 |   1.4809 |     45.404 |     1.1
   42 |   1.4840 |     46.175 |   1.4755 |     45.466 |     1.1
   43 |   1.4782 |     46.142 |   1.4707 |     45.496 |     1.1
   44 |   1.4729 |     46.175 |   1.4662 |     45.558 |     1.1
   45 |   1.4667 |     46.061 |   1.4596 |     45.404 |     1.2
   46 |   1.4599 |     46.061 |   1.4557 |     45.558 |     1.2
   47 |   1.4574 |     46.099 |   1.4508 |     45.527 |     1.2
   48 |   1.4526 |     46.110 |   1.4455 |     45.588 |     1.2
   49 |   1.4467 |     46.115 |   1.4407 |     45.558 |     1.3
   50 |   1.4425 |     46.066 |   1.4363 |     45.374 |     1.3
   51 |   1.4352 |     45.996 |   1.4317 |     45.588 |     1.3
   52 |   1.4326 |     46.028 |   1.4266 |     45.435 |     1.4
   53 |   1.4246 |     46.012 |   1.4225 |     45.435 |     1.4
   54 |   1.4183 |     46.012 |   1.4189 |     45.435 |     1.4
   55 |   1.4158 |     46.050 |   1.4148 |     45.435 |     1.4
   56 |   1.4097 |     45.915 |   1.4112 |     45.435 |     1.5
   57 |   1.4062 |     45.980 |   1.4059 |     45.435 |     1.5
   58 |   1.4002 |     45.958 |   1.4007 |     45.374 |     1.5
   59 |   1.3936 |     45.925 |   1.3962 |     45.588 |     1.5
   60 |   1.3894 |     45.898 |   1.3922 |     45.558 |     1.6
   61 |   1.3835 |     45.812 |   1.3888 |     45.312 |     1.6
   62 |   1.3766 |     45.741 |   1.3830 |     45.221 |     1.6
   63 |   1.3703 |     45.514 |   1.3767 |     45.129 |     1.6
   64 |   1.3659 |     45.427 |   1.3723 |     45.221 |     1.7
   65 |   1.3570 |     45.129 |   1.3697 |     45.159 |     1.7
   66 |   1.3557 |     45.037 |   1.3637 |     45.159 |     1.7
   67 |   1.3484 |     44.907 |   1.3600 |     44.669 |     1.7
   68 |   1.3426 |     44.685 |   1.3563 |     44.884 |     1.8
   69 |   1.3353 |     44.511 |   1.3514 |     44.853 |     1.8
   70 |   1.3281 |     44.381 |   1.3468 |     44.669 |     1.8
   71 |   1.3225 |     44.192 |   1.3412 |     44.638 |     1.8
   72 |   1.3176 |     44.143 |   1.3384 |     44.638 |     1.9
   73 |   1.3113 |     44.099 |   1.3346 |     44.393 |     1.9
   74 |   1.3087 |     43.764 |   1.3310 |     44.363 |     1.9
   75 |   1.3013 |     43.525 |   1.3276 |     43.781 |     1.9
   76 |   1.2979 |     43.298 |   1.3258 |     43.321 |     2.0
   77 |   1.2891 |     43.037 |   1.3222 |     43.474 |     2.0
   78 |   1.2857 |     42.907 |   1.3230 |     43.505 |     2.0
   79 |   1.2817 |     42.647 |   1.3178 |     43.352 |     2.0
   80 |   1.2753 |     42.680 |   1.3155 |     43.382 |     2.1
   81 |   1.2734 |     42.507 |   1.3124 |     43.413 |     2.1
   82 |   1.2673 |     42.344 |   1.3112 |     43.199 |     2.1
   83 |   1.2656 |     42.208 |   1.3086 |     43.321 |     2.2
   84 |   1.2589 |     42.051 |   1.3044 |     43.290 |     2.2
   85 |   1.2575 |     42.122 |   1.3032 |     42.984 |     2.2
   86 |   1.2504 |     41.916 |   1.2972 |     42.831 |     2.2
   87 |   1.2457 |     41.661 |   1.3023 |     43.444 |     2.3
   88 |   1.2460 |     41.824 |   1.2964 |     43.076 |     2.3
   89 |   1.2360 |     41.396 |   1.2948 |     43.045 |     2.3
   90 |   1.2344 |     41.531 |   1.2918 |     43.168 |     2.3
   91 |   1.2319 |     41.602 |   1.2940 |     43.290 |     2.4
   92 |   1.2279 |     41.228 |   1.2880 |     43.137 |     2.4
   93 |   1.2213 |     41.065 |   1.2899 |     42.770 |     2.4
   94 |   1.2231 |     41.114 |   1.2861 |     42.862 |     2.4
   95 |   1.2188 |     40.897 |   1.2838 |     43.137 |     2.5
   96 |   1.2139 |     40.892 |   1.2855 |     43.137 |     2.5
   97 |   1.2101 |     41.071 |   1.2812 |     42.892 |     2.5
   98 |   1.2065 |     40.876 |   1.2810 |     42.800 |     2.5
   99 |   1.2038 |     40.849 |   1.2813 |     42.984 |     2.6
  100 |   1.2039 |     40.794 |   1.2817 |     42.892 |     2.6
  101 |   1.2013 |     40.458 |   1.2808 |     42.678 |     2.6
  102 |   1.2014 |     40.572 |   1.2828 |     43.260 |     2.6
  103 |   1.1904 |     40.296 |   1.2768 |     43.045 |     2.7
  104 |   1.1866 |     40.285 |   1.2774 |     42.862 |     2.7
  105 |   1.1883 |     40.285 |   1.2783 |     42.616 |     2.7
  106 |   1.1829 |     39.906 |   1.2780 |     42.463 |     2.7
  107 |   1.1830 |     39.906 |   1.2807 |     42.739 |     2.8
  108 |   1.1772 |     39.819 |   1.2728 |     42.433 |     2.8
  109 |   1.1760 |     40.025 |   1.2712 |     42.984 |     2.8
  110 |   1.1724 |     39.667 |   1.2692 |     42.953 |     2.8
  111 |   1.1677 |     39.402 |   1.2687 |     42.004 |     2.9
  112 |   1.1633 |     39.033 |   1.2713 |     42.279 |     2.9
  113 |   1.1600 |     39.033 |   1.2638 |     42.157 |     2.9
  114 |   1.1596 |     38.833 |   1.2684 |     42.157 |     3.0
  115 |   1.1604 |     39.142 |   1.2672 |     42.188 |     3.0
  116 |   1.1560 |     38.594 |   1.2602 |     42.126 |     3.0
  117 |   1.1502 |     38.383 |   1.2618 |     42.065 |     3.0
  118 |   1.1479 |     38.307 |   1.2656 |     42.065 |     3.1
  119 |   1.1454 |     38.242 |   1.2601 |     42.371 |     3.1
  120 |   1.1434 |     38.242 |   1.2625 |     42.218 |     3.1
  121 |   1.1391 |     38.264 |   1.2611 |     42.096 |     3.1
  122 |   1.1395 |     37.993 |   1.2598 |     42.525 |     3.2
  123 |   1.1373 |     38.112 |   1.2603 |     41.789 |     3.2
  124 |   1.1284 |     37.679 |   1.2575 |     41.605 |     3.2
  125 |   1.1299 |     37.115 |   1.2567 |     41.942 |     3.2
  126 |   1.1257 |     37.663 |   1.2531 |     41.881 |     3.3
  127 |   1.1228 |     37.283 |   1.2539 |     41.942 |     3.3
  128 |   1.1263 |     37.175 |   1.2546 |     41.820 |     3.3
  129 |   1.1192 |     37.381 |   1.2587 |     42.065 |     3.3
  130 |   1.1148 |     37.104 |   1.2556 |     41.759 |     3.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 501,090

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4663 |     88.215 |   3.2128 |     82.966 |     0.0
    2 |   2.8321 |     83.312 |   2.6498 |     83.303 |     0.0
    3 |   2.5186 |     62.657 |   2.4269 |     58.824 |     0.1
    4 |   2.3279 |     59.049 |   2.2684 |     58.824 |     0.1
    5 |   2.1940 |     59.049 |   2.1517 |     58.824 |     0.1
    6 |   2.0938 |     58.518 |   2.0577 |     53.768 |     0.1
    7 |   2.0093 |     55.364 |   1.9777 |     53.768 |     0.1
    8 |   1.9387 |     52.492 |   1.9066 |     48.438 |     0.1
    9 |   1.8682 |     48.618 |   1.8351 |     48.376 |     0.2
   10 |   1.8006 |     48.613 |   1.7672 |     48.346 |     0.2
   11 |   1.7417 |     48.602 |   1.7100 |     48.346 |     0.2
   12 |   1.6894 |     48.613 |   1.6604 |     48.346 |     0.2
   13 |   1.6475 |     47.248 |   1.6183 |     45.466 |     0.2
   14 |   1.6089 |     46.207 |   1.5841 |     45.466 |     0.2
   15 |   1.5775 |     46.213 |   1.5578 |     45.466 |     0.3
   16 |   1.5513 |     46.207 |   1.5360 |     45.466 |     0.3
   17 |   1.5327 |     46.213 |   1.5182 |     45.466 |     0.3
   18 |   1.5178 |     46.213 |   1.5031 |     45.466 |     0.3
   19 |   1.5023 |     46.207 |   1.4902 |     45.466 |     0.3
   20 |   1.4912 |     46.213 |   1.4797 |     45.466 |     0.3
   21 |   1.4810 |     46.213 |   1.4697 |     45.466 |     0.4
   22 |   1.4714 |     46.213 |   1.4610 |     45.466 |     0.4
   23 |   1.4618 |     46.207 |   1.4540 |     45.466 |     0.4
   24 |   1.4561 |     46.240 |   1.4479 |     45.466 |     0.4
   25 |   1.4481 |     46.207 |   1.4428 |     45.466 |     0.4
   26 |   1.4448 |     46.213 |   1.4377 |     45.466 |     0.4
   27 |   1.4443 |     46.207 |   1.4332 |     45.466 |     0.5
   28 |   1.4356 |     46.213 |   1.4297 |     45.466 |     0.5
   29 |   1.4331 |     46.223 |   1.4254 |     45.466 |     0.5
   30 |   1.4291 |     46.202 |   1.4222 |     45.466 |     0.5
   31 |   1.4221 |     46.218 |   1.4187 |     45.466 |     0.5
   32 |   1.4207 |     46.218 |   1.4146 |     45.466 |     0.5
   33 |   1.4173 |     46.207 |   1.4117 |     45.496 |     0.6
   34 |   1.4091 |     46.121 |   1.4074 |     45.404 |     0.6
   35 |   1.4060 |     45.920 |   1.4037 |     45.190 |     0.6
   36 |   1.4027 |     45.714 |   1.3984 |     44.914 |     0.6
   37 |   1.3929 |     45.373 |   1.3912 |     44.884 |     0.6
   38 |   1.3852 |     45.248 |   1.3860 |     44.914 |     0.6
   39 |   1.3760 |     44.977 |   1.3801 |     44.638 |     0.7
   40 |   1.3689 |     44.761 |   1.3775 |     44.547 |     0.7
   41 |   1.3638 |     44.598 |   1.3695 |     44.638 |     0.7
   42 |   1.3537 |     44.403 |   1.3651 |     44.485 |     0.7
   43 |   1.3460 |     44.267 |   1.3597 |     44.485 |     0.7
   44 |   1.3396 |     44.154 |   1.3656 |     44.822 |     0.7
   45 |   1.3317 |     44.127 |   1.3533 |     44.669 |     0.8
   46 |   1.3208 |     43.845 |   1.3466 |     44.516 |     0.8
   47 |   1.3117 |     43.791 |   1.3391 |     44.332 |     0.8
   48 |   1.2987 |     43.563 |   1.3354 |     44.393 |     0.8
   49 |   1.2915 |     43.525 |   1.3232 |     44.026 |     0.8
   50 |   1.2786 |     42.902 |   1.3165 |     43.781 |     0.8
   51 |   1.2647 |     42.469 |   1.3119 |     43.689 |     0.9
   52 |   1.2570 |     42.078 |   1.2961 |     43.076 |     0.9
   53 |   1.2462 |     41.889 |   1.2932 |     42.923 |     0.9
   54 |   1.2356 |     41.434 |   1.2928 |     42.433 |     0.9
   55 |   1.2293 |     40.838 |   1.2855 |     42.341 |     0.9
   56 |   1.2250 |     40.892 |   1.2822 |     42.494 |     0.9
   57 |   1.2149 |     40.686 |   1.2773 |     42.126 |     1.0
   58 |   1.2084 |     40.529 |   1.2812 |     42.218 |     1.0
   59 |   1.2019 |     40.383 |   1.2714 |     42.157 |     1.0
   60 |   1.1923 |     40.160 |   1.2720 |     42.096 |     1.0
   61 |   1.1871 |     39.895 |   1.2654 |     41.789 |     1.0
   62 |   1.1803 |     39.597 |   1.2592 |     41.759 |     1.0
   63 |   1.1727 |     39.472 |   1.2612 |     41.605 |     1.1
   64 |   1.1683 |     39.093 |   1.2595 |     41.820 |     1.1
   65 |   1.1571 |     38.876 |   1.2528 |     41.452 |     1.1
   66 |   1.1521 |     38.383 |   1.2561 |     41.513 |     1.1
   67 |   1.1461 |     38.248 |   1.2533 |     41.452 |     1.1
   68 |   1.1411 |     38.194 |   1.2530 |     41.544 |     1.2
   69 |   1.1336 |     37.533 |   1.2503 |     41.391 |     1.2
   70 |   1.1284 |     37.749 |   1.2490 |     41.146 |     1.2
   71 |   1.1207 |     37.386 |   1.2492 |     41.238 |     1.2
   72 |   1.1167 |     36.926 |   1.2392 |     41.115 |     1.2
   73 |   1.1074 |     37.029 |   1.2383 |     40.625 |     1.2
   74 |   1.1010 |     36.801 |   1.2396 |     41.054 |     1.3
   75 |   1.0978 |     36.584 |   1.2451 |     41.176 |     1.3
   76 |   1.0898 |     36.227 |   1.2303 |     40.625 |     1.3
   77 |   1.0846 |     35.929 |   1.2356 |     40.165 |     1.3
   78 |   1.0837 |     35.712 |   1.2290 |     40.227 |     1.3
   79 |   1.0775 |     35.809 |   1.2294 |     40.564 |     1.3
   80 |   1.0646 |     35.284 |   1.2260 |     39.828 |     1.4
   81 |   1.0589 |     34.948 |   1.2288 |     40.135 |     1.4
   82 |   1.0570 |     34.715 |   1.2367 |     40.441 |     1.4
   83 |   1.0502 |     34.607 |   1.2211 |     39.522 |     1.4
   84 |   1.0434 |     34.319 |   1.2154 |     38.725 |     1.4
   85 |   1.0363 |     33.788 |   1.2160 |     39.246 |     1.5
   86 |   1.0271 |     33.480 |   1.2158 |     38.971 |     1.5
   87 |   1.0240 |     33.350 |   1.2314 |     39.093 |     1.5
   88 |   1.0170 |     33.225 |   1.2155 |     38.603 |     1.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 749,890

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2518 |     81.789 |   2.7441 |     83.333 |     0.0
    2 |   2.4834 |     65.106 |   2.2383 |     58.824 |     0.0
    3 |   2.0841 |     57.255 |   1.9913 |     53.891 |     0.1
    4 |   1.9066 |     50.975 |   1.8459 |     48.346 |     0.1
    5 |   1.7866 |     48.602 |   1.7322 |     48.346 |     0.1
    6 |   1.6862 |     48.602 |   1.6360 |     48.346 |     0.1
    7 |   1.6020 |     48.212 |   1.5627 |     45.466 |     0.1
    8 |   1.5467 |     46.202 |   1.5131 |     45.466 |     0.2
    9 |   1.5047 |     46.218 |   1.4805 |     45.466 |     0.2
   10 |   1.4759 |     46.213 |   1.4586 |     45.466 |     0.2
   11 |   1.4561 |     46.294 |   1.4439 |     45.466 |     0.2
   12 |   1.4438 |     46.202 |   1.4331 |     45.466 |     0.2
   13 |   1.4331 |     46.202 |   1.4242 |     45.466 |     0.2
   14 |   1.4248 |     46.158 |   1.4163 |     45.987 |     0.3
   15 |   1.4166 |     46.267 |   1.4085 |     45.466 |     0.3
   16 |   1.4082 |     46.375 |   1.4013 |     45.466 |     0.3
   17 |   1.3988 |     46.213 |   1.3942 |     45.466 |     0.3
   18 |   1.3910 |     46.213 |   1.3894 |     45.466 |     0.3
   19 |   1.3861 |     46.278 |   1.3829 |     45.282 |     0.4
   20 |   1.3742 |     46.055 |   1.3777 |     45.067 |     0.4
   21 |   1.3669 |     45.562 |   1.3719 |     45.251 |     0.4
   22 |   1.3621 |     45.562 |   1.3681 |     44.792 |     0.4
   23 |   1.3523 |     45.145 |   1.3624 |     43.995 |     0.4
   24 |   1.3482 |     44.815 |   1.3563 |     44.424 |     0.4
   25 |   1.3401 |     44.701 |   1.3547 |     44.026 |     0.5
   26 |   1.3376 |     44.473 |   1.3578 |     44.547 |     0.5
   27 |   1.3324 |     44.506 |   1.3480 |     43.903 |     0.5
   28 |   1.3241 |     44.343 |   1.3469 |     44.638 |     0.5
   29 |   1.3222 |     44.403 |   1.3398 |     43.413 |     0.5
   30 |   1.3132 |     44.148 |   1.3386 |     43.566 |     0.6
   31 |   1.3096 |     43.780 |   1.3451 |     44.424 |     0.6
   32 |   1.3055 |     43.904 |   1.3362 |     43.627 |     0.6
   33 |   1.3017 |     43.677 |   1.3452 |     44.240 |     0.6
   34 |   1.2961 |     43.536 |   1.3344 |     43.903 |     0.6
   35 |   1.2913 |     43.433 |   1.3328 |     44.393 |     0.6
   36 |   1.2878 |     43.433 |   1.3356 |     43.658 |     0.7
   37 |   1.2839 |     43.444 |   1.3274 |     43.689 |     0.7
   38 |   1.2775 |     43.151 |   1.3277 |     43.260 |     0.7
   39 |   1.2730 |     43.195 |   1.3205 |     43.903 |     0.7
   40 |   1.2664 |     42.842 |   1.3199 |     42.892 |     0.7
   41 |   1.2666 |     42.756 |   1.3119 |     42.923 |     0.8
   42 |   1.2596 |     42.414 |   1.3173 |     42.616 |     0.8
   43 |   1.2551 |     42.192 |   1.3104 |     42.555 |     0.8
   44 |   1.2519 |     41.986 |   1.3097 |     42.371 |     0.8
   45 |   1.2432 |     41.623 |   1.3027 |     42.096 |     0.8
   46 |   1.2413 |     41.184 |   1.2989 |     41.513 |     0.8
   47 |   1.2317 |     41.163 |   1.2939 |     41.728 |     0.9
   48 |   1.2271 |     40.903 |   1.2910 |     41.360 |     0.9
   49 |   1.2181 |     40.393 |   1.2858 |     41.513 |     0.9
   50 |   1.2151 |     40.496 |   1.2773 |     41.667 |     0.9
   51 |   1.2101 |     40.355 |   1.2737 |     41.360 |     0.9
   52 |   1.2024 |     40.307 |   1.2686 |     41.636 |     1.0
   53 |   1.1957 |     40.150 |   1.2674 |     41.207 |     1.0
   54 |   1.1885 |     39.949 |   1.2621 |     41.299 |     1.0
   55 |   1.1844 |     39.987 |   1.2568 |     41.391 |     1.0
   56 |   1.1777 |     39.862 |   1.2785 |     41.422 |     1.0
   57 |   1.1802 |     39.792 |   1.2512 |     40.809 |     1.0
   58 |   1.1658 |     39.586 |   1.2440 |     40.901 |     1.1
   59 |   1.1645 |     39.602 |   1.2487 |     40.656 |     1.1
   60 |   1.1576 |     39.272 |   1.2438 |     40.748 |     1.1
   61 |   1.1577 |     39.028 |   1.2489 |     41.299 |     1.1
   62 |   1.1516 |     39.185 |   1.2404 |     40.380 |     1.1
   63 |   1.1417 |     38.735 |   1.2311 |     40.748 |     1.2
   64 |   1.1391 |     38.611 |   1.2315 |     40.594 |     1.2
   65 |   1.1322 |     38.264 |   1.2305 |     40.594 |     1.2
   66 |   1.1295 |     38.145 |   1.2280 |     40.288 |     1.2
   67 |   1.1200 |     38.237 |   1.2240 |     39.982 |     1.2
   68 |   1.1141 |     37.950 |   1.2232 |     40.196 |     1.3
   69 |   1.1100 |     37.673 |   1.2184 |     40.196 |     1.3
   70 |   1.1008 |     37.305 |   1.2152 |     39.859 |     1.3
   71 |   1.0987 |     37.294 |   1.2124 |     39.308 |     1.3
   72 |   1.0926 |     37.056 |   1.2116 |     39.706 |     1.3
   73 |   1.0869 |     36.823 |   1.2111 |     39.583 |     1.3
   74 |   1.0746 |     36.568 |   1.2115 |     39.706 |     1.4
   75 |   1.0693 |     36.070 |   1.2060 |     38.971 |     1.4
   76 |   1.0618 |     35.929 |   1.2083 |     39.154 |     1.4
   77 |   1.0597 |     35.837 |   1.2011 |     38.787 |     1.4
   78 |   1.0515 |     35.734 |   1.1939 |     38.879 |     1.4
   79 |   1.0438 |     35.354 |   1.1924 |     38.634 |     1.4
   80 |   1.0342 |     35.360 |   1.1918 |     38.664 |     1.5
   81 |   1.0328 |     35.284 |   1.1991 |     38.634 |     1.5
   82 |   1.0247 |     35.073 |   1.1876 |     38.358 |     1.5
   83 |   1.0184 |     34.769 |   1.1898 |     38.511 |     1.5
   84 |   1.0049 |     34.249 |   1.1850 |     38.021 |     1.5
   85 |   1.0041 |     34.054 |   1.1731 |     37.868 |     1.6
   86 |   0.9941 |     33.864 |   1.1856 |     38.297 |     1.6
   87 |   0.9865 |     33.604 |   1.1926 |     38.480 |     1.6
   88 |   0.9816 |     33.350 |   1.1737 |     37.469 |     1.6
   89 |   0.9753 |     32.846 |   1.1852 |     38.113 |     1.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 782,146

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8283 |     73.700 |   2.3942 |     58.824 |     0.0
    2 |   2.1693 |     58.615 |   1.9618 |     58.701 |     0.1
    3 |   1.8312 |     50.780 |   1.6938 |     48.346 |     0.1
    4 |   1.6235 |     46.819 |   1.5483 |     45.466 |     0.1
    5 |   1.5235 |     46.213 |   1.4858 |     45.466 |     0.2
    6 |   1.4762 |     46.088 |   1.4539 |     45.466 |     0.2
    7 |   1.4534 |     46.207 |   1.4368 |     45.987 |     0.2
    8 |   1.4364 |     46.294 |   1.4237 |     45.987 |     0.3
    9 |   1.4286 |     46.180 |   1.4124 |     45.466 |     0.3
   10 |   1.4188 |     46.272 |   1.4072 |     45.466 |     0.3
   11 |   1.4066 |     46.272 |   1.3981 |     45.466 |     0.4
   12 |   1.3974 |     46.359 |   1.3883 |     45.987 |     0.4
   13 |   1.3887 |     46.121 |   1.3749 |     45.404 |     0.4
   14 |   1.3736 |     45.860 |   1.3699 |     45.282 |     0.5
   15 |   1.3677 |     45.817 |   1.3648 |     44.945 |     0.5
   16 |   1.3580 |     45.362 |   1.3425 |     44.608 |     0.5
   17 |   1.3431 |     44.636 |   1.3270 |     42.770 |     0.6
   18 |   1.3285 |     44.365 |   1.3150 |     43.168 |     0.6
   19 |   1.3149 |     43.899 |   1.3034 |     43.321 |     0.6
   20 |   1.3056 |     43.688 |   1.2924 |     42.862 |     0.7
   21 |   1.3005 |     43.498 |   1.2903 |     42.218 |     0.7
   22 |   1.2925 |     43.146 |   1.2821 |     41.667 |     0.7
   23 |   1.2800 |     42.788 |   1.2772 |     41.115 |     0.8
   24 |   1.2671 |     42.322 |   1.2668 |     41.360 |     0.8
   25 |   1.2646 |     42.241 |   1.2570 |     41.575 |     0.9
   26 |   1.2570 |     42.095 |   1.2525 |     40.993 |     0.9
   27 |   1.2540 |     41.818 |   1.2517 |     40.901 |     0.9
   28 |   1.2440 |     41.455 |   1.2382 |     40.594 |     1.0
   29 |   1.2308 |     40.962 |   1.2318 |     40.196 |     1.0
   30 |   1.2202 |     40.762 |   1.2269 |     39.767 |     1.0
   31 |   1.2158 |     40.610 |   1.2267 |     39.982 |     1.1
   32 |   1.2130 |     40.529 |   1.2169 |     40.043 |     1.1
   33 |   1.2007 |     40.290 |   1.2133 |     39.859 |     1.1
   34 |   1.1943 |     40.150 |   1.2071 |     39.277 |     1.2
   35 |   1.1882 |     39.787 |   1.2037 |     39.461 |     1.2
   36 |   1.1775 |     39.841 |   1.2006 |     39.369 |     1.2
   37 |   1.1700 |     39.358 |   1.1902 |     39.062 |     1.3
   38 |   1.1654 |     39.331 |   1.1890 |     39.185 |     1.3
   39 |   1.1579 |     39.055 |   1.1884 |     39.308 |     1.3
   40 |   1.1518 |     38.936 |   1.1789 |     39.277 |     1.4
   41 |   1.1468 |     38.947 |   1.1800 |     39.216 |     1.4
   42 |   1.1449 |     38.584 |   1.1747 |     38.909 |     1.4
   43 |   1.1364 |     38.643 |   1.1672 |     38.542 |     1.5
   44 |   1.1305 |     38.329 |   1.1675 |     38.603 |     1.5
   45 |   1.1237 |     38.280 |   1.1620 |     38.021 |     1.5
   46 |   1.1222 |     38.053 |   1.1510 |     38.358 |     1.6
   47 |   1.1129 |     38.139 |   1.1481 |     38.205 |     1.6
   48 |   1.1058 |     37.386 |   1.1491 |     38.450 |     1.6
   49 |   1.1084 |     37.923 |   1.1389 |     38.205 |     1.7
   50 |   1.0998 |     37.673 |   1.1510 |     38.205 |     1.7
   51 |   1.0942 |     37.121 |   1.1463 |     37.898 |     1.7
   52 |   1.0832 |     37.218 |   1.1330 |     37.837 |     1.8
   53 |   1.0743 |     36.812 |   1.1451 |     37.714 |     1.8
   54 |   1.0793 |     36.985 |   1.1381 |     37.806 |     1.8
   55 |   1.0728 |     36.601 |   1.1243 |     37.776 |     1.9
   56 |   1.0639 |     36.238 |   1.1251 |     36.795 |     1.9
   57 |   1.0582 |     35.983 |   1.1257 |     37.714 |     1.9
   58 |   1.0488 |     35.999 |   1.1240 |     37.377 |     2.0
   59 |   1.0466 |     35.788 |   1.1204 |     37.347 |     2.0
   60 |   1.0446 |     35.544 |   1.1246 |     37.316 |     2.0
   61 |   1.0319 |     35.067 |   1.1158 |     37.040 |     2.1
   62 |   1.0308 |     34.872 |   1.1084 |     36.489 |     2.1
   63 |   1.0216 |     34.834 |   1.1076 |     36.183 |     2.1
   64 |   1.0227 |     34.704 |   1.1062 |     35.815 |     2.2
   65 |   1.0164 |     34.628 |   1.0989 |     35.172 |     2.2
   66 |   1.0074 |     34.298 |   1.0932 |     35.784 |     2.2
   67 |   1.0154 |     34.552 |   1.0857 |     35.386 |     2.3
   68 |   1.0037 |     34.254 |   1.1126 |     36.366 |     2.3
   69 |   0.9970 |     34.038 |   1.1017 |     35.509 |     2.3
   70 |   0.9932 |     33.610 |   1.0983 |     34.773 |     2.4
   71 |   0.9901 |     33.550 |   1.0875 |     35.202 |     2.4
   72 |   0.9946 |     33.745 |   1.0799 |     34.436 |     2.4
   73 |   0.9786 |     33.263 |   1.0814 |     34.559 |     2.5
   74 |   0.9719 |     32.900 |   1.0907 |     35.263 |     2.5
   75 |   0.9691 |     32.922 |   1.0804 |     34.436 |     2.5
   76 |   0.9626 |     32.786 |   1.0778 |     35.080 |     2.6
   77 |   0.9621 |     32.775 |   1.0826 |     35.018 |     2.6
   78 |   0.9562 |     32.309 |   1.0653 |     34.712 |     2.6
   79 |   0.9477 |     32.065 |   1.0544 |     33.885 |     2.7
   80 |   0.9392 |     31.854 |   1.0590 |     34.130 |     2.7
   81 |   0.9386 |     31.448 |   1.0505 |     33.854 |     2.7
   82 |   0.9290 |     31.383 |   1.0603 |     34.283 |     2.8
   83 |   0.9339 |     31.383 |   1.0675 |     34.528 |     2.8
   84 |   0.9343 |     31.762 |   1.0632 |     34.069 |     2.8
   85 |   0.9665 |     32.867 |   1.0525 |     34.099 |     2.9
   86 |   0.9386 |     31.778 |   1.0437 |     33.487 |     2.9
   87 |   0.9247 |     31.534 |   1.0267 |     33.487 |     2.9
   88 |   0.9141 |     30.798 |   1.0456 |     34.130 |     3.0
   89 |   0.9146 |     31.123 |   1.0460 |     33.272 |     3.0
   90 |   0.9052 |     30.917 |   1.0397 |     34.161 |     3.0
   91 |   0.8982 |     30.337 |   1.0364 |     33.793 |     3.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 169,730

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8817 |     76.555 |   2.3666 |     58.824 |     0.0
    2 |   2.1605 |     57.775 |   1.9875 |     53.891 |     0.0
    3 |   1.8731 |     51.745 |   1.7410 |     48.346 |     0.0
    4 |   1.6581 |     47.502 |   1.5596 |     45.466 |     0.1
    5 |   1.5280 |     46.261 |   1.4839 |     45.466 |     0.1
    6 |   1.4740 |     46.359 |   1.4519 |     45.466 |     0.1
    7 |   1.4407 |     46.505 |   1.4164 |     45.466 |     0.1
    8 |   1.4101 |     46.104 |   1.3936 |     45.772 |     0.1
    9 |   1.3852 |     45.692 |   1.3774 |     45.312 |     0.1
   10 |   1.3629 |     45.194 |   1.3560 |     44.638 |     0.1
   11 |   1.3314 |     44.793 |   1.3272 |     45.129 |     0.2
   12 |   1.3119 |     44.695 |   1.3051 |     44.363 |     0.2
   13 |   1.2913 |     43.883 |   1.3077 |     43.689 |     0.2
   14 |   1.2769 |     43.249 |   1.2889 |     43.903 |     0.2
   15 |   1.2643 |     43.054 |   1.2803 |     43.321 |     0.2
   16 |   1.2561 |     43.113 |   1.2720 |     43.505 |     0.2
   17 |   1.2426 |     42.788 |   1.2628 |     42.984 |     0.2
   18 |   1.2299 |     42.414 |   1.2555 |     43.352 |     0.3
   19 |   1.2226 |     42.154 |   1.2452 |     42.463 |     0.3
   20 |   1.2124 |     41.780 |   1.2497 |     42.953 |     0.3
   21 |   1.2111 |     41.748 |   1.2364 |     42.188 |     0.3
   22 |   1.1934 |     40.897 |   1.2348 |     42.463 |     0.3
   23 |   1.1841 |     40.729 |   1.2307 |     42.065 |     0.3
   24 |   1.1771 |     40.578 |   1.2105 |     40.993 |     0.3
   25 |   1.1672 |     40.399 |   1.2132 |     41.176 |     0.4
   26 |   1.1588 |     40.090 |   1.2068 |     41.912 |     0.4
   27 |   1.1530 |     39.841 |   1.2086 |     42.065 |     0.4
   28 |   1.1515 |     39.852 |   1.1986 |     41.299 |     0.4
   29 |   1.1448 |     39.694 |   1.1929 |     40.839 |     0.4
   30 |   1.1289 |     39.212 |   1.1855 |     40.564 |     0.4
   31 |   1.1205 |     38.920 |   1.1802 |     40.472 |     0.4
   32 |   1.1104 |     38.540 |   1.1782 |     39.185 |     0.5
   33 |   1.0973 |     38.069 |   1.1663 |     39.614 |     0.5
   34 |   1.0875 |     37.489 |   1.1535 |     39.093 |     0.5
   35 |   1.0777 |     37.148 |   1.1505 |     39.614 |     0.5
   36 |   1.0636 |     36.660 |   1.1393 |     38.971 |     0.5
   37 |   1.0612 |     36.357 |   1.1425 |     39.246 |     0.5
   38 |   1.0544 |     36.010 |   1.1305 |     38.756 |     0.5
   39 |   1.0452 |     35.891 |   1.1191 |     38.756 |     0.5
   40 |   1.0271 |     35.257 |   1.1185 |     38.051 |     0.6
   41 |   1.0164 |     34.926 |   1.1225 |     38.051 |     0.6
   42 |   1.0213 |     34.926 |   1.1187 |     37.868 |     0.6
   43 |   1.0145 |     34.682 |   1.1085 |     37.684 |     0.6
   44 |   1.0012 |     34.249 |   1.1141 |     37.623 |     0.6
   45 |   0.9932 |     33.783 |   1.1149 |     37.623 |     0.6
   46 |   0.9918 |     33.610 |   1.0962 |     37.255 |     0.6
   47 |   0.9761 |     33.133 |   1.0848 |     36.152 |     0.7
   48 |   0.9698 |     32.943 |   1.0981 |     37.316 |     0.7
   49 |   0.9653 |     32.672 |   1.0901 |     36.213 |     0.7
   50 |   0.9505 |     32.185 |   1.0906 |     35.784 |     0.7
   51 |   0.9489 |     32.055 |   1.0937 |     36.826 |     0.7
   52 |   0.9396 |     31.719 |   1.0818 |     35.968 |     0.7
   53 |   0.9388 |     31.870 |   1.0920 |     36.091 |     0.7
   54 |   0.9274 |     31.464 |   1.0771 |     35.999 |     0.8
   55 |   0.9136 |     30.873 |   1.0815 |     35.846 |     0.8
   56 |   0.9130 |     31.052 |   1.0686 |     35.355 |     0.8
   57 |   0.9153 |     30.705 |   1.0885 |     35.938 |     0.8
   58 |   0.9088 |     30.657 |   1.0712 |     35.294 |     0.8
   59 |   0.9023 |     30.695 |   1.0811 |     35.662 |     0.8
   60 |   0.9041 |     30.380 |   1.0683 |     35.049 |     0.8
   61 |   0.8905 |     29.920 |   1.0648 |     35.110 |     0.8
   62 |   0.8863 |     29.931 |   1.0666 |     34.896 |     0.9
   63 |   0.8746 |     29.524 |   1.0780 |     34.620 |     0.9
   64 |   0.8747 |     29.410 |   1.0747 |     34.926 |     0.9
   65 |   0.8773 |     29.811 |   1.0687 |     35.417 |     0.9
   66 |   0.8648 |     29.053 |   1.0558 |     34.988 |     0.9
   67 |   0.8632 |     28.896 |   1.0750 |     35.570 |     0.9
   68 |   0.9047 |     30.559 |   1.0610 |     35.172 |     0.9
   69 |   0.8611 |     29.010 |   1.0697 |     34.773 |     1.0
   70 |   0.8446 |     28.571 |   1.0596 |     34.314 |     1.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 488,866

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3575 |     84.244 |   3.0165 |     83.333 |     0.0
    2 |   2.6613 |     67.284 |   2.4013 |     58.915 |     0.0
    3 |   2.2580 |     58.322 |   2.1547 |     56.679 |     0.1
    4 |   2.0752 |     55.857 |   2.0161 |     56.556 |     0.1
    5 |   1.9562 |     49.556 |   1.9095 |     48.376 |     0.1
    6 |   1.8571 |     48.461 |   1.8169 |     48.162 |     0.1
    7 |   1.7709 |     48.304 |   1.7343 |     48.039 |     0.1
    8 |   1.6982 |     48.288 |   1.6672 |     48.039 |     0.1
    9 |   1.6375 |     48.277 |   1.6101 |     48.100 |     0.2
   10 |   1.5848 |     48.223 |   1.5637 |     48.039 |     0.2
   11 |   1.5420 |     46.158 |   1.5248 |     45.037 |     0.2
   12 |   1.5072 |     45.720 |   1.4961 |     45.037 |     0.2
   13 |   1.4750 |     45.584 |   1.4672 |     44.975 |     0.2
   14 |   1.4469 |     45.411 |   1.4530 |     44.945 |     0.3
   15 |   1.4261 |     45.226 |   1.4206 |     44.577 |     0.3
   16 |   1.3987 |     44.950 |   1.4014 |     44.393 |     0.3
   17 |   1.3790 |     44.798 |   1.3819 |     44.424 |     0.3
   18 |   1.3591 |     44.528 |   1.3696 |     43.811 |     0.3
   19 |   1.3425 |     44.300 |   1.3529 |     43.873 |     0.4
   20 |   1.3261 |     43.883 |   1.3419 |     43.995 |     0.4
   21 |   1.3117 |     43.558 |   1.3290 |     43.627 |     0.4
   22 |   1.2945 |     43.157 |   1.3200 |     43.627 |     0.4
   23 |   1.2828 |     41.434 |   1.3042 |     41.299 |     0.4
   24 |   1.2655 |     40.442 |   1.2947 |     41.575 |     0.5
   25 |   1.2560 |     40.090 |   1.2882 |     41.268 |     0.5
   26 |   1.2414 |     39.597 |   1.2795 |     41.023 |     0.5
   27 |   1.2303 |     39.364 |   1.2687 |     40.594 |     0.5
   28 |   1.2152 |     38.882 |   1.2622 |     40.074 |     0.5
   29 |   1.2074 |     38.519 |   1.2541 |     40.074 |     0.5
   30 |   1.1926 |     38.085 |   1.2435 |     39.675 |     0.6
   31 |   1.1815 |     37.646 |   1.2383 |     39.859 |     0.6
   32 |   1.1720 |     37.435 |   1.2293 |     39.430 |     0.6
   33 |   1.1639 |     36.942 |   1.2206 |     39.277 |     0.6
   34 |   1.1498 |     36.731 |   1.2164 |     39.338 |     0.6
   35 |   1.1386 |     36.395 |   1.2050 |     39.277 |     0.7
   36 |   1.1312 |     35.923 |   1.1994 |     38.848 |     0.7
   37 |   1.1175 |     35.338 |   1.1933 |     38.971 |     0.7
   38 |   1.1080 |     35.213 |   1.1901 |     38.450 |     0.7
   39 |   1.0966 |     34.617 |   1.1828 |     38.174 |     0.7
   40 |   1.0869 |     34.466 |   1.1731 |     38.021 |     0.7
   41 |   1.0765 |     34.314 |   1.1671 |     37.837 |     0.8
   42 |   1.0671 |     33.864 |   1.1621 |     37.806 |     0.8
   43 |   1.0579 |     33.507 |   1.1546 |     37.347 |     0.8
   44 |   1.0486 |     32.894 |   1.1513 |     36.887 |     0.8
   45 |   1.0381 |     32.553 |   1.1400 |     36.213 |     0.8
   46 |   1.0288 |     31.973 |   1.1363 |     36.121 |     0.9
   47 |   1.0201 |     31.697 |   1.1325 |     36.183 |     0.9
   48 |   1.0072 |     31.258 |   1.1277 |     35.938 |     0.9
   49 |   0.9987 |     30.787 |   1.1252 |     35.539 |     0.9
   50 |   0.9908 |     30.575 |   1.1187 |     36.029 |     0.9
   51 |   0.9833 |     30.077 |   1.1123 |     35.018 |     1.0
   52 |   0.9732 |     29.893 |   1.1025 |     34.467 |     1.0
   53 |   0.9657 |     29.584 |   1.1067 |     35.018 |     1.0
   54 |   0.9576 |     29.270 |   1.0988 |     34.283 |     1.0
   55 |   0.9482 |     29.015 |   1.0923 |     34.069 |     1.0
   56 |   0.9432 |     28.706 |   1.0952 |     34.436 |     1.1
   57 |   0.9331 |     28.267 |   1.0878 |     33.977 |     1.1
   58 |   0.9227 |     28.002 |   1.0807 |     33.854 |     1.1
   59 |   0.9141 |     27.747 |   1.0754 |     33.885 |     1.1
   60 |   0.9045 |     27.498 |   1.0729 |     33.517 |     1.1
   61 |   0.8972 |     27.113 |   1.0716 |     34.099 |     1.1
   62 |   0.8886 |     26.718 |   1.0720 |     34.007 |     1.2
   63 |   0.8809 |     26.691 |   1.0638 |     33.824 |     1.2
   64 |   0.8720 |     26.333 |   1.0619 |     33.578 |     1.2
   65 |   0.8654 |     26.257 |   1.0571 |     33.824 |     1.2
   66 |   0.8604 |     26.225 |   1.0520 |     33.180 |     1.2
   67 |   0.8522 |     25.867 |   1.0457 |     32.629 |     1.3
   68 |   0.8447 |     25.444 |   1.0506 |     32.996 |     1.3
   69 |   0.8368 |     25.114 |   1.0443 |     32.996 |     1.3
   70 |   0.8286 |     25.130 |   1.0393 |     32.904 |     1.3
   71 |   0.8271 |     24.865 |   1.0442 |     32.659 |     1.3
   72 |   0.8152 |     24.642 |   1.0350 |     32.629 |     1.4
   73 |   0.8080 |     24.290 |   1.0395 |     32.782 |     1.4
   74 |   0.8047 |     24.420 |   1.0330 |     32.721 |     1.4
   75 |   0.7941 |     24.025 |   1.0301 |     32.690 |     1.4
   76 |   0.7879 |     23.721 |   1.0390 |     33.058 |     1.4
   77 |   0.7821 |     23.597 |   1.0247 |     32.200 |     1.4
   78 |   0.7724 |     23.299 |   1.0237 |     32.843 |     1.5
   79 |   0.7692 |     23.201 |   1.0240 |     32.292 |     1.5
   80 |   0.7624 |     23.033 |   1.0246 |     31.893 |     1.5
   81 |   0.7573 |     22.887 |   1.0192 |     32.384 |     1.5
   82 |   0.7504 |     22.741 |   1.0169 |     32.016 |     1.5
   83 |   0.7433 |     22.345 |   1.0171 |     31.648 |     1.6
   84 |   0.7373 |     22.112 |   1.0193 |     32.138 |     1.6
   85 |   0.7326 |     22.036 |   1.0082 |     31.832 |     1.6
   86 |   0.7249 |     21.803 |   1.0093 |     32.230 |     1.6
   87 |   0.7190 |     21.576 |   1.0109 |     31.403 |     1.6
   88 |   0.7156 |     21.261 |   1.0057 |     31.464 |     1.7
   89 |   0.7096 |     21.121 |   1.0079 |     31.036 |     1.7
   90 |   0.7040 |     21.115 |   0.9993 |     30.852 |     1.7
   91 |   0.6963 |     20.730 |   1.0049 |     30.882 |     1.7
   92 |   0.6935 |     20.665 |   1.0060 |     31.219 |     1.7
   93 |   0.6871 |     20.557 |   1.0026 |     31.097 |     1.7
   94 |   0.6829 |     20.292 |   0.9970 |     30.607 |     1.8
   95 |   0.6756 |     19.896 |   0.9971 |     30.484 |     1.8
   96 |   0.6664 |     19.836 |   0.9949 |     30.300 |     1.8
   97 |   0.6618 |     19.495 |   0.9976 |     30.515 |     1.8
   98 |   0.6601 |     19.327 |   1.0007 |     30.362 |     1.8
   99 |   0.6540 |     19.435 |   0.9921 |     30.607 |     1.9
  100 |   0.6557 |     19.403 |   0.9913 |     29.933 |     1.9
  101 |   0.6448 |     18.948 |   0.9880 |     30.178 |     1.9
  102 |   0.6382 |     18.872 |   0.9853 |     30.086 |     1.9
  103 |   0.6334 |     18.585 |   0.9878 |     29.320 |     1.9
  104 |   0.6272 |     18.363 |   0.9838 |     29.871 |     2.0
  105 |   0.6251 |     18.168 |   0.9918 |     29.810 |     2.0
  106 |   0.6194 |     18.298 |   0.9869 |     29.565 |     2.0
  107 |   0.6129 |     18.016 |   0.9918 |     29.779 |     2.0
  108 |   0.6099 |     17.832 |   0.9850 |     29.841 |     2.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 198,818

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5488 |     98.786 |   3.5057 |     98.131 |     0.0
    2 |   3.3336 |     86.194 |   3.0854 |     80.974 |     0.0
    3 |   2.9184 |     82.038 |   2.8120 |     83.333 |     0.0
    4 |   2.7256 |     72.816 |   2.6658 |     67.341 |     0.1
    5 |   2.5873 |     66.699 |   2.5396 |     58.824 |     0.1
    6 |   2.4692 |     59.049 |   2.4344 |     58.824 |     0.1
    7 |   2.3761 |     59.049 |   2.3521 |     58.824 |     0.1
    8 |   2.3039 |     59.049 |   2.2874 |     58.824 |     0.1
    9 |   2.2461 |     59.049 |   2.2343 |     58.824 |     0.1
   10 |   2.1956 |     59.049 |   2.1895 |     58.824 |     0.1
   11 |   2.1576 |     59.049 |   2.1505 |     58.824 |     0.2
   12 |   2.1198 |     59.049 |   2.1153 |     58.824 |     0.2
   13 |   2.0880 |     57.624 |   2.0823 |     53.768 |     0.2
   14 |   2.0572 |     55.640 |   2.0505 |     53.799 |     0.2
   15 |   2.0284 |     54.080 |   2.0202 |     53.768 |     0.2
   16 |   1.9978 |     53.874 |   1.9915 |     53.768 |     0.2
   17 |   1.9742 |     53.869 |   1.9641 |     53.768 |     0.2
   18 |   1.9453 |     53.863 |   1.9378 |     53.768 |     0.3
   19 |   1.9209 |     53.863 |   1.9116 |     53.768 |     0.3
   20 |   1.8957 |     53.657 |   1.8846 |     52.911 |     0.3
   21 |   1.8684 |     50.894 |   1.8574 |     48.591 |     0.3
   22 |   1.8445 |     48.645 |   1.8318 |     48.376 |     0.3
   23 |   1.8216 |     48.613 |   1.8075 |     48.346 |     0.3
   24 |   1.7970 |     48.597 |   1.7841 |     48.346 |     0.3
   25 |   1.7744 |     48.607 |   1.7608 |     48.346 |     0.4
   26 |   1.7526 |     48.602 |   1.7383 |     48.346 |     0.4
   27 |   1.7323 |     48.597 |   1.7167 |     48.346 |     0.4
   28 |   1.7108 |     48.607 |   1.6946 |     48.346 |     0.4
   29 |   1.6868 |     48.607 |   1.6736 |     48.254 |     0.4
   30 |   1.6675 |     47.334 |   1.6522 |     45.466 |     0.4
   31 |   1.6512 |     46.207 |   1.6323 |     45.466 |     0.4
   32 |   1.6310 |     46.175 |   1.6144 |     45.404 |     0.5
   33 |   1.6125 |     46.148 |   1.5977 |     45.404 |     0.5
   34 |   1.5995 |     46.148 |   1.5826 |     45.404 |     0.5
   35 |   1.5810 |     46.131 |   1.5682 |     45.404 |     0.5
   36 |   1.5681 |     46.137 |   1.5536 |     45.404 |     0.5
   37 |   1.5511 |     46.093 |   1.5386 |     45.404 |     0.5
   38 |   1.5370 |     46.050 |   1.5251 |     45.343 |     0.5
   39 |   1.5247 |     46.039 |   1.5134 |     45.343 |     0.6
   40 |   1.5145 |     46.028 |   1.5039 |     45.343 |     0.6
   41 |   1.5088 |     46.012 |   1.4948 |     45.404 |     0.6
   42 |   1.4988 |     46.034 |   1.4874 |     45.343 |     0.6
   43 |   1.4899 |     46.039 |   1.4801 |     45.343 |     0.6
   44 |   1.4808 |     46.007 |   1.4729 |     45.343 |     0.6
   45 |   1.4725 |     46.001 |   1.4668 |     45.343 |     0.6
   46 |   1.4668 |     46.007 |   1.4603 |     45.343 |     0.7
   47 |   1.4594 |     46.007 |   1.4541 |     45.343 |     0.7
   48 |   1.4528 |     45.996 |   1.4494 |     45.343 |     0.7
   49 |   1.4475 |     45.996 |   1.4439 |     45.312 |     0.7
   50 |   1.4430 |     45.990 |   1.4387 |     45.312 |     0.7
   51 |   1.4364 |     45.985 |   1.4343 |     45.312 |     0.7
   52 |   1.4311 |     45.920 |   1.4293 |     45.312 |     0.7
   53 |   1.4261 |     45.985 |   1.4255 |     45.312 |     0.8
   54 |   1.4195 |     45.969 |   1.4222 |     45.251 |     0.8
   55 |   1.4139 |     45.644 |   1.4186 |     45.129 |     0.8
   56 |   1.4109 |     45.514 |   1.4156 |     44.975 |     0.8
   57 |   1.4044 |     45.048 |   1.4107 |     44.884 |     0.8
   58 |   1.4000 |     44.658 |   1.4074 |     44.945 |     0.8
   59 |   1.3948 |     44.593 |   1.4040 |     44.608 |     0.8
   60 |   1.3904 |     44.262 |   1.4022 |     44.853 |     0.8
   61 |   1.3851 |     44.175 |   1.3973 |     44.638 |     0.9
   62 |   1.3794 |     43.644 |   1.3943 |     44.608 |     0.9
   63 |   1.3730 |     43.720 |   1.3899 |     44.547 |     0.9
   64 |   1.3699 |     43.568 |   1.3872 |     44.669 |     0.9
   65 |   1.3652 |     43.417 |   1.3835 |     44.547 |     0.9
   66 |   1.3583 |     43.178 |   1.3834 |     44.393 |     0.9
   67 |   1.3546 |     43.108 |   1.3787 |     44.669 |     0.9
   68 |   1.3485 |     42.994 |   1.3756 |     44.332 |     1.0
   69 |   1.3420 |     42.842 |   1.3717 |     44.148 |     1.0
   70 |   1.3383 |     42.723 |   1.3696 |     43.995 |     1.0
   71 |   1.3365 |     42.582 |   1.3688 |     44.056 |     1.0
   72 |   1.3292 |     42.507 |   1.3656 |     43.934 |     1.0
   73 |   1.3241 |     42.328 |   1.3622 |     43.903 |     1.0
   74 |   1.3233 |     42.311 |   1.3594 |     43.689 |     1.0
   75 |   1.3152 |     42.317 |   1.3568 |     43.536 |     1.1
   76 |   1.3112 |     42.143 |   1.3535 |     43.290 |     1.1
   77 |   1.3091 |     42.116 |   1.3519 |     43.597 |     1.1
   78 |   1.3057 |     41.948 |   1.3503 |     43.995 |     1.1
   79 |   1.3006 |     41.921 |   1.3483 |     43.505 |     1.1
   80 |   1.2958 |     41.862 |   1.3479 |     43.566 |     1.1
   81 |   1.2928 |     41.699 |   1.3424 |     42.555 |     1.1
   82 |   1.2881 |     41.510 |   1.3415 |     42.862 |     1.2
   83 |   1.2831 |     41.482 |   1.3385 |     42.616 |     1.2
   84 |   1.2781 |     41.445 |   1.3364 |     42.708 |     1.2
   85 |   1.2737 |     41.439 |   1.3343 |     42.678 |     1.2
   86 |   1.2708 |     41.423 |   1.3327 |     42.525 |     1.2
   87 |   1.2667 |     41.374 |   1.3324 |     42.463 |     1.2
   88 |   1.2643 |     41.217 |   1.3281 |     42.555 |     1.2
   89 |   1.2621 |     41.466 |   1.3250 |     42.310 |     1.2
   90 |   1.2562 |     41.228 |   1.3235 |     42.433 |     1.3
   91 |   1.2543 |     41.168 |   1.3198 |     42.402 |     1.3
   92 |   1.2492 |     40.854 |   1.3244 |     42.034 |     1.3
   93 |   1.2454 |     40.518 |   1.3208 |     41.452 |     1.3
   94 |   1.2419 |     40.464 |   1.3148 |     41.483 |     1.3
   95 |   1.2362 |     40.122 |   1.3102 |     41.176 |     1.3
   96 |   1.2328 |     40.106 |   1.3145 |     41.238 |     1.3
   97 |   1.2326 |     40.030 |   1.3147 |     41.422 |     1.4
   98 |   1.2275 |     39.808 |   1.3074 |     41.146 |     1.4
   99 |   1.2226 |     39.797 |   1.3061 |     41.207 |     1.4
  100 |   1.2191 |     39.613 |   1.3017 |     40.931 |     1.4
  101 |   1.2146 |     39.445 |   1.3012 |     41.023 |     1.4
  102 |   1.2117 |     39.575 |   1.3003 |     41.115 |     1.4
  103 |   1.2076 |     39.326 |   1.2981 |     40.993 |     1.4
  104 |   1.2049 |     39.337 |   1.2979 |     41.207 |     1.5
  105 |   1.2027 |     39.115 |   1.2968 |     41.023 |     1.5
  106 |   1.1959 |     39.093 |   1.2947 |     40.901 |     1.5
  107 |   1.1953 |     38.855 |   1.2952 |     40.870 |     1.5
  108 |   1.1902 |     38.844 |   1.2905 |     41.023 |     1.5
  109 |   1.1855 |     38.757 |   1.2868 |     40.349 |     1.5
  110 |   1.1835 |     38.557 |   1.2850 |     40.227 |     1.5
  111 |   1.1783 |     38.389 |   1.2813 |     40.135 |     1.6
  112 |   1.1773 |     38.405 |   1.2865 |     40.809 |     1.6
  113 |   1.1715 |     38.080 |   1.2789 |     40.349 |     1.6
  114 |   1.1683 |     38.210 |   1.2819 |     40.257 |     1.6
  115 |   1.1652 |     37.971 |   1.2786 |     40.472 |     1.6
  116 |   1.1617 |     37.928 |   1.2801 |     40.104 |     1.6
  117 |   1.1578 |     37.782 |   1.2762 |     40.227 |     1.6
  118 |   1.1570 |     37.695 |   1.2787 |     40.196 |     1.6
  119 |   1.1517 |     37.625 |   1.2737 |     39.890 |     1.7
  120 |   1.1468 |     37.495 |   1.2739 |     40.227 |     1.7
  121 |   1.1425 |     37.522 |   1.2690 |     40.104 |     1.7
  122 |   1.1399 |     37.533 |   1.2684 |     40.196 |     1.7
  123 |   1.1387 |     37.473 |   1.2660 |     40.074 |     1.7
  124 |   1.1374 |     37.348 |   1.2716 |     40.165 |     1.7
  125 |   1.1344 |     37.283 |   1.2694 |     40.104 |     1.7
  126 |   1.1284 |     37.224 |   1.2658 |     39.737 |     1.8
  127 |   1.1293 |     37.115 |   1.2665 |     39.798 |     1.8
  128 |   1.1203 |     37.099 |   1.2604 |     39.890 |     1.8
  129 |   1.1197 |     36.926 |   1.2629 |     39.920 |     1.8
  130 |   1.1169 |     37.034 |   1.2626 |     39.706 |     1.8
  131 |   1.1120 |     36.942 |   1.2595 |     39.737 |     1.8
  132 |   1.1135 |     37.002 |   1.2622 |     39.308 |     1.8
  133 |   1.1070 |     36.855 |   1.2627 |     39.859 |     1.8
  134 |   1.1040 |     36.763 |   1.2621 |     39.706 |     1.9
  135 |   1.1044 |     36.682 |   1.2601 |     39.859 |     1.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,108,674

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2192 |     60.398 |   1.6216 |     48.346 |     0.1
    2 |   1.4896 |     46.467 |   1.4242 |     45.496 |     0.1
    3 |   1.4152 |     46.267 |   1.3998 |     45.527 |     0.2
    4 |   1.3939 |     46.034 |   1.3699 |     45.159 |     0.2
    5 |   1.3637 |     45.248 |   1.3425 |     43.995 |     0.3
    6 |   1.3435 |     44.706 |   1.3235 |     43.597 |     0.4
    7 |   1.3263 |     44.338 |   1.3207 |     43.811 |     0.4
    8 |   1.3109 |     43.834 |   1.3031 |     42.586 |     0.5
    9 |   1.2866 |     43.119 |   1.2829 |     42.065 |     0.5
   10 |   1.2535 |     41.900 |   1.2722 |     41.360 |     0.6
   11 |   1.2220 |     41.179 |   1.2286 |     39.828 |     0.7
   12 |   1.1997 |     40.171 |   1.1959 |     38.725 |     0.7
   13 |   1.1715 |     39.429 |   1.1915 |     38.051 |     0.8
   14 |   1.1378 |     38.253 |   1.1728 |     38.480 |     0.8
   15 |   1.1241 |     37.787 |   1.1393 |     37.469 |     0.9
   16 |   1.0863 |     36.503 |   1.1224 |     36.458 |     0.9
   17 |   1.0579 |     35.761 |   1.0892 |     35.907 |     1.0
   18 |   1.0255 |     34.612 |   1.0704 |     35.080 |     1.1
   19 |   1.0083 |     33.821 |   1.0534 |     33.762 |     1.1
   20 |   0.9834 |     33.252 |   1.0633 |     34.957 |     1.2
   21 |   0.9877 |     33.480 |   1.0462 |     35.202 |     1.2
   22 |   0.9527 |     32.271 |   1.0035 |     33.425 |     1.3
   23 |   0.9096 |     30.716 |   0.9908 |     33.088 |     1.4
   24 |   0.8900 |     30.234 |   0.9918 |     32.721 |     1.4
   25 |   0.8641 |     28.966 |   0.9915 |     32.996 |     1.5
   26 |   0.8575 |     28.728 |   0.9811 |     33.272 |     1.5
   27 |   0.8336 |     28.283 |   0.9667 |     32.874 |     1.6
   28 |   0.8165 |     27.650 |   0.9533 |     31.434 |     1.7
   29 |   0.7998 |     27.097 |   0.9697 |     32.659 |     1.7
   30 |   0.7931 |     26.810 |   0.9401 |     31.434 |     1.8
   31 |   0.7644 |     25.943 |   0.9565 |     31.434 |     1.8
   32 |   0.7310 |     24.355 |   0.9378 |     30.453 |     1.9
   33 |   0.7290 |     24.707 |   0.9293 |     30.392 |     2.0
   34 |   0.7269 |     24.518 |   0.9468 |     31.066 |     2.0
   35 |   0.7322 |     24.686 |   0.9064 |     29.871 |     2.1
   36 |   0.6894 |     23.142 |   0.9385 |     30.147 |     2.1
   37 |   0.6885 |     23.640 |   0.8888 |     28.830 |     2.2
   38 |   0.6449 |     21.738 |   0.9048 |     29.350 |     2.3
   39 |   0.6179 |     21.191 |   0.9176 |     28.952 |     2.3
   40 |   0.5985 |     20.199 |   0.9380 |     29.871 |     2.4
   41 |   0.5868 |     19.853 |   0.8919 |     27.972 |     2.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,398,562

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7763 |     72.784 |   2.3311 |     58.824 |     0.1
    2 |   2.0831 |     57.336 |   1.8995 |     54.197 |     0.2
    3 |   1.7746 |     49.865 |   1.6675 |     48.376 |     0.2
    4 |   1.6052 |     46.841 |   1.5398 |     45.466 |     0.3
    5 |   1.5119 |     46.196 |   1.4805 |     45.404 |     0.4
    6 |   1.4671 |     46.191 |   1.4441 |     45.558 |     0.5
    7 |   1.4380 |     45.698 |   1.4211 |     45.374 |     0.6
    8 |   1.4209 |     45.443 |   1.4063 |     45.190 |     0.6
    9 |   1.4044 |     45.194 |   1.3941 |     44.700 |     0.7
   10 |   1.3879 |     45.042 |   1.3787 |     44.424 |     0.8
   11 |   1.3740 |     44.977 |   1.3621 |     44.516 |     0.9
   12 |   1.3634 |     44.966 |   1.3524 |     44.179 |     1.0
   13 |   1.3632 |     45.010 |   1.3494 |     44.700 |     1.0
   14 |   1.3427 |     44.327 |   1.3387 |     43.627 |     1.1
   15 |   1.3303 |     43.682 |   1.3250 |     43.627 |     1.2
   16 |   1.3048 |     43.178 |   1.3021 |     42.279 |     1.3
   17 |   1.3025 |     43.298 |   1.2981 |     42.616 |     1.4
   18 |   1.2887 |     42.989 |   1.2821 |     42.065 |     1.4
   19 |   1.2732 |     42.577 |   1.2845 |     41.942 |     1.5
   20 |   1.2655 |     42.517 |   1.2755 |     41.973 |     1.6
   21 |   1.2604 |     42.252 |   1.2606 |     41.391 |     1.7
   22 |   1.2465 |     42.143 |   1.2520 |     41.238 |     1.7
   23 |   1.2297 |     41.645 |   1.2397 |     40.656 |     1.8
   24 |   1.2251 |     41.282 |   1.2489 |     41.452 |     1.9
   25 |   1.2210 |     41.228 |   1.2359 |     40.502 |     2.0
   26 |   1.2123 |     41.016 |   1.2415 |     40.380 |     2.1
   27 |   1.2103 |     41.011 |   1.2186 |     40.043 |     2.1
   28 |   1.2069 |     40.816 |   1.2464 |     40.349 |     2.2
   29 |   1.2162 |     41.147 |   1.2137 |     39.859 |     2.3
   30 |   1.2051 |     40.523 |   1.2219 |     40.196 |     2.4
   31 |   1.1963 |     40.361 |   1.2168 |     39.890 |     2.5
   32 |   1.1880 |     40.399 |   1.2039 |     39.828 |     2.5
   33 |   1.1776 |     40.160 |   1.2025 |     39.982 |     2.6
   34 |   1.1657 |     39.814 |   1.1870 |     38.695 |     2.7
   35 |   1.1553 |     39.375 |   1.1829 |     39.675 |     2.8
   36 |   1.1474 |     39.391 |   1.1856 |     39.246 |     2.9
   37 |   1.1364 |     38.909 |   1.1894 |     39.277 |     2.9
   38 |   1.1395 |     39.212 |   1.1751 |     39.614 |     3.0
   39 |   1.1378 |     39.234 |   1.1661 |     38.971 |     3.1
   40 |   1.1413 |     39.082 |   1.1789 |     39.706 |     3.2
   41 |   1.1299 |     38.622 |   1.1611 |     39.828 |     3.3
   42 |   1.1201 |     38.410 |   1.1544 |     39.246 |     3.3
   43 |   1.1186 |     38.427 |   1.1428 |     38.909 |     3.4
   44 |   1.1416 |     39.163 |   1.1829 |     39.369 |     3.5
   45 |   1.1562 |     39.640 |   1.1801 |     39.553 |     3.6
   46 |   1.1406 |     39.120 |   1.1715 |     39.461 |     3.7
   47 |   1.1204 |     38.524 |   1.1662 |     39.308 |     3.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,812,226

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1620 |     84.368 |   2.6619 |     83.333 |     0.1
    2 |   2.4002 |     61.032 |   2.2119 |     58.303 |     0.2
    3 |   2.0715 |     55.581 |   1.9769 |     53.799 |     0.2
    4 |   1.8982 |     51.571 |   1.8311 |     48.438 |     0.3
    5 |   1.7698 |     48.607 |   1.7099 |     48.346 |     0.4
    6 |   1.6667 |     48.607 |   1.6152 |     48.376 |     0.5
    7 |   1.5849 |     46.261 |   1.5492 |     45.466 |     0.6
    8 |   1.5287 |     46.213 |   1.5058 |     45.466 |     0.6
    9 |   1.4945 |     46.207 |   1.4764 |     45.466 |     0.7
   10 |   1.4704 |     46.386 |   1.4561 |     45.466 |     0.8
   11 |   1.4532 |     46.321 |   1.4422 |     45.466 |     0.9
   12 |   1.4386 |     46.234 |   1.4275 |     45.466 |     1.0
   13 |   1.4231 |     46.256 |   1.4177 |     45.037 |     1.0
   14 |   1.4107 |     46.055 |   1.4060 |     45.098 |     1.1
   15 |   1.3973 |     45.898 |   1.3933 |     45.067 |     1.2
   16 |   1.3845 |     45.687 |   1.3853 |     45.067 |     1.3
   17 |   1.3729 |     45.584 |   1.3745 |     44.945 |     1.4
   18 |   1.3642 |     45.080 |   1.3619 |     44.056 |     1.4
   19 |   1.3495 |     44.663 |   1.3595 |     44.179 |     1.5
   20 |   1.3375 |     44.213 |   1.3529 |     43.750 |     1.6
   21 |   1.3305 |     43.915 |   1.3419 |     43.321 |     1.7
   22 |   1.3132 |     43.498 |   1.3333 |     43.168 |     1.8
   23 |   1.3057 |     43.081 |   1.3308 |     42.555 |     1.8
   24 |   1.2907 |     42.777 |   1.3195 |     42.831 |     1.9
   25 |   1.2784 |     41.986 |   1.3116 |     42.494 |     2.0
   26 |   1.2628 |     41.558 |   1.3081 |     42.157 |     2.1
   27 |   1.2518 |     40.914 |   1.2959 |     41.544 |     2.2
   28 |   1.2382 |     40.242 |   1.2882 |     41.330 |     2.2
   29 |   1.2229 |     39.738 |   1.2779 |     41.299 |     2.3
   30 |   1.2071 |     39.066 |   1.2667 |     40.564 |     2.4
   31 |   1.1906 |     38.470 |   1.2636 |     40.901 |     2.5
   32 |   1.1733 |     37.982 |   1.2587 |     40.288 |     2.6
   33 |   1.1592 |     37.565 |   1.2441 |     39.491 |     2.6
   34 |   1.1436 |     37.229 |   1.2395 |     39.798 |     2.7
   35 |   1.1287 |     36.947 |   1.2253 |     39.338 |     2.8
   36 |   1.1140 |     35.826 |   1.2185 |     39.522 |     2.9
   37 |   1.0950 |     35.636 |   1.2165 |     38.634 |     3.0
   38 |   1.0801 |     34.953 |   1.2141 |     37.929 |     3.0
   39 |   1.0633 |     34.287 |   1.2021 |     38.480 |     3.1
   40 |   1.0479 |     33.621 |   1.1988 |     38.725 |     3.2
   41 |   1.0376 |     33.203 |   1.1880 |     37.531 |     3.3
   42 |   1.0175 |     32.705 |   1.1777 |     37.347 |     3.4
   43 |   1.0058 |     32.114 |   1.1814 |     37.347 |     3.4
   44 |   0.9902 |     31.540 |   1.1695 |     36.060 |     3.5
   45 |   0.9737 |     30.765 |   1.1554 |     35.999 |     3.6
   46 |   0.9609 |     30.635 |   1.1569 |     35.938 |     3.7
   47 |   0.9490 |     29.925 |   1.1516 |     35.662 |     3.7
   48 |   0.9340 |     29.568 |   1.1441 |     35.631 |     3.8
   49 |   0.9168 |     29.026 |   1.1375 |     35.447 |     3.9
   50 |   0.9089 |     28.684 |   1.1404 |     34.865 |     4.0
   51 |   0.8940 |     28.040 |   1.1318 |     35.049 |     4.1
   52 |   0.8831 |     27.590 |   1.1295 |     35.202 |     4.1
   53 |   0.8733 |     27.454 |   1.1280 |     34.375 |     4.2
   54 |   0.8547 |     26.761 |   1.1258 |     34.589 |     4.3
   55 |   0.8488 |     26.360 |   1.1165 |     34.007 |     4.4
   56 |   0.8337 |     25.634 |   1.1006 |     33.180 |     4.5
   57 |   0.8207 |     25.352 |   1.0985 |     33.303 |     4.5
   58 |   0.8098 |     24.653 |   1.0992 |     33.058 |     4.6
   59 |   0.7978 |     24.621 |   1.1050 |     32.996 |     4.7
   60 |   0.7845 |     24.036 |   1.1016 |     33.272 |     4.8
   61 |   0.7798 |     23.667 |   1.0983 |     32.843 |     4.9
   62 |   0.7662 |     23.515 |   1.0947 |     32.782 |     4.9
   63 |   0.7501 |     22.784 |   1.0914 |     32.843 |     5.0
   64 |   0.7475 |     22.703 |   1.0987 |     32.659 |     5.1
   65 |   0.7326 |     22.399 |   1.0970 |     33.058 |     5.2
   66 |   0.7171 |     21.895 |   1.0977 |     32.414 |     5.3
   67 |   0.7114 |     21.592 |   1.0979 |     31.985 |     5.3
   68 |   0.7043 |     21.180 |   1.0839 |     31.893 |     5.4
   69 |   0.6954 |     20.931 |   1.0916 |     32.322 |     5.5
   70 |   0.6772 |     20.416 |   1.0866 |     31.832 |     5.6
   71 |   0.6690 |     19.896 |   1.0887 |     32.384 |     5.7
   72 |   0.6585 |     19.993 |   1.0903 |     31.801 |     5.7
   73 |   0.6509 |     19.240 |   1.0826 |     31.526 |     5.8
   74 |   0.6426 |     19.262 |   1.0987 |     30.882 |     5.9
   75 |   0.6347 |     18.850 |   1.0954 |     31.832 |     6.0
   76 |   0.6256 |     18.829 |   1.1012 |     30.882 |     6.1
   77 |   0.6128 |     18.341 |   1.0924 |     30.300 |     6.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 2,829,698

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1452 |     85.051 |   2.6377 |     83.333 |     0.1
    2 |   2.3585 |     60.446 |   2.1458 |     56.740 |     0.2
    3 |   2.0574 |     55.722 |   1.9535 |     53.799 |     0.3
    4 |   1.9008 |     52.389 |   1.8214 |     48.346 |     0.3
    5 |   1.7811 |     48.803 |   1.7210 |     48.346 |     0.4
    6 |   1.6930 |     48.635 |   1.6419 |     48.346 |     0.5
    7 |   1.6202 |     48.613 |   1.5758 |     48.346 |     0.6
    8 |   1.5609 |     48.483 |   1.5238 |     47.396 |     0.7
    9 |   1.5170 |     46.435 |   1.4881 |     45.466 |     0.8
   10 |   1.4852 |     46.213 |   1.4619 |     45.466 |     0.9
   11 |   1.4608 |     46.229 |   1.4454 |     45.466 |     1.0
   12 |   1.4468 |     46.223 |   1.4309 |     45.466 |     1.0
   13 |   1.4334 |     46.305 |   1.4196 |     45.466 |     1.1
   14 |   1.4201 |     46.186 |   1.4096 |     45.466 |     1.2
   15 |   1.4096 |     46.223 |   1.3982 |     45.466 |     1.3
   16 |   1.4011 |     46.169 |   1.3889 |     45.466 |     1.4
   17 |   1.3894 |     45.953 |   1.3795 |     45.190 |     1.5
   18 |   1.3782 |     45.898 |   1.3731 |     45.098 |     1.6
   19 |   1.3700 |     45.422 |   1.3649 |     44.087 |     1.6
   20 |   1.3586 |     44.880 |   1.3586 |     43.781 |     1.7
   21 |   1.3487 |     44.614 |   1.3505 |     43.444 |     1.8
   22 |   1.3416 |     44.392 |   1.3423 |     43.689 |     1.9
   23 |   1.3328 |     44.240 |   1.3362 |     43.658 |     2.0
   24 |   1.3240 |     43.986 |   1.3294 |     43.290 |     2.1
   25 |   1.3156 |     43.861 |   1.3231 |     43.719 |     2.2
   26 |   1.3046 |     43.720 |   1.3202 |     43.352 |     2.2
   27 |   1.2974 |     43.270 |   1.3122 |     43.352 |     2.3
   28 |   1.2911 |     43.173 |   1.3088 |     43.076 |     2.4
   29 |   1.2824 |     42.956 |   1.3026 |     43.199 |     2.5
   30 |   1.2683 |     42.561 |   1.2979 |     43.229 |     2.6
   31 |   1.2600 |     41.840 |   1.2841 |     41.820 |     2.7
   32 |   1.2475 |     41.174 |   1.2817 |     41.299 |     2.8
   33 |   1.2344 |     40.496 |   1.2731 |     41.575 |     2.8
   34 |   1.2289 |     40.350 |   1.2712 |     41.391 |     2.9
   35 |   1.2223 |     39.716 |   1.2645 |     41.023 |     3.0
   36 |   1.2087 |     39.321 |   1.2560 |     40.748 |     3.1
   37 |   1.1975 |     39.147 |   1.2512 |     40.839 |     3.2
   38 |   1.1921 |     38.936 |   1.2446 |     40.625 |     3.3
   39 |   1.1769 |     38.340 |   1.2377 |     40.257 |     3.4
   40 |   1.1674 |     38.259 |   1.2307 |     40.135 |     3.4
   41 |   1.1536 |     37.554 |   1.2279 |     40.012 |     3.5
   42 |   1.1438 |     37.446 |   1.2155 |     39.461 |     3.6
   43 |   1.1341 |     37.251 |   1.2142 |     39.890 |     3.7
   44 |   1.1231 |     36.877 |   1.2080 |     39.461 |     3.8
   45 |   1.1104 |     36.443 |   1.2049 |     39.400 |     3.9
   46 |   1.1026 |     36.145 |   1.1977 |     39.308 |     4.0
   47 |   1.0911 |     35.707 |   1.1891 |     38.909 |     4.0
   48 |   1.0781 |     35.306 |   1.1929 |     39.369 |     4.1
   49 |   1.0708 |     34.926 |   1.1808 |     38.603 |     4.2
   50 |   1.0611 |     34.406 |   1.1685 |     38.756 |     4.3
   51 |   1.0473 |     34.341 |   1.1690 |     37.990 |     4.4
   52 |   1.0387 |     34.130 |   1.1652 |     37.776 |     4.5
   53 |   1.0308 |     33.788 |   1.1725 |     37.408 |     4.5
   54 |   1.0186 |     33.572 |   1.1543 |     37.163 |     4.6
   55 |   1.0059 |     32.531 |   1.1518 |     36.949 |     4.7
   56 |   0.9912 |     32.483 |   1.1440 |     36.857 |     4.8
   57 |   0.9805 |     31.903 |   1.1446 |     37.040 |     4.9
   58 |   0.9744 |     31.524 |   1.1421 |     36.336 |     5.0
   59 |   0.9619 |     30.776 |   1.1349 |     36.734 |     5.1
   60 |   0.9536 |     30.684 |   1.1239 |     36.213 |     5.1
   61 |   0.9397 |     30.012 |   1.1259 |     35.692 |     5.2
   62 |   0.9274 |     29.730 |   1.1273 |     35.600 |     5.3
   63 |   0.9190 |     29.183 |   1.1195 |     35.325 |     5.4
   64 |   0.9098 |     28.955 |   1.1006 |     34.498 |     5.5
   65 |   0.8949 |     28.403 |   1.1031 |     34.498 |     5.6
   66 |   0.8876 |     28.191 |   1.0994 |     34.344 |     5.7
   67 |   0.8773 |     28.083 |   1.1028 |     33.578 |     5.7
   68 |   0.8686 |     27.444 |   1.0967 |     33.977 |     5.8
   69 |   0.8537 |     27.281 |   1.0977 |     33.701 |     5.9
   70 |   0.8492 |     27.026 |   1.0881 |     33.915 |     6.0
   71 |   0.8362 |     26.604 |   1.0914 |     33.211 |     6.1
   72 |   0.8328 |     26.625 |   1.0904 |     33.088 |     6.2
   73 |   0.8203 |     26.008 |   1.0868 |     33.303 |     6.3
   74 |   0.8079 |     25.558 |   1.0833 |     33.088 |     6.3
   75 |   0.8013 |     25.401 |   1.0798 |     33.272 |     6.4
   76 |   0.7925 |     24.967 |   1.0773 |     33.272 |     6.5
   77 |   0.7800 |     24.697 |   1.0822 |     32.812 |     6.6
   78 |   0.7720 |     24.198 |   1.0721 |     32.721 |     6.7
   79 |   0.7645 |     24.436 |   1.0871 |     33.119 |     6.8
   80 |   0.7637 |     24.290 |   1.0703 |     32.353 |     6.9
   81 |   0.7463 |     23.840 |   1.0827 |     31.832 |     6.9
   82 |   0.7434 |     23.418 |   1.0770 |     32.475 |     7.0
   83 |   0.7286 |     23.207 |   1.0672 |     31.495 |     7.1
   84 |   0.7223 |     22.963 |   1.0671 |     31.740 |     7.2
   85 |   0.7185 |     22.757 |   1.0707 |     32.353 |     7.3
   86 |   0.7074 |     22.150 |   1.0682 |     31.434 |     7.4
   87 |   0.7037 |     22.258 |   1.0598 |     31.495 |     7.5
   88 |   0.6955 |     21.684 |   1.0662 |     31.342 |     7.5
   89 |   0.6866 |     21.738 |   1.0632 |     31.005 |     7.6
   90 |   0.6781 |     21.326 |   1.0577 |     31.219 |     7.7
   91 |   0.6680 |     21.061 |   1.0576 |     31.219 |     7.8
   92 |   0.6616 |     20.747 |   1.0587 |     30.821 |     7.9
   93 |   0.6577 |     20.622 |   1.0682 |     31.066 |     8.0
   94 |   0.6554 |     20.985 |   1.0514 |     30.545 |     8.0
   95 |   0.6401 |     20.129 |   1.0536 |     30.760 |     8.1
   96 |   0.6389 |     20.069 |   1.0590 |     30.453 |     8.2
   97 |   0.6318 |     20.075 |   1.0514 |     30.270 |     8.3
   98 |   0.6262 |     19.842 |   1.0555 |     30.208 |     8.4
   99 |   0.6168 |     19.284 |   1.0435 |     30.300 |     8.5
  100 |   0.6112 |     19.094 |   1.0525 |     29.841 |     8.6
  101 |   0.6041 |     18.823 |   1.0527 |     29.994 |     8.6
  102 |   0.5927 |     18.536 |   1.0542 |     30.086 |     8.7
  103 |   0.5843 |     18.086 |   1.0548 |     29.749 |     8.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 851,362

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5034 |     66.986 |   1.9922 |     57.200 |     0.0
    2 |   1.7493 |     49.512 |   1.5531 |     45.466 |     0.1
    3 |   1.4916 |     45.839 |   1.4339 |     44.577 |     0.1
    4 |   1.4179 |     45.508 |   1.3963 |     44.608 |     0.1
    5 |   1.3818 |     44.354 |   1.3567 |     43.689 |     0.2
    6 |   1.3506 |     43.964 |   1.3382 |     43.321 |     0.2
    7 |   1.3287 |     43.590 |   1.3236 |     42.708 |     0.2
    8 |   1.3129 |     43.401 |   1.2968 |     42.188 |     0.3
    9 |   1.2895 |     42.897 |   1.2870 |     41.452 |     0.3
   10 |   1.2709 |     42.279 |   1.2646 |     41.544 |     0.3
   11 |   1.2554 |     42.100 |   1.2558 |     41.268 |     0.4
   12 |   1.2293 |     41.038 |   1.2334 |     40.993 |     0.4
   13 |   1.2040 |     40.307 |   1.2213 |     39.920 |     0.4
   14 |   1.1903 |     39.375 |   1.2018 |     38.388 |     0.5
   15 |   1.1617 |     38.773 |   1.1846 |     37.500 |     0.5
   16 |   1.1401 |     37.749 |   1.1717 |     37.684 |     0.5
   17 |   1.1176 |     37.142 |   1.1661 |     37.040 |     0.5
   18 |   1.0921 |     36.449 |   1.1520 |     37.531 |     0.6
   19 |   1.0705 |     35.663 |   1.1333 |     36.795 |     0.6
   20 |   1.0430 |     34.764 |   1.1234 |     36.550 |     0.6
   21 |   1.0331 |     34.525 |   1.0977 |     35.478 |     0.7
   22 |   1.0076 |     33.485 |   1.0942 |     35.325 |     0.7
   23 |   0.9899 |     33.176 |   1.0837 |     35.294 |     0.7
   24 |   0.9706 |     32.174 |   1.0889 |     35.662 |     0.8
   25 |   0.9451 |     31.681 |   1.0614 |     34.314 |     0.8
   26 |   0.9267 |     30.911 |   1.0708 |     34.957 |     0.8
   27 |   0.9101 |     30.359 |   1.0564 |     34.926 |     0.9
   28 |   0.8863 |     29.790 |   1.0436 |     33.824 |     0.9
   29 |   0.8768 |     29.340 |   1.0325 |     33.977 |     0.9
   30 |   0.8548 |     28.365 |   1.0320 |     33.609 |     1.0
   31 |   0.8392 |     27.937 |   1.0520 |     33.762 |     1.0
   32 |   0.8274 |     27.812 |   1.0407 |     34.222 |     1.0
   33 |   0.8259 |     27.509 |   1.0287 |     33.272 |     1.1
   34 |   0.8137 |     27.189 |   1.0177 |     32.904 |     1.1
   35 |   0.8024 |     26.533 |   1.0248 |     32.751 |     1.1
   36 |   0.7908 |     26.187 |   1.0173 |     33.088 |     1.1
   37 |   0.7779 |     25.981 |   1.0071 |     32.138 |     1.2
   38 |   0.7626 |     25.629 |   1.0049 |     32.445 |     1.2
   39 |   0.7396 |     24.821 |   1.0091 |     32.261 |     1.2
   40 |   0.7247 |     24.063 |   1.0120 |     32.292 |     1.3
   41 |   0.7238 |     24.301 |   1.0201 |     32.047 |     1.3
   42 |   0.7082 |     23.781 |   1.0102 |     31.893 |     1.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 359,298

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4365 |     87.603 |   3.1616 |     81.893 |     0.0
    2 |   2.8214 |     77.639 |   2.5905 |     67.249 |     0.0
    3 |   2.4531 |     63.920 |   2.3507 |     59.559 |     0.1
    4 |   2.2774 |     59.086 |   2.2059 |     58.793 |     0.1
    5 |   2.1528 |     58.095 |   2.0896 |     57.659 |     0.1
    6 |   2.0461 |     54.969 |   1.9929 |     51.134 |     0.1
    7 |   1.9612 |     50.932 |   1.9164 |     48.866 |     0.2
    8 |   1.8886 |     49.919 |   1.8462 |     48.775 |     0.2
    9 |   1.8230 |     49.426 |   1.7797 |     48.407 |     0.2
   10 |   1.7587 |     49.057 |   1.7175 |     48.223 |     0.2
   11 |   1.6992 |     48.770 |   1.6609 |     48.131 |     0.3
   12 |   1.6442 |     48.526 |   1.6101 |     48.100 |     0.3
   13 |   1.5982 |     48.174 |   1.5694 |     48.131 |     0.3
   14 |   1.5601 |     47.307 |   1.5363 |     45.251 |     0.3
   15 |   1.5325 |     46.110 |   1.5083 |     45.190 |     0.3
   16 |   1.5019 |     45.963 |   1.4834 |     45.098 |     0.4
   17 |   1.4787 |     45.790 |   1.4620 |     45.190 |     0.4
   18 |   1.4587 |     45.785 |   1.4440 |     45.098 |     0.4
   19 |   1.4377 |     45.671 |   1.4278 |     45.159 |     0.4
   20 |   1.4205 |     45.514 |   1.4115 |     45.098 |     0.5
   21 |   1.4040 |     45.389 |   1.3985 |     44.577 |     0.5
   22 |   1.3910 |     44.928 |   1.3847 |     44.240 |     0.5
   23 |   1.3761 |     44.712 |   1.3747 |     44.301 |     0.5
   24 |   1.3583 |     44.484 |   1.3599 |     43.658 |     0.6
   25 |   1.3487 |     43.991 |   1.3493 |     43.382 |     0.6
   26 |   1.3390 |     43.801 |   1.3416 |     43.137 |     0.6
   27 |   1.3247 |     43.314 |   1.3328 |     42.800 |     0.6
   28 |   1.3152 |     43.065 |   1.3234 |     42.923 |     0.6
   29 |   1.3072 |     42.658 |   1.3154 |     42.034 |     0.7
   30 |   1.2954 |     42.344 |   1.3077 |     42.555 |     0.7
   31 |   1.2863 |     42.225 |   1.3005 |     41.667 |     0.7
   32 |   1.2743 |     41.667 |   1.2940 |     41.575 |     0.7
   33 |   1.2672 |     41.499 |   1.2894 |     41.483 |     0.8
   34 |   1.2584 |     40.951 |   1.2791 |     40.993 |     0.8
   35 |   1.2496 |     40.805 |   1.2756 |     40.686 |     0.8
   36 |   1.2425 |     40.599 |   1.2642 |     39.951 |     0.8
   37 |   1.2340 |     40.236 |   1.2605 |     40.411 |     0.8
   38 |   1.2209 |     39.629 |   1.2511 |     39.798 |     0.9
   39 |   1.2190 |     39.716 |   1.2454 |     39.737 |     0.9
   40 |   1.2099 |     39.201 |   1.2410 |     39.675 |     0.9
   41 |   1.1996 |     38.735 |   1.2295 |     39.246 |     0.9
   42 |   1.1914 |     38.345 |   1.2254 |     38.634 |     1.0
   43 |   1.1848 |     38.188 |   1.2225 |     38.603 |     1.0
   44 |   1.1766 |     37.879 |   1.2143 |     38.511 |     1.0
   45 |   1.1665 |     37.533 |   1.2088 |     38.174 |     1.0
   46 |   1.1613 |     37.511 |   1.2002 |     38.450 |     1.1
   47 |   1.1515 |     37.310 |   1.1995 |     38.082 |     1.1
   48 |   1.1456 |     36.985 |   1.1902 |     38.113 |     1.1
   49 |   1.1361 |     36.844 |   1.1847 |     37.745 |     1.1
   50 |   1.1322 |     36.536 |   1.1751 |     37.531 |     1.1
   51 |   1.1229 |     36.438 |   1.1733 |     37.592 |     1.2
   52 |   1.1131 |     35.956 |   1.1730 |     37.776 |     1.2
   53 |   1.1057 |     35.902 |   1.1645 |     37.347 |     1.2
   54 |   1.1011 |     35.950 |   1.1625 |     37.255 |     1.2
   55 |   1.0961 |     35.577 |   1.1619 |     38.205 |     1.3
   56 |   1.0911 |     35.528 |   1.1485 |     36.765 |     1.3
   57 |   1.0798 |     35.029 |   1.1470 |     37.010 |     1.3
   58 |   1.0735 |     35.105 |   1.1405 |     36.857 |     1.3
   59 |   1.0657 |     34.677 |   1.1326 |     36.366 |     1.3
   60 |   1.0594 |     34.888 |   1.1326 |     36.550 |     1.4
   61 |   1.0507 |     34.271 |   1.1260 |     36.121 |     1.4
   62 |   1.0509 |     34.086 |   1.1267 |     35.754 |     1.4
   63 |   1.0392 |     33.550 |   1.1220 |     36.152 |     1.4
   64 |   1.0349 |     33.653 |   1.1178 |     36.060 |     1.5
   65 |   1.0283 |     33.696 |   1.1162 |     36.121 |     1.5
   66 |   1.0270 |     33.322 |   1.1051 |     35.355 |     1.5
   67 |   1.0145 |     33.160 |   1.1056 |     35.447 |     1.5
   68 |   1.0118 |     32.716 |   1.1000 |     35.172 |     1.5
   69 |   1.0015 |     32.607 |   1.0977 |     34.835 |     1.6
   70 |   0.9992 |     32.098 |   1.0945 |     34.865 |     1.6
   71 |   0.9930 |     32.049 |   1.0898 |     35.202 |     1.6
   72 |   0.9915 |     31.605 |   1.0847 |     34.314 |     1.6
   73 |   0.9829 |     31.545 |   1.0842 |     34.620 |     1.7
   74 |   0.9784 |     31.291 |   1.0858 |     35.233 |     1.7
   75 |   0.9706 |     31.090 |   1.0785 |     34.222 |     1.7
   76 |   0.9682 |     30.901 |   1.0733 |     34.344 |     1.7
   77 |   0.9593 |     30.781 |   1.0686 |     34.069 |     1.7
   78 |   0.9564 |     30.543 |   1.0687 |     34.099 |     1.8
   79 |   0.9474 |     30.137 |   1.0650 |     33.977 |     1.8
   80 |   0.9446 |     30.169 |   1.0620 |     34.130 |     1.8
   81 |   0.9364 |     30.142 |   1.0655 |     34.222 |     1.8
   82 |   0.9348 |     29.860 |   1.0597 |     34.161 |     1.9
   83 |   0.9292 |     29.519 |   1.0532 |     34.252 |     1.9
   84 |   0.9223 |     29.508 |   1.0553 |     34.038 |     1.9
   85 |   0.9157 |     29.150 |   1.0473 |     33.885 |     1.9
   86 |   0.9106 |     29.123 |   1.0492 |     34.069 |     1.9
   87 |   0.9048 |     28.603 |   1.0405 |     33.701 |     2.0
   88 |   0.9026 |     28.934 |   1.0413 |     33.578 |     2.0
   89 |   0.8965 |     28.381 |   1.0432 |     33.150 |     2.0
   90 |   0.8914 |     28.338 |   1.0378 |     33.272 |     2.0
   91 |   0.8815 |     27.861 |   1.0427 |     33.119 |     2.0
   92 |   0.8789 |     27.807 |   1.0325 |     33.578 |     2.1
   93 |   0.8757 |     27.482 |   1.0415 |     33.425 |     2.1
   94 |   0.8725 |     27.698 |   1.0245 |     33.058 |     2.1
   95 |   0.8676 |     27.509 |   1.0276 |     33.058 |     2.1
   96 |   0.8611 |     27.221 |   1.0262 |     32.904 |     2.2
   97 |   0.8554 |     27.016 |   1.0251 |     32.721 |     2.2
   98 |   0.8505 |     26.907 |   1.0229 |     32.629 |     2.2
   99 |   0.8425 |     26.604 |   1.0173 |     32.384 |     2.2
  100 |   0.8430 |     26.826 |   1.0137 |     32.506 |     2.2
  101 |   0.8352 |     26.284 |   1.0125 |     32.322 |     2.3
  102 |   0.8295 |     26.057 |   1.0125 |     32.353 |     2.3
  103 |   0.8271 |     26.149 |   1.0127 |     32.138 |     2.3
  104 |   0.8235 |     26.295 |   1.0140 |     31.832 |     2.3
  105 |   0.8154 |     25.737 |   1.0111 |     32.047 |     2.4
  106 |   0.8145 |     25.461 |   1.0052 |     31.985 |     2.4
  107 |   0.8072 |     25.238 |   1.0101 |     32.475 |     2.4
  108 |   0.8084 |     25.195 |   1.0001 |     31.618 |     2.4
  109 |   0.7987 |     24.886 |   1.0027 |     31.740 |     2.4
  110 |   0.7983 |     24.902 |   0.9994 |     31.771 |     2.5
  111 |   0.7902 |     24.594 |   0.9934 |     31.526 |     2.5
  112 |   0.7928 |     24.865 |   0.9984 |     31.771 |     2.5
  113 |   0.7857 |     24.296 |   1.0003 |     31.434 |     2.5
  114 |   0.7801 |     24.453 |   0.9944 |     31.219 |     2.6
  115 |   0.7744 |     24.339 |   0.9925 |     31.648 |     2.6
  116 |   0.7712 |     24.073 |   0.9921 |     31.710 |     2.6
  117 |   0.7676 |     24.041 |   0.9958 |     31.219 |     2.6
  118 |   0.7611 |     23.673 |   0.9870 |     30.882 |     2.6
  119 |   0.7627 |     23.510 |   0.9912 |     31.189 |     2.7
  120 |   0.7543 |     23.456 |   0.9860 |     31.219 |     2.7
  121 |   0.7531 |     23.602 |   0.9843 |     30.974 |     2.7
  122 |   0.7488 |     23.158 |   0.9859 |     30.852 |     2.7
  123 |   0.7462 |     23.402 |   0.9863 |     31.311 |     2.8
  124 |   0.7397 |     22.963 |   0.9880 |     31.434 |     2.8
  125 |   0.7376 |     22.708 |   0.9830 |     31.066 |     2.8
  126 |   0.7330 |     22.730 |   0.9758 |     30.821 |     2.8
  127 |   0.7347 |     22.708 |   0.9812 |     30.760 |     2.8
  128 |   0.7231 |     22.475 |   0.9835 |     30.944 |     2.9
  129 |   0.7212 |     22.155 |   0.9729 |     30.239 |     2.9
  130 |   0.7126 |     22.161 |   0.9780 |     30.882 |     2.9
  131 |   0.7146 |     21.939 |   0.9788 |     30.913 |     2.9
  132 |   0.7123 |     21.852 |   0.9882 |     30.453 |     3.0
  133 |   0.7091 |     22.052 |   0.9826 |     31.158 |     3.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 383,202

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4787 |     89.656 |   3.4339 |     82.935 |     0.0
    2 |   3.2523 |     83.149 |   3.0130 |     83.333 |     0.0
    3 |   2.8676 |     83.214 |   2.7838 |     81.893 |     0.1
    4 |   2.7106 |     81.301 |   2.6657 |     65.901 |     0.1
    5 |   2.5968 |     66.379 |   2.5561 |     66.912 |     0.1
    6 |   2.4914 |     64.602 |   2.4605 |     58.824 |     0.1
    7 |   2.4046 |     59.049 |   2.3766 |     58.824 |     0.1
    8 |   2.3250 |     59.049 |   2.3041 |     58.824 |     0.1
    9 |   2.2602 |     59.049 |   2.2448 |     58.824 |     0.2
   10 |   2.2060 |     59.049 |   2.1927 |     58.824 |     0.2
   11 |   2.1576 |     59.049 |   2.1465 |     58.824 |     0.2
   12 |   2.1169 |     59.049 |   2.1044 |     58.824 |     0.2
   13 |   2.0754 |     59.049 |   2.0645 |     58.824 |     0.2
   14 |   2.0372 |     59.049 |   2.0259 |     58.824 |     0.2
   15 |   2.0020 |     59.049 |   1.9909 |     58.824 |     0.3
   16 |   1.9715 |     59.038 |   1.9587 |     58.824 |     0.3
   17 |   1.9381 |     57.607 |   1.9287 |     58.824 |     0.3
   18 |   1.9116 |     53.153 |   1.9008 |     49.510 |     0.3
   19 |   1.8877 |     49.274 |   1.8743 |     48.315 |     0.3
   20 |   1.8610 |     48.683 |   1.8492 |     48.376 |     0.3
   21 |   1.8357 |     48.640 |   1.8250 |     48.284 |     0.4
   22 |   1.8153 |     48.629 |   1.8013 |     48.284 |     0.4
   23 |   1.7884 |     48.575 |   1.7780 |     48.284 |     0.4
   24 |   1.7694 |     48.597 |   1.7552 |     48.284 |     0.4
   25 |   1.7444 |     48.548 |   1.7333 |     48.284 |     0.4
   26 |   1.7231 |     48.532 |   1.7117 |     48.254 |     0.4
   27 |   1.7041 |     48.537 |   1.6918 |     48.254 |     0.5
   28 |   1.6859 |     48.488 |   1.6734 |     48.254 |     0.5
   29 |   1.6654 |     48.467 |   1.6537 |     48.254 |     0.5
   30 |   1.6459 |     48.445 |   1.6320 |     48.223 |     0.5
   31 |   1.6266 |     47.361 |   1.6104 |     45.312 |     0.5
   32 |   1.6050 |     46.023 |   1.5931 |     45.312 |     0.5
   33 |   1.5883 |     46.028 |   1.5786 |     45.343 |     0.6
   34 |   1.5768 |     46.001 |   1.5656 |     45.343 |     0.6
   35 |   1.5640 |     46.034 |   1.5533 |     45.312 |     0.6
   36 |   1.5530 |     46.012 |   1.5421 |     45.404 |     0.6
   37 |   1.5447 |     46.001 |   1.5326 |     45.343 |     0.6
   38 |   1.5329 |     46.012 |   1.5238 |     45.251 |     0.6
   39 |   1.5215 |     45.974 |   1.5156 |     45.312 |     0.7
   40 |   1.5147 |     46.012 |   1.5050 |     45.343 |     0.7
   41 |   1.5066 |     45.996 |   1.4976 |     45.282 |     0.7
   42 |   1.4985 |     46.001 |   1.4902 |     45.312 |     0.7
   43 |   1.4912 |     45.931 |   1.4835 |     45.282 |     0.7
   44 |   1.4872 |     45.915 |   1.4778 |     45.282 |     0.7
   45 |   1.4790 |     45.947 |   1.4720 |     45.312 |     0.8
   46 |   1.4732 |     45.904 |   1.4668 |     45.343 |     0.8
   47 |   1.4699 |     45.925 |   1.4615 |     45.374 |     0.8
   48 |   1.4647 |     45.931 |   1.4572 |     45.466 |     0.8
   49 |   1.4558 |     45.833 |   1.4533 |     45.404 |     0.8
   50 |   1.4522 |     45.898 |   1.4489 |     45.435 |     0.8
   51 |   1.4485 |     45.774 |   1.4457 |     45.374 |     0.9
   52 |   1.4453 |     45.714 |   1.4426 |     45.343 |     0.9
   53 |   1.4433 |     45.535 |   1.4385 |     45.374 |     0.9
   54 |   1.4382 |     45.644 |   1.4353 |     45.404 |     0.9
   55 |   1.4342 |     45.497 |   1.4323 |     45.343 |     0.9
   56 |   1.4320 |     45.481 |   1.4297 |     45.221 |     0.9
   57 |   1.4299 |     45.351 |   1.4268 |     45.067 |     1.0
   58 |   1.4253 |     45.346 |   1.4229 |     45.435 |     1.0
   59 |   1.4210 |     45.161 |   1.4202 |     45.221 |     1.0
   60 |   1.4192 |     45.178 |   1.4172 |     45.067 |     1.0
   61 |   1.4154 |     45.048 |   1.4140 |     44.945 |     1.0
   62 |   1.4121 |     44.939 |   1.4113 |     44.700 |     1.0
   63 |   1.4093 |     44.782 |   1.4088 |     44.792 |     1.1
   64 |   1.4054 |     44.652 |   1.4069 |     44.485 |     1.1
   65 |   1.4012 |     44.549 |   1.4029 |     44.393 |     1.1
   66 |   1.3967 |     44.555 |   1.4009 |     44.332 |     1.1
   67 |   1.3945 |     44.565 |   1.3966 |     44.332 |     1.1
   68 |   1.3941 |     44.473 |   1.3951 |     44.240 |     1.1
   69 |   1.3878 |     44.457 |   1.3925 |     44.363 |     1.2
   70 |   1.3880 |     44.381 |   1.3898 |     44.271 |     1.2
   71 |   1.3831 |     44.403 |   1.3879 |     44.332 |     1.2
   72 |   1.3793 |     44.327 |   1.3852 |     44.240 |     1.2
   73 |   1.3780 |     44.224 |   1.3837 |     44.240 |     1.2
   74 |   1.3752 |     44.235 |   1.3817 |     44.087 |     1.2
   75 |   1.3710 |     44.262 |   1.3799 |     44.179 |     1.3
   76 |   1.3687 |     44.181 |   1.3773 |     44.026 |     1.3
   77 |   1.3656 |     44.164 |   1.3778 |     43.964 |     1.3
   78 |   1.3622 |     44.170 |   1.3744 |     44.179 |     1.3
   79 |   1.3606 |     44.137 |   1.3738 |     44.118 |     1.3
   80 |   1.3581 |     44.034 |   1.3722 |     43.934 |     1.3
   81 |   1.3537 |     44.018 |   1.3704 |     44.026 |     1.4
   82 |   1.3535 |     44.040 |   1.3701 |     43.934 |     1.4
   83 |   1.3490 |     43.986 |   1.3689 |     44.026 |     1.4
   84 |   1.3476 |     44.007 |   1.3665 |     43.934 |     1.4
   85 |   1.3464 |     43.948 |   1.3675 |     43.903 |     1.4
   86 |   1.3431 |     43.915 |   1.3647 |     44.056 |     1.4
   87 |   1.3368 |     43.888 |   1.3632 |     43.873 |     1.5
   88 |   1.3380 |     43.850 |   1.3627 |     43.934 |     1.5
   89 |   1.3331 |     43.807 |   1.3633 |     43.719 |     1.5
   90 |   1.3322 |     43.823 |   1.3616 |     43.995 |     1.5
   91 |   1.3309 |     43.764 |   1.3578 |     43.719 |     1.5
   92 |   1.3292 |     43.688 |   1.3605 |     43.781 |     1.5
   93 |   1.3246 |     43.720 |   1.3607 |     43.781 |     1.6
   94 |   1.3235 |     43.617 |   1.3616 |     43.781 |     1.6
   95 |   1.3220 |     43.612 |   1.3567 |     43.658 |     1.6
   96 |   1.3214 |     43.585 |   1.3571 |     43.689 |     1.6
   97 |   1.3184 |     43.460 |   1.3546 |     43.781 |     1.6
   98 |   1.3168 |     43.482 |   1.3565 |     43.842 |     1.6
   99 |   1.3134 |     43.422 |   1.3539 |     43.689 |     1.7
  100 |   1.3131 |     43.325 |   1.3564 |     43.811 |     1.7
  101 |   1.3111 |     43.325 |   1.3539 |     43.873 |     1.7
  102 |   1.3080 |     43.243 |   1.3500 |     43.658 |     1.7
  103 |   1.3057 |     43.200 |   1.3485 |     43.658 |     1.7
  104 |   1.3030 |     43.249 |   1.3516 |     43.934 |     1.7
  105 |   1.3045 |     43.151 |   1.3493 |     44.026 |     1.8
  106 |   1.3006 |     43.113 |   1.3489 |     43.811 |     1.8
  107 |   1.2997 |     43.113 |   1.3464 |     43.658 |     1.8
  108 |   1.2948 |     42.918 |   1.3480 |     43.903 |     1.8
  109 |   1.2924 |     42.972 |   1.3450 |     43.781 |     1.8
  110 |   1.2925 |     42.935 |   1.3461 |     43.719 |     1.8
  111 |   1.2888 |     42.940 |   1.3464 |     43.964 |     1.9
  112 |   1.2853 |     42.880 |   1.3433 |     43.934 |     1.9
  113 |   1.2846 |     42.756 |   1.3430 |     43.719 |     1.9
  114 |   1.2780 |     42.815 |   1.3424 |     43.781 |     1.9
  115 |   1.2787 |     42.572 |   1.3448 |     43.781 |     1.9
  116 |   1.2749 |     42.609 |   1.3449 |     43.781 |     1.9
  117 |   1.2714 |     42.604 |   1.3403 |     43.505 |     2.0
  118 |   1.2682 |     42.452 |   1.3390 |     43.750 |     2.0
  119 |   1.2657 |     42.555 |   1.3333 |     43.719 |     2.0
  120 |   1.2654 |     42.441 |   1.3319 |     43.536 |     2.0
  121 |   1.2576 |     42.328 |   1.3318 |     43.658 |     2.0
  122 |   1.2539 |     42.301 |   1.3338 |     43.536 |     2.0
  123 |   1.2519 |     42.203 |   1.3283 |     43.566 |     2.1
  124 |   1.2489 |     42.078 |   1.3303 |     43.658 |     2.1
  125 |   1.2444 |     41.992 |   1.3265 |     43.382 |     2.1
  126 |   1.2422 |     41.927 |   1.3254 |     43.413 |     2.1
  127 |   1.2390 |     41.791 |   1.3261 |     43.505 |     2.1
  128 |   1.2359 |     41.656 |   1.3225 |     43.290 |     2.1
  129 |   1.2351 |     41.558 |   1.3212 |     43.260 |     2.2
  130 |   1.2315 |     41.499 |   1.3219 |     43.076 |     2.2
  131 |   1.2242 |     41.298 |   1.3212 |     42.770 |     2.2
  132 |   1.2239 |     41.179 |   1.3196 |     43.015 |     2.2
  133 |   1.2204 |     41.190 |   1.3202 |     42.953 |     2.2
  134 |   1.2182 |     41.065 |   1.3211 |     42.984 |     2.2
  135 |   1.2161 |     41.060 |   1.3165 |     42.800 |     2.3
  136 |   1.2107 |     40.821 |   1.3144 |     42.800 |     2.3
  137 |   1.2078 |     40.643 |   1.3100 |     42.402 |     2.3
  138 |   1.2041 |     40.599 |   1.3101 |     42.555 |     2.3
  139 |   1.2011 |     40.572 |   1.3117 |     42.249 |     2.3
  140 |   1.1989 |     40.312 |   1.3118 |     42.525 |     2.3
  141 |   1.1949 |     40.377 |   1.3060 |     42.126 |     2.4
  142 |   1.1910 |     40.225 |   1.3083 |     42.341 |     2.4
  143 |   1.1880 |     40.074 |   1.3062 |     42.096 |     2.4
  144 |   1.1865 |     40.079 |   1.3018 |     42.096 |     2.4
  145 |   1.1852 |     39.960 |   1.3037 |     42.341 |     2.4
  146 |   1.1808 |     39.868 |   1.3067 |     42.004 |     2.5
  147 |   1.1764 |     39.857 |   1.3033 |     42.096 |     2.5
  148 |   1.1758 |     39.830 |   1.2984 |     41.728 |     2.5
  149 |   1.1708 |     39.700 |   1.3052 |     41.881 |     2.5
  150 |   1.1701 |     39.689 |   1.2985 |     42.218 |     2.5
  151 |   1.1683 |     39.635 |   1.2989 |     42.218 |     2.5
  152 |   1.1650 |     39.624 |   1.2996 |     41.850 |     2.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 720,290

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2399 |     87.099 |   2.7225 |     83.333 |     0.0
    2 |   2.4754 |     64.142 |   2.2603 |     58.824 |     0.0
    3 |   2.1121 |     56.822 |   1.9891 |     54.044 |     0.1
    4 |   1.9158 |     51.062 |   1.8350 |     48.346 |     0.1
    5 |   1.7858 |     48.857 |   1.7231 |     48.346 |     0.1
    6 |   1.6865 |     48.640 |   1.6270 |     48.346 |     0.1
    7 |   1.5992 |     47.388 |   1.5517 |     45.466 |     0.2
    8 |   1.5345 |     46.213 |   1.5023 |     45.466 |     0.2
    9 |   1.4938 |     46.213 |   1.4717 |     45.466 |     0.2
   10 |   1.4659 |     46.213 |   1.4497 |     45.466 |     0.2
   11 |   1.4471 |     46.240 |   1.4339 |     45.466 |     0.3
   12 |   1.4348 |     46.251 |   1.4217 |     45.466 |     0.3
   13 |   1.4199 |     46.115 |   1.4106 |     45.435 |     0.3
   14 |   1.4093 |     46.137 |   1.4007 |     45.282 |     0.3
   15 |   1.3955 |     46.001 |   1.3886 |     45.006 |     0.4
   16 |   1.3850 |     45.638 |   1.3801 |     45.098 |     0.4
   17 |   1.3726 |     45.297 |   1.3672 |     45.037 |     0.4
   18 |   1.3610 |     45.172 |   1.3558 |     44.730 |     0.4
   19 |   1.3451 |     44.977 |   1.3438 |     43.781 |     0.5
   20 |   1.3355 |     44.614 |   1.3365 |     43.536 |     0.5
   21 |   1.3233 |     44.403 |   1.3245 |     43.781 |     0.5
   22 |   1.3164 |     43.910 |   1.3196 |     43.382 |     0.5
   23 |   1.3077 |     43.796 |   1.3130 |     43.260 |     0.5
   24 |   1.2979 |     43.482 |   1.3058 |     42.953 |     0.6
   25 |   1.2896 |     43.493 |   1.3015 |     42.770 |     0.6
   26 |   1.2810 |     43.195 |   1.2960 |     42.525 |     0.6
   27 |   1.2733 |     42.978 |   1.2899 |     43.352 |     0.6
   28 |   1.2686 |     42.972 |   1.2840 |     43.107 |     0.7
   29 |   1.2583 |     42.637 |   1.2794 |     42.739 |     0.7
   30 |   1.2540 |     42.512 |   1.2711 |     42.004 |     0.7
   31 |   1.2422 |     41.835 |   1.2656 |     41.912 |     0.7
   32 |   1.2350 |     41.455 |   1.2599 |     41.667 |     0.8
   33 |   1.2251 |     41.098 |   1.2539 |     41.820 |     0.8
   34 |   1.2136 |     40.984 |   1.2473 |     41.636 |     0.8
   35 |   1.2055 |     40.800 |   1.2415 |     41.299 |     0.8
   36 |   1.1944 |     40.177 |   1.2344 |     40.349 |     0.9
   37 |   1.1861 |     40.020 |   1.2261 |     40.533 |     0.9
   38 |   1.1727 |     39.310 |   1.2154 |     40.288 |     0.9
   39 |   1.1611 |     38.806 |   1.2087 |     39.522 |     0.9
   40 |   1.1459 |     38.237 |   1.2071 |     40.135 |     0.9
   41 |   1.1356 |     37.982 |   1.1883 |     39.001 |     1.0
   42 |   1.1214 |     37.381 |   1.1851 |     38.971 |     1.0
   43 |   1.1118 |     37.056 |   1.1644 |     38.143 |     1.0
   44 |   1.0983 |     36.422 |   1.1635 |     38.450 |     1.0
   45 |   1.0843 |     35.950 |   1.1549 |     37.960 |     1.1
   46 |   1.0744 |     35.750 |   1.1433 |     37.377 |     1.1
   47 |   1.0598 |     34.905 |   1.1365 |     37.469 |     1.1
   48 |   1.0490 |     34.233 |   1.1319 |     36.857 |     1.1
   49 |   1.0395 |     33.984 |   1.1248 |     37.102 |     1.2
   50 |   1.0278 |     33.713 |   1.1145 |     36.489 |     1.2
   51 |   1.0194 |     33.312 |   1.1095 |     36.305 |     1.2
   52 |   1.0076 |     32.867 |   1.1035 |     36.029 |     1.2
   53 |   1.0007 |     32.667 |   1.1017 |     36.458 |     1.2
   54 |   0.9907 |     32.374 |   1.0848 |     35.478 |     1.3
   55 |   0.9821 |     31.708 |   1.0839 |     35.355 |     1.3
   56 |   0.9696 |     31.675 |   1.0851 |     35.723 |     1.3
   57 |   0.9607 |     31.448 |   1.0785 |     35.417 |     1.3
   58 |   0.9508 |     30.906 |   1.0716 |     34.896 |     1.4
   59 |   0.9408 |     30.586 |   1.0784 |     35.355 |     1.4
   60 |   0.9363 |     30.478 |   1.0607 |     34.651 |     1.4
   61 |   0.9272 |     29.941 |   1.0578 |     34.406 |     1.4
   62 |   0.9172 |     29.725 |   1.0586 |     33.640 |     1.5
   63 |   0.9058 |     29.318 |   1.0615 |     34.498 |     1.5
   64 |   0.8994 |     29.215 |   1.0543 |     33.762 |     1.5
   65 |   0.8925 |     28.858 |   1.0511 |     34.161 |     1.5
   66 |   0.8825 |     28.619 |   1.0483 |     33.303 |     1.6
   67 |   0.8780 |     28.451 |   1.0398 |     33.824 |     1.6
   68 |   0.8684 |     28.267 |   1.0487 |     34.130 |     1.6
   69 |   0.8591 |     28.018 |   1.0343 |     33.241 |     1.6
   70 |   0.8543 |     27.920 |   1.0334 |     33.333 |     1.6
   71 |   0.8438 |     27.346 |   1.0346 |     33.456 |     1.7
   72 |   0.8367 |     27.184 |   1.0417 |     32.690 |     1.7
   73 |   0.8298 |     26.956 |   1.0332 |     32.567 |     1.7
   74 |   0.8238 |     26.620 |   1.0297 |     33.027 |     1.7
   75 |   0.8184 |     26.560 |   1.0287 |     32.537 |     1.8
   76 |   0.8129 |     26.116 |   1.0209 |     32.537 |     1.8
   77 |   0.8050 |     25.786 |   1.0165 |     32.598 |     1.8
   78 |   0.7920 |     25.601 |   1.0160 |     32.200 |     1.8
   79 |   0.7894 |     25.423 |   1.0147 |     31.801 |     1.9
   80 |   0.7828 |     25.228 |   1.0207 |     31.771 |     1.9
   81 |   0.7739 |     25.022 |   1.0096 |     31.495 |     1.9
   82 |   0.7685 |     24.621 |   1.0238 |     32.322 |     1.9
   83 |   0.7613 |     24.718 |   1.0053 |     31.281 |     1.9
   84 |   0.7549 |     24.301 |   1.0137 |     31.863 |     2.0
   85 |   0.7521 |     24.095 |   1.0144 |     31.801 |     2.0
   86 |   0.7439 |     23.889 |   0.9958 |     31.066 |     2.0
   87 |   0.7384 |     23.662 |   1.0056 |     31.495 |     2.0
   88 |   0.7323 |     23.792 |   1.0167 |     30.944 |     2.1
   89 |   0.7252 |     23.521 |   1.0059 |     31.036 |     2.1
   90 |   0.7171 |     23.109 |   1.0071 |     30.974 |     2.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 294,050

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4886 |     88.757 |   3.3478 |     83.058 |     0.0
    2 |   2.9301 |     81.567 |   2.6807 |     81.893 |     0.0
    3 |   2.5267 |     64.505 |   2.4135 |     58.824 |     0.0
    4 |   2.3291 |     59.011 |   2.2715 |     58.824 |     0.1
    5 |   2.2128 |     58.631 |   2.1696 |     58.824 |     0.1
    6 |   2.1244 |     57.932 |   2.0828 |     58.824 |     0.1
    7 |   2.0434 |     56.784 |   2.0047 |     53.830 |     0.1
    8 |   1.9735 |     55.895 |   1.9376 |     53.768 |     0.1
    9 |   1.9079 |     53.793 |   1.8730 |     48.346 |     0.1
   10 |   1.8474 |     49.458 |   1.8095 |     48.346 |     0.1
   11 |   1.7861 |     48.992 |   1.7510 |     48.346 |     0.2
   12 |   1.7293 |     48.770 |   1.6980 |     48.346 |     0.2
   13 |   1.6823 |     48.304 |   1.6522 |     45.466 |     0.2
   14 |   1.6392 |     46.711 |   1.6124 |     45.466 |     0.2
   15 |   1.6037 |     46.223 |   1.5792 |     45.466 |     0.2
   16 |   1.5713 |     46.213 |   1.5525 |     45.466 |     0.2
   17 |   1.5490 |     46.213 |   1.5314 |     45.466 |     0.2
   18 |   1.5308 |     46.207 |   1.5136 |     45.466 |     0.3
   19 |   1.5108 |     46.213 |   1.4982 |     45.466 |     0.3
   20 |   1.4965 |     46.207 |   1.4860 |     45.466 |     0.3
   21 |   1.4864 |     46.213 |   1.4757 |     45.466 |     0.3
   22 |   1.4792 |     46.213 |   1.4666 |     45.466 |     0.3
   23 |   1.4680 |     46.213 |   1.4591 |     45.466 |     0.3
   24 |   1.4629 |     46.213 |   1.4524 |     45.466 |     0.3
   25 |   1.4553 |     46.207 |   1.4464 |     45.466 |     0.4
   26 |   1.4491 |     46.213 |   1.4413 |     45.466 |     0.4
   27 |   1.4454 |     46.207 |   1.4365 |     45.466 |     0.4
   28 |   1.4414 |     46.207 |   1.4321 |     45.466 |     0.4
   29 |   1.4361 |     46.261 |   1.4282 |     45.466 |     0.4
   30 |   1.4305 |     46.234 |   1.4246 |     45.466 |     0.4
   31 |   1.4292 |     46.267 |   1.4216 |     45.466 |     0.4
   32 |   1.4255 |     46.186 |   1.4187 |     45.466 |     0.5
   33 |   1.4235 |     46.207 |   1.4159 |     45.466 |     0.5
   34 |   1.4194 |     46.267 |   1.4137 |     45.466 |     0.5
   35 |   1.4173 |     46.142 |   1.4115 |     45.466 |     0.5
   36 |   1.4133 |     46.245 |   1.4091 |     45.466 |     0.5
   37 |   1.4129 |     46.245 |   1.4075 |     45.466 |     0.5
   38 |   1.4115 |     46.202 |   1.4057 |     45.466 |     0.5
   39 |   1.4103 |     46.261 |   1.4045 |     45.466 |     0.6
   40 |   1.4070 |     46.196 |   1.4015 |     45.466 |     0.6
   41 |   1.4007 |     46.229 |   1.3954 |     44.884 |     0.6
   42 |   1.3931 |     45.898 |   1.3862 |     44.884 |     0.6
   43 |   1.3809 |     45.877 |   1.3790 |     45.067 |     0.6
   44 |   1.3743 |     45.801 |   1.3723 |     44.853 |     0.6
   45 |   1.3688 |     45.557 |   1.3694 |     44.884 |     0.6
   46 |   1.3614 |     45.606 |   1.3652 |     45.006 |     0.7
   47 |   1.3561 |     45.492 |   1.3613 |     45.098 |     0.7
   48 |   1.3497 |     45.465 |   1.3548 |     45.006 |     0.7
   49 |   1.3433 |     45.205 |   1.3518 |     44.669 |     0.7
   50 |   1.3400 |     44.923 |   1.3476 |     44.516 |     0.7
   51 |   1.3326 |     44.771 |   1.3445 |     44.547 |     0.7
   52 |   1.3279 |     44.750 |   1.3394 |     44.210 |     0.7
   53 |   1.3253 |     44.565 |   1.3366 |     44.271 |     0.8
   54 |   1.3169 |     44.213 |   1.3357 |     44.332 |     0.8
   55 |   1.3118 |     44.121 |   1.3279 |     44.363 |     0.8
   56 |   1.3055 |     43.991 |   1.3240 |     44.271 |     0.8
   57 |   1.2976 |     43.997 |   1.3197 |     44.210 |     0.8
   58 |   1.2938 |     43.693 |   1.3167 |     44.363 |     0.8
   59 |   1.2892 |     43.829 |   1.3176 |     44.363 |     0.8
   60 |   1.2841 |     43.764 |   1.3039 |     44.179 |     0.8
   61 |   1.2818 |     43.894 |   1.3030 |     44.210 |     0.9
   62 |   1.2719 |     43.525 |   1.2993 |     44.210 |     0.9
   63 |   1.2688 |     43.336 |   1.2954 |     43.934 |     0.9
   64 |   1.2651 |     43.460 |   1.2930 |     44.179 |     0.9
   65 |   1.2586 |     43.379 |   1.2897 |     44.087 |     0.9
   66 |   1.2538 |     43.189 |   1.2837 |     43.781 |     0.9
   67 |   1.2496 |     42.994 |   1.2836 |     44.424 |     0.9
   68 |   1.2450 |     43.070 |   1.2801 |     43.750 |     1.0
   69 |   1.2433 |     42.702 |   1.2774 |     43.781 |     1.0
   70 |   1.2362 |     42.447 |   1.2724 |     43.566 |     1.0
   71 |   1.2316 |     42.436 |   1.2717 |     43.352 |     1.0
   72 |   1.2323 |     42.577 |   1.2679 |     42.984 |     1.0
   73 |   1.2267 |     42.160 |   1.2676 |     43.076 |     1.0
   74 |   1.2233 |     42.100 |   1.2654 |     43.168 |     1.0
   75 |   1.2200 |     42.252 |   1.2601 |     42.739 |     1.1
   76 |   1.2153 |     41.764 |   1.2624 |     43.045 |     1.1
   77 |   1.2118 |     41.851 |   1.2551 |     42.433 |     1.1
   78 |   1.2061 |     41.683 |   1.2536 |     43.015 |     1.1
   79 |   1.2039 |     41.623 |   1.2532 |     42.555 |     1.1
   80 |   1.1996 |     41.320 |   1.2502 |     42.371 |     1.1
   81 |   1.1984 |     41.304 |   1.2497 |     42.433 |     1.1
   82 |   1.1932 |     41.369 |   1.2511 |     42.218 |     1.2
   83 |   1.1922 |     41.417 |   1.2410 |     42.249 |     1.2
   84 |   1.1870 |     41.011 |   1.2412 |     42.188 |     1.2
   85 |   1.1827 |     40.870 |   1.2383 |     42.126 |     1.2
   86 |   1.1799 |     40.951 |   1.2352 |     42.494 |     1.2
   87 |   1.1798 |     40.816 |   1.2379 |     42.708 |     1.2
   88 |   1.1771 |     40.968 |   1.2399 |     42.678 |     1.2
   89 |   1.1742 |     40.691 |   1.2291 |     42.402 |     1.3
   90 |   1.1665 |     40.588 |   1.2357 |     42.034 |     1.3
   91 |   1.1671 |     40.437 |   1.2329 |     41.850 |     1.3
   92 |   1.1644 |     40.578 |   1.2276 |     42.188 |     1.3
   93 |   1.1581 |     40.269 |   1.2258 |     41.759 |     1.3
   94 |   1.1583 |     40.361 |   1.2355 |     42.188 |     1.3
   95 |   1.1531 |     40.122 |   1.2222 |     41.667 |     1.3
   96 |   1.1522 |     40.274 |   1.2220 |     41.483 |     1.4
   97 |   1.1489 |     40.258 |   1.2298 |     41.728 |     1.4
   98 |   1.1469 |     40.258 |   1.2221 |     41.728 |     1.4
   99 |   1.1414 |     39.792 |   1.2153 |     41.452 |     1.4
  100 |   1.1406 |     39.862 |   1.2205 |     41.636 |     1.4
  101 |   1.1374 |     39.673 |   1.2160 |     41.575 |     1.4
  102 |   1.1333 |     39.781 |   1.2163 |     41.513 |     1.4
  103 |   1.1327 |     39.684 |   1.2130 |     41.360 |     1.5
  104 |   1.1341 |     39.548 |   1.2162 |     41.299 |     1.5
  105 |   1.1282 |     39.472 |   1.2121 |     41.299 |     1.5
  106 |   1.1253 |     39.543 |   1.2142 |     41.054 |     1.5
  107 |   1.1242 |     39.537 |   1.2130 |     41.207 |     1.5
  108 |   1.1215 |     39.537 |   1.2121 |     41.023 |     1.5
  109 |   1.1175 |     39.088 |   1.2183 |     41.422 |     1.5
  110 |   1.1146 |     39.120 |   1.2056 |     40.809 |     1.6
  111 |   1.1120 |     39.098 |   1.2071 |     40.870 |     1.6
  112 |   1.1124 |     38.903 |   1.2075 |     40.809 |     1.6
  113 |   1.1087 |     38.860 |   1.2080 |     40.870 |     1.6
  114 |   1.1024 |     38.383 |   1.2054 |     40.901 |     1.6
  115 |   1.1035 |     38.594 |   1.2010 |     40.809 |     1.6
  116 |   1.1048 |     38.464 |   1.2037 |     41.207 |     1.6
  117 |   1.1051 |     38.649 |   1.2006 |     40.564 |     1.6
  118 |   1.0978 |     38.069 |   1.2010 |     40.288 |     1.7
  119 |   1.0955 |     38.150 |   1.2010 |     40.962 |     1.7
  120 |   1.0914 |     38.188 |   1.2076 |     40.839 |     1.7
  121 |   1.0938 |     38.134 |   1.2034 |     40.472 |     1.7
  122 |   1.0881 |     37.738 |   1.1945 |     40.625 |     1.7
  123 |   1.0850 |     37.619 |   1.1957 |     40.809 |     1.7
  124 |   1.0846 |     37.711 |   1.1996 |     40.686 |     1.7
  125 |   1.0809 |     37.527 |   1.1953 |     40.319 |     1.8
  126 |   1.0766 |     37.294 |   1.1953 |     40.135 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 440,226

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4299 |     87.939 |   3.1030 |     82.935 |     0.0
    2 |   2.7937 |     80.581 |   2.6125 |     65.809 |     0.0
    3 |   2.4686 |     59.401 |   2.3516 |     58.824 |     0.1
    4 |   2.2585 |     57.781 |   2.1725 |     58.180 |     0.1
    5 |   2.1115 |     56.518 |   2.0502 |     55.147 |     0.1
    6 |   2.0097 |     54.643 |   1.9579 |     49.418 |     0.1
    7 |   1.9252 |     50.087 |   1.8823 |     48.346 |     0.2
    8 |   1.8573 |     49.209 |   1.8215 |     48.376 |     0.2
    9 |   1.8042 |     48.943 |   1.7656 |     48.346 |     0.2
   10 |   1.7492 |     48.792 |   1.7148 |     48.346 |     0.2
   11 |   1.6992 |     48.640 |   1.6713 |     48.346 |     0.3
   12 |   1.6608 |     48.624 |   1.6336 |     48.346 |     0.3
   13 |   1.6262 |     48.624 |   1.5999 |     48.346 |     0.3
   14 |   1.5917 |     48.602 |   1.5702 |     48.346 |     0.3
   15 |   1.5655 |     48.142 |   1.5450 |     45.925 |     0.4
   16 |   1.5416 |     46.462 |   1.5222 |     45.466 |     0.4
   17 |   1.5223 |     46.131 |   1.5026 |     45.466 |     0.4
   18 |   1.5014 |     46.066 |   1.4870 |     45.466 |     0.4
   19 |   1.4867 |     46.088 |   1.4734 |     45.466 |     0.4
   20 |   1.4750 |     46.007 |   1.4609 |     45.435 |     0.5
   21 |   1.4628 |     45.958 |   1.4495 |     45.435 |     0.5
   22 |   1.4490 |     46.023 |   1.4401 |     45.159 |     0.5
   23 |   1.4429 |     45.947 |   1.4313 |     45.190 |     0.5
   24 |   1.4311 |     45.931 |   1.4234 |     45.190 |     0.6
   25 |   1.4253 |     45.860 |   1.4155 |     45.129 |     0.6
   26 |   1.4167 |     45.942 |   1.4092 |     45.221 |     0.6
   27 |   1.4110 |     46.050 |   1.4016 |     45.129 |     0.6
   28 |   1.4025 |     45.904 |   1.3968 |     45.312 |     0.7
   29 |   1.3917 |     45.817 |   1.3899 |     45.067 |     0.7
   30 |   1.3881 |     45.676 |   1.3835 |     45.067 |     0.7
   31 |   1.3802 |     45.703 |   1.3784 |     45.098 |     0.7
   32 |   1.3736 |     45.508 |   1.3727 |     44.792 |     0.8
   33 |   1.3673 |     45.172 |   1.3666 |     44.638 |     0.8
   34 |   1.3596 |     45.346 |   1.3606 |     44.608 |     0.8
   35 |   1.3551 |     45.064 |   1.3572 |     44.424 |     0.8
   36 |   1.3489 |     45.048 |   1.3524 |     43.964 |     0.8
   37 |   1.3433 |     44.484 |   1.3452 |     43.597 |     0.9
   38 |   1.3332 |     44.202 |   1.3392 |     43.444 |     0.9
   39 |   1.3269 |     44.051 |   1.3342 |     43.934 |     0.9
   40 |   1.3202 |     43.866 |   1.3303 |     43.934 |     0.9
   41 |   1.3137 |     43.650 |   1.3309 |     43.352 |     1.0
   42 |   1.3097 |     43.493 |   1.3225 |     43.076 |     1.0
   43 |   1.3061 |     43.319 |   1.3198 |     43.321 |     1.0
   44 |   1.3000 |     43.270 |   1.3173 |     43.168 |     1.0
   45 |   1.2953 |     43.070 |   1.3121 |     43.107 |     1.1
   46 |   1.2884 |     42.935 |   1.3092 |     42.923 |     1.1
   47 |   1.2818 |     42.805 |   1.3065 |     42.984 |     1.1
   48 |   1.2770 |     42.745 |   1.3019 |     42.892 |     1.1
   49 |   1.2717 |     42.544 |   1.2992 |     42.708 |     1.2
   50 |   1.2666 |     42.441 |   1.2964 |     42.923 |     1.2
   51 |   1.2623 |     42.523 |   1.2950 |     42.862 |     1.2
   52 |   1.2566 |     42.263 |   1.2904 |     42.494 |     1.2
   53 |   1.2507 |     42.241 |   1.2880 |     42.616 |     1.2
   54 |   1.2444 |     42.122 |   1.2862 |     42.555 |     1.3
   55 |   1.2390 |     41.992 |   1.2815 |     42.647 |     1.3
   56 |   1.2323 |     41.802 |   1.2844 |     42.984 |     1.3
   57 |   1.2292 |     42.089 |   1.2790 |     42.770 |     1.3
   58 |   1.2236 |     41.450 |   1.2759 |     42.586 |     1.4
   59 |   1.2170 |     41.569 |   1.2715 |     42.678 |     1.4
   60 |   1.2124 |     41.336 |   1.2691 |     42.708 |     1.4
   61 |   1.2064 |     41.103 |   1.2654 |     42.616 |     1.4
   62 |   1.1996 |     40.843 |   1.2637 |     42.249 |     1.5
   63 |   1.1936 |     40.859 |   1.2572 |     41.881 |     1.5
   64 |   1.1860 |     40.437 |   1.2569 |     42.126 |     1.5
   65 |   1.1822 |     39.944 |   1.2559 |     41.759 |     1.5
   66 |   1.1732 |     39.732 |   1.2548 |     41.667 |     1.6
   67 |   1.1683 |     39.516 |   1.2556 |     41.636 |     1.6
   68 |   1.1628 |     39.055 |   1.2450 |     41.299 |     1.6
   69 |   1.1556 |     39.185 |   1.2434 |     40.901 |     1.6
   70 |   1.1511 |     38.752 |   1.2399 |     40.993 |     1.6
   71 |   1.1482 |     38.389 |   1.2391 |     40.043 |     1.7
   72 |   1.1408 |     38.047 |   1.2391 |     40.870 |     1.7
   73 |   1.1349 |     38.134 |   1.2298 |     40.809 |     1.7
   74 |   1.1302 |     37.516 |   1.2332 |     40.441 |     1.7
   75 |   1.1237 |     37.695 |   1.2235 |     40.165 |     1.8
   76 |   1.1177 |     37.587 |   1.2267 |     40.441 |     1.8
   77 |   1.1118 |     37.424 |   1.2240 |     40.196 |     1.8
   78 |   1.1067 |     36.964 |   1.2223 |     40.441 |     1.8
   79 |   1.1001 |     36.904 |   1.2129 |     39.737 |     1.9
   80 |   1.0945 |     36.834 |   1.2163 |     39.338 |     1.9
   81 |   1.0926 |     36.741 |   1.2142 |     39.798 |     1.9
   82 |   1.0843 |     36.508 |   1.2113 |     39.982 |     1.9
   83 |   1.0791 |     36.183 |   1.2067 |     39.737 |     1.9
   84 |   1.0730 |     36.010 |   1.2032 |     39.583 |     2.0
   85 |   1.0646 |     35.728 |   1.1998 |     39.583 |     2.0
   86 |   1.0631 |     35.446 |   1.1976 |     39.369 |     2.0
   87 |   1.0537 |     35.127 |   1.2019 |     39.308 |     2.0
   88 |   1.0502 |     35.262 |   1.1939 |     39.400 |     2.1
   89 |   1.0485 |     34.888 |   1.1908 |     38.542 |     2.1
   90 |   1.0399 |     34.612 |   1.1898 |     39.553 |     2.1
   91 |   1.0338 |     34.357 |   1.1906 |     38.542 |     2.1
   92 |   1.0301 |     34.114 |   1.1865 |     39.124 |     2.2
   93 |   1.0222 |     34.065 |   1.1892 |     38.848 |     2.2
   94 |   1.0182 |     33.577 |   1.1760 |     38.388 |     2.2
   95 |   1.0135 |     33.366 |   1.1752 |     38.021 |     2.2
   96 |   1.0026 |     33.073 |   1.1707 |     37.623 |     2.2
   97 |   0.9981 |     32.488 |   1.1732 |     36.979 |     2.3
   98 |   0.9968 |     32.510 |   1.1600 |     36.949 |     2.3
   99 |   0.9880 |     32.179 |   1.1644 |     36.581 |     2.3
  100 |   0.9802 |     31.713 |   1.1665 |     37.255 |     2.3
  101 |   0.9760 |     31.372 |   1.1527 |     36.366 |     2.4
  102 |   0.9736 |     31.123 |   1.1612 |     36.489 |     2.4
  103 |   0.9626 |     30.917 |   1.1565 |     36.275 |     2.4
  104 |   0.9557 |     30.608 |   1.1510 |     35.846 |     2.4
  105 |   0.9507 |     30.277 |   1.1507 |     35.662 |     2.5
  106 |   0.9452 |     30.137 |   1.1425 |     35.631 |     2.5
  107 |   0.9334 |     29.622 |   1.1439 |     35.263 |     2.5
  108 |   0.9373 |     29.660 |   1.1434 |     35.355 |     2.5
  109 |   0.9294 |     29.568 |   1.1345 |     35.202 |     2.5
  110 |   0.9222 |     28.988 |   1.1400 |     35.049 |     2.6
  111 |   0.9158 |     28.847 |   1.1412 |     34.957 |     2.6
  112 |   0.9100 |     28.771 |   1.1394 |     34.651 |     2.6
  113 |   0.9046 |     28.717 |   1.1286 |     34.926 |     2.6
  114 |   0.8955 |     28.294 |   1.1259 |     34.375 |     2.7
  115 |   0.8918 |     28.267 |   1.1279 |     34.651 |     2.7
  116 |   0.8863 |     28.007 |   1.1233 |     34.773 |     2.7
  117 |   0.8798 |     28.029 |   1.1159 |     34.375 |     2.7
  118 |   0.8726 |     27.438 |   1.1154 |     34.007 |     2.8
  119 |   0.8654 |     27.189 |   1.1096 |     34.314 |     2.8
  120 |   0.8643 |     27.140 |   1.1059 |     34.436 |     2.8
  121 |   0.8576 |     27.146 |   1.1166 |     34.069 |     2.8
  122 |   0.8529 |     26.707 |   1.1037 |     34.007 |     2.8
  123 |   0.8467 |     26.533 |   1.1192 |     34.375 |     2.9
  124 |   0.8390 |     26.409 |   1.1053 |     33.732 |     2.9
  125 |   0.8376 |     26.295 |   1.1089 |     33.946 |     2.9
  126 |   0.8302 |     26.002 |   1.1000 |     33.364 |     2.9
  127 |   0.8301 |     26.100 |   1.1028 |     33.395 |     3.0
  128 |   0.8237 |     25.742 |   1.0906 |     33.211 |     3.0
  129 |   0.8165 |     25.726 |   1.0950 |     33.303 |     3.0
  130 |   0.8082 |     25.352 |   1.1005 |     33.303 |     3.0
  131 |   0.8057 |     24.902 |   1.0890 |     33.425 |     3.1
  132 |   0.8017 |     25.238 |   1.0933 |     32.445 |     3.1
  133 |   0.7986 |     24.859 |   1.0895 |     32.445 |     3.1
  134 |   0.7923 |     24.805 |   1.0926 |     32.751 |     3.1
  135 |   0.7871 |     24.632 |   1.0922 |     33.027 |     3.2
  136 |   0.7800 |     24.247 |   1.0844 |     32.506 |     3.2
  137 |   0.7780 |     24.214 |   1.0850 |     32.353 |     3.2
  138 |   0.7731 |     24.155 |   1.0771 |     32.445 |     3.2
  139 |   0.7644 |     23.976 |   1.0955 |     32.629 |     3.2
  140 |   0.7632 |     23.808 |   1.0857 |     32.230 |     3.3
  141 |   0.7602 |     23.803 |   1.0900 |     32.200 |     3.3
  142 |   0.7504 |     23.662 |   1.0827 |     32.292 |     3.3
  143 |   0.7486 |     23.667 |   1.0768 |     32.230 |     3.4
  144 |   0.7418 |     23.228 |   1.0844 |     32.384 |     3.4
  145 |   0.7404 |     23.077 |   1.0800 |     32.353 |     3.4
  146 |   0.7380 |     23.212 |   1.0867 |     31.832 |     3.4
  147 |   0.7313 |     23.228 |   1.0797 |     32.292 |     3.5
  148 |   0.7297 |     23.033 |   1.0719 |     31.648 |     3.5
  149 |   0.7270 |     22.757 |   1.0717 |     31.924 |     3.5
  150 |   0.7232 |     22.876 |   1.0827 |     31.648 |     3.5
  151 |   0.7193 |     22.453 |   1.0826 |     31.618 |     3.6
  152 |   0.7124 |     22.524 |   1.0748 |     31.771 |     3.6
  153 |   0.7099 |     22.410 |   1.0811 |     31.679 |     3.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 599,522

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8906 |     78.609 |   2.4367 |     66.912 |     0.0
    2 |   2.2084 |     59.059 |   1.9990 |     53.830 |     0.1
    3 |   1.8860 |     51.685 |   1.7426 |     48.346 |     0.1
    4 |   1.6812 |     48.412 |   1.5919 |     45.466 |     0.1
    5 |   1.5625 |     46.261 |   1.5131 |     45.987 |     0.1
    6 |   1.5044 |     46.261 |   1.4740 |     45.987 |     0.2
    7 |   1.4725 |     46.326 |   1.4519 |     45.466 |     0.2
    8 |   1.4557 |     46.272 |   1.4373 |     45.987 |     0.2
    9 |   1.4406 |     46.316 |   1.4275 |     45.987 |     0.2
   10 |   1.4293 |     46.294 |   1.4180 |     45.987 |     0.3
   11 |   1.4203 |     46.278 |   1.4102 |     45.466 |     0.3
   12 |   1.4146 |     46.305 |   1.4058 |     45.466 |     0.3
   13 |   1.4131 |     46.272 |   1.4023 |     45.987 |     0.3
   14 |   1.4108 |     46.175 |   1.4001 |     45.466 |     0.4
   15 |   1.4063 |     46.288 |   1.3969 |     45.067 |     0.4
   16 |   1.4021 |     46.288 |   1.3970 |     45.282 |     0.4
   17 |   1.4011 |     46.413 |   1.3898 |     45.466 |     0.5
   18 |   1.3954 |     46.272 |   1.3831 |     44.945 |     0.5
   19 |   1.3880 |     46.240 |   1.3761 |     45.466 |     0.5
   20 |   1.3793 |     46.294 |   1.3701 |     45.221 |     0.5
   21 |   1.3767 |     46.207 |   1.3652 |     45.741 |     0.6
   22 |   1.3630 |     46.072 |   1.3525 |     44.975 |     0.6
   23 |   1.3518 |     45.270 |   1.3393 |     43.719 |     0.6
   24 |   1.3481 |     44.934 |   1.3308 |     43.536 |     0.6
   25 |   1.3340 |     44.739 |   1.3182 |     43.750 |     0.7
   26 |   1.3199 |     44.419 |   1.3041 |     43.168 |     0.7
   27 |   1.3121 |     44.235 |   1.3049 |     43.382 |     0.7
   28 |   1.3093 |     44.365 |   1.2882 |     43.199 |     0.7
   29 |   1.2993 |     44.240 |   1.2785 |     43.199 |     0.8
   30 |   1.2931 |     44.116 |   1.2771 |     43.199 |     0.8
   31 |   1.2821 |     43.801 |   1.2701 |     42.953 |     0.8
   32 |   1.2780 |     43.747 |   1.2662 |     42.555 |     0.8
   33 |   1.2705 |     43.525 |   1.2591 |     42.126 |     0.9
   34 |   1.2639 |     43.476 |   1.2609 |     42.402 |     0.9
   35 |   1.2608 |     43.260 |   1.2623 |     42.402 |     0.9
   36 |   1.2523 |     43.027 |   1.2494 |     41.728 |     0.9
   37 |   1.2479 |     42.783 |   1.2467 |     42.586 |     1.0
   38 |   1.2473 |     43.195 |   1.2418 |     42.402 |     1.0
   39 |   1.2419 |     42.815 |   1.2365 |     42.218 |     1.0
   40 |   1.2373 |     42.593 |   1.2305 |     42.218 |     1.0
   41 |   1.2290 |     42.582 |   1.2243 |     41.759 |     1.1
   42 |   1.2215 |     42.404 |   1.2193 |     41.268 |     1.1
   43 |   1.2179 |     41.845 |   1.2142 |     41.667 |     1.1
   44 |   1.2080 |     41.737 |   1.2106 |     40.931 |     1.1
   45 |   1.2063 |     41.650 |   1.2143 |     41.176 |     1.2
   46 |   1.2040 |     41.672 |   1.2071 |     41.054 |     1.2
   47 |   1.2001 |     41.450 |   1.2011 |     41.207 |     1.2
   48 |   1.1957 |     41.390 |   1.2014 |     41.054 |     1.2
   49 |   1.1832 |     41.038 |   1.1952 |     41.483 |     1.3
   50 |   1.1860 |     40.914 |   1.1889 |     40.594 |     1.3
   51 |   1.1755 |     40.746 |   1.1895 |     40.625 |     1.3
   52 |   1.1722 |     40.377 |   1.1790 |     40.533 |     1.4
   53 |   1.1685 |     40.144 |   1.1677 |     39.553 |     1.4
   54 |   1.1633 |     40.290 |   1.1653 |     40.104 |     1.4
   55 |   1.1581 |     40.133 |   1.1631 |     39.491 |     1.4
   56 |   1.1643 |     39.906 |   1.1704 |     39.706 |     1.5
   57 |   1.1474 |     39.570 |   1.1583 |     39.828 |     1.5
   58 |   1.1444 |     39.293 |   1.1456 |     38.143 |     1.5
   59 |   1.1419 |     39.017 |   1.1602 |     39.093 |     1.5
   60 |   1.1466 |     39.283 |   1.1566 |     38.817 |     1.6
   61 |   1.1369 |     38.768 |   1.1470 |     38.542 |     1.6
   62 |   1.1453 |     39.163 |   1.1531 |     38.113 |     1.6
   63 |   1.1267 |     38.367 |   1.1439 |     38.358 |     1.6
   64 |   1.1297 |     38.437 |   1.1495 |     38.327 |     1.7
   65 |   1.1299 |     38.421 |   1.1464 |     37.837 |     1.7
   66 |   1.1142 |     38.069 |   1.1346 |     37.837 |     1.7
   67 |   1.1041 |     37.971 |   1.1341 |     38.082 |     1.7
   68 |   1.0998 |     37.576 |   1.1437 |     38.113 |     1.8
   69 |   1.1072 |     37.538 |   1.1264 |     37.316 |     1.8
   70 |   1.0980 |     37.137 |   1.1263 |     38.021 |     1.8
   71 |   1.1024 |     37.457 |   1.1362 |     38.695 |     1.8
   72 |   1.1211 |     37.858 |   1.1282 |     37.745 |     1.9
   73 |   1.1044 |     37.560 |   1.1308 |     38.143 |     1.9
   74 |   1.0974 |     37.343 |   1.1237 |     37.561 |     1.9
   75 |   1.0937 |     36.920 |   1.1189 |     37.561 |     1.9
   76 |   1.0767 |     36.834 |   1.1163 |     37.929 |     2.0
   77 |   1.0816 |     36.790 |   1.1256 |     37.898 |     2.0
   78 |   1.0768 |     36.676 |   1.1061 |     37.806 |     2.0
   79 |   1.0693 |     36.248 |   1.1047 |     37.623 |     2.1
   80 |   1.0621 |     35.788 |   1.0918 |     36.918 |     2.1
   81 |   1.0599 |     36.135 |   1.1080 |     37.224 |     2.1
   82 |   1.0576 |     35.761 |   1.1093 |     37.102 |     2.1
   83 |   1.0627 |     35.940 |   1.0934 |     37.010 |     2.2
   84 |   1.0467 |     35.717 |   1.0899 |     37.040 |     2.2
   85 |   1.0391 |     35.078 |   1.0893 |     36.581 |     2.2
   86 |   1.0356 |     35.625 |   1.0930 |     36.765 |     2.2
   87 |   1.0419 |     35.224 |   1.0932 |     37.500 |     2.3
   88 |   1.0393 |     35.360 |   1.1117 |     37.255 |     2.3
   89 |   1.0379 |     35.517 |   1.0976 |     36.949 |     2.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,123,874

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3335 |     63.871 |   1.7002 |     52.543 |     0.0
    2 |   1.5201 |     46.684 |   1.4335 |     45.987 |     0.1
    3 |   1.4202 |     46.305 |   1.4020 |     45.466 |     0.1
    4 |   1.4044 |     46.348 |   1.3811 |     45.466 |     0.1
    5 |   1.3939 |     46.261 |   1.3863 |     45.558 |     0.1
    6 |   1.3807 |     46.213 |   1.3668 |     45.466 |     0.2
    7 |   1.3600 |     45.833 |   1.3401 |     44.914 |     0.2
    8 |   1.3155 |     44.750 |   1.2922 |     43.015 |     0.2
    9 |   1.2820 |     44.078 |   1.2811 |     43.689 |     0.2
   10 |   1.2615 |     43.466 |   1.2576 |     42.923 |     0.3
   11 |   1.2373 |     42.572 |   1.2318 |     42.402 |     0.3
   12 |   1.2205 |     42.062 |   1.2224 |     41.146 |     0.3
   13 |   1.2000 |     41.260 |   1.1942 |     40.502 |     0.3
   14 |   1.1883 |     41.044 |   1.1842 |     40.165 |     0.4
   15 |   1.1670 |     40.106 |   1.1664 |     39.491 |     0.4
   16 |   1.1467 |     39.451 |   1.1449 |     38.909 |     0.4
   17 |   1.1293 |     39.055 |   1.1483 |     39.093 |     0.5
   18 |   1.1091 |     38.172 |   1.1307 |     38.695 |     0.5
   19 |   1.1043 |     38.486 |   1.1143 |     37.806 |     0.5
   20 |   1.0858 |     37.256 |   1.1159 |     37.929 |     0.5
   21 |   1.0712 |     37.191 |   1.0917 |     36.366 |     0.6
   22 |   1.0497 |     36.151 |   1.0940 |     36.857 |     0.6
   23 |   1.0288 |     35.381 |   1.0850 |     36.397 |     0.6
   24 |   1.0061 |     34.748 |   1.0782 |     36.520 |     0.6
   25 |   0.9891 |     33.951 |   1.0591 |     35.172 |     0.7
   26 |   0.9778 |     33.664 |   1.0402 |     34.743 |     0.7
   27 |   0.9616 |     33.111 |   1.0284 |     34.283 |     0.7
   28 |   0.9425 |     32.531 |   1.0216 |     33.854 |     0.7
   29 |   0.9284 |     32.006 |   1.0121 |     33.548 |     0.8
   30 |   0.9052 |     31.134 |   1.0080 |     33.640 |     0.8
   31 |   0.8870 |     30.109 |   0.9906 |     32.445 |     0.8
   32 |   0.8754 |     29.763 |   0.9833 |     32.200 |     0.9
   33 |   0.8557 |     29.015 |   0.9891 |     32.721 |     0.9
   34 |   0.8503 |     29.215 |   1.0081 |     32.414 |     0.9
   35 |   0.8200 |     27.855 |   0.9553 |     31.342 |     0.9
   36 |   0.8075 |     27.595 |   0.9553 |     31.189 |     1.0
   37 |   0.7971 |     27.026 |   0.9606 |     31.771 |     1.0
   38 |   0.7905 |     26.793 |   0.9526 |     30.484 |     1.0
   39 |   0.7763 |     26.441 |   0.9482 |     30.423 |     1.0
   40 |   0.7547 |     25.683 |   0.9495 |     30.331 |     1.1
   41 |   0.7399 |     25.358 |   0.9253 |     29.473 |     1.1
   42 |   0.7332 |     24.886 |   0.9181 |     29.350 |     1.1
   43 |   0.7195 |     24.339 |   0.9449 |     29.841 |     1.1
   44 |   0.7096 |     24.334 |   0.9240 |     29.350 |     1.2
   45 |   0.7002 |     23.765 |   0.9168 |     28.860 |     1.2
   46 |   0.6809 |     23.266 |   0.9303 |     28.431 |     1.2
   47 |   0.6771 |     23.261 |   0.9341 |     29.228 |     1.2
   48 |   0.6627 |     22.594 |   0.9113 |     28.094 |     1.3
   49 |   0.6528 |     21.966 |   0.9097 |     27.665 |     1.3
   50 |   0.6389 |     21.884 |   0.9277 |     28.064 |     1.3
   51 |   0.6348 |     21.630 |   0.9165 |     27.941 |     1.4
   52 |   0.6170 |     20.752 |   0.9448 |     28.523 |     1.4
   53 |   0.6151 |     20.958 |   0.9109 |     27.574 |     1.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 198,402

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4206 |     85.690 |   3.1703 |     83.333 |     0.0
    2 |   2.8421 |     81.410 |   2.6445 |     67.341 |     0.0
    3 |   2.5075 |     63.166 |   2.4018 |     58.854 |     0.0
    4 |   2.3109 |     58.675 |   2.2420 |     58.824 |     0.1
    5 |   2.1830 |     57.586 |   2.1323 |     57.996 |     0.1
    6 |   2.0896 |     56.632 |   2.0466 |     54.534 |     0.1
    7 |   2.0073 |     55.857 |   1.9705 |     54.779 |     0.1
    8 |   1.9403 |     53.489 |   1.8996 |     48.928 |     0.1
    9 |   1.8733 |     49.973 |   1.8360 |     48.346 |     0.1
   10 |   1.8083 |     48.965 |   1.7754 |     48.254 |     0.1
   11 |   1.7522 |     48.819 |   1.7185 |     48.192 |     0.1
   12 |   1.6968 |     48.656 |   1.6685 |     48.346 |     0.2
   13 |   1.6505 |     48.380 |   1.6233 |     46.630 |     0.2
   14 |   1.6072 |     46.852 |   1.5834 |     45.527 |     0.2
   15 |   1.5705 |     46.251 |   1.5486 |     45.404 |     0.2
   16 |   1.5354 |     46.240 |   1.5169 |     45.496 |     0.2
   17 |   1.5054 |     46.191 |   1.4910 |     45.527 |     0.2
   18 |   1.4817 |     46.175 |   1.4711 |     45.527 |     0.2
   19 |   1.4598 |     46.126 |   1.4498 |     45.374 |     0.3
   20 |   1.4425 |     46.055 |   1.4328 |     45.129 |     0.3
   21 |   1.4236 |     45.920 |   1.4192 |     45.067 |     0.3
   22 |   1.4103 |     45.698 |   1.4081 |     45.006 |     0.3
   23 |   1.3982 |     45.508 |   1.3955 |     44.853 |     0.3
   24 |   1.3843 |     45.205 |   1.3865 |     44.761 |     0.3
   25 |   1.3739 |     44.891 |   1.3756 |     44.761 |     0.3
   26 |   1.3619 |     44.847 |   1.3674 |     44.455 |     0.4
   27 |   1.3499 |     44.403 |   1.3590 |     44.271 |     0.4
   28 |   1.3434 |     44.105 |   1.3526 |     44.577 |     0.4
   29 |   1.3345 |     44.045 |   1.3447 |     44.424 |     0.4
   30 |   1.3254 |     43.617 |   1.3384 |     44.455 |     0.4
   31 |   1.3190 |     43.433 |   1.3335 |     43.934 |     0.4
   32 |   1.3115 |     42.799 |   1.3270 |     43.842 |     0.4
   33 |   1.3041 |     42.674 |   1.3197 |     42.953 |     0.4
   34 |   1.2952 |     41.932 |   1.3150 |     42.402 |     0.5
   35 |   1.2887 |     41.558 |   1.3095 |     42.770 |     0.5
   36 |   1.2810 |     41.428 |   1.3062 |     42.310 |     0.5
   37 |   1.2780 |     41.347 |   1.3006 |     41.605 |     0.5
   38 |   1.2678 |     40.746 |   1.2935 |     41.759 |     0.5
   39 |   1.2609 |     40.453 |   1.2880 |     41.575 |     0.5
   40 |   1.2552 |     40.187 |   1.2853 |     41.299 |     0.5
   41 |   1.2474 |     40.068 |   1.2819 |     41.299 |     0.6
   42 |   1.2443 |     40.139 |   1.2767 |     40.502 |     0.6
   43 |   1.2365 |     39.396 |   1.2698 |     40.686 |     0.6
   44 |   1.2309 |     39.326 |   1.2673 |     40.257 |     0.6
   45 |   1.2226 |     39.039 |   1.2633 |     40.074 |     0.6
   46 |   1.2158 |     39.109 |   1.2582 |     39.828 |     0.6
   47 |   1.2089 |     38.838 |   1.2506 |     39.951 |     0.6
   48 |   1.2029 |     38.681 |   1.2483 |     40.165 |     0.6
   49 |   1.1993 |     38.194 |   1.2416 |     40.135 |     0.7
   50 |   1.1872 |     37.793 |   1.2385 |     39.737 |     0.7
   51 |   1.1822 |     38.015 |   1.2365 |     39.645 |     0.7
   52 |   1.1806 |     37.901 |   1.2313 |     39.737 |     0.7
   53 |   1.1706 |     37.598 |   1.2244 |     38.879 |     0.7
   54 |   1.1644 |     37.365 |   1.2219 |     39.185 |     0.7
   55 |   1.1590 |     37.386 |   1.2172 |     39.154 |     0.7
   56 |   1.1540 |     37.061 |   1.2162 |     39.062 |     0.8
   57 |   1.1484 |     36.877 |   1.2102 |     38.388 |     0.8
   58 |   1.1434 |     36.796 |   1.2088 |     38.664 |     0.8
   59 |   1.1381 |     36.731 |   1.1988 |     38.940 |     0.8
   60 |   1.1349 |     36.449 |   1.1987 |     38.327 |     0.8
   61 |   1.1268 |     36.617 |   1.1972 |     38.266 |     0.8
   62 |   1.1243 |     36.232 |   1.1930 |     38.082 |     0.8
   63 |   1.1169 |     36.205 |   1.1893 |     37.898 |     0.8
   64 |   1.1123 |     36.005 |   1.1853 |     37.898 |     0.9
   65 |   1.1051 |     35.766 |   1.1855 |     37.837 |     0.9
   66 |   1.1055 |     35.701 |   1.1799 |     37.898 |     0.9
   67 |   1.1001 |     35.761 |   1.1776 |     37.837 |     0.9
   68 |   1.0941 |     35.571 |   1.1750 |     37.929 |     0.9
   69 |   1.0914 |     35.262 |   1.1711 |     37.745 |     0.9
   70 |   1.0849 |     35.154 |   1.1682 |     37.592 |     0.9
   71 |   1.0836 |     35.127 |   1.1653 |     37.531 |     1.0
   72 |   1.0739 |     34.688 |   1.1625 |     37.316 |     1.0
   73 |   1.0710 |     34.574 |   1.1586 |     36.887 |     1.0
   74 |   1.0623 |     34.233 |   1.1538 |     37.102 |     1.0
   75 |   1.0616 |     34.151 |   1.1550 |     37.224 |     1.0
   76 |   1.0580 |     34.108 |   1.1487 |     36.734 |     1.0
   77 |   1.0512 |     33.919 |   1.1518 |     36.887 |     1.0
   78 |   1.0518 |     33.713 |   1.1459 |     37.040 |     1.0
   79 |   1.0425 |     33.474 |   1.1437 |     37.194 |     1.1
   80 |   1.0389 |     33.577 |   1.1355 |     36.734 |     1.1
   81 |   1.0315 |     33.003 |   1.1342 |     36.458 |     1.1
   82 |   1.0274 |     33.214 |   1.1319 |     35.907 |     1.1
   83 |   1.0233 |     32.689 |   1.1298 |     36.765 |     1.1
   84 |   1.0164 |     32.477 |   1.1266 |     36.275 |     1.1
   85 |   1.0139 |     32.705 |   1.1253 |     36.458 |     1.1
   86 |   1.0135 |     32.428 |   1.1208 |     35.876 |     1.2
   87 |   1.0028 |     32.250 |   1.1203 |     35.999 |     1.2
   88 |   1.0003 |     32.071 |   1.1167 |     35.784 |     1.2
   89 |   0.9926 |     32.006 |   1.1138 |     35.968 |     1.2
   90 |   0.9933 |     31.751 |   1.1113 |     36.091 |     1.2
   91 |   0.9851 |     31.637 |   1.1073 |     35.600 |     1.2
   92 |   0.9799 |     31.144 |   1.1055 |     35.478 |     1.2
   93 |   0.9768 |     31.247 |   1.1031 |     35.325 |     1.2
   94 |   0.9694 |     30.922 |   1.0974 |     35.509 |     1.3
   95 |   0.9676 |     30.960 |   1.0942 |     35.294 |     1.3
   96 |   0.9702 |     30.955 |   1.0939 |     34.865 |     1.3
   97 |   0.9650 |     30.565 |   1.0869 |     34.743 |     1.3
   98 |   0.9522 |     30.500 |   1.0792 |     34.467 |     1.3
   99 |   0.9502 |     30.207 |   1.0853 |     34.835 |     1.3
  100 |   0.9477 |     30.164 |   1.0833 |     34.528 |     1.3
  101 |   0.9425 |     30.185 |   1.0828 |     34.589 |     1.4
  102 |   0.9353 |     29.649 |   1.0872 |     34.130 |     1.4
  103 |   0.9344 |     29.557 |   1.0768 |     34.375 |     1.4
  104 |   0.9244 |     29.037 |   1.0757 |     33.670 |     1.4
  105 |   0.9214 |     29.178 |   1.0682 |     33.425 |     1.4
  106 |   0.9186 |     29.345 |   1.0679 |     32.904 |     1.4
  107 |   0.9128 |     29.075 |   1.0673 |     32.996 |     1.4
  108 |   0.9104 |     28.798 |   1.0637 |     33.241 |     1.4
  109 |   0.9091 |     28.755 |   1.0642 |     32.782 |     1.5
  110 |   0.8999 |     28.435 |   1.0638 |     33.180 |     1.5
  111 |   0.8960 |     28.300 |   1.0592 |     32.629 |     1.5
  112 |   0.8928 |     28.218 |   1.0501 |     32.506 |     1.5
  113 |   0.8858 |     28.116 |   1.0488 |     32.629 |     1.5
  114 |   0.8803 |     27.845 |   1.0513 |     32.874 |     1.5
  115 |   0.8787 |     27.633 |   1.0470 |     32.537 |     1.5
  116 |   0.8738 |     27.438 |   1.0443 |     32.782 |     1.6
  117 |   0.8701 |     27.303 |   1.0450 |     32.904 |     1.6
  118 |   0.8676 |     27.070 |   1.0408 |     32.200 |     1.6
  119 |   0.8614 |     26.956 |   1.0429 |     32.169 |     1.6
  120 |   0.8555 |     26.631 |   1.0375 |     32.322 |     1.6
  121 |   0.8534 |     26.506 |   1.0390 |     32.322 |     1.6
  122 |   0.8506 |     26.582 |   1.0352 |     32.108 |     1.6
  123 |   0.8443 |     26.344 |   1.0330 |     32.016 |     1.6
  124 |   0.8394 |     26.138 |   1.0322 |     31.863 |     1.7
  125 |   0.8319 |     25.981 |   1.0324 |     32.414 |     1.7
  126 |   0.8301 |     26.029 |   1.0307 |     32.322 |     1.7
  127 |   0.8293 |     26.084 |   1.0278 |     32.200 |     1.7
  128 |   0.8239 |     25.737 |   1.0267 |     32.077 |     1.7
  129 |   0.8186 |     25.520 |   1.0230 |     32.016 |     1.7
  130 |   0.8155 |     25.358 |   1.0258 |     31.373 |     1.7
  131 |   0.8159 |     25.331 |   1.0186 |     31.158 |     1.8
  132 |   0.8101 |     25.255 |   1.0216 |     31.924 |     1.8
  133 |   0.8048 |     25.157 |   1.0166 |     31.434 |     1.8
  134 |   0.7978 |     24.984 |   1.0250 |     31.464 |     1.8
  135 |   0.8018 |     24.957 |   1.0201 |     31.311 |     1.8
  136 |   0.7974 |     24.772 |   1.0187 |     31.219 |     1.8
  137 |   0.7880 |     24.843 |   1.0142 |     31.403 |     1.8
  138 |   0.7902 |     24.745 |   1.0109 |     30.974 |     1.8
  139 |   0.7788 |     24.225 |   1.0099 |     31.434 |     1.9
  140 |   0.7795 |     24.371 |   1.0074 |     30.882 |     1.9
  141 |   0.7704 |     24.003 |   1.0085 |     30.699 |     1.9
  142 |   0.7705 |     23.938 |   0.9987 |     30.882 |     1.9
  143 |   0.7659 |     23.971 |   1.0020 |     31.097 |     1.9
  144 |   0.7618 |     23.862 |   0.9948 |     30.882 |     1.9
  145 |   0.7622 |     23.694 |   1.0009 |     30.882 |     1.9
  146 |   0.7595 |     23.884 |   1.0010 |     31.036 |     2.0
  147 |   0.7520 |     23.304 |   1.0073 |     30.821 |     2.0
  148 |   0.7507 |     23.385 |   0.9984 |     30.913 |     2.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 655,298

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3228 |     62.413 |   1.7106 |     48.346 |     0.0
    2 |   1.5364 |     47.112 |   1.4350 |     45.987 |     0.0
    3 |   1.4237 |     46.283 |   1.3971 |     45.496 |     0.0
    4 |   1.3902 |     45.996 |   1.3661 |     45.772 |     0.1
    5 |   1.3480 |     45.069 |   1.3292 |     44.118 |     0.1
    6 |   1.2977 |     43.390 |   1.2671 |     41.513 |     0.1
    7 |   1.2538 |     42.154 |   1.2338 |     41.023 |     0.1
    8 |   1.2204 |     41.076 |   1.2257 |     41.759 |     0.1
    9 |   1.1833 |     39.814 |   1.1957 |     39.859 |     0.1
   10 |   1.1525 |     38.974 |   1.1794 |     38.388 |     0.1
   11 |   1.1225 |     37.825 |   1.1706 |     38.388 |     0.2
   12 |   1.1011 |     36.958 |   1.1520 |     38.511 |     0.2
   13 |   1.0738 |     36.200 |   1.1254 |     37.500 |     0.2
   14 |   1.0462 |     35.446 |   1.1067 |     36.979 |     0.2
   15 |   1.0232 |     34.731 |   1.1135 |     36.060 |     0.2
   16 |   0.9996 |     33.788 |   1.0848 |     36.581 |     0.2
   17 |   0.9827 |     33.117 |   1.0882 |     36.489 |     0.2
   18 |   0.9565 |     32.510 |   1.0640 |     35.907 |     0.3
   19 |   0.9328 |     31.730 |   1.0584 |     35.600 |     0.3
   20 |   0.8970 |     30.223 |   1.0274 |     34.436 |     0.3
   21 |   0.8808 |     29.671 |   1.0274 |     33.854 |     0.3
   22 |   0.8600 |     29.015 |   1.0117 |     33.487 |     0.3
   23 |   0.8461 |     28.359 |   1.0288 |     34.069 |     0.3
   24 |   0.8262 |     27.850 |   0.9852 |     31.832 |     0.3
   25 |   0.7960 |     26.479 |   0.9881 |     31.985 |     0.4
   26 |   0.7731 |     25.802 |   0.9756 |     31.648 |     0.4
   27 |   0.7474 |     24.854 |   0.9959 |     31.127 |     0.4
   28 |   0.7279 |     24.019 |   0.9609 |     30.392 |     0.4
   29 |   0.7048 |     22.990 |   0.9640 |     29.810 |     0.4
   30 |   0.6900 |     22.605 |   0.9608 |     29.810 |     0.4
   31 |   0.6627 |     21.462 |   0.9706 |     29.963 |     0.4
   32 |   0.6574 |     21.435 |   0.9518 |     29.626 |     0.5
   33 |   0.6514 |     20.812 |   0.9852 |     29.136 |     0.5
   34 |   0.6223 |     20.313 |   0.9431 |     28.646 |     0.5
   35 |   0.6118 |     19.858 |   0.9788 |     28.799 |     0.5
   36 |   0.5918 |     19.137 |   0.9398 |     27.757 |     0.5
   37 |   0.5757 |     18.601 |   0.9593 |     28.646 |     0.5
   38 |   0.5578 |     18.113 |   0.9468 |     27.727 |     0.5
   39 |   0.5425 |     17.246 |   0.9586 |     28.033 |     0.6
   40 |   0.5355 |     17.214 |   0.9553 |     27.880 |     0.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 368,994

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4737 |     67.008 |   1.9412 |     52.022 |     0.0
    2 |   1.7016 |     48.445 |   1.5294 |     45.466 |     0.0
    3 |   1.4559 |     45.416 |   1.4060 |     44.301 |     0.1
    4 |   1.3653 |     43.791 |   1.3341 |     41.728 |     0.1
    5 |   1.3020 |     42.257 |   1.2906 |     41.422 |     0.1
    6 |   1.2557 |     40.832 |   1.2575 |     40.717 |     0.1
    7 |   1.2120 |     39.689 |   1.2112 |     39.706 |     0.1
    8 |   1.1756 |     38.903 |   1.1850 |     38.511 |     0.2
    9 |   1.1327 |     37.386 |   1.1644 |     37.377 |     0.2
   10 |   1.0961 |     36.173 |   1.1372 |     36.275 |     0.2
   11 |   1.0617 |     34.639 |   1.1078 |     36.121 |     0.2
   12 |   1.0253 |     33.518 |   1.0874 |     35.080 |     0.2
   13 |   0.9971 |     32.190 |   1.0565 |     34.161 |     0.3
   14 |   0.9622 |     30.770 |   1.0517 |     33.640 |     0.3
   15 |   0.9257 |     29.606 |   1.0327 |     33.027 |     0.3
   16 |   0.8971 |     28.999 |   0.9993 |     32.475 |     0.3
   17 |   0.8720 |     27.872 |   0.9905 |     31.801 |     0.3
   18 |   0.8321 |     26.317 |   0.9792 |     32.169 |     0.4
   19 |   0.8113 |     25.856 |   0.9719 |     31.281 |     0.4
   20 |   0.7838 |     24.946 |   0.9603 |     31.127 |     0.4
   21 |   0.7495 |     23.656 |   0.9613 |     31.189 |     0.4
   22 |   0.7287 |     23.093 |   0.9507 |     30.331 |     0.4
   23 |   0.7106 |     22.329 |   0.9517 |     30.331 |     0.5
   24 |   0.6859 |     22.036 |   0.9475 |     29.105 |     0.5
   25 |   0.6688 |     21.261 |   0.9586 |     29.841 |     0.5
   26 |   0.6511 |     20.476 |   0.9458 |     28.248 |     0.5
   27 |   0.6260 |     19.782 |   0.9350 |     28.554 |     0.5
   28 |   0.6058 |     18.883 |   0.9399 |     28.738 |     0.6
   29 |   0.5838 |     18.514 |   0.9379 |     27.512 |     0.6
   30 |   0.5665 |     18.059 |   0.9388 |     28.615 |     0.6
   31 |   0.5545 |     17.414 |   0.9387 |     28.646 |     0.6
   32 |   0.5550 |     17.474 |   0.9343 |     27.941 |     0.6
   33 |   0.5233 |     16.791 |   0.9309 |     27.849 |     0.7
   34 |   0.5073 |     15.919 |   0.9389 |     27.451 |     0.7
   35 |   0.4971 |     15.518 |   0.9357 |     27.298 |     0.7
   36 |   0.4915 |     15.903 |   0.9374 |     27.359 |     0.7
   37 |   0.4633 |     14.814 |   0.9327 |     27.543 |     0.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,737,058

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8115 |     76.365 |   2.3748 |     58.824 |     0.1
    2 |   2.1781 |     58.420 |   2.0139 |     57.904 |     0.1
    3 |   1.8876 |     54.107 |   1.7519 |     48.346 |     0.2
    4 |   1.6608 |     47.589 |   1.5762 |     45.466 |     0.3
    5 |   1.5425 |     46.288 |   1.4974 |     45.466 |     0.3
    6 |   1.4812 |     46.213 |   1.4499 |     45.466 |     0.4
    7 |   1.4459 |     46.245 |   1.4311 |     45.466 |     0.4
    8 |   1.4301 |     46.137 |   1.4123 |     45.864 |     0.5
    9 |   1.4144 |     46.213 |   1.4006 |     45.466 |     0.6
   10 |   1.3992 |     46.213 |   1.3884 |     45.496 |     0.6
   11 |   1.3824 |     46.158 |   1.3662 |     45.435 |     0.7
   12 |   1.3689 |     45.909 |   1.3548 |     44.669 |     0.8
   13 |   1.3531 |     45.920 |   1.3503 |     45.098 |     0.8
   14 |   1.3421 |     45.671 |   1.3338 |     44.669 |     0.9
   15 |   1.3280 |     45.362 |   1.3247 |     44.975 |     0.9
   16 |   1.3207 |     45.357 |   1.3089 |     43.964 |     1.0
   17 |   1.3048 |     44.490 |   1.3008 |     43.352 |     1.1
   18 |   1.2954 |     43.980 |   1.2901 |     43.076 |     1.1
   19 |   1.2862 |     43.585 |   1.2826 |     42.923 |     1.2
   20 |   1.2711 |     43.411 |   1.2697 |     42.494 |     1.3
   21 |   1.2625 |     43.075 |   1.2598 |     42.279 |     1.3
   22 |   1.2537 |     42.837 |   1.2599 |     42.249 |     1.4
   23 |   1.2435 |     42.241 |   1.2540 |     42.463 |     1.4
   24 |   1.2410 |     42.154 |   1.2464 |     41.513 |     1.5
   25 |   1.2317 |     42.024 |   1.2413 |     42.218 |     1.6
   26 |   1.2401 |     42.404 |   1.2707 |     43.505 |     1.6
   27 |   1.2589 |     42.962 |   1.2411 |     41.728 |     1.7
   28 |   1.2391 |     42.436 |   1.2257 |     41.422 |     1.8
   29 |   1.2271 |     42.246 |   1.2303 |     42.371 |     1.8
   30 |   1.2149 |     41.748 |   1.2233 |     41.452 |     1.9
   31 |   1.2084 |     41.558 |   1.2126 |     41.759 |     2.0
   32 |   1.2011 |     41.472 |   1.2087 |     41.636 |     2.0
   33 |   1.1938 |     41.358 |   1.2135 |     41.483 |     2.1
   34 |   1.1833 |     40.886 |   1.2054 |     41.391 |     2.1
   35 |   1.1799 |     40.984 |   1.1947 |     40.043 |     2.2
   36 |   1.1697 |     40.106 |   1.1967 |     39.920 |     2.3
   37 |   1.1674 |     40.399 |   1.1838 |     40.441 |     2.3
   38 |   1.1545 |     39.835 |   1.1755 |     39.706 |     2.4
   39 |   1.1508 |     39.483 |   1.1649 |     39.154 |     2.5
   40 |   1.1443 |     39.066 |   1.1629 |     39.093 |     2.5
   41 |   1.1319 |     38.649 |   1.1751 |     39.614 |     2.6
   42 |   1.1486 |     39.456 |   1.1641 |     38.971 |     2.7
   43 |   1.1233 |     38.291 |   1.1546 |     39.308 |     2.7
   44 |   1.1134 |     38.215 |   1.1569 |     37.745 |     2.8
   45 |   1.1070 |     37.852 |   1.1371 |     38.143 |     2.8
   46 |   1.1073 |     37.641 |   1.1438 |     37.776 |     2.9
   47 |   1.0987 |     37.495 |   1.1248 |     36.887 |     3.0
   48 |   1.0862 |     36.871 |   1.1224 |     37.500 |     3.0
   49 |   1.0752 |     36.633 |   1.1035 |     36.826 |     3.1
   50 |   1.0717 |     36.378 |   1.1080 |     36.826 |     3.2
   51 |   1.0683 |     36.135 |   1.0996 |     36.060 |     3.2
   52 |   1.0597 |     35.593 |   1.1062 |     36.366 |     3.3
   53 |   1.0449 |     35.241 |   1.1008 |     36.366 |     3.4
   54 |   1.0457 |     35.495 |   1.0911 |     35.662 |     3.4
   55 |   1.0406 |     35.311 |   1.0949 |     36.458 |     3.5
   56 |   1.0299 |     34.710 |   1.0826 |     34.957 |     3.5
   57 |   1.0323 |     34.943 |   1.0808 |     35.263 |     3.6
   58 |   1.0210 |     34.460 |   1.0930 |     35.938 |     3.7
   59 |   1.0249 |     34.515 |   1.0764 |     35.263 |     3.7
   60 |   1.0137 |     34.076 |   1.0630 |     34.957 |     3.8
   61 |   1.0036 |     33.772 |   1.0650 |     35.202 |     3.9
   62 |   1.0131 |     34.244 |   1.0639 |     34.835 |     3.9
   63 |   1.0002 |     33.984 |   1.0604 |     34.957 |     4.0
   64 |   1.0034 |     33.962 |   1.0710 |     35.846 |     4.1
   65 |   0.9936 |     33.561 |   1.0747 |     36.581 |     4.1
   66 |   0.9860 |     33.577 |   1.0563 |     34.957 |     4.2
   67 |   0.9774 |     33.501 |   1.0494 |     34.988 |     4.2
   68 |   0.9639 |     32.651 |   1.0482 |     35.018 |     4.3
   69 |   0.9605 |     32.651 |   1.0577 |     34.651 |     4.4
   70 |   0.9673 |     32.775 |   1.0353 |     34.865 |     4.4
   71 |   0.9521 |     32.261 |   1.0319 |     34.896 |     4.5
   72 |   0.9528 |     32.396 |   1.0403 |     34.191 |     4.6
   73 |   0.9441 |     31.925 |   1.0347 |     34.038 |     4.6
   74 |   0.9440 |     31.897 |   1.0394 |     34.130 |     4.7
   75 |   0.9306 |     31.437 |   1.0188 |     33.548 |     4.8
   76 |   0.9237 |     31.090 |   1.0359 |     34.804 |     4.8
   77 |   0.9187 |     30.987 |   1.0105 |     33.272 |     4.9
   78 |   0.9133 |     31.188 |   1.0046 |     33.517 |     4.9
   79 |   0.9357 |     31.551 |   1.0270 |     33.272 |     5.0
   80 |   0.9217 |     31.096 |   1.0153 |     33.548 |     5.1
   81 |   0.9232 |     31.329 |   1.0134 |     32.996 |     5.1
   82 |   0.9624 |     32.374 |   1.0273 |     33.487 |     5.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 735,842

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2430 |     61.232 |   1.6217 |     45.466 |     0.0
    2 |   1.4884 |     46.256 |   1.4173 |     45.987 |     0.0
    3 |   1.3901 |     45.828 |   1.3534 |     44.271 |     0.1
    4 |   1.3300 |     44.240 |   1.3040 |     43.382 |     0.1
    5 |   1.2798 |     42.853 |   1.2595 |     41.667 |     0.1
    6 |   1.2288 |     41.239 |   1.2183 |     39.277 |     0.1
    7 |   1.1869 |     39.738 |   1.1935 |     39.062 |     0.2
    8 |   1.1547 |     38.649 |   1.1794 |     39.491 |     0.2
    9 |   1.1147 |     37.722 |   1.1338 |     37.806 |     0.2
   10 |   1.0699 |     36.508 |   1.0944 |     36.489 |     0.2
   11 |   1.0185 |     34.563 |   1.0858 |     35.600 |     0.2
   12 |   0.9826 |     32.759 |   1.0520 |     33.915 |     0.3
   13 |   0.9349 |     31.383 |   1.0124 |     32.108 |     0.3
   14 |   0.8929 |     29.389 |   0.9926 |     32.292 |     0.3
   15 |   0.8374 |     27.731 |   0.9803 |     31.434 |     0.3
   16 |   0.7950 |     26.165 |   0.9543 |     30.331 |     0.3
   17 |   0.7588 |     24.865 |   0.9702 |     31.373 |     0.4
   18 |   0.7180 |     23.494 |   0.9522 |     30.576 |     0.4
   19 |   0.6700 |     21.825 |   0.9279 |     29.075 |     0.4
   20 |   0.6355 |     20.497 |   0.9195 |     29.289 |     0.4
   21 |   0.6112 |     20.010 |   0.9322 |     28.646 |     0.5
   22 |   0.5716 |     18.498 |   0.9259 |     28.278 |     0.5
   23 |   0.5479 |     17.940 |   0.9096 |     27.328 |     0.5
   24 |   0.5097 |     16.412 |   0.9148 |     27.451 |     0.5
   25 |   0.4751 |     14.982 |   0.9407 |     27.237 |     0.5
   26 |   0.4522 |     14.315 |   0.9074 |     27.083 |     0.6
   27 |   0.4356 |     14.158 |   0.9476 |     27.298 |     0.6
   28 |   0.4115 |     13.047 |   0.9205 |     26.103 |     0.6
   29 |   0.3952 |     12.803 |   0.9442 |     26.838 |     0.6
   30 |   0.3865 |     12.381 |   0.9133 |     25.398 |     0.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,075,746

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1103 |     82.093 |   2.6120 |     73.009 |     0.0
    2 |   2.3535 |     59.444 |   2.1974 |     56.311 |     0.1
    3 |   2.0761 |     56.123 |   1.9915 |     53.830 |     0.1
    4 |   1.9087 |     52.937 |   1.8487 |     49.540 |     0.1
    5 |   1.7816 |     48.797 |   1.7247 |     48.438 |     0.2
    6 |   1.6721 |     48.602 |   1.6244 |     48.346 |     0.2
    7 |   1.5894 |     47.410 |   1.5553 |     45.496 |     0.2
    8 |   1.5314 |     46.213 |   1.5072 |     45.558 |     0.3
    9 |   1.4856 |     46.218 |   1.4683 |     45.466 |     0.3
   10 |   1.4485 |     45.844 |   1.4380 |     44.669 |     0.3
   11 |   1.4164 |     44.842 |   1.4131 |     44.210 |     0.4
   12 |   1.3931 |     44.322 |   1.3932 |     42.800 |     0.4
   13 |   1.3666 |     43.373 |   1.3710 |     42.525 |     0.4
   14 |   1.3439 |     42.420 |   1.3571 |     42.555 |     0.5
   15 |   1.3267 |     41.986 |   1.3401 |     41.544 |     0.5
   16 |   1.3069 |     41.515 |   1.3254 |     41.422 |     0.5
   17 |   1.2906 |     41.049 |   1.3077 |     41.207 |     0.6
   18 |   1.2706 |     40.458 |   1.2971 |     40.564 |     0.6
   19 |   1.2543 |     40.036 |   1.2831 |     40.043 |     0.6
   20 |   1.2363 |     39.245 |   1.2715 |     39.706 |     0.7
   21 |   1.2206 |     38.318 |   1.2612 |     39.553 |     0.7
   22 |   1.2049 |     37.939 |   1.2452 |     38.756 |     0.7
   23 |   1.1873 |     37.327 |   1.2341 |     38.205 |     0.7
   24 |   1.1712 |     36.340 |   1.2219 |     37.469 |     0.8
   25 |   1.1571 |     36.097 |   1.2102 |     36.949 |     0.8
   26 |   1.1374 |     35.409 |   1.2014 |     36.581 |     0.8
   27 |   1.1221 |     34.813 |   1.1941 |     36.152 |     0.9
   28 |   1.1081 |     34.552 |   1.1843 |     36.336 |     0.9
   29 |   1.0947 |     34.076 |   1.1704 |     36.550 |     0.9
   30 |   1.0780 |     33.350 |   1.1622 |     35.815 |     1.0
   31 |   1.0663 |     32.965 |   1.1551 |     35.723 |     1.0
   32 |   1.0520 |     32.477 |   1.1453 |     35.754 |     1.0
   33 |   1.0376 |     31.963 |   1.1388 |     35.294 |     1.1
   34 |   1.0225 |     31.497 |   1.1267 |     35.172 |     1.1
   35 |   1.0086 |     31.009 |   1.1268 |     35.018 |     1.1
   36 |   0.9955 |     30.868 |   1.1145 |     34.620 |     1.2
   37 |   0.9830 |     30.413 |   1.1133 |     34.651 |     1.2
   38 |   0.9664 |     30.039 |   1.1014 |     34.620 |     1.2
   39 |   0.9511 |     29.725 |   1.0995 |     34.681 |     1.3
   40 |   0.9375 |     29.075 |   1.0918 |     33.885 |     1.3
   41 |   0.9255 |     29.042 |   1.0913 |     33.915 |     1.3
   42 |   0.9132 |     28.424 |   1.0876 |     33.977 |     1.4
   43 |   0.8989 |     27.899 |   1.0830 |     33.609 |     1.4
   44 |   0.8862 |     27.595 |   1.0662 |     33.058 |     1.4
   45 |   0.8724 |     27.194 |   1.0712 |     33.303 |     1.5
   46 |   0.8579 |     26.685 |   1.0676 |     33.027 |     1.5
   47 |   0.8447 |     26.273 |   1.0640 |     32.598 |     1.5
   48 |   0.8334 |     25.862 |   1.0612 |     32.721 |     1.6
   49 |   0.8183 |     25.558 |   1.0594 |     32.690 |     1.6
   50 |   0.8091 |     25.060 |   1.0461 |     32.261 |     1.6
   51 |   0.7987 |     24.821 |   1.0499 |     32.016 |     1.7
   52 |   0.7843 |     24.138 |   1.0469 |     31.893 |     1.7
   53 |   0.7765 |     23.759 |   1.0379 |     31.679 |     1.7
   54 |   0.7662 |     23.337 |   1.0440 |     31.495 |     1.8
   55 |   0.7486 |     22.860 |   1.0348 |     31.526 |     1.8
   56 |   0.7370 |     22.258 |   1.0375 |     31.219 |     1.8
   57 |   0.7247 |     21.955 |   1.0310 |     31.281 |     1.9
   58 |   0.7176 |     21.765 |   1.0452 |     31.189 |     1.9
   59 |   0.7091 |     21.375 |   1.0302 |     30.882 |     1.9
   60 |   0.6928 |     21.408 |   1.0215 |     30.423 |     1.9
   61 |   0.6844 |     20.958 |   1.0162 |     30.116 |     2.0
   62 |   0.6756 |     20.145 |   1.0270 |     30.300 |     2.0
   63 |   0.6639 |     19.836 |   1.0184 |     29.871 |     2.0
   64 |   0.6507 |     19.267 |   1.0289 |     30.208 |     2.1
   65 |   0.6437 |     19.300 |   1.0222 |     29.994 |     2.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,701,954

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5016 |     67.398 |   2.0137 |     53.922 |     0.1
    2 |   1.8096 |     51.766 |   1.6139 |     48.346 |     0.1
    3 |   1.5294 |     46.110 |   1.4591 |     45.466 |     0.2
    4 |   1.4384 |     45.676 |   1.4106 |     44.424 |     0.3
    5 |   1.4027 |     44.956 |   1.3757 |     43.781 |     0.3
    6 |   1.3682 |     44.251 |   1.3472 |     43.015 |     0.4
    7 |   1.3383 |     43.509 |   1.3273 |     42.984 |     0.4
    8 |   1.3178 |     42.994 |   1.3024 |     42.249 |     0.5
    9 |   1.2939 |     42.507 |   1.2905 |     41.636 |     0.6
   10 |   1.2693 |     42.100 |   1.2687 |     41.238 |     0.6
   11 |   1.2542 |     41.293 |   1.2604 |     40.594 |     0.7
   12 |   1.2383 |     41.163 |   1.2405 |     40.594 |     0.8
   13 |   1.2185 |     40.599 |   1.2342 |     40.074 |     0.8
   14 |   1.1971 |     39.667 |   1.2090 |     39.216 |     0.9
   15 |   1.1827 |     39.180 |   1.2122 |     39.185 |     0.9
   16 |   1.1698 |     38.790 |   1.1983 |     39.430 |     1.0
   17 |   1.1552 |     37.944 |   1.1892 |     37.806 |     1.1
   18 |   1.1434 |     37.495 |   1.1813 |     38.450 |     1.1
   19 |   1.1311 |     37.397 |   1.1830 |     37.439 |     1.2
   20 |   1.1085 |     36.476 |   1.1598 |     37.469 |     1.3
   21 |   1.0953 |     36.281 |   1.1547 |     37.286 |     1.3
   22 |   1.0821 |     35.777 |   1.1428 |     36.703 |     1.4
   23 |   1.0770 |     35.403 |   1.1333 |     36.734 |     1.4
   24 |   1.0697 |     35.365 |   1.1322 |     36.857 |     1.5
   25 |   1.0538 |     34.888 |   1.1290 |     35.815 |     1.6
   26 |   1.0432 |     34.525 |   1.1365 |     36.857 |     1.6
   27 |   1.0241 |     33.946 |   1.1269 |     37.010 |     1.7
   28 |   1.0118 |     33.642 |   1.1231 |     36.428 |     1.8
   29 |   1.0042 |     33.523 |   1.1263 |     37.010 |     1.8
   30 |   0.9959 |     32.932 |   1.1108 |     36.458 |     1.9
   31 |   0.9910 |     32.867 |   1.1074 |     35.355 |     1.9
   32 |   0.9864 |     32.987 |   1.1045 |     36.458 |     2.0
   33 |   0.9608 |     31.800 |   1.0810 |     34.712 |     2.1
   34 |   0.9659 |     32.120 |   1.1072 |     36.183 |     2.1
   35 |   0.9549 |     31.816 |   1.0957 |     35.723 |     2.2
   36 |   0.9410 |     31.171 |   1.1006 |     35.631 |     2.3
   37 |   0.9447 |     31.204 |   1.0845 |     35.386 |     2.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 805,826

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3024 |     62.289 |   1.7035 |     50.000 |     0.0
    2 |   1.5295 |     47.193 |   1.4276 |     45.006 |     0.0
    3 |   1.4215 |     46.191 |   1.3992 |     45.466 |     0.1
    4 |   1.3952 |     45.822 |   1.3650 |     44.118 |     0.1
    5 |   1.3607 |     44.880 |   1.3370 |     43.750 |     0.1
    6 |   1.3428 |     44.831 |   1.3237 |     44.087 |     0.1
    7 |   1.3282 |     44.630 |   1.3091 |     44.730 |     0.2
    8 |   1.3068 |     44.278 |   1.3029 |     43.505 |     0.2
    9 |   1.2942 |     43.796 |   1.2945 |     43.352 |     0.2
   10 |   1.2768 |     43.173 |   1.2826 |     42.402 |     0.2
   11 |   1.2625 |     42.962 |   1.2690 |     42.249 |     0.3
   12 |   1.2384 |     42.187 |   1.2634 |     41.115 |     0.3
   13 |   1.2205 |     41.510 |   1.2401 |     41.023 |     0.3
   14 |   1.2056 |     40.832 |   1.2274 |     40.564 |     0.3
   15 |   1.1873 |     40.475 |   1.2115 |     40.104 |     0.4
   16 |   1.1707 |     40.079 |   1.1901 |     40.012 |     0.4
   17 |   1.1447 |     39.478 |   1.1789 |     39.246 |     0.4
   18 |   1.1278 |     38.800 |   1.1858 |     39.798 |     0.4
   19 |   1.1159 |     38.817 |   1.1525 |     39.614 |     0.5
   20 |   1.0884 |     37.890 |   1.1486 |     39.246 |     0.5
   21 |   1.0657 |     36.796 |   1.1156 |     37.286 |     0.5
   22 |   1.0547 |     36.525 |   1.1136 |     37.776 |     0.5
   23 |   1.0376 |     36.064 |   1.0952 |     37.929 |     0.5
   24 |   1.0122 |     35.111 |   1.0758 |     35.784 |     0.6
   25 |   0.9940 |     34.455 |   1.0622 |     36.121 |     0.6
   26 |   0.9664 |     33.561 |   1.0523 |     34.988 |     0.6
   27 |   0.9503 |     32.781 |   1.0649 |     35.692 |     0.6
   28 |   0.9338 |     32.130 |   1.0362 |     35.080 |     0.7
   29 |   0.9079 |     31.404 |   1.0392 |     35.417 |     0.7
   30 |   0.8985 |     30.890 |   1.0470 |     35.049 |     0.7
   31 |   0.8774 |     30.077 |   1.0175 |     33.425 |     0.7
   32 |   0.8574 |     29.150 |   1.0238 |     32.445 |     0.8
   33 |   0.8338 |     28.419 |   1.0280 |     33.487 |     0.8
   34 |   0.8156 |     27.541 |   0.9995 |     32.138 |     0.8
   35 |   0.7995 |     27.205 |   0.9962 |     32.138 |     0.8
   36 |   0.7721 |     26.024 |   0.9828 |     31.679 |     0.9
   37 |   0.7690 |     26.268 |   0.9821 |     31.587 |     0.9
   38 |   0.7623 |     25.564 |   0.9886 |     31.618 |     0.9
   39 |   0.7354 |     25.016 |   0.9813 |     31.250 |     0.9
   40 |   0.7249 |     24.707 |   0.9744 |     30.821 |     0.9
   41 |   0.7260 |     24.659 |   0.9714 |     31.373 |     1.0
   42 |   0.7000 |     23.673 |   0.9895 |     30.576 |     1.0
   43 |   0.6786 |     23.011 |   0.9595 |     30.699 |     1.0
   44 |   0.6604 |     22.350 |   0.9563 |     29.994 |     1.0
   45 |   0.6591 |     22.605 |   0.9381 |     29.136 |     1.1
   46 |   0.6365 |     21.375 |   0.9262 |     28.860 |     1.1
   47 |   0.6238 |     21.234 |   0.9378 |     28.922 |     1.1
   48 |   0.6109 |     20.736 |   0.9253 |     28.523 |     1.1
   49 |   0.6060 |     20.617 |   0.9350 |     29.289 |     1.2
   50 |   0.5997 |     20.118 |   0.9420 |     28.462 |     1.2
   51 |   0.6026 |     20.292 |   0.9434 |     28.983 |     1.2
   52 |   0.5754 |     19.788 |   0.9415 |     28.523 |     1.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 2,467,042

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5741 |     68.086 |   2.0003 |     53.799 |     0.1
    2 |   1.7638 |     49.989 |   1.5643 |     45.496 |     0.2
    3 |   1.5080 |     46.359 |   1.4488 |     45.987 |     0.3
    4 |   1.4385 |     46.202 |   1.4177 |     45.466 |     0.3
    5 |   1.4194 |     46.256 |   1.4051 |     45.466 |     0.4
    6 |   1.4099 |     46.294 |   1.3981 |     45.466 |     0.5
    7 |   1.4014 |     46.408 |   1.3948 |     45.466 |     0.6
    8 |   1.3993 |     46.164 |   1.3900 |     45.466 |     0.7
    9 |   1.3952 |     46.001 |   1.3854 |     45.466 |     0.8
   10 |   1.3909 |     45.893 |   1.3699 |     44.884 |     0.9
   11 |   1.3755 |     45.118 |   1.3594 |     43.934 |     0.9
   12 |   1.3643 |     44.972 |   1.3488 |     43.689 |     1.0
   13 |   1.3577 |     44.706 |   1.3440 |     43.781 |     1.1
   14 |   1.3478 |     44.701 |   1.3384 |     43.903 |     1.2
   15 |   1.3412 |     44.777 |   1.3344 |     44.087 |     1.3
   16 |   1.3383 |     44.549 |   1.3266 |     43.689 |     1.4
   17 |   1.3284 |     44.484 |   1.3292 |     43.750 |     1.5
   18 |   1.3272 |     44.311 |   1.3187 |     43.536 |     1.5
   19 |   1.3239 |     44.251 |   1.3103 |     44.148 |     1.6
   20 |   1.3130 |     43.953 |   1.3019 |     43.076 |     1.7
   21 |   1.3037 |     43.579 |   1.2998 |     42.616 |     1.8
   22 |   1.2995 |     43.715 |   1.2910 |     42.678 |     1.9
   23 |   1.2957 |     43.699 |   1.2838 |     42.188 |     2.0
   24 |   1.2892 |     43.482 |   1.2775 |     42.126 |     2.0
   25 |   1.2841 |     43.346 |   1.2790 |     42.616 |     2.1
   26 |   1.2778 |     43.119 |   1.2741 |     42.157 |     2.2
   27 |   1.2705 |     43.032 |   1.2658 |     41.667 |     2.3
   28 |   1.2685 |     42.729 |   1.2648 |     42.402 |     2.4
   29 |   1.2580 |     42.599 |   1.2538 |     41.513 |     2.5
   30 |   1.2510 |     42.593 |   1.2526 |     42.157 |     2.6
   31 |   1.2562 |     42.664 |   1.2451 |     41.483 |     2.6
   32 |   1.2402 |     42.089 |   1.2444 |     41.330 |     2.7
   33 |   1.2376 |     42.171 |   1.2288 |     41.697 |     2.8
   34 |   1.2307 |     41.889 |   1.2289 |     41.544 |     2.9
   35 |   1.2242 |     41.840 |   1.2342 |     41.575 |     3.0
   36 |   1.2182 |     41.428 |   1.2227 |     41.605 |     3.1
   37 |   1.2073 |     41.461 |   1.2097 |     40.993 |     3.2
   38 |   1.2053 |     41.087 |   1.2043 |     39.798 |     3.2
   39 |   1.1917 |     40.442 |   1.1955 |     39.798 |     3.3
   40 |   1.1844 |     40.318 |   1.1899 |     39.737 |     3.4
   41 |   1.1850 |     40.269 |   1.1984 |     39.798 |     3.5
   42 |   1.1770 |     39.922 |   1.1832 |     38.756 |     3.6
   43 |   1.1841 |     40.101 |   1.1941 |     39.461 |     3.7
   44 |   1.1765 |     39.776 |   1.1764 |     39.124 |     3.8
   45 |   1.1670 |     39.521 |   1.1710 |     38.235 |     3.8
   46 |   1.1569 |     39.120 |   1.1747 |     38.572 |     3.9
   47 |   1.1553 |     39.261 |   1.1629 |     38.634 |     4.0
   48 |   1.1523 |     39.088 |   1.1558 |     37.837 |     4.1
   49 |   1.1549 |     39.201 |   1.1552 |     38.817 |     4.2
   50 |   1.1430 |     38.773 |   1.1452 |     38.327 |     4.3
   51 |   1.1389 |     38.632 |   1.1617 |     38.817 |     4.3
   52 |   1.1334 |     38.638 |   1.1573 |     39.062 |     4.4
   53 |   1.1207 |     38.188 |   1.1341 |     38.021 |     4.5
   54 |   1.1178 |     37.793 |   1.1434 |     38.725 |     4.6
   55 |   1.1208 |     38.150 |   1.1627 |     38.297 |     4.7
   56 |   1.1240 |     38.502 |   1.1364 |     37.929 |     4.8
   57 |   1.1121 |     37.858 |   1.1425 |     38.051 |     4.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,535,042

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3285 |     64.191 |   1.6839 |     48.346 |     0.0
    2 |   1.5146 |     46.603 |   1.4323 |     45.466 |     0.1
    3 |   1.4205 |     46.223 |   1.3997 |     45.466 |     0.1
    4 |   1.4066 |     46.381 |   1.3918 |     45.987 |     0.2
    5 |   1.3972 |     46.202 |   1.3853 |     45.466 |     0.2
    6 |   1.3912 |     46.104 |   1.3718 |     45.129 |     0.3
    7 |   1.3698 |     45.145 |   1.3464 |     44.118 |     0.3
    8 |   1.3492 |     44.712 |   1.3316 |     43.842 |     0.3
    9 |   1.3335 |     44.511 |   1.3281 |     43.352 |     0.4
   10 |   1.3258 |     44.083 |   1.3112 |     43.750 |     0.4
   11 |   1.3165 |     44.110 |   1.3013 |     43.076 |     0.5
   12 |   1.3025 |     43.493 |   1.3027 |     43.444 |     0.5
   13 |   1.2898 |     43.336 |   1.2917 |     43.229 |     0.6
   14 |   1.2812 |     43.021 |   1.2713 |     41.881 |     0.6
   15 |   1.2709 |     42.685 |   1.2730 |     42.065 |     0.6
   16 |   1.2613 |     42.674 |   1.2632 |     41.912 |     0.7
   17 |   1.2511 |     42.425 |   1.2471 |     41.513 |     0.7
   18 |   1.2386 |     41.818 |   1.2437 |     41.115 |     0.8
   19 |   1.2226 |     41.472 |   1.2433 |     42.034 |     0.8
   20 |   1.2143 |     41.423 |   1.2367 |     41.544 |     0.9
   21 |   1.1948 |     40.794 |   1.2182 |     40.931 |     0.9
   22 |   1.1881 |     40.491 |   1.2338 |     41.268 |     0.9
   23 |   1.1819 |     40.009 |   1.2033 |     39.553 |     1.0
   24 |   1.1600 |     39.299 |   1.2028 |     39.001 |     1.0
   25 |   1.1512 |     39.342 |   1.1904 |     38.695 |     1.1
   26 |   1.1315 |     38.464 |   1.1755 |     39.185 |     1.1
   27 |   1.1229 |     38.107 |   1.1648 |     38.848 |     1.2
   28 |   1.1092 |     37.841 |   1.1559 |     37.684 |     1.2
   29 |   1.1034 |     37.224 |   1.1605 |     38.266 |     1.2
   30 |   1.0816 |     36.698 |   1.1737 |     38.756 |     1.3
   31 |   1.0847 |     36.974 |   1.1527 |     39.093 |     1.3
   32 |   1.0567 |     35.934 |   1.1502 |     38.327 |     1.4
   33 |   1.0474 |     35.322 |   1.1398 |     37.806 |     1.4
   34 |   1.0377 |     34.943 |   1.1513 |     38.235 |     1.5
   35 |   1.0236 |     34.672 |   1.1280 |     36.612 |     1.5
   36 |   1.0151 |     34.395 |   1.1252 |     36.703 |     1.6
   37 |   1.0181 |     34.298 |   1.1153 |     36.152 |     1.6
   38 |   1.0065 |     34.086 |   1.1267 |     37.469 |     1.6
   39 |   0.9895 |     33.561 |   1.1040 |     35.447 |     1.7
   40 |   0.9753 |     33.041 |   1.1046 |     36.397 |     1.7
   41 |   0.9620 |     32.959 |   1.1041 |     35.938 |     1.8
   42 |   0.9529 |     32.336 |   1.0914 |     35.049 |     1.8
   43 |   0.9433 |     32.347 |   1.1129 |     35.876 |     1.9
   44 |   0.9408 |     32.109 |   1.0833 |     34.988 |     1.9
   45 |   0.9178 |     31.052 |   1.0873 |     34.804 |     1.9
   46 |   0.9205 |     31.339 |   1.0799 |     34.804 |     2.0
   47 |   0.9069 |     30.998 |   1.0994 |     35.233 |     2.0
   48 |   0.8997 |     30.711 |   1.1039 |     35.325 |     2.1
   49 |   0.8869 |     30.261 |   1.0991 |     35.631 |     2.1
   50 |   0.8696 |     29.839 |   1.0924 |     35.325 |     2.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 505,730

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.3020 |     62.034 |   1.6630 |     48.376 |     0.0
    2 |   1.5041 |     46.381 |   1.4188 |     45.987 |     0.0
    3 |   1.4009 |     46.251 |   1.3649 |     43.842 |     0.0
    4 |   1.3505 |     44.739 |   1.3277 |     44.118 |     0.1
    5 |   1.3072 |     43.997 |   1.2822 |     43.107 |     0.1
    6 |   1.2712 |     42.615 |   1.2617 |     41.360 |     0.1
    7 |   1.2400 |     41.564 |   1.2474 |     41.268 |     0.1
    8 |   1.2104 |     40.605 |   1.2219 |     40.257 |     0.1
    9 |   1.1761 |     39.445 |   1.1880 |     39.124 |     0.1
   10 |   1.1421 |     38.275 |   1.1572 |     37.929 |     0.1
   11 |   1.0995 |     37.554 |   1.1234 |     35.968 |     0.2
   12 |   1.0645 |     35.869 |   1.1097 |     35.815 |     0.2
   13 |   1.0378 |     35.067 |   1.0739 |     34.957 |     0.2
   14 |   0.9937 |     33.344 |   1.0647 |     34.528 |     0.2
   15 |   0.9568 |     32.477 |   1.0339 |     33.732 |     0.2
   16 |   0.9229 |     30.890 |   1.0469 |     33.395 |     0.2
   17 |   0.8958 |     30.109 |   0.9923 |     32.384 |     0.2
   18 |   0.8580 |     28.934 |   0.9842 |     31.648 |     0.3
   19 |   0.8285 |     27.530 |   0.9674 |     30.515 |     0.3
   20 |   0.7977 |     26.934 |   0.9498 |     30.300 |     0.3
   21 |   0.7667 |     25.309 |   0.9453 |     29.626 |     0.3
   22 |   0.7278 |     24.209 |   0.9280 |     29.259 |     0.3
   23 |   0.7019 |     23.125 |   0.9410 |     28.860 |     0.3
   24 |   0.6781 |     22.107 |   0.9379 |     28.493 |     0.3
   25 |   0.6441 |     21.191 |   0.9268 |     28.217 |     0.4
   26 |   0.6581 |     21.722 |   0.9394 |     28.768 |     0.4
   27 |   0.6095 |     20.237 |   0.9195 |     27.237 |     0.4
   28 |   0.5788 |     19.284 |   0.9146 |     28.002 |     0.4
   29 |   0.5618 |     18.493 |   0.9436 |     28.064 |     0.4
   30 |   0.5361 |     17.707 |   0.9106 |     26.991 |     0.4
   31 |   0.5121 |     17.192 |   0.9198 |     27.237 |     0.4
   32 |   0.4988 |     16.477 |   0.9316 |     27.114 |     0.5
   33 |   0.4866 |     16.125 |   0.9317 |     26.593 |     0.5
   34 |   0.4704 |     15.632 |   0.9278 |     26.471 |     0.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 2,275,714

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2109 |     82.591 |   2.7243 |     81.893 |     0.1
    2 |   2.4779 |     64.971 |   2.2785 |     58.824 |     0.1
    3 |   2.1324 |     57.732 |   2.0311 |     58.548 |     0.2
    4 |   1.9434 |     53.544 |   1.8794 |     48.468 |     0.2
    5 |   1.8138 |     48.629 |   1.7573 |     48.346 |     0.3
    6 |   1.7094 |     48.602 |   1.6613 |     48.346 |     0.4
    7 |   1.6233 |     48.602 |   1.5763 |     48.346 |     0.4
    8 |   1.5500 |     46.733 |   1.5175 |     45.466 |     0.5
    9 |   1.5058 |     46.213 |   1.4820 |     45.466 |     0.5
   10 |   1.4759 |     46.213 |   1.4593 |     45.466 |     0.6
   11 |   1.4559 |     46.337 |   1.4436 |     45.466 |     0.7
   12 |   1.4409 |     46.267 |   1.4319 |     45.466 |     0.7
   13 |   1.4298 |     46.326 |   1.4215 |     46.140 |     0.8
   14 |   1.4205 |     46.196 |   1.4114 |     45.466 |     0.8
   15 |   1.4104 |     46.316 |   1.4044 |     45.741 |     0.9
   16 |   1.3975 |     46.207 |   1.3903 |     45.466 |     1.0
   17 |   1.3852 |     46.186 |   1.3806 |     45.496 |     1.0
   18 |   1.3740 |     45.828 |   1.3694 |     44.975 |     1.1
   19 |   1.3584 |     45.248 |   1.3593 |     44.179 |     1.1
   20 |   1.3502 |     45.064 |   1.3520 |     44.210 |     1.2
   21 |   1.3398 |     44.972 |   1.3442 |     44.179 |     1.3
   22 |   1.3290 |     44.836 |   1.3383 |     44.087 |     1.3
   23 |   1.3166 |     44.739 |   1.3285 |     44.301 |     1.4
   24 |   1.3019 |     44.593 |   1.3363 |     43.934 |     1.4
   25 |   1.2895 |     44.099 |   1.3100 |     43.689 |     1.5
   26 |   1.2768 |     43.585 |   1.3018 |     43.444 |     1.6
   27 |   1.2624 |     42.745 |   1.2881 |     42.433 |     1.6
   28 |   1.2514 |     42.057 |   1.2794 |     42.463 |     1.7
   29 |   1.2348 |     41.558 |   1.2731 |     42.923 |     1.7
   30 |   1.2220 |     41.287 |   1.2633 |     42.249 |     1.8
   31 |   1.2100 |     40.859 |   1.2515 |     42.371 |     1.9
   32 |   1.1927 |     40.350 |   1.2464 |     42.126 |     1.9
   33 |   1.1783 |     39.749 |   1.2362 |     42.065 |     2.0
   34 |   1.1651 |     39.337 |   1.2286 |     41.728 |     2.0
   35 |   1.1502 |     38.578 |   1.2156 |     41.054 |     2.1
   36 |   1.1426 |     38.215 |   1.2175 |     40.962 |     2.2
   37 |   1.1278 |     37.836 |   1.2030 |     40.227 |     2.2
   38 |   1.1117 |     37.023 |   1.2016 |     40.564 |     2.3
   39 |   1.0987 |     36.590 |   1.2030 |     39.798 |     2.3
   40 |   1.0882 |     36.042 |   1.1935 |     40.319 |     2.4
   41 |   1.0725 |     35.983 |   1.1789 |     39.553 |     2.5
   42 |   1.0582 |     35.344 |   1.1763 |     40.043 |     2.5
   43 |   1.0467 |     34.883 |   1.1705 |     39.124 |     2.6
   44 |   1.0318 |     34.466 |   1.1707 |     39.430 |     2.6
   45 |   1.0203 |     34.021 |   1.1616 |     38.879 |     2.7
   46 |   1.0082 |     33.415 |   1.1591 |     38.082 |     2.8
   47 |   0.9953 |     33.176 |   1.1597 |     38.143 |     2.8
   48 |   0.9823 |     32.347 |   1.1598 |     38.082 |     2.9
   49 |   0.9721 |     32.087 |   1.1522 |     37.745 |     2.9
   50 |   0.9612 |     31.643 |   1.1638 |     38.327 |     3.0
   51 |   0.9538 |     31.253 |   1.1470 |     37.347 |     3.1
   52 |   0.9424 |     30.689 |   1.1396 |     36.703 |     3.1
   53 |   0.9338 |     30.066 |   1.1454 |     37.224 |     3.2
   54 |   0.9213 |     29.833 |   1.1456 |     37.255 |     3.2
   55 |   0.9082 |     29.058 |   1.1473 |     36.949 |     3.3
   56 |   0.9014 |     28.814 |   1.1524 |     37.806 |     3.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 634,210

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4902 |     62.435 |   1.9761 |     54.044 |     0.0
    2 |   1.7400 |     49.187 |   1.5493 |     45.496 |     0.1
    3 |   1.4882 |     45.785 |   1.4226 |     45.067 |     0.1
    4 |   1.4062 |     44.733 |   1.3708 |     43.290 |     0.1
    5 |   1.3554 |     43.547 |   1.3407 |     42.525 |     0.1
    6 |   1.3214 |     43.054 |   1.3034 |     41.207 |     0.2
    7 |   1.2860 |     42.062 |   1.2717 |     41.483 |     0.2
    8 |   1.2529 |     41.233 |   1.2537 |     40.319 |     0.2
    9 |   1.2247 |     40.057 |   1.2212 |     39.369 |     0.2
   10 |   1.1968 |     39.212 |   1.2062 |     38.603 |     0.3
   11 |   1.1660 |     38.139 |   1.1970 |     38.327 |     0.3
   12 |   1.1380 |     37.516 |   1.1679 |     37.163 |     0.3
   13 |   1.1122 |     36.514 |   1.1520 |     36.244 |     0.3
   14 |   1.0964 |     36.091 |   1.1275 |     35.172 |     0.4
   15 |   1.0705 |     35.176 |   1.1143 |     35.049 |     0.4
   16 |   1.0348 |     34.070 |   1.1015 |     35.018 |     0.4
   17 |   1.0182 |     33.604 |   1.0842 |     34.804 |     0.4
   18 |   0.9868 |     32.537 |   1.0820 |     34.436 |     0.5
   19 |   0.9632 |     31.627 |   1.0536 |     32.812 |     0.5
   20 |   0.9420 |     31.253 |   1.0492 |     34.038 |     0.5
   21 |   0.9220 |     30.440 |   1.0374 |     33.180 |     0.5
   22 |   0.9148 |     30.451 |   1.0344 |     32.629 |     0.6
   23 |   0.8777 |     29.129 |   1.0195 |     32.169 |     0.6
   24 |   0.8691 |     28.647 |   1.0208 |     32.414 |     0.6
   25 |   0.8442 |     27.872 |   1.0112 |     32.322 |     0.6
   26 |   0.8255 |     27.379 |   0.9998 |     31.679 |     0.7
   27 |   0.8089 |     26.680 |   0.9923 |     31.526 |     0.7
   28 |   0.7988 |     26.197 |   0.9948 |     31.955 |     0.7
   29 |   0.7891 |     26.019 |   1.0018 |     32.108 |     0.7
   30 |   0.7743 |     25.336 |   0.9821 |     30.852 |     0.8
   31 |   0.7559 |     24.604 |   0.9847 |     30.668 |     0.8
   32 |   0.7394 |     24.502 |   0.9784 |     30.484 |     0.8
   33 |   0.7254 |     23.851 |   0.9766 |     30.790 |     0.8
   34 |   0.7254 |     24.225 |   0.9747 |     30.790 |     0.9
   35 |   0.6918 |     22.860 |   0.9669 |     30.116 |     0.9
   36 |   0.6879 |     22.632 |   0.9837 |     30.055 |     0.9
   37 |   0.6863 |     22.757 |   0.9696 |     30.116 |     0.9
   38 |   0.6700 |     21.803 |   0.9705 |     29.933 |     1.0
   39 |   0.6570 |     21.906 |   0.9710 |     29.963 |     1.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 296,258

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8512 |     75.591 |   2.4806 |     66.452 |     0.0
    2 |   2.2911 |     60.506 |   2.1034 |     58.854 |     0.0
    3 |   1.9578 |     55.202 |   1.8039 |     51.838 |     0.1
    4 |   1.6875 |     48.499 |   1.5709 |     45.558 |     0.1
    5 |   1.5245 |     45.660 |   1.4765 |     45.282 |     0.1
    6 |   1.4553 |     45.254 |   1.4297 |     44.884 |     0.1
    7 |   1.4134 |     45.292 |   1.3937 |     44.700 |     0.2
    8 |   1.3812 |     44.679 |   1.3688 |     44.455 |     0.2
    9 |   1.3516 |     44.159 |   1.3374 |     42.433 |     0.2
   10 |   1.3242 |     42.978 |   1.3137 |     42.096 |     0.2
   11 |   1.3013 |     42.333 |   1.2929 |     40.809 |     0.2
   12 |   1.2765 |     42.133 |   1.2755 |     40.931 |     0.3
   13 |   1.2590 |     41.650 |   1.2603 |     39.737 |     0.3
   14 |   1.2404 |     41.027 |   1.2362 |     39.400 |     0.3
   15 |   1.2180 |     40.047 |   1.2267 |     39.124 |     0.3
   16 |   1.2047 |     39.586 |   1.2143 |     39.124 |     0.4
   17 |   1.1830 |     39.201 |   1.2077 |     39.246 |     0.4
   18 |   1.1684 |     39.033 |   1.1975 |     38.817 |     0.4
   19 |   1.1520 |     38.481 |   1.1857 |     38.051 |     0.4
   20 |   1.1368 |     37.809 |   1.1743 |     37.439 |     0.4
   21 |   1.1263 |     37.787 |   1.1588 |     37.408 |     0.5
   22 |   1.1046 |     36.736 |   1.1531 |     36.918 |     0.5
   23 |   1.0901 |     36.346 |   1.1509 |     36.029 |     0.5
   24 |   1.0811 |     35.891 |   1.1434 |     36.489 |     0.5
   25 |   1.0662 |     35.344 |   1.1319 |     35.938 |     0.6
   26 |   1.0514 |     34.498 |   1.1266 |     36.029 |     0.6
   27 |   1.0348 |     34.227 |   1.1082 |     35.233 |     0.6
   28 |   1.0293 |     33.723 |   1.1118 |     35.447 |     0.6
   29 |   1.0098 |     33.322 |   1.1044 |     35.846 |     0.6
   30 |   1.0059 |     32.943 |   1.0954 |     35.447 |     0.7
   31 |   0.9942 |     32.667 |   1.0813 |     34.375 |     0.7
   32 |   0.9863 |     32.369 |   1.0841 |     35.294 |     0.7
   33 |   0.9714 |     32.190 |   1.0818 |     35.018 |     0.7
   34 |   0.9548 |     31.372 |   1.0586 |     33.670 |     0.7
   35 |   0.9504 |     31.334 |   1.0644 |     34.130 |     0.8
   36 |   0.9464 |     30.830 |   1.0635 |     34.099 |     0.8
   37 |   0.9310 |     30.565 |   1.0427 |     33.058 |     0.8
   38 |   0.9204 |     29.741 |   1.0441 |     33.180 |     0.8
   39 |   0.9144 |     30.007 |   1.0495 |     32.996 |     0.9
   40 |   0.9002 |     29.324 |   1.0466 |     32.475 |     0.9
   41 |   0.8998 |     29.524 |   1.0291 |     32.322 |     0.9
   42 |   0.8901 |     28.814 |   1.0286 |     31.801 |     0.9
   43 |   0.8753 |     28.598 |   1.0245 |     32.414 |     0.9
   44 |   0.8715 |     28.473 |   1.0359 |     33.241 |     1.0
   45 |   0.8681 |     28.121 |   1.0106 |     31.740 |     1.0
   46 |   0.8943 |     29.340 |   1.0371 |     32.414 |     1.0
   47 |   0.8611 |     27.904 |   1.0120 |     31.985 |     1.0
   48 |   0.8412 |     27.427 |   1.0123 |     31.189 |     1.1
   49 |   0.8279 |     26.745 |   1.0118 |     31.219 |     1.1
   50 |   0.8289 |     27.021 |   0.9921 |     31.219 |     1.1
   51 |   0.8146 |     26.490 |   1.0060 |     31.924 |     1.1
   52 |   0.8198 |     26.907 |   1.0062 |     31.587 |     1.1
   53 |   0.7974 |     25.997 |   0.9957 |     31.495 |     1.2
   54 |   0.7919 |     25.899 |   0.9966 |     30.821 |     1.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 169,730

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.9203 |     78.208 |   2.4664 |     66.912 |     0.0
    2 |   2.2035 |     59.753 |   1.9939 |     53.768 |     0.0
    3 |   1.8438 |     51.181 |   1.7057 |     48.346 |     0.0
    4 |   1.6342 |     46.391 |   1.5590 |     45.466 |     0.1
    5 |   1.5239 |     46.272 |   1.4874 |     45.987 |     0.1
    6 |   1.4721 |     46.234 |   1.4485 |     45.466 |     0.1
    7 |   1.4430 |     46.278 |   1.4318 |     45.987 |     0.1
    8 |   1.4308 |     46.229 |   1.4173 |     45.466 |     0.1
    9 |   1.4200 |     46.267 |   1.4095 |     45.466 |     0.1
   10 |   1.4129 |     46.223 |   1.4016 |     45.864 |     0.1
   11 |   1.4032 |     46.158 |   1.3931 |     45.803 |     0.1
   12 |   1.3897 |     46.326 |   1.3761 |     44.884 |     0.2
   13 |   1.3688 |     45.936 |   1.3588 |     45.282 |     0.2
   14 |   1.3482 |     45.492 |   1.3391 |     43.811 |     0.2
   15 |   1.3245 |     44.230 |   1.3168 |     43.658 |     0.2
   16 |   1.3023 |     43.904 |   1.3017 |     42.953 |     0.2
   17 |   1.2879 |     43.503 |   1.2870 |     43.382 |     0.2
   18 |   1.2727 |     43.606 |   1.2847 |     43.199 |     0.2
   19 |   1.2596 |     42.929 |   1.2717 |     43.076 |     0.3
   20 |   1.2445 |     42.257 |   1.2671 |     42.188 |     0.3
   21 |   1.2296 |     41.607 |   1.2529 |     42.004 |     0.3
   22 |   1.2155 |     41.336 |   1.2503 |     41.942 |     0.3
   23 |   1.2028 |     40.946 |   1.2360 |     41.667 |     0.3
   24 |   1.1847 |     40.431 |   1.2266 |     40.656 |     0.3
   25 |   1.1727 |     40.106 |   1.2135 |     40.533 |     0.3
   26 |   1.1526 |     39.743 |   1.2053 |     40.778 |     0.3
   27 |   1.1410 |     39.337 |   1.2079 |     40.870 |     0.4
   28 |   1.1289 |     38.871 |   1.2117 |     41.636 |     0.4
   29 |   1.1177 |     38.421 |   1.2037 |     40.502 |     0.4
   30 |   1.0974 |     37.543 |   1.1713 |     39.430 |     0.4
   31 |   1.0929 |     37.657 |   1.1596 |     38.419 |     0.4
   32 |   1.0752 |     36.676 |   1.1508 |     39.308 |     0.4
   33 |   1.0651 |     36.590 |   1.1597 |     39.277 |     0.4
   34 |   1.0548 |     35.977 |   1.1515 |     39.062 |     0.5
   35 |   1.0398 |     35.826 |   1.1572 |     39.154 |     0.5
   36 |   1.0265 |     35.268 |   1.1537 |     38.388 |     0.5
   37 |   1.0223 |     35.230 |   1.1422 |     38.419 |     0.5
   38 |   1.0040 |     34.617 |   1.1439 |     38.848 |     0.5
   39 |   0.9996 |     34.406 |   1.1323 |     37.960 |     0.5
   40 |   0.9801 |     33.772 |   1.1370 |     38.082 |     0.5
   41 |   0.9690 |     33.436 |   1.1316 |     37.623 |     0.5
   42 |   0.9576 |     32.819 |   1.1333 |     37.653 |     0.6
   43 |   0.9437 |     32.483 |   1.1262 |     36.765 |     0.6
   44 |   0.9318 |     31.968 |   1.1387 |     36.642 |     0.6
   45 |   0.9212 |     31.605 |   1.1278 |     36.826 |     0.6
   46 |   0.9094 |     31.074 |   1.1257 |     36.213 |     0.6
   47 |   0.9019 |     31.014 |   1.1149 |     36.642 |     0.6
   48 |   0.8905 |     30.380 |   1.1172 |     35.570 |     0.6
   49 |   0.8851 |     30.218 |   1.1212 |     35.938 |     0.7
   50 |   0.8729 |     29.519 |   1.1168 |     35.600 |     0.7
   51 |   0.8599 |     29.459 |   1.1374 |     36.029 |     0.7
   52 |   0.8530 |     29.215 |   1.1069 |     35.754 |     0.7
   53 |   0.8415 |     28.609 |   1.1067 |     34.283 |     0.7
   54 |   0.8311 |     28.153 |   1.1422 |     36.275 |     0.7
   55 |   0.8340 |     28.473 |   1.1223 |     35.539 |     0.7
   56 |   0.8186 |     27.671 |   1.1186 |     35.110 |     0.7
   57 |   0.8046 |     27.433 |   1.1193 |     35.325 |     0.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 564,738

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4061 |     89.884 |   3.2651 |     83.333 |     0.0
    2 |   3.0601 |     82.163 |   2.8964 |     71.538 |     0.0
    3 |   2.7784 |     68.260 |   2.6943 |     67.126 |     0.1
    4 |   2.6145 |     67.111 |   2.5598 |     66.912 |     0.1
    5 |   2.4964 |     66.515 |   2.4604 |     64.216 |     0.1
    6 |   2.4029 |     60.831 |   2.3732 |     59.069 |     0.1
    7 |   2.3227 |     59.092 |   2.2956 |     58.609 |     0.2
    8 |   2.2469 |     58.984 |   2.2240 |     58.609 |     0.2
    9 |   2.1832 |     58.799 |   2.1622 |     58.303 |     0.2
   10 |   2.1188 |     58.528 |   2.0965 |     58.333 |     0.2
   11 |   2.0610 |     57.596 |   2.0426 |     55.147 |     0.3
   12 |   2.0092 |     52.991 |   1.9904 |     49.694 |     0.3
   13 |   1.9601 |     49.810 |   1.9417 |     49.203 |     0.3
   14 |   1.9135 |     49.128 |   1.8955 |     48.775 |     0.3
   15 |   1.8714 |     48.640 |   1.8545 |     48.376 |     0.4
   16 |   1.8294 |     48.353 |   1.8161 |     48.407 |     0.4
   17 |   1.7920 |     48.320 |   1.7762 |     48.162 |     0.4
   18 |   1.7576 |     48.288 |   1.7440 |     48.315 |     0.4
   19 |   1.7238 |     48.250 |   1.7133 |     48.376 |     0.5
   20 |   1.6947 |     48.185 |   1.6807 |     48.100 |     0.5
   21 |   1.6646 |     48.169 |   1.6495 |     48.039 |     0.5
   22 |   1.6354 |     47.117 |   1.6256 |     45.312 |     0.5
   23 |   1.6148 |     45.904 |   1.6076 |     45.374 |     0.5
   24 |   1.5974 |     45.953 |   1.5834 |     45.159 |     0.6
   25 |   1.5707 |     45.844 |   1.5625 |     45.190 |     0.6
   26 |   1.5522 |     45.898 |   1.5444 |     45.343 |     0.6
   27 |   1.5332 |     45.882 |   1.5298 |     45.282 |     0.6
   28 |   1.5191 |     45.839 |   1.5140 |     45.343 |     0.7
   29 |   1.5020 |     45.839 |   1.4991 |     45.282 |     0.7
   30 |   1.4870 |     45.795 |   1.4844 |     45.159 |     0.7
   31 |   1.4709 |     45.774 |   1.4707 |     45.282 |     0.7
   32 |   1.4568 |     45.590 |   1.4596 |     45.159 |     0.8
   33 |   1.4530 |     45.524 |   1.4539 |     45.221 |     0.8
   34 |   1.4347 |     45.221 |   1.4421 |     44.975 |     0.8
   35 |   1.4219 |     44.934 |   1.4282 |     44.485 |     0.8
   36 |   1.4081 |     44.766 |   1.4223 |     44.669 |     0.8
   37 |   1.3985 |     44.273 |   1.4106 |     44.455 |     0.9
   38 |   1.3866 |     44.240 |   1.4034 |     44.455 |     0.9
   39 |   1.3764 |     44.040 |   1.3938 |     44.393 |     0.9
   40 |   1.3647 |     43.666 |   1.3819 |     44.210 |     0.9
   41 |   1.3528 |     43.531 |   1.3741 |     44.240 |     1.0
   42 |   1.3455 |     43.401 |   1.3654 |     43.658 |     1.0
   43 |   1.3345 |     42.940 |   1.3557 |     43.995 |     1.0
   44 |   1.3195 |     42.777 |   1.3491 |     43.811 |     1.0
   45 |   1.3106 |     42.366 |   1.3391 |     43.199 |     1.1
   46 |   1.3057 |     42.187 |   1.3324 |     42.923 |     1.1
   47 |   1.2892 |     41.542 |   1.3225 |     41.667 |     1.1
   48 |   1.2785 |     40.675 |   1.3164 |     41.513 |     1.1
   49 |   1.2689 |     40.269 |   1.3098 |     40.931 |     1.2
   50 |   1.2605 |     39.965 |   1.2999 |     40.411 |     1.2
   51 |   1.2507 |     39.602 |   1.2922 |     40.104 |     1.2
   52 |   1.2414 |     39.104 |   1.2848 |     39.522 |     1.2
   53 |   1.2342 |     38.844 |   1.2799 |     39.430 |     1.2
   54 |   1.2239 |     38.421 |   1.2686 |     39.154 |     1.3
   55 |   1.2164 |     38.042 |   1.2651 |     38.879 |     1.3
   56 |   1.2088 |     37.652 |   1.2586 |     38.848 |     1.3
   57 |   1.1991 |     37.175 |   1.2531 |     38.327 |     1.3
   58 |   1.1908 |     36.920 |   1.2481 |     38.143 |     1.4
   59 |   1.1819 |     36.763 |   1.2395 |     37.868 |     1.4
   60 |   1.1732 |     36.389 |   1.2340 |     37.806 |     1.4
   61 |   1.1659 |     36.286 |   1.2302 |     37.653 |     1.4
   62 |   1.1589 |     36.124 |   1.2238 |     37.714 |     1.5
   63 |   1.1497 |     35.723 |   1.2178 |     37.469 |     1.5
   64 |   1.1425 |     35.419 |   1.2200 |     37.929 |     1.5
   65 |   1.1333 |     35.268 |   1.2096 |     37.500 |     1.5
   66 |   1.1371 |     35.636 |   1.2059 |     37.102 |     1.6
   67 |   1.1217 |     34.997 |   1.1977 |     37.224 |     1.6
   68 |   1.1096 |     34.552 |   1.1924 |     37.071 |     1.6
   69 |   1.1039 |     34.460 |   1.1886 |     37.040 |     1.6
   70 |   1.0952 |     34.260 |   1.1837 |     36.550 |     1.6
   71 |   1.0861 |     33.919 |   1.1810 |     36.765 |     1.7
   72 |   1.0790 |     33.642 |   1.1754 |     36.213 |     1.7
   73 |   1.0732 |     33.377 |   1.1736 |     36.581 |     1.7
   74 |   1.0712 |     33.377 |   1.1706 |     36.550 |     1.7
   75 |   1.0616 |     32.900 |   1.1596 |     36.244 |     1.8
   76 |   1.0539 |     32.624 |   1.1605 |     35.938 |     1.8
   77 |   1.0460 |     32.358 |   1.1613 |     35.907 |     1.8
   78 |   1.0414 |     32.326 |   1.1515 |     35.876 |     1.8
   79 |   1.0336 |     31.935 |   1.1471 |     35.815 |     1.9
   80 |   1.0301 |     31.778 |   1.1490 |     35.570 |     1.9
   81 |   1.0237 |     31.529 |   1.1400 |     35.570 |     1.9
   82 |   1.0128 |     31.193 |   1.1416 |     35.202 |     1.9
   83 |   1.0069 |     31.047 |   1.1395 |     35.846 |     1.9
   84 |   1.0050 |     31.041 |   1.1347 |     35.325 |     2.0
   85 |   0.9962 |     30.592 |   1.1315 |     35.662 |     2.0
   86 |   0.9885 |     30.467 |   1.1249 |     35.662 |     2.0
   87 |   0.9847 |     30.212 |   1.1263 |     35.233 |     2.0
   88 |   0.9775 |     29.898 |   1.1153 |     34.743 |     2.1
   89 |   0.9694 |     29.638 |   1.1173 |     34.743 |     2.1
   90 |   0.9661 |     29.167 |   1.1141 |     34.988 |     2.1
   91 |   0.9578 |     29.091 |   1.1116 |     34.620 |     2.1
   92 |   0.9540 |     28.993 |   1.1088 |     34.589 |     2.2
   93 |   0.9518 |     28.744 |   1.1083 |     34.712 |     2.2
   94 |   0.9447 |     28.847 |   1.1060 |     34.926 |     2.2
   95 |   0.9345 |     28.392 |   1.0953 |     33.977 |     2.2
   96 |   0.9294 |     28.273 |   1.0988 |     34.436 |     2.3
   97 |   0.9255 |     27.975 |   1.0930 |     34.406 |     2.3
   98 |   0.9187 |     28.018 |   1.0900 |     33.609 |     2.3
   99 |   0.9135 |     27.687 |   1.0951 |     34.681 |     2.3
  100 |   0.9103 |     27.590 |   1.0895 |     33.701 |     2.3
  101 |   0.9047 |     27.259 |   1.0870 |     33.609 |     2.4
  102 |   0.8968 |     27.146 |   1.0853 |     33.609 |     2.4
  103 |   0.8956 |     26.967 |   1.0846 |     34.252 |     2.4
  104 |   0.8868 |     26.810 |   1.0724 |     33.425 |     2.4
  105 |   0.8819 |     26.615 |   1.0798 |     33.578 |     2.5
  106 |   0.8815 |     26.468 |   1.0813 |     33.977 |     2.5
  107 |   0.8748 |     26.360 |   1.0711 |     33.333 |     2.5
  108 |   0.8722 |     26.170 |   1.0734 |     33.150 |     2.5
  109 |   0.8668 |     26.170 |   1.0705 |     33.762 |     2.6
  110 |   0.8582 |     25.764 |   1.0678 |     33.303 |     2.6
  111 |   0.8525 |     25.699 |   1.0680 |     33.762 |     2.6
  112 |   0.8495 |     25.368 |   1.0687 |     33.793 |     2.6
  113 |   0.8443 |     25.347 |   1.0624 |     33.211 |     2.7
  114 |   0.8375 |     24.967 |   1.0629 |     33.211 |     2.7
  115 |   0.8306 |     24.978 |   1.0574 |     33.241 |     2.7
  116 |   0.8311 |     24.854 |   1.0574 |     32.966 |     2.7
  117 |   0.8253 |     24.794 |   1.0550 |     33.088 |     2.7
  118 |   0.8224 |     24.648 |   1.0546 |     33.180 |     2.8
  119 |   0.8172 |     24.594 |   1.0559 |     33.456 |     2.8
  120 |   0.8111 |     24.144 |   1.0608 |     32.966 |     2.8
  121 |   0.8057 |     24.258 |   1.0541 |     32.904 |     2.8
  122 |   0.8021 |     24.204 |   1.0532 |     32.966 |     2.9
  123 |   0.7973 |     23.835 |   1.0538 |     32.874 |     2.9
  124 |   0.7974 |     23.824 |   1.0578 |     33.058 |     2.9
  125 |   0.7900 |     23.510 |   1.0596 |     33.272 |     2.9
  126 |   0.7852 |     23.266 |   1.0531 |     32.629 |     3.0
  127 |   0.7841 |     23.412 |   1.0498 |     32.843 |     3.0
  128 |   0.7770 |     23.353 |   1.0509 |     32.751 |     3.0
  129 |   0.7744 |     23.212 |   1.0488 |     32.384 |     3.0
  130 |   0.7704 |     22.968 |   1.0481 |     32.721 |     3.0
  131 |   0.7679 |     22.806 |   1.0602 |     32.690 |     3.1
  132 |   0.7606 |     22.583 |   1.0475 |     32.292 |     3.1
  133 |   0.7592 |     22.659 |   1.0417 |     31.863 |     3.1
  134 |   0.7528 |     22.269 |   1.0434 |     32.200 |     3.1
  135 |   0.7468 |     22.161 |   1.0540 |     32.598 |     3.2
  136 |   0.7444 |     22.112 |   1.0418 |     31.648 |     3.2
  137 |   0.7400 |     22.090 |   1.0487 |     31.801 |     3.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 299,522

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4513 |     84.964 |   3.3783 |     85.417 |     0.0
    2 |   3.1754 |     84.964 |   2.9779 |     85.417 |     0.0
    3 |   2.8475 |     84.964 |   2.7642 |     85.417 |     0.1
    4 |   2.6793 |     69.311 |   2.6239 |     57.812 |     0.1
    5 |   2.5571 |     59.114 |   2.5183 |     58.824 |     0.1
    6 |   2.4638 |     59.049 |   2.4378 |     58.824 |     0.1
    7 |   2.3919 |     59.049 |   2.3717 |     58.824 |     0.1
    8 |   2.3304 |     59.049 |   2.3152 |     58.824 |     0.2
    9 |   2.2767 |     59.049 |   2.2658 |     58.824 |     0.2
   10 |   2.2322 |     59.038 |   2.2212 |     58.824 |     0.2
   11 |   2.1900 |     59.016 |   2.1807 |     58.211 |     0.2
   12 |   2.1550 |     58.079 |   2.1425 |     58.027 |     0.3
   13 |   2.1143 |     56.724 |   2.1061 |     54.197 |     0.3
   14 |   2.0805 |     54.562 |   2.0719 |     53.891 |     0.3
   15 |   2.0472 |     54.178 |   2.0383 |     53.952 |     0.3
   16 |   2.0178 |     54.226 |   2.0063 |     53.860 |     0.3
   17 |   1.9829 |     54.064 |   1.9692 |     53.799 |     0.4
   18 |   1.9458 |     53.945 |   1.9313 |     53.217 |     0.4
   19 |   1.9082 |     51.290 |   1.8942 |     48.529 |     0.4
   20 |   1.8737 |     48.407 |   1.8583 |     48.376 |     0.4
   21 |   1.8408 |     48.597 |   1.8254 |     48.376 |     0.4
   22 |   1.8099 |     48.602 |   1.7953 |     48.346 |     0.5
   23 |   1.7819 |     48.602 |   1.7677 |     48.346 |     0.5
   24 |   1.7554 |     48.602 |   1.7423 |     48.346 |     0.5
   25 |   1.7331 |     48.607 |   1.7190 |     48.346 |     0.5
   26 |   1.7092 |     48.607 |   1.6975 |     48.346 |     0.6
   27 |   1.6892 |     48.607 |   1.6777 |     48.346 |     0.6
   28 |   1.6715 |     48.607 |   1.6596 |     48.254 |     0.6
   29 |   1.6552 |     47.946 |   1.6429 |     45.558 |     0.6
   30 |   1.6394 |     46.218 |   1.6276 |     45.466 |     0.6
   31 |   1.6232 |     46.213 |   1.6128 |     45.466 |     0.7
   32 |   1.6097 |     46.213 |   1.5995 |     45.466 |     0.7
   33 |   1.5951 |     46.213 |   1.5869 |     45.466 |     0.7
   34 |   1.5844 |     46.213 |   1.5748 |     45.466 |     0.7
   35 |   1.5725 |     46.213 |   1.5639 |     45.466 |     0.7
   36 |   1.5600 |     46.213 |   1.5536 |     45.466 |     0.8
   37 |   1.5511 |     46.213 |   1.5440 |     45.466 |     0.8
   38 |   1.5453 |     46.213 |   1.5354 |     45.466 |     0.8
   39 |   1.5344 |     46.202 |   1.5270 |     45.466 |     0.8
   40 |   1.5266 |     46.207 |   1.5197 |     45.466 |     0.9
   41 |   1.5199 |     46.213 |   1.5123 |     45.466 |     0.9
   42 |   1.5129 |     46.213 |   1.5051 |     45.466 |     0.9
   43 |   1.5038 |     46.213 |   1.4987 |     45.466 |     0.9
   44 |   1.5010 |     46.213 |   1.4925 |     45.466 |     0.9
   45 |   1.4933 |     46.213 |   1.4870 |     45.466 |     1.0
   46 |   1.4853 |     46.213 |   1.4820 |     45.466 |     1.0
   47 |   1.4818 |     46.213 |   1.4768 |     45.466 |     1.0
   48 |   1.4764 |     46.213 |   1.4725 |     45.466 |     1.0
   49 |   1.4698 |     46.213 |   1.4678 |     45.466 |     1.0
   50 |   1.4665 |     46.218 |   1.4632 |     45.466 |     1.1
   51 |   1.4607 |     46.207 |   1.4589 |     45.466 |     1.1
   52 |   1.4605 |     46.202 |   1.4554 |     45.466 |     1.1
   53 |   1.4530 |     46.218 |   1.4517 |     45.466 |     1.1
   54 |   1.4498 |     46.202 |   1.4482 |     45.527 |     1.2
   55 |   1.4452 |     46.218 |   1.4430 |     45.527 |     1.2
   56 |   1.4402 |     46.196 |   1.4386 |     45.496 |     1.2
   57 |   1.4346 |     46.213 |   1.4337 |     45.435 |     1.2
   58 |   1.4291 |     46.207 |   1.4304 |     45.404 |     1.2
   59 |   1.4240 |     46.169 |   1.4260 |     45.496 |     1.3
   60 |   1.4219 |     46.083 |   1.4221 |     45.466 |     1.3
   61 |   1.4154 |     45.936 |   1.4181 |     45.251 |     1.3
   62 |   1.4135 |     45.925 |   1.4143 |     45.190 |     1.3
   63 |   1.4077 |     45.871 |   1.4105 |     45.251 |     1.3
   64 |   1.4033 |     45.828 |   1.4080 |     45.190 |     1.4
   65 |   1.3988 |     45.611 |   1.4053 |     45.006 |     1.4
   66 |   1.3933 |     45.459 |   1.4002 |     45.006 |     1.4
   67 |   1.3895 |     45.367 |   1.3991 |     44.822 |     1.4
   68 |   1.3853 |     45.183 |   1.3942 |     44.730 |     1.4
   69 |   1.3794 |     44.907 |   1.3894 |     44.700 |     1.5
   70 |   1.3742 |     44.809 |   1.3909 |     44.455 |     1.5
   71 |   1.3719 |     44.739 |   1.3847 |     44.485 |     1.5
   72 |   1.3669 |     44.538 |   1.3818 |     44.577 |     1.5
   73 |   1.3621 |     44.571 |   1.3812 |     44.516 |     1.6
   74 |   1.3590 |     44.425 |   1.3767 |     44.516 |     1.6
   75 |   1.3535 |     44.370 |   1.3762 |     44.608 |     1.6
   76 |   1.3530 |     44.278 |   1.3742 |     44.547 |     1.6
   77 |   1.3469 |     44.246 |   1.3728 |     44.485 |     1.6
   78 |   1.3429 |     44.192 |   1.3674 |     44.669 |     1.7
   79 |   1.3386 |     44.040 |   1.3654 |     44.424 |     1.7
   80 |   1.3352 |     44.062 |   1.3634 |     44.669 |     1.7
   81 |   1.3308 |     43.937 |   1.3613 |     44.577 |     1.7
   82 |   1.3265 |     43.866 |   1.3615 |     44.608 |     1.7
   83 |   1.3267 |     43.736 |   1.3598 |     44.485 |     1.8
   84 |   1.3187 |     43.764 |   1.3560 |     44.455 |     1.8
   85 |   1.3177 |     43.623 |   1.3549 |     44.547 |     1.8
   86 |   1.3114 |     43.666 |   1.3518 |     44.332 |     1.8
   87 |   1.3077 |     43.406 |   1.3519 |     44.485 |     1.8
   88 |   1.3061 |     43.314 |   1.3488 |     43.995 |     1.9
   89 |   1.3012 |     43.113 |   1.3456 |     43.842 |     1.9
   90 |   1.2992 |     42.989 |   1.3449 |     43.934 |     1.9
   91 |   1.2930 |     42.875 |   1.3450 |     43.934 |     1.9
   92 |   1.2904 |     42.414 |   1.3421 |     43.842 |     1.9
   93 |   1.2868 |     42.593 |   1.3428 |     43.811 |     2.0
   94 |   1.2821 |     42.203 |   1.3370 |     43.658 |     2.0
   95 |   1.2790 |     42.274 |   1.3319 |     43.444 |     2.0
   96 |   1.2741 |     42.041 |   1.3318 |     43.352 |     2.0
   97 |   1.2718 |     41.770 |   1.3321 |     43.352 |     2.0
   98 |   1.2660 |     41.629 |   1.3306 |     43.199 |     2.1
   99 |   1.2630 |     41.385 |   1.3267 |     42.678 |     2.1
  100 |   1.2565 |     41.076 |   1.3268 |     42.800 |     2.1
  101 |   1.2511 |     40.995 |   1.3219 |     42.616 |     2.1
  102 |   1.2485 |     40.930 |   1.3183 |     42.371 |     2.1
  103 |   1.2454 |     40.502 |   1.3183 |     42.739 |     2.2
  104 |   1.2392 |     40.426 |   1.3136 |     42.371 |     2.2
  105 |   1.2368 |     39.922 |   1.3112 |     42.678 |     2.2
  106 |   1.2344 |     39.868 |   1.3126 |     42.463 |     2.2
  107 |   1.2281 |     39.944 |   1.3084 |     42.034 |     2.3
  108 |   1.2255 |     39.532 |   1.3068 |     42.433 |     2.3
  109 |   1.2218 |     39.629 |   1.3048 |     42.310 |     2.3
  110 |   1.2115 |     39.223 |   1.3000 |     41.850 |     2.3
  111 |   1.2120 |     39.266 |   1.2991 |     42.341 |     2.3
  112 |   1.2079 |     39.098 |   1.3001 |     41.789 |     2.4
  113 |   1.2038 |     38.985 |   1.2932 |     41.973 |     2.4
  114 |   1.1982 |     38.865 |   1.2937 |     41.636 |     2.4
  115 |   1.1916 |     38.584 |   1.2968 |     41.973 |     2.4
  116 |   1.1913 |     38.779 |   1.2898 |     41.667 |     2.4
  117 |   1.1848 |     38.524 |   1.2886 |     41.973 |     2.5
  118 |   1.1823 |     38.383 |   1.2844 |     41.575 |     2.5
  119 |   1.1779 |     38.221 |   1.2838 |     41.759 |     2.5
  120 |   1.1721 |     38.242 |   1.2846 |     41.483 |     2.5
  121 |   1.1717 |     38.047 |   1.2821 |     40.931 |     2.5
  122 |   1.1658 |     37.776 |   1.2771 |     41.207 |     2.6
  123 |   1.1614 |     37.776 |   1.2837 |     41.176 |     2.6
  124 |   1.1588 |     37.679 |   1.2762 |     40.993 |     2.6
  125 |   1.1558 |     37.533 |   1.2765 |     41.422 |     2.6
  126 |   1.1517 |     37.348 |   1.2750 |     40.962 |     2.6
  127 |   1.1440 |     37.343 |   1.2783 |     41.238 |     2.7
  128 |   1.1421 |     37.321 |   1.2757 |     40.931 |     2.7
  129 |   1.1396 |     36.980 |   1.2718 |     40.686 |     2.7
  130 |   1.1328 |     36.958 |   1.2727 |     40.625 |     2.7
  131 |   1.1307 |     36.844 |   1.2740 |     40.502 |     2.7
  132 |   1.1257 |     36.611 |   1.2686 |     40.411 |     2.8
  133 |   1.1252 |     36.508 |   1.2679 |     40.319 |     2.8
  134 |   1.1181 |     36.508 |   1.2667 |     39.890 |     2.8
  135 |   1.1160 |     36.416 |   1.2649 |     40.472 |     2.8
  136 |   1.1118 |     36.178 |   1.2626 |     40.319 |     2.8
  137 |   1.1114 |     36.107 |   1.2609 |     40.257 |     2.9
  138 |   1.1054 |     35.777 |   1.2634 |     40.012 |     2.9
  139 |   1.1017 |     35.820 |   1.2640 |     40.043 |     2.9
  140 |   1.0961 |     35.474 |   1.2574 |     39.767 |     2.9
  141 |   1.0943 |     35.495 |   1.2612 |     40.472 |     3.0
  142 |   1.0880 |     35.338 |   1.2613 |     40.349 |     3.0
  143 |   1.0860 |     35.040 |   1.2600 |     39.675 |     3.0
  144 |   1.0821 |     35.035 |   1.2596 |     40.074 |     3.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 2,177,890

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1316 |     83.642 |   2.6286 |     83.333 |     0.1
    2 |   2.3687 |     60.436 |   2.1811 |     58.701 |     0.1
    3 |   2.0683 |     56.236 |   1.9773 |     57.322 |     0.2
    4 |   1.9090 |     53.256 |   1.8404 |     48.407 |     0.3
    5 |   1.7924 |     49.133 |   1.7303 |     48.346 |     0.3
    6 |   1.6948 |     48.629 |   1.6435 |     48.346 |     0.4
    7 |   1.6205 |     48.553 |   1.5763 |     48.346 |     0.4
    8 |   1.5597 |     46.494 |   1.5268 |     45.496 |     0.5
    9 |   1.5165 |     46.213 |   1.4891 |     45.496 |     0.6
   10 |   1.4843 |     46.207 |   1.4632 |     45.496 |     0.6
   11 |   1.4619 |     46.207 |   1.4434 |     45.466 |     0.7
   12 |   1.4423 |     46.218 |   1.4270 |     45.496 |     0.8
   13 |   1.4249 |     46.202 |   1.4141 |     45.496 |     0.8
   14 |   1.4083 |     46.202 |   1.3960 |     45.466 |     0.9
   15 |   1.3910 |     46.121 |   1.3801 |     45.374 |     0.9
   16 |   1.3727 |     45.850 |   1.3603 |     44.332 |     1.0
   17 |   1.3527 |     44.999 |   1.3439 |     43.934 |     1.1
   18 |   1.3352 |     43.980 |   1.3291 |     43.168 |     1.1
   19 |   1.3172 |     43.910 |   1.3279 |     43.444 |     1.2
   20 |   1.3075 |     43.455 |   1.3075 |     42.647 |     1.3
   21 |   1.2899 |     43.005 |   1.2995 |     41.728 |     1.3
   22 |   1.2807 |     42.328 |   1.2928 |     42.218 |     1.4
   23 |   1.2640 |     42.100 |   1.2886 |     41.973 |     1.5
   24 |   1.2517 |     41.650 |   1.2735 |     41.697 |     1.5
   25 |   1.2397 |     41.314 |   1.2683 |     41.176 |     1.6
   26 |   1.2252 |     40.886 |   1.2656 |     41.544 |     1.6
   27 |   1.2134 |     40.605 |   1.2477 |     40.686 |     1.7
   28 |   1.2001 |     40.020 |   1.2368 |     40.625 |     1.8
   29 |   1.1854 |     39.499 |   1.2309 |     40.564 |     1.8
   30 |   1.1694 |     39.201 |   1.2144 |     40.349 |     1.9
   31 |   1.1518 |     38.611 |   1.2080 |     39.982 |     2.0
   32 |   1.1344 |     37.820 |   1.2008 |     39.185 |     2.0
   33 |   1.1184 |     37.121 |   1.1889 |     39.246 |     2.1
   34 |   1.1047 |     36.796 |   1.1750 |     37.990 |     2.1
   35 |   1.0867 |     35.739 |   1.1634 |     37.561 |     2.2
   36 |   1.0687 |     35.029 |   1.1508 |     37.531 |     2.3
   37 |   1.0513 |     34.222 |   1.1482 |     36.765 |     2.3
   38 |   1.0353 |     33.696 |   1.1385 |     36.121 |     2.4
   39 |   1.0183 |     32.954 |   1.1234 |     35.907 |     2.5
   40 |   1.0017 |     32.141 |   1.1169 |     35.478 |     2.5
   41 |   0.9851 |     31.664 |   1.1119 |     35.172 |     2.6
   42 |   0.9705 |     31.036 |   1.0995 |     34.773 |     2.7
   43 |   0.9564 |     30.624 |   1.0887 |     34.498 |     2.7
   44 |   0.9393 |     30.164 |   1.0810 |     34.161 |     2.8
   45 |   0.9302 |     30.050 |   1.0871 |     34.743 |     2.8
   46 |   0.9136 |     29.020 |   1.0705 |     33.824 |     2.9
   47 |   0.9014 |     28.625 |   1.0704 |     34.069 |     3.0
   48 |   0.8891 |     28.381 |   1.0599 |     33.701 |     3.0
   49 |   0.8740 |     27.877 |   1.0586 |     33.609 |     3.1
   50 |   0.8650 |     27.400 |   1.0556 |     33.364 |     3.2
   51 |   0.8543 |     27.265 |   1.0436 |     32.782 |     3.2
   52 |   0.8390 |     26.788 |   1.0457 |     33.058 |     3.3
   53 |   0.8333 |     26.333 |   1.0502 |     33.303 |     3.3
   54 |   0.8185 |     25.840 |   1.0461 |     32.966 |     3.4
   55 |   0.8142 |     25.937 |   1.0299 |     31.924 |     3.5
   56 |   0.8065 |     25.493 |   1.0301 |     32.077 |     3.5
   57 |   0.7916 |     25.076 |   1.0159 |     32.047 |     3.6
   58 |   0.7806 |     24.881 |   1.0175 |     31.985 |     3.7
   59 |   0.7764 |     24.583 |   1.0184 |     31.801 |     3.7
   60 |   0.7657 |     24.171 |   1.0119 |     31.832 |     3.8
   61 |   0.7595 |     24.176 |   1.0141 |     31.679 |     3.9
   62 |   0.7478 |     23.602 |   0.9974 |     31.710 |     3.9
   63 |   0.7416 |     23.467 |   1.0077 |     31.618 |     4.0
   64 |   0.7327 |     23.174 |   1.0079 |     31.250 |     4.0
   65 |   0.7274 |     22.930 |   1.0024 |     30.913 |     4.1
   66 |   0.7154 |     22.605 |   0.9957 |     30.729 |     4.2
   67 |   0.7126 |     22.800 |   1.0012 |     30.545 |     4.2
   68 |   0.7018 |     22.237 |   0.9918 |     31.036 |     4.3
   69 |   0.6911 |     21.982 |   1.0016 |     30.790 |     4.4
   70 |   0.6840 |     21.841 |   0.9950 |     30.270 |     4.4
   71 |   0.6753 |     21.402 |   0.9899 |     30.392 |     4.5
   72 |   0.6735 |     21.164 |   0.9847 |     30.362 |     4.5
   73 |   0.6592 |     20.828 |   0.9837 |     29.718 |     4.6
   74 |   0.6570 |     20.611 |   0.9840 |     29.534 |     4.7
   75 |   0.6477 |     20.438 |   0.9866 |     30.086 |     4.7
   76 |   0.6416 |     20.059 |   0.9948 |     29.504 |     4.8
   77 |   0.6358 |     19.966 |   0.9884 |     29.259 |     4.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 240,418

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.9133 |     79.010 |   2.4727 |     66.912 |     0.0
    2 |   2.2314 |     59.054 |   2.0208 |     58.824 |     0.0
    3 |   1.8894 |     52.520 |   1.7349 |     50.521 |     0.1
    4 |   1.6515 |     46.895 |   1.5635 |     45.466 |     0.1
    5 |   1.5345 |     46.267 |   1.4927 |     45.466 |     0.1
    6 |   1.4843 |     46.267 |   1.4583 |     45.466 |     0.1
    7 |   1.4598 |     46.256 |   1.4394 |     45.466 |     0.1
    8 |   1.4417 |     46.191 |   1.4270 |     45.987 |     0.2
    9 |   1.4319 |     46.088 |   1.4177 |     45.466 |     0.2
   10 |   1.4213 |     46.218 |   1.4106 |     45.466 |     0.2
   11 |   1.4151 |     46.180 |   1.4045 |     45.527 |     0.2
   12 |   1.4090 |     46.093 |   1.4013 |     45.527 |     0.2
   13 |   1.4049 |     46.207 |   1.3972 |     45.527 |     0.2
   14 |   1.3986 |     46.055 |   1.3939 |     45.588 |     0.3
   15 |   1.3984 |     45.974 |   1.3867 |     45.466 |     0.3
   16 |   1.3902 |     45.931 |   1.3844 |     45.312 |     0.3
   17 |   1.3864 |     45.649 |   1.3818 |     45.404 |     0.3
   18 |   1.3807 |     45.535 |   1.3780 |     44.975 |     0.3
   19 |   1.3762 |     45.394 |   1.3701 |     45.067 |     0.4
   20 |   1.3695 |     45.167 |   1.3628 |     45.221 |     0.4
   21 |   1.3556 |     45.010 |   1.3545 |     45.374 |     0.4
   22 |   1.3460 |     44.761 |   1.3401 |     45.159 |     0.4
   23 |   1.3304 |     44.197 |   1.3256 |     43.903 |     0.4
   24 |   1.3189 |     44.024 |   1.3098 |     43.260 |     0.4
   25 |   1.3102 |     43.753 |   1.3010 |     42.616 |     0.5
   26 |   1.2969 |     43.471 |   1.2927 |     42.678 |     0.5
   27 |   1.2888 |     43.092 |   1.2931 |     42.708 |     0.5
   28 |   1.2804 |     42.945 |   1.2838 |     42.157 |     0.5
   29 |   1.2699 |     42.685 |   1.2693 |     42.249 |     0.5
   30 |   1.2583 |     42.181 |   1.2625 |     42.004 |     0.6
   31 |   1.2482 |     41.959 |   1.2572 |     41.636 |     0.6
   32 |   1.2395 |     41.732 |   1.2516 |     41.575 |     0.6
   33 |   1.2310 |     41.510 |   1.2457 |     41.146 |     0.6
   34 |   1.2263 |     41.363 |   1.2450 |     40.962 |     0.6
   35 |   1.2200 |     41.174 |   1.2298 |     40.993 |     0.6
   36 |   1.2123 |     40.702 |   1.2390 |     41.422 |     0.7
   37 |   1.2075 |     40.735 |   1.2250 |     40.349 |     0.7
   38 |   1.1979 |     40.594 |   1.2225 |     40.074 |     0.7
   39 |   1.1973 |     40.247 |   1.2212 |     40.196 |     0.7
   40 |   1.1980 |     40.307 |   1.2166 |     40.319 |     0.7
   41 |   1.1869 |     39.862 |   1.2107 |     39.828 |     0.8
   42 |   1.1777 |     39.792 |   1.2180 |     40.319 |     0.8
   43 |   1.1759 |     39.700 |   1.2031 |     39.920 |     0.8
   44 |   1.1703 |     39.445 |   1.2069 |     40.196 |     0.8
   45 |   1.1685 |     39.651 |   1.1990 |     39.706 |     0.8
   46 |   1.1609 |     39.429 |   1.2028 |     39.767 |     0.8
   47 |   1.1533 |     39.348 |   1.1888 |     39.890 |     0.9
   48 |   1.1547 |     39.196 |   1.1966 |     40.135 |     0.9
   49 |   1.1578 |     39.451 |   1.1957 |     40.135 |     0.9
   50 |   1.1525 |     39.250 |   1.1880 |     39.859 |     0.9
   51 |   1.1445 |     39.082 |   1.1860 |     39.890 |     0.9
   52 |   1.1423 |     39.044 |   1.1834 |     39.767 |     1.0
   53 |   1.1314 |     38.898 |   1.1843 |     39.645 |     1.0
   54 |   1.1284 |     38.497 |   1.1880 |     40.012 |     1.0
   55 |   1.1226 |     38.486 |   1.1921 |     39.828 |     1.0
   56 |   1.1201 |     38.497 |   1.1826 |     40.135 |     1.0
   57 |   1.1163 |     38.508 |   1.1856 |     39.338 |     1.0
   58 |   1.1154 |     38.551 |   1.1832 |     39.706 |     1.1
   59 |   1.1091 |     38.183 |   1.1774 |     39.522 |     1.1
   60 |   1.1053 |     38.107 |   1.1773 |     39.369 |     1.1
   61 |   1.1017 |     37.982 |   1.1616 |     39.062 |     1.1
   62 |   1.0995 |     37.961 |   1.1796 |     39.093 |     1.1
   63 |   1.0961 |     37.955 |   1.1813 |     39.798 |     1.2
   64 |   1.0897 |     37.717 |   1.1861 |     39.154 |     1.2
   65 |   1.0930 |     37.793 |   1.1791 |     38.971 |     1.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,022,306

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4942 |     87.760 |   3.4225 |     85.417 |     0.0
    2 |   3.2463 |     81.556 |   3.0155 |     81.893 |     0.1
    3 |   2.8707 |     81.404 |   2.7758 |     81.893 |     0.1
    4 |   2.7120 |     81.329 |   2.6663 |     81.893 |     0.2
    5 |   2.6091 |     75.829 |   2.5708 |     65.901 |     0.2
    6 |   2.5231 |     60.804 |   2.4889 |     58.824 |     0.3
    7 |   2.4545 |     60.371 |   2.4215 |     58.824 |     0.3
    8 |   2.3897 |     59.802 |   2.3612 |     58.824 |     0.3
    9 |   2.3332 |     59.412 |   2.3080 |     58.824 |     0.4
   10 |   2.2860 |     58.929 |   2.2600 |     58.824 |     0.4
   11 |   2.2420 |     58.648 |   2.2175 |     58.824 |     0.5
   12 |   2.2031 |     58.252 |   2.1772 |     58.824 |     0.5
   13 |   2.1653 |     58.133 |   2.1390 |     58.824 |     0.5
   14 |   2.1303 |     58.079 |   2.1028 |     58.824 |     0.6
   15 |   2.0961 |     57.613 |   2.0702 |     58.824 |     0.6
   16 |   2.0652 |     57.304 |   2.0387 |     58.824 |     0.7
   17 |   2.0383 |     57.103 |   2.0083 |     58.824 |     0.7
   18 |   2.0102 |     56.692 |   1.9803 |     58.793 |     0.7
   19 |   1.9819 |     56.123 |   1.9520 |     58.701 |     0.8
   20 |   1.9513 |     55.413 |   1.9228 |     55.913 |     0.8
   21 |   1.9229 |     53.549 |   1.8940 |     51.134 |     0.9
   22 |   1.8967 |     51.934 |   1.8676 |     53.094 |     0.9
   23 |   1.8697 |     51.062 |   1.8419 |     50.337 |     1.0
   24 |   1.8462 |     50.748 |   1.8176 |     48.376 |     1.0
   25 |   1.8209 |     50.385 |   1.7938 |     48.346 |     1.0
   26 |   1.7954 |     49.902 |   1.7709 |     48.346 |     1.1
   27 |   1.7752 |     49.740 |   1.7492 |     48.346 |     1.1
   28 |   1.7542 |     49.464 |   1.7284 |     48.346 |     1.2
   29 |   1.7322 |     49.128 |   1.7083 |     48.346 |     1.2
   30 |   1.7107 |     48.391 |   1.6881 |     46.722 |     1.2
   31 |   1.6932 |     47.816 |   1.6697 |     45.466 |     1.3
   32 |   1.6744 |     47.015 |   1.6532 |     45.466 |     1.3
   33 |   1.6581 |     46.689 |   1.6379 |     45.496 |     1.4
   34 |   1.6434 |     46.521 |   1.6233 |     45.466 |     1.4
   35 |   1.6287 |     46.402 |   1.6098 |     45.466 |     1.5
   36 |   1.6149 |     46.429 |   1.5972 |     45.466 |     1.5
   37 |   1.6038 |     46.391 |   1.5856 |     45.466 |     1.5
   38 |   1.5919 |     46.337 |   1.5746 |     45.466 |     1.6
   39 |   1.5811 |     46.310 |   1.5643 |     45.466 |     1.6
   40 |   1.5716 |     46.251 |   1.5548 |     45.466 |     1.7
   41 |   1.5619 |     46.256 |   1.5459 |     45.466 |     1.7
   42 |   1.5535 |     46.223 |   1.5381 |     45.466 |     1.7
   43 |   1.5448 |     46.223 |   1.5306 |     45.466 |     1.8
   44 |   1.5375 |     46.218 |   1.5237 |     45.466 |     1.8
   45 |   1.5306 |     46.218 |   1.5173 |     45.466 |     1.9
   46 |   1.5257 |     46.218 |   1.5112 |     45.466 |     1.9
   47 |   1.5187 |     46.218 |   1.5057 |     45.466 |     2.0
   48 |   1.5153 |     46.223 |   1.5005 |     45.466 |     2.0
   49 |   1.5063 |     46.213 |   1.4954 |     45.466 |     2.0
   50 |   1.5038 |     46.213 |   1.4905 |     45.466 |     2.1
   51 |   1.4970 |     46.213 |   1.4858 |     45.466 |     2.1
   52 |   1.4918 |     46.213 |   1.4816 |     45.466 |     2.2
   53 |   1.4888 |     46.218 |   1.4775 |     45.466 |     2.2
   54 |   1.4842 |     46.207 |   1.4736 |     45.466 |     2.2
   55 |   1.4814 |     46.207 |   1.4699 |     45.466 |     2.3
   56 |   1.4754 |     46.202 |   1.4667 |     45.466 |     2.3
   57 |   1.4734 |     46.240 |   1.4634 |     45.466 |     2.4
   58 |   1.4725 |     46.207 |   1.4602 |     45.466 |     2.4
   59 |   1.4692 |     46.202 |   1.4574 |     45.466 |     2.5
   60 |   1.4642 |     46.191 |   1.4548 |     45.466 |     2.5
   61 |   1.4610 |     46.207 |   1.4522 |     45.466 |     2.5
   62 |   1.4567 |     46.213 |   1.4495 |     45.466 |     2.6
   63 |   1.4562 |     46.240 |   1.4473 |     45.466 |     2.6
   64 |   1.4561 |     46.245 |   1.4450 |     45.466 |     2.7
   65 |   1.4520 |     46.202 |   1.4424 |     45.466 |     2.7
   66 |   1.4483 |     46.186 |   1.4404 |     45.466 |     2.8
   67 |   1.4491 |     46.267 |   1.4385 |     45.466 |     2.8
   68 |   1.4475 |     46.272 |   1.4361 |     45.466 |     2.8
   69 |   1.4416 |     46.148 |   1.4342 |     45.466 |     2.9
   70 |   1.4398 |     46.261 |   1.4320 |     45.466 |     2.9
   71 |   1.4356 |     46.088 |   1.4301 |     45.466 |     3.0
   72 |   1.4357 |     46.234 |   1.4275 |     45.466 |     3.0
   73 |   1.4330 |     46.137 |   1.4253 |     45.466 |     3.0
   74 |   1.4298 |     46.045 |   1.4218 |     45.343 |     3.1
   75 |   1.4236 |     45.806 |   1.4186 |     45.221 |     3.1
   76 |   1.4205 |     45.541 |   1.4160 |     45.037 |     3.2
   77 |   1.4186 |     45.508 |   1.4117 |     44.730 |     3.2
   78 |   1.4132 |     45.400 |   1.4100 |     44.363 |     3.3
   79 |   1.4110 |     45.172 |   1.4075 |     44.516 |     3.3
   80 |   1.4054 |     45.161 |   1.4039 |     44.516 |     3.3
   81 |   1.4013 |     45.059 |   1.4021 |     44.853 |     3.4
   82 |   1.4026 |     44.928 |   1.3990 |     44.577 |     3.4
   83 |   1.3941 |     45.069 |   1.3953 |     44.393 |     3.5
   84 |   1.3901 |     44.939 |   1.3930 |     44.485 |     3.5
   85 |   1.3889 |     44.885 |   1.3904 |     44.638 |     3.5
   86 |   1.3807 |     44.755 |   1.3887 |     44.761 |     3.6
   87 |   1.3786 |     44.652 |   1.3861 |     44.393 |     3.6
   88 |   1.3717 |     44.695 |   1.3866 |     44.301 |     3.7
   89 |   1.3693 |     44.636 |   1.3836 |     44.516 |     3.7
   90 |   1.3638 |     44.517 |   1.3820 |     43.964 |     3.8
   91 |   1.3646 |     44.593 |   1.3796 |     44.301 |     3.8
   92 |   1.3647 |     44.522 |   1.3789 |     44.363 |     3.8
   93 |   1.3539 |     44.414 |   1.3771 |     44.301 |     3.9
   94 |   1.3542 |     44.565 |   1.3769 |     44.118 |     3.9
   95 |   1.3531 |     44.495 |   1.3735 |     44.577 |     4.0
   96 |   1.3489 |     44.506 |   1.3749 |     44.516 |     4.0
   97 |   1.3446 |     44.397 |   1.3732 |     44.945 |     4.0
   98 |   1.3428 |     44.300 |   1.3702 |     44.669 |     4.1
   99 |   1.3383 |     44.311 |   1.3685 |     44.608 |     4.1
  100 |   1.3371 |     44.213 |   1.3663 |     44.547 |     4.2
  101 |   1.3315 |     44.295 |   1.3663 |     44.730 |     4.2
  102 |   1.3305 |     44.116 |   1.3650 |     44.393 |     4.3
  103 |   1.3267 |     43.801 |   1.3646 |     44.516 |     4.3
  104 |   1.3243 |     43.872 |   1.3608 |     44.087 |     4.3
  105 |   1.3193 |     43.753 |   1.3592 |     44.179 |     4.4
  106 |   1.3199 |     43.715 |   1.3587 |     44.179 |     4.4
  107 |   1.3146 |     43.449 |   1.3575 |     44.056 |     4.5
  108 |   1.3123 |     43.520 |   1.3571 |     43.658 |     4.5
  109 |   1.3122 |     43.449 |   1.3599 |     44.730 |     4.5
  110 |   1.3060 |     43.265 |   1.3560 |     43.689 |     4.6
  111 |   1.3032 |     43.455 |   1.3545 |     43.811 |     4.6
  112 |   1.3061 |     43.308 |   1.3490 |     43.566 |     4.7
  113 |   1.3038 |     43.151 |   1.3521 |     43.597 |     4.7
  114 |   1.2985 |     43.108 |   1.3489 |     44.148 |     4.8
  115 |   1.2941 |     42.859 |   1.3494 |     44.118 |     4.8
  116 |   1.2950 |     42.680 |   1.3461 |     43.627 |     4.8
  117 |   1.2905 |     42.626 |   1.3451 |     43.658 |     4.9
  118 |   1.2890 |     42.891 |   1.3484 |     44.056 |     4.9
  119 |   1.2872 |     42.582 |   1.3449 |     43.689 |     5.0
  120 |   1.2821 |     42.398 |   1.3423 |     43.474 |     5.0
  121 |   1.2775 |     42.517 |   1.3393 |     43.536 |     5.0
  122 |   1.2808 |     42.582 |   1.3373 |     43.627 |     5.1
  123 |   1.2779 |     42.339 |   1.3299 |     43.413 |     5.1
  124 |   1.2711 |     42.311 |   1.3297 |     43.413 |     5.2
  125 |   1.2694 |     42.371 |   1.3338 |     43.873 |     5.2
  126 |   1.2668 |     42.306 |   1.3300 |     43.444 |     5.3
  127 |   1.2648 |     42.414 |   1.3262 |     43.597 |     5.3
  128 |   1.2641 |     42.333 |   1.3243 |     43.229 |     5.3
  129 |   1.2592 |     42.257 |   1.3192 |     43.229 |     5.4
  130 |   1.2586 |     42.149 |   1.3209 |     43.199 |     5.4
  131 |   1.2529 |     41.959 |   1.3217 |     43.260 |     5.5
  132 |   1.2552 |     42.404 |   1.3129 |     43.229 |     5.5
  133 |   1.2537 |     42.230 |   1.3146 |     43.199 |     5.6
  134 |   1.2446 |     41.824 |   1.3138 |     42.923 |     5.6
  135 |   1.2422 |     41.732 |   1.3080 |     42.862 |     5.6
  136 |   1.2439 |     41.726 |   1.3124 |     43.107 |     5.7
  137 |   1.2399 |     41.683 |   1.3098 |     43.260 |     5.7
  138 |   1.2356 |     41.428 |   1.3064 |     42.984 |     5.8
  139 |   1.2333 |     41.379 |   1.3067 |     42.831 |     5.8
  140 |   1.2291 |     41.206 |   1.3051 |     42.862 |     5.8
  141 |   1.2287 |     41.342 |   1.3049 |     42.984 |     5.9
  142 |   1.2282 |     41.331 |   1.2991 |     42.862 |     5.9
  143 |   1.2249 |     41.217 |   1.3024 |     42.862 |     6.0
  144 |   1.2218 |     41.125 |   1.2987 |     42.616 |     6.0
  145 |   1.2190 |     41.038 |   1.2929 |     42.770 |     6.1
  146 |   1.2172 |     40.919 |   1.2985 |     42.800 |     6.1
  147 |   1.2163 |     40.995 |   1.2957 |     42.678 |     6.1
  148 |   1.2161 |     41.076 |   1.2934 |     42.831 |     6.2
  149 |   1.2122 |     40.886 |   1.2910 |     42.616 |     6.2
  150 |   1.2104 |     40.865 |   1.2941 |     42.739 |     6.3
  151 |   1.2120 |     40.865 |   1.2893 |     42.555 |     6.3
  152 |   1.2059 |     40.751 |   1.2951 |     42.678 |     6.3
  153 |   1.2027 |     40.783 |   1.2838 |     42.249 |     6.4
  154 |   1.2027 |     40.697 |   1.2865 |     42.586 |     6.4
  155 |   1.1995 |     40.811 |   1.2852 |     42.126 |     6.5
  156 |   1.1990 |     40.621 |   1.2876 |     42.586 |     6.5
  157 |   1.1972 |     40.702 |   1.2880 |     42.249 |     6.6
  158 |   1.1929 |     40.702 |   1.2816 |     42.218 |     6.6
  159 |   1.1956 |     40.616 |   1.2820 |     42.065 |     6.6
  160 |   1.1900 |     40.496 |   1.2821 |     42.310 |     6.7
  161 |   1.1878 |     40.312 |   1.2853 |     42.586 |     6.7
  162 |   1.1873 |     40.247 |   1.2822 |     42.157 |     6.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 190,370

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8970 |     78.609 |   2.4573 |     66.513 |     0.0
    2 |   2.2191 |     58.989 |   2.0450 |     58.793 |     0.0
    3 |   1.9013 |     53.408 |   1.7644 |     48.346 |     0.0
    4 |   1.6768 |     47.887 |   1.5990 |     45.466 |     0.1
    5 |   1.5583 |     46.213 |   1.5161 |     45.466 |     0.1
    6 |   1.4972 |     46.213 |   1.4711 |     45.466 |     0.1
    7 |   1.4632 |     46.218 |   1.4480 |     45.466 |     0.1
    8 |   1.4431 |     46.223 |   1.4297 |     45.466 |     0.1
    9 |   1.4254 |     46.191 |   1.4175 |     45.466 |     0.1
   10 |   1.4158 |     46.245 |   1.4052 |     45.987 |     0.1
   11 |   1.4047 |     46.359 |   1.3941 |     45.466 |     0.2
   12 |   1.3870 |     46.251 |   1.3800 |     46.078 |     0.2
   13 |   1.3687 |     45.893 |   1.3577 |     44.455 |     0.2
   14 |   1.3502 |     44.809 |   1.3398 |     43.964 |     0.2
   15 |   1.3282 |     44.338 |   1.3237 |     43.627 |     0.2
   16 |   1.3062 |     43.363 |   1.3106 |     42.310 |     0.2
   17 |   1.2898 |     42.794 |   1.2959 |     42.096 |     0.2
   18 |   1.2761 |     42.290 |   1.2836 |     41.115 |     0.2
   19 |   1.2585 |     41.791 |   1.2674 |     41.728 |     0.3
   20 |   1.2451 |     41.260 |   1.2586 |     41.176 |     0.3
   21 |   1.2294 |     40.827 |   1.2443 |     40.839 |     0.3
   22 |   1.2164 |     40.518 |   1.2301 |     40.472 |     0.3
   23 |   1.1948 |     39.971 |   1.2171 |     40.135 |     0.3
   24 |   1.1797 |     39.212 |   1.2132 |     39.553 |     0.3
   25 |   1.1621 |     39.033 |   1.1952 |     39.430 |     0.3
   26 |   1.1374 |     38.492 |   1.1882 |     39.491 |     0.4
   27 |   1.1186 |     38.085 |   1.1789 |     38.971 |     0.4
   28 |   1.1016 |     37.175 |   1.1714 |     38.909 |     0.4
   29 |   1.0884 |     36.855 |   1.1627 |     38.266 |     0.4
   30 |   1.0716 |     36.346 |   1.1496 |     37.898 |     0.4
   31 |   1.0600 |     35.744 |   1.1478 |     38.235 |     0.4
   32 |   1.0422 |     35.230 |   1.1293 |     36.887 |     0.4
   33 |   1.0287 |     34.682 |   1.1349 |     37.561 |     0.4
   34 |   1.0136 |     33.967 |   1.1272 |     37.163 |     0.5
   35 |   1.0005 |     33.431 |   1.1272 |     36.428 |     0.5
   36 |   0.9904 |     32.857 |   1.1186 |     37.163 |     0.5
   37 |   0.9775 |     32.223 |   1.1098 |     35.784 |     0.5
   38 |   0.9605 |     31.713 |   1.1065 |     36.029 |     0.5
   39 |   0.9497 |     31.242 |   1.0967 |     35.815 |     0.5
   40 |   0.9372 |     30.982 |   1.1091 |     36.029 |     0.5
   41 |   0.9257 |     30.689 |   1.0927 |     35.631 |     0.6
   42 |   0.9174 |     30.169 |   1.1046 |     35.570 |     0.6
   43 |   0.9042 |     29.763 |   1.0943 |     35.539 |     0.6
   44 |   0.8931 |     29.465 |   1.1004 |     35.692 |     0.6
   45 |   0.8937 |     29.476 |   1.0890 |     34.988 |     0.6
   46 |   0.8705 |     28.533 |   1.1037 |     35.478 |     0.6
   47 |   0.8688 |     28.674 |   1.0897 |     34.681 |     0.6
   48 |   0.8571 |     28.213 |   1.0901 |     34.498 |     0.6
   49 |   0.8454 |     27.948 |   1.0989 |     35.539 |     0.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 1,107,586

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4110 |     92.669 |   3.0441 |     68.903 |     0.0
    2 |   2.7767 |     74.041 |   2.6170 |     59.252 |     0.1
    3 |   2.4973 |     59.498 |   2.3937 |     58.824 |     0.1
    4 |   2.3156 |     58.577 |   2.2383 |     58.824 |     0.2
    5 |   2.1854 |     57.959 |   2.1247 |     58.793 |     0.2
    6 |   2.0855 |     56.968 |   2.0268 |     58.640 |     0.2
    7 |   1.9995 |     55.413 |   1.9444 |     51.991 |     0.3
    8 |   1.9217 |     51.766 |   1.8709 |     48.560 |     0.3
    9 |   1.8549 |     49.940 |   1.8093 |     48.346 |     0.4
   10 |   1.7981 |     49.653 |   1.7575 |     48.376 |     0.4
   11 |   1.7467 |     49.160 |   1.7114 |     48.376 |     0.5
   12 |   1.7045 |     48.976 |   1.6719 |     48.376 |     0.5
   13 |   1.6650 |     48.775 |   1.6349 |     48.346 |     0.5
   14 |   1.6314 |     48.645 |   1.6033 |     48.376 |     0.6
   15 |   1.5998 |     48.597 |   1.5739 |     48.346 |     0.6
   16 |   1.5709 |     48.510 |   1.5477 |     48.346 |     0.7
   17 |   1.5489 |     47.800 |   1.5264 |     45.680 |     0.7
   18 |   1.5235 |     46.467 |   1.5043 |     45.466 |     0.7
   19 |   1.5025 |     46.180 |   1.4865 |     45.435 |     0.8
   20 |   1.4869 |     46.131 |   1.4697 |     45.466 |     0.8
   21 |   1.4729 |     46.223 |   1.4533 |     45.343 |     0.9
   22 |   1.4533 |     46.137 |   1.4385 |     45.895 |     0.9
   23 |   1.4374 |     46.093 |   1.4239 |     45.435 |     1.0
   24 |   1.4255 |     45.801 |   1.4128 |     45.312 |     1.0
   25 |   1.4092 |     45.411 |   1.4008 |     44.853 |     1.0
   26 |   1.3976 |     45.199 |   1.3887 |     44.455 |     1.1
   27 |   1.3839 |     45.156 |   1.3785 |     44.914 |     1.1
   28 |   1.3742 |     44.712 |   1.3740 |     44.271 |     1.2
   29 |   1.3648 |     44.587 |   1.3609 |     43.199 |     1.2
   30 |   1.3543 |     44.295 |   1.3529 |     43.290 |     1.2
   31 |   1.3442 |     43.823 |   1.3471 |     42.831 |     1.3
   32 |   1.3357 |     43.563 |   1.3411 |     43.474 |     1.3
   33 |   1.3266 |     43.682 |   1.3349 |     43.199 |     1.4
   34 |   1.3198 |     43.341 |   1.3291 |     42.525 |     1.4
   35 |   1.3134 |     43.151 |   1.3234 |     43.199 |     1.5
   36 |   1.3029 |     43.216 |   1.3228 |     43.015 |     1.5
   37 |   1.2977 |     43.032 |   1.3206 |     42.678 |     1.5
   38 |   1.2907 |     43.016 |   1.3115 |     43.045 |     1.6
   39 |   1.2845 |     42.680 |   1.3094 |     42.739 |     1.6
   40 |   1.2794 |     42.404 |   1.3050 |     42.953 |     1.7
   41 |   1.2725 |     42.512 |   1.2999 |     42.616 |     1.7
   42 |   1.2659 |     42.355 |   1.2926 |     42.341 |     1.7
   43 |   1.2621 |     42.160 |   1.2921 |     42.555 |     1.8
   44 |   1.2519 |     42.008 |   1.2868 |     41.881 |     1.8
   45 |   1.2459 |     41.808 |   1.2830 |     42.157 |     1.9
   46 |   1.2413 |     41.721 |   1.2788 |     42.004 |     1.9
   47 |   1.2333 |     41.553 |   1.2768 |     41.973 |     2.0
   48 |   1.2297 |     41.260 |   1.2698 |     42.279 |     2.0
   49 |   1.2188 |     41.450 |   1.2676 |     41.268 |     2.0
   50 |   1.2190 |     41.081 |   1.2668 |     42.341 |     2.1
   51 |   1.2101 |     40.643 |   1.2625 |     41.667 |     2.1
   52 |   1.2013 |     40.724 |   1.2584 |     41.422 |     2.2
   53 |   1.1944 |     40.415 |   1.2506 |     42.218 |     2.2
   54 |   1.1909 |     40.345 |   1.2457 |     41.483 |     2.2
   55 |   1.1871 |     40.166 |   1.2462 |     42.034 |     2.3
   56 |   1.1779 |     40.063 |   1.2450 |     41.238 |     2.3
   57 |   1.1745 |     39.976 |   1.2355 |     41.575 |     2.4
   58 |   1.1673 |     39.824 |   1.2405 |     41.391 |     2.4
   59 |   1.1632 |     39.689 |   1.2311 |     41.391 |     2.5
   60 |   1.1537 |     39.299 |   1.2277 |     41.330 |     2.5
   61 |   1.1480 |     39.364 |   1.2259 |     41.085 |     2.5
   62 |   1.1436 |     38.995 |   1.2275 |     40.901 |     2.6
   63 |   1.1395 |     38.882 |   1.2201 |     41.299 |     2.6
   64 |   1.1308 |     38.632 |   1.2232 |     41.023 |     2.7
   65 |   1.1269 |     38.345 |   1.2209 |     40.931 |     2.7
   66 |   1.1211 |     38.221 |   1.2144 |     40.625 |     2.7
   67 |   1.1188 |     38.356 |   1.2155 |     40.319 |     2.8
   68 |   1.1130 |     38.031 |   1.2098 |     40.625 |     2.8
   69 |   1.1064 |     37.787 |   1.2075 |     40.594 |     2.9
   70 |   1.1016 |     37.484 |   1.2034 |     39.706 |     2.9
   71 |   1.0956 |     37.354 |   1.1990 |     39.890 |     3.0
   72 |   1.0905 |     37.012 |   1.1963 |     39.859 |     3.0
   73 |   1.0864 |     37.099 |   1.2021 |     39.675 |     3.0
   74 |   1.0825 |     36.676 |   1.1947 |     39.185 |     3.1
   75 |   1.0766 |     36.628 |   1.2066 |     40.472 |     3.1
   76 |   1.0715 |     36.373 |   1.1929 |     39.553 |     3.2
   77 |   1.0697 |     36.546 |   1.1918 |     40.043 |     3.2
   78 |   1.0600 |     35.967 |   1.1967 |     40.043 |     3.2
   79 |   1.0571 |     36.124 |   1.1854 |     39.982 |     3.3
   80 |   1.0554 |     35.734 |   1.1838 |     39.338 |     3.3
   81 |   1.0490 |     35.750 |   1.1796 |     39.859 |     3.4
   82 |   1.0420 |     35.446 |   1.1816 |     39.277 |     3.4
   83 |   1.0409 |     35.197 |   1.1752 |     39.583 |     3.5
   84 |   1.0329 |     35.165 |   1.1833 |     39.308 |     3.5
   85 |   1.0283 |     35.089 |   1.1760 |     39.246 |     3.5
   86 |   1.0243 |     34.894 |   1.1817 |     39.491 |     3.6
   87 |   1.0216 |     34.482 |   1.1854 |     38.909 |     3.6
   88 |   1.0194 |     34.552 |   1.1732 |     39.185 |     3.7
   89 |   1.0125 |     34.054 |   1.1723 |     38.634 |     3.7
   90 |   1.0081 |     34.092 |   1.1598 |     38.388 |     3.7
   91 |   1.0064 |     34.189 |   1.1633 |     38.603 |     3.8
   92 |   0.9982 |     33.664 |   1.1623 |     37.868 |     3.8
   93 |   0.9976 |     33.648 |   1.1655 |     38.143 |     3.9
   94 |   0.9908 |     33.198 |   1.1568 |     37.500 |     3.9
   95 |   0.9870 |     33.057 |   1.1532 |     38.051 |     4.0
   96 |   0.9829 |     32.894 |   1.1556 |     37.714 |     4.0
   97 |   0.9774 |     32.867 |   1.1575 |     37.745 |     4.0
   98 |   0.9692 |     32.266 |   1.1573 |     37.408 |     4.1
   99 |   0.9703 |     32.347 |   1.1556 |     36.918 |     4.1
  100 |   0.9656 |     32.179 |   1.1499 |     37.745 |     4.2
  101 |   0.9588 |     32.000 |   1.1570 |     37.408 |     4.2
  102 |   0.9593 |     31.773 |   1.1513 |     37.377 |     4.2
  103 |   0.9559 |     31.881 |   1.1479 |     36.673 |     4.3
  104 |   0.9481 |     31.524 |   1.1450 |     36.520 |     4.3
  105 |   0.9431 |     31.291 |   1.1478 |     36.887 |     4.4
  106 |   0.9430 |     31.090 |   1.1510 |     36.734 |     4.4
  107 |   0.9394 |     31.253 |   1.1440 |     36.734 |     4.5
  108 |   0.9363 |     30.857 |   1.1457 |     36.673 |     4.5
  109 |   0.9278 |     30.711 |   1.1439 |     36.152 |     4.5
  110 |   0.9237 |     30.326 |   1.1407 |     36.550 |     4.6
  111 |   0.9241 |     30.608 |   1.1406 |     36.213 |     4.6
  112 |   0.9203 |     30.353 |   1.1343 |     35.723 |     4.7
  113 |   0.9199 |     30.212 |   1.1340 |     36.581 |     4.7
  114 |   0.9112 |     30.028 |   1.1320 |     35.723 |     4.7
  115 |   0.9106 |     29.914 |   1.1372 |     35.815 |     4.8
  116 |   0.9052 |     29.898 |   1.1454 |     35.754 |     4.8
  117 |   0.9009 |     29.676 |   1.1353 |     36.060 |     4.9
  118 |   0.8963 |     29.681 |   1.1383 |     36.091 |     4.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 418,978

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5362 |     96.245 |   3.4730 |     96.661 |     0.0
    2 |   3.2775 |     80.413 |   3.0244 |     80.423 |     0.0
    3 |   2.8778 |     83.095 |   2.7906 |     83.333 |     0.1
    4 |   2.7254 |     81.914 |   2.6818 |     67.341 |     0.1
    5 |   2.6182 |     67.777 |   2.5779 |     67.341 |     0.1
    6 |   2.5159 |     64.884 |   2.4721 |     58.824 |     0.1
    7 |   2.4206 |     60.479 |   2.3831 |     58.824 |     0.1
    8 |   2.3441 |     58.924 |   2.3152 |     58.824 |     0.2
    9 |   2.2843 |     58.366 |   2.2611 |     58.824 |     0.2
   10 |   2.2353 |     58.138 |   2.2153 |     58.824 |     0.2
   11 |   2.1947 |     57.797 |   2.1745 |     58.824 |     0.2
   12 |   2.1583 |     57.781 |   2.1380 |     58.824 |     0.2
   13 |   2.1223 |     57.526 |   2.1033 |     58.824 |     0.3
   14 |   2.0898 |     57.353 |   2.0705 |     58.824 |     0.3
   15 |   2.0579 |     56.984 |   2.0394 |     58.824 |     0.3
   16 |   2.0270 |     56.838 |   2.0102 |     58.119 |     0.3
   17 |   2.0014 |     56.399 |   1.9826 |     57.904 |     0.3
   18 |   1.9741 |     55.906 |   1.9543 |     56.771 |     0.4
   19 |   1.9464 |     55.342 |   1.9260 |     54.596 |     0.4
   20 |   1.9203 |     53.500 |   1.8982 |     50.521 |     0.4
   21 |   1.8898 |     52.156 |   1.8712 |     48.468 |     0.4
   22 |   1.8658 |     50.558 |   1.8454 |     48.407 |     0.4
   23 |   1.8405 |     49.886 |   1.8201 |     48.438 |     0.5
   24 |   1.8152 |     49.664 |   1.7947 |     48.346 |     0.5
   25 |   1.7903 |     49.366 |   1.7692 |     48.346 |     0.5
   26 |   1.7623 |     49.176 |   1.7445 |     48.346 |     0.5
   27 |   1.7398 |     49.025 |   1.7196 |     48.346 |     0.5
   28 |   1.7172 |     48.840 |   1.6960 |     48.346 |     0.6
   29 |   1.6957 |     48.093 |   1.6741 |     46.906 |     0.6
   30 |   1.6738 |     46.977 |   1.6544 |     45.466 |     0.6
   31 |   1.6566 |     46.397 |   1.6363 |     45.466 |     0.6
   32 |   1.6367 |     46.261 |   1.6199 |     45.466 |     0.6
   33 |   1.6223 |     46.234 |   1.6054 |     45.496 |     0.7
   34 |   1.6069 |     46.234 |   1.5907 |     45.466 |     0.7
   35 |   1.5914 |     46.240 |   1.5763 |     45.466 |     0.7
   36 |   1.5758 |     46.186 |   1.5647 |     45.466 |     0.7
   37 |   1.5693 |     46.223 |   1.5538 |     45.466 |     0.7
   38 |   1.5545 |     46.202 |   1.5440 |     45.466 |     0.8
   39 |   1.5466 |     46.207 |   1.5337 |     45.466 |     0.8
   40 |   1.5382 |     46.186 |   1.5245 |     45.466 |     0.8
   41 |   1.5279 |     46.196 |   1.5154 |     45.466 |     0.8
   42 |   1.5195 |     46.175 |   1.5076 |     45.466 |     0.8
   43 |   1.5106 |     46.186 |   1.5001 |     45.466 |     0.9
   44 |   1.5053 |     46.191 |   1.4924 |     45.466 |     0.9
   45 |   1.4970 |     46.164 |   1.4864 |     45.466 |     0.9
   46 |   1.4911 |     46.207 |   1.4807 |     45.466 |     0.9
   47 |   1.4841 |     46.186 |   1.4752 |     45.466 |     0.9
   48 |   1.4777 |     46.180 |   1.4699 |     45.466 |     1.0
   49 |   1.4736 |     46.207 |   1.4648 |     45.466 |     1.0
   50 |   1.4677 |     46.180 |   1.4604 |     45.466 |     1.0
   51 |   1.4634 |     46.175 |   1.4556 |     45.466 |     1.0
   52 |   1.4610 |     46.164 |   1.4512 |     45.404 |     1.0
   53 |   1.4542 |     46.180 |   1.4468 |     45.466 |     1.1
   54 |   1.4488 |     46.164 |   1.4423 |     45.466 |     1.1
   55 |   1.4474 |     46.158 |   1.4387 |     45.466 |     1.1
   56 |   1.4425 |     46.131 |   1.4352 |     45.435 |     1.1
   57 |   1.4385 |     46.131 |   1.4311 |     45.343 |     1.1
   58 |   1.4363 |     46.104 |   1.4277 |     45.343 |     1.2
   59 |   1.4316 |     46.121 |   1.4243 |     45.282 |     1.2
   60 |   1.4285 |     46.121 |   1.4204 |     45.221 |     1.2
   61 |   1.4246 |     46.104 |   1.4171 |     45.282 |     1.2
   62 |   1.4198 |     46.093 |   1.4150 |     45.282 |     1.2
   63 |   1.4186 |     46.061 |   1.4142 |     45.221 |     1.3
   64 |   1.4161 |     46.077 |   1.4092 |     45.282 |     1.3
   65 |   1.4142 |     46.018 |   1.4076 |     45.282 |     1.3
   66 |   1.4102 |     45.963 |   1.4054 |     45.190 |     1.3
   67 |   1.4083 |     45.920 |   1.4039 |     45.221 |     1.3
   68 |   1.4034 |     45.709 |   1.4010 |     45.006 |     1.4
   69 |   1.4011 |     45.481 |   1.4002 |     44.761 |     1.4
   70 |   1.3995 |     45.351 |   1.3972 |     44.608 |     1.4
   71 |   1.3961 |     45.400 |   1.3972 |     44.638 |     1.4
   72 |   1.3940 |     45.216 |   1.3960 |     44.669 |     1.4
   73 |   1.3916 |     45.205 |   1.3957 |     44.669 |     1.5
   74 |   1.3901 |     45.118 |   1.3924 |     44.547 |     1.5
   75 |   1.3891 |     45.042 |   1.3904 |     44.669 |     1.5
   76 |   1.3865 |     44.993 |   1.3904 |     44.608 |     1.5
   77 |   1.3832 |     44.939 |   1.3896 |     44.853 |     1.5
   78 |   1.3820 |     44.874 |   1.3901 |     44.822 |     1.6
   79 |   1.3788 |     44.815 |   1.3876 |     44.914 |     1.6
   80 |   1.3768 |     44.804 |   1.3852 |     44.761 |     1.6
   81 |   1.3761 |     44.788 |   1.3812 |     44.363 |     1.6
   82 |   1.3729 |     44.712 |   1.3831 |     44.700 |     1.6
   83 |   1.3741 |     44.771 |   1.3853 |     44.853 |     1.7
   84 |   1.3729 |     44.750 |   1.3804 |     44.608 |     1.7
   85 |   1.3699 |     44.658 |   1.3809 |     44.700 |     1.7
   86 |   1.3651 |     44.663 |   1.3790 |     44.577 |     1.7
   87 |   1.3641 |     44.528 |   1.3804 |     44.761 |     1.7
   88 |   1.3613 |     44.484 |   1.3764 |     44.547 |     1.8
   89 |   1.3629 |     44.571 |   1.3760 |     44.485 |     1.8
   90 |   1.3613 |     44.484 |   1.3752 |     44.516 |     1.8
   91 |   1.3590 |     44.419 |   1.3765 |     44.516 |     1.8
   92 |   1.3594 |     44.430 |   1.3735 |     44.485 |     1.8
   93 |   1.3577 |     44.452 |   1.3719 |     44.393 |     1.9
   94 |   1.3550 |     44.365 |   1.3726 |     44.577 |     1.9
   95 |   1.3529 |     44.387 |   1.3706 |     44.516 |     1.9
   96 |   1.3514 |     44.278 |   1.3692 |     44.547 |     1.9
   97 |   1.3517 |     44.305 |   1.3705 |     44.485 |     1.9
   98 |   1.3506 |     44.365 |   1.3670 |     44.301 |     2.0
   99 |   1.3481 |     44.235 |   1.3680 |     44.455 |     2.0
  100 |   1.3442 |     44.164 |   1.3664 |     44.271 |     2.0
  101 |   1.3453 |     44.230 |   1.3698 |     44.638 |     2.0
  102 |   1.3460 |     44.251 |   1.3652 |     44.332 |     2.0
  103 |   1.3409 |     44.094 |   1.3630 |     44.210 |     2.1
  104 |   1.3393 |     44.116 |   1.3636 |     44.240 |     2.1
  105 |   1.3394 |     44.219 |   1.3620 |     44.301 |     2.1
  106 |   1.3350 |     44.062 |   1.3651 |     44.455 |     2.1
  107 |   1.3379 |     44.208 |   1.3627 |     44.485 |     2.1
  108 |   1.3342 |     44.089 |   1.3598 |     44.240 |     2.2
  109 |   1.3325 |     44.007 |   1.3616 |     44.363 |     2.2
  110 |   1.3312 |     44.013 |   1.3608 |     44.363 |     2.2
  111 |   1.3314 |     44.089 |   1.3596 |     44.393 |     2.2
  112 |   1.3329 |     44.007 |   1.3585 |     44.271 |     2.2
  113 |   1.3288 |     43.986 |   1.3571 |     44.424 |     2.3
  114 |   1.3266 |     43.894 |   1.3568 |     44.424 |     2.3
  115 |   1.3315 |     43.975 |   1.3555 |     44.332 |     2.3
  116 |   1.3240 |     43.937 |   1.3542 |     44.516 |     2.3
  117 |   1.3205 |     43.780 |   1.3568 |     44.179 |     2.3
  118 |   1.3255 |     43.801 |   1.3518 |     44.271 |     2.4
  119 |   1.3227 |     43.818 |   1.3529 |     44.240 |     2.4
  120 |   1.3198 |     43.774 |   1.3541 |     44.332 |     2.4
  121 |   1.3173 |     43.796 |   1.3546 |     44.424 |     2.4
  122 |   1.3154 |     43.709 |   1.3502 |     44.179 |     2.4
  123 |   1.3134 |     43.823 |   1.3547 |     44.271 |     2.5
  124 |   1.3118 |     43.758 |   1.3531 |     44.271 |     2.5
  125 |   1.3116 |     43.796 |   1.3578 |     44.455 |     2.5
  126 |   1.3126 |     43.812 |   1.3538 |     44.485 |     2.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 676,770

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5298 |     67.696 |   1.9826 |     56.495 |     0.0
    2 |   1.7594 |     49.848 |   1.5699 |     45.466 |     0.1
    3 |   1.5077 |     46.299 |   1.4552 |     45.987 |     0.1
    4 |   1.4436 |     46.283 |   1.4216 |     45.466 |     0.1
    5 |   1.4203 |     46.370 |   1.4072 |     46.017 |     0.1
    6 |   1.4091 |     46.402 |   1.3994 |     45.466 |     0.2
    7 |   1.4020 |     46.213 |   1.3900 |     45.466 |     0.2
    8 |   1.3991 |     46.213 |   1.3855 |     45.466 |     0.2
    9 |   1.3903 |     46.299 |   1.3760 |     45.680 |     0.2
   10 |   1.3824 |     46.099 |   1.3669 |     45.496 |     0.3
   11 |   1.3700 |     45.254 |   1.3472 |     44.056 |     0.3
   12 |   1.3495 |     44.782 |   1.3394 |     44.240 |     0.3
   13 |   1.3330 |     44.571 |   1.3227 |     43.382 |     0.3
   14 |   1.3197 |     44.360 |   1.3112 |     44.179 |     0.4
   15 |   1.3070 |     44.154 |   1.2999 |     42.525 |     0.4
   16 |   1.2946 |     43.363 |   1.2918 |     42.310 |     0.4
   17 |   1.2825 |     42.956 |   1.2809 |     41.176 |     0.5
   18 |   1.2720 |     42.788 |   1.2743 |     42.157 |     0.5
   19 |   1.2622 |     42.420 |   1.2786 |     41.697 |     0.5
   20 |   1.2513 |     42.214 |   1.2659 |     41.452 |     0.5
   21 |   1.2409 |     41.759 |   1.2611 |     41.483 |     0.6
   22 |   1.2286 |     41.650 |   1.2406 |     40.686 |     0.6
   23 |   1.2209 |     41.439 |   1.2350 |     40.901 |     0.6
   24 |   1.2129 |     41.342 |   1.2380 |     40.257 |     0.6
   25 |   1.2009 |     40.886 |   1.2228 |     40.625 |     0.7
   26 |   1.1894 |     40.366 |   1.2236 |     40.564 |     0.7
   27 |   1.1759 |     40.020 |   1.2190 |     40.319 |     0.7
   28 |   1.1732 |     40.020 |   1.2021 |     39.277 |     0.7
   29 |   1.1553 |     39.250 |   1.1927 |     39.767 |     0.8
   30 |   1.1406 |     38.849 |   1.1888 |     39.216 |     0.8
   31 |   1.1283 |     38.215 |   1.1793 |     39.308 |     0.8
   32 |   1.1103 |     37.549 |   1.1805 |     39.032 |     0.9
   33 |   1.0984 |     37.029 |   1.1724 |     39.246 |     0.9
   34 |   1.0820 |     36.525 |   1.1603 |     38.971 |     0.9
   35 |   1.0677 |     36.048 |   1.1565 |     38.205 |     0.9
   36 |   1.0592 |     35.782 |   1.1397 |     37.592 |     1.0
   37 |   1.0453 |     35.176 |   1.1400 |     37.102 |     1.0
   38 |   1.0343 |     34.796 |   1.1341 |     37.960 |     1.0
   39 |   1.0251 |     34.325 |   1.1359 |     36.642 |     1.0
   40 |   1.0161 |     34.384 |   1.1238 |     37.132 |     1.1
   41 |   0.9927 |     33.474 |   1.1035 |     36.152 |     1.1
   42 |   0.9733 |     32.672 |   1.1178 |     36.397 |     1.1
   43 |   0.9655 |     32.580 |   1.0998 |     35.478 |     1.1
   44 |   0.9522 |     31.800 |   1.0888 |     34.589 |     1.2
   45 |   0.9458 |     31.849 |   1.0855 |     35.110 |     1.2
   46 |   0.9377 |     31.562 |   1.0875 |     34.681 |     1.2
   47 |   0.9295 |     30.846 |   1.0758 |     34.467 |     1.2
   48 |   0.9142 |     30.760 |   1.0760 |     34.375 |     1.3
   49 |   0.8931 |     30.131 |   1.0591 |     34.007 |     1.3
   50 |   0.8805 |     29.454 |   1.0651 |     33.854 |     1.3
   51 |   0.8707 |     29.069 |   1.0646 |     34.130 |     1.4
   52 |   0.8512 |     28.441 |   1.0527 |     33.303 |     1.4
   53 |   0.8484 |     28.441 |   1.0485 |     33.211 |     1.4
   54 |   0.8373 |     28.121 |   1.0509 |     34.038 |     1.4
   55 |   0.8399 |     28.153 |   1.0448 |     33.793 |     1.5
   56 |   0.8311 |     27.866 |   1.0417 |     33.854 |     1.5
   57 |   0.8160 |     27.677 |   1.0364 |     33.272 |     1.5
   58 |   0.7917 |     26.647 |   1.0232 |     31.924 |     1.5
   59 |   0.7847 |     26.360 |   1.0169 |     32.598 |     1.6
   60 |   0.7710 |     25.824 |   1.0316 |     33.333 |     1.6
   61 |   0.7654 |     25.975 |   1.0025 |     32.475 |     1.6
   62 |   0.7540 |     25.390 |   1.0221 |     32.782 |     1.6
   63 |   0.7460 |     25.087 |   1.0350 |     33.425 |     1.7
   64 |   0.7342 |     24.745 |   0.9989 |     31.893 |     1.7
   65 |   0.7340 |     24.567 |   1.0107 |     32.353 |     1.7
   66 |   0.7157 |     23.949 |   1.0142 |     32.292 |     1.7
   67 |   0.7119 |     23.819 |   1.0026 |     31.189 |     1.8
   68 |   0.7119 |     23.878 |   1.0117 |     31.710 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 400,802

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1317 |     58.214 |   1.5077 |     45.282 |     0.0
    2 |   1.4080 |     45.514 |   1.3356 |     43.750 |     0.0
    3 |   1.3011 |     42.707 |   1.2477 |     40.288 |     0.0
    4 |   1.2227 |     40.318 |   1.2018 |     39.338 |     0.1
    5 |   1.1636 |     38.800 |   1.1331 |     37.010 |     0.1
    6 |   1.1013 |     36.909 |   1.0932 |     36.305 |     0.1
    7 |   1.0425 |     34.374 |   1.0427 |     34.069 |     0.1
    8 |   0.9836 |     32.060 |   0.9970 |     32.721 |     0.1
    9 |   0.9200 |     30.007 |   0.9599 |     32.077 |     0.1
   10 |   0.8710 |     28.749 |   0.9324 |     30.852 |     0.1
   11 |   0.8141 |     26.463 |   0.8945 |     29.105 |     0.2
   12 |   0.7627 |     24.669 |   0.8715 |     28.676 |     0.2
   13 |   0.7167 |     23.152 |   0.8486 |     27.328 |     0.2
   14 |   0.6692 |     21.446 |   0.8371 |     27.543 |     0.2
   15 |   0.6261 |     19.668 |   0.8322 |     26.501 |     0.2
   16 |   0.5971 |     19.273 |   0.7999 |     25.184 |     0.2
   17 |   0.5546 |     17.734 |   0.8120 |     25.429 |     0.2
   18 |   0.5362 |     17.290 |   0.8003 |     25.184 |     0.3
   19 |   0.4989 |     16.071 |   0.8152 |     24.571 |     0.3
   20 |   0.4726 |     15.187 |   0.8221 |     24.786 |     0.3
   21 |   0.4457 |     14.516 |   0.7922 |     23.376 |     0.3
   22 |   0.4278 |     13.833 |   0.8002 |     23.652 |     0.3
   23 |   0.4039 |     12.841 |   0.8046 |     23.836 |     0.3
   24 |   0.3871 |     12.793 |   0.8072 |     23.315 |     0.3
   25 |   0.3767 |     12.738 |   0.7950 |     22.763 |     0.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 388,386

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4106 |     87.002 |   3.1084 |     81.342 |     0.0
    2 |   2.7480 |     72.323 |   2.5389 |     62.224 |     0.0
    3 |   2.4195 |     60.067 |   2.3272 |     58.762 |     0.1
    4 |   2.2489 |     58.615 |   2.1837 |     58.395 |     0.1
    5 |   2.1227 |     57.661 |   2.0676 |     57.230 |     0.1
    6 |   2.0160 |     53.516 |   1.9627 |     49.755 |     0.1
    7 |   1.9180 |     49.675 |   1.8691 |     48.866 |     0.2
    8 |   1.8316 |     48.965 |   1.7881 |     48.223 |     0.2
    9 |   1.7582 |     48.570 |   1.7144 |     48.009 |     0.2
   10 |   1.6891 |     48.423 |   1.6527 |     48.039 |     0.2
   11 |   1.6352 |     48.277 |   1.6039 |     48.039 |     0.3
   12 |   1.5866 |     46.484 |   1.5608 |     45.129 |     0.3
   13 |   1.5462 |     45.963 |   1.5265 |     45.129 |     0.3
   14 |   1.5192 |     45.866 |   1.5018 |     45.129 |     0.3
   15 |   1.4888 |     45.904 |   1.4774 |     45.129 |     0.4
   16 |   1.4697 |     45.850 |   1.4567 |     45.190 |     0.4
   17 |   1.4479 |     45.817 |   1.4410 |     45.098 |     0.4
   18 |   1.4315 |     45.698 |   1.4237 |     45.037 |     0.4
   19 |   1.4139 |     45.584 |   1.4100 |     44.945 |     0.4
   20 |   1.4011 |     45.351 |   1.4000 |     44.975 |     0.5
   21 |   1.3855 |     45.292 |   1.3863 |     44.547 |     0.5
   22 |   1.3723 |     44.923 |   1.3742 |     44.730 |     0.5
   23 |   1.3603 |     44.587 |   1.3651 |     44.547 |     0.5
   24 |   1.3478 |     43.758 |   1.3545 |     44.363 |     0.6
   25 |   1.3341 |     43.791 |   1.3479 |     44.240 |     0.6
   26 |   1.3266 |     43.216 |   1.3359 |     43.597 |     0.6
   27 |   1.3145 |     42.832 |   1.3264 |     42.862 |     0.6
   28 |   1.3032 |     42.322 |   1.3198 |     42.279 |     0.7
   29 |   1.2927 |     41.900 |   1.3118 |     42.157 |     0.7
   30 |   1.2832 |     41.347 |   1.3037 |     41.544 |     0.7
   31 |   1.2708 |     41.016 |   1.2925 |     40.839 |     0.7
   32 |   1.2593 |     40.632 |   1.2817 |     40.686 |     0.7
   33 |   1.2498 |     40.263 |   1.2739 |     40.564 |     0.8
   34 |   1.2380 |     39.814 |   1.2676 |     39.828 |     0.8
   35 |   1.2293 |     39.586 |   1.2586 |     39.369 |     0.8
   36 |   1.2186 |     39.174 |   1.2515 |     39.706 |     0.8
   37 |   1.2076 |     38.708 |   1.2482 |     39.828 |     0.9
   38 |   1.1988 |     38.600 |   1.2375 |     39.369 |     0.9
   39 |   1.1877 |     38.053 |   1.2314 |     38.634 |     0.9
   40 |   1.1765 |     37.722 |   1.2224 |     38.572 |     0.9
   41 |   1.1660 |     37.446 |   1.2160 |     38.388 |     0.9
   42 |   1.1587 |     37.164 |   1.2083 |     38.113 |     1.0
   43 |   1.1501 |     36.823 |   1.2031 |     37.990 |     1.0
   44 |   1.1397 |     36.514 |   1.1958 |     37.960 |     1.0
   45 |   1.1305 |     36.107 |   1.1893 |     37.469 |     1.0
   46 |   1.1249 |     36.005 |   1.1859 |     37.714 |     1.1
   47 |   1.1129 |     35.620 |   1.1777 |     37.347 |     1.1
   48 |   1.1023 |     35.511 |   1.1801 |     37.071 |     1.1
   49 |   1.0975 |     35.138 |   1.1672 |     37.102 |     1.1
   50 |   1.0906 |     34.742 |   1.1640 |     36.765 |     1.2
   51 |   1.0828 |     34.531 |   1.1598 |     36.857 |     1.2
   52 |   1.0668 |     34.341 |   1.1495 |     36.887 |     1.2
   53 |   1.0573 |     33.555 |   1.1492 |     36.336 |     1.2
   54 |   1.0515 |     33.718 |   1.1428 |     36.397 |     1.2
   55 |   1.0463 |     33.696 |   1.1366 |     35.784 |     1.3
   56 |   1.0356 |     32.824 |   1.1350 |     35.999 |     1.3
   57 |   1.0276 |     32.764 |   1.1278 |     35.662 |     1.3
   58 |   1.0188 |     32.228 |   1.1228 |     35.509 |     1.3
   59 |   1.0100 |     32.136 |   1.1161 |     34.835 |     1.4
   60 |   1.0034 |     31.767 |   1.1113 |     35.049 |     1.4
   61 |   0.9966 |     31.372 |   1.1134 |     35.355 |     1.4
   62 |   0.9853 |     31.177 |   1.1031 |     34.926 |     1.4
   63 |   0.9815 |     30.906 |   1.1033 |     35.080 |     1.4
   64 |   0.9731 |     30.760 |   1.0955 |     34.681 |     1.5
   65 |   0.9645 |     30.407 |   1.0898 |     34.589 |     1.5
   66 |   0.9605 |     30.180 |   1.0850 |     34.222 |     1.5
   67 |   0.9508 |     29.947 |   1.0854 |     34.007 |     1.5
   68 |   0.9446 |     29.849 |   1.0817 |     33.762 |     1.6
   69 |   0.9366 |     29.578 |   1.0741 |     34.007 |     1.6
   70 |   0.9273 |     29.156 |   1.0706 |     33.915 |     1.6
   71 |   0.9273 |     29.026 |   1.0655 |     33.854 |     1.6
   72 |   0.9166 |     29.015 |   1.0655 |     33.915 |     1.7
   73 |   0.9058 |     28.565 |   1.0594 |     33.732 |     1.7
   74 |   0.9061 |     28.305 |   1.0536 |     33.333 |     1.7
   75 |   0.9058 |     28.609 |   1.0481 |     33.241 |     1.7
   76 |   0.8885 |     27.893 |   1.0483 |     33.119 |     1.7
   77 |   0.8834 |     27.839 |   1.0424 |     32.996 |     1.8
   78 |   0.8815 |     27.595 |   1.0412 |     33.211 |     1.8
   79 |   0.8766 |     27.395 |   1.0415 |     33.241 |     1.8
   80 |   0.8644 |     27.406 |   1.0390 |     33.058 |     1.8
   81 |   0.8603 |     26.967 |   1.0264 |     32.812 |     1.9
   82 |   0.8527 |     26.793 |   1.0340 |     32.904 |     1.9
   83 |   0.8477 |     26.647 |   1.0206 |     32.966 |     1.9
   84 |   0.8412 |     26.490 |   1.0242 |     32.874 |     1.9
   85 |   0.8374 |     26.355 |   1.0189 |     32.445 |     1.9
   86 |   0.8255 |     26.111 |   1.0203 |     32.721 |     2.0
   87 |   0.8242 |     25.883 |   1.0128 |     31.955 |     2.0
   88 |   0.8211 |     25.601 |   1.0101 |     32.292 |     2.0
   89 |   0.8118 |     25.390 |   1.0103 |     32.200 |     2.0
   90 |   0.8078 |     25.184 |   1.0089 |     31.556 |     2.1
   91 |   0.8000 |     25.033 |   1.0032 |     31.955 |     2.1
   92 |   0.7957 |     24.935 |   1.0009 |     31.985 |     2.1
   93 |   0.7894 |     24.496 |   0.9964 |     31.556 |     2.1
   94 |   0.7865 |     24.686 |   0.9936 |     31.832 |     2.1
   95 |   0.7807 |     24.496 |   1.0002 |     31.526 |     2.2
   96 |   0.7714 |     24.019 |   0.9918 |     31.526 |     2.2
   97 |   0.7695 |     23.618 |   0.9933 |     31.464 |     2.2
   98 |   0.7654 |     23.922 |   0.9911 |     31.219 |     2.2
   99 |   0.7589 |     23.564 |   0.9849 |     31.066 |     2.3
  100 |   0.7558 |     23.656 |   0.9828 |     30.913 |     2.3
  101 |   0.7478 |     23.353 |   0.9869 |     30.821 |     2.3
  102 |   0.7427 |     23.077 |   0.9874 |     31.127 |     2.3
  103 |   0.7381 |     23.028 |   0.9799 |     30.423 |     2.3
  104 |   0.7332 |     22.415 |   0.9737 |     30.208 |     2.4
  105 |   0.7279 |     22.453 |   0.9834 |     30.790 |     2.4
  106 |   0.7288 |     22.686 |   0.9775 |     30.607 |     2.4
  107 |   0.7209 |     22.356 |   0.9801 |     30.576 |     2.4
  108 |   0.7141 |     22.096 |   0.9759 |     30.147 |     2.5
  109 |   0.7121 |     21.939 |   0.9660 |     30.239 |     2.5
  110 |   0.7055 |     21.771 |   0.9734 |     30.300 |     2.5
  111 |   0.7013 |     21.679 |   0.9717 |     30.055 |     2.5
  112 |   0.6972 |     21.397 |   0.9726 |     29.749 |     2.5
  113 |   0.6978 |     21.337 |   0.9765 |     29.994 |     2.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 710,050

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5429 |     67.880 |   1.9785 |     53.799 |     0.0
    2 |   1.7728 |     50.043 |   1.5817 |     45.466 |     0.1
    3 |   1.5132 |     46.229 |   1.4509 |     45.987 |     0.1
    4 |   1.4421 |     46.316 |   1.4196 |     45.466 |     0.1
    5 |   1.4188 |     46.229 |   1.4070 |     45.466 |     0.1
    6 |   1.4116 |     46.245 |   1.4018 |     45.466 |     0.2
    7 |   1.4045 |     46.337 |   1.3968 |     45.987 |     0.2
    8 |   1.4007 |     46.283 |   1.3915 |     45.466 |     0.2
    9 |   1.3980 |     46.245 |   1.3872 |     45.466 |     0.2
   10 |   1.3917 |     46.121 |   1.3857 |     45.496 |     0.3
   11 |   1.3799 |     45.638 |   1.3585 |     44.455 |     0.3
   12 |   1.3622 |     44.874 |   1.3408 |     44.118 |     0.3
   13 |   1.3462 |     44.858 |   1.3364 |     43.903 |     0.3
   14 |   1.3401 |     44.679 |   1.3315 |     44.026 |     0.4
   15 |   1.3269 |     44.701 |   1.3155 |     44.240 |     0.4
   16 |   1.3214 |     44.435 |   1.3083 |     43.964 |     0.4
   17 |   1.3142 |     44.278 |   1.3063 |     43.781 |     0.4
   18 |   1.3019 |     43.888 |   1.2928 |     43.045 |     0.5
   19 |   1.2917 |     43.211 |   1.2888 |     42.433 |     0.5
   20 |   1.2856 |     43.081 |   1.2783 |     41.850 |     0.5
   21 |   1.2758 |     42.712 |   1.2797 |     42.463 |     0.6
   22 |   1.2602 |     42.214 |   1.2748 |     42.555 |     0.6
   23 |   1.2561 |     42.387 |   1.2676 |     42.004 |     0.6
   24 |   1.2495 |     42.062 |   1.2616 |     41.850 |     0.6
   25 |   1.2467 |     41.965 |   1.2492 |     42.004 |     0.7
   26 |   1.2389 |     41.618 |   1.2497 |     42.188 |     0.7
   27 |   1.2239 |     41.282 |   1.2455 |     41.452 |     0.7
   28 |   1.2142 |     41.022 |   1.2340 |     40.380 |     0.7
   29 |   1.2054 |     40.485 |   1.2320 |     40.441 |     0.8
   30 |   1.1927 |     39.841 |   1.2327 |     39.982 |     0.8
   31 |   1.1839 |     40.236 |   1.2236 |     39.430 |     0.8
   32 |   1.1811 |     40.198 |   1.2225 |     40.074 |     0.8
   33 |   1.1761 |     40.160 |   1.2131 |     39.001 |     0.9
   34 |   1.1660 |     39.391 |   1.2089 |     39.093 |     0.9
   35 |   1.1622 |     39.082 |   1.1948 |     38.113 |     0.9
   36 |   1.1479 |     38.833 |   1.2075 |     39.246 |     0.9
   37 |   1.1436 |     38.865 |   1.1955 |     38.480 |     1.0
   38 |   1.1341 |     38.362 |   1.1967 |     39.369 |     1.0
   39 |   1.1344 |     38.291 |   1.1829 |     38.205 |     1.0
   40 |   1.1255 |     38.199 |   1.1720 |     37.868 |     1.0
   41 |   1.1184 |     37.841 |   1.1664 |     37.960 |     1.1
   42 |   1.1056 |     37.598 |   1.1694 |     38.051 |     1.1
   43 |   1.0993 |     37.354 |   1.1689 |     38.358 |     1.1
   44 |   1.0899 |     36.969 |   1.1860 |     38.909 |     1.2
   45 |   1.0956 |     37.294 |   1.1594 |     38.021 |     1.2
   46 |   1.0702 |     36.568 |   1.1569 |     38.725 |     1.2
   47 |   1.0595 |     36.129 |   1.1456 |     37.653 |     1.2
   48 |   1.0573 |     35.777 |   1.1452 |     37.714 |     1.3
   49 |   1.0523 |     35.744 |   1.1532 |     38.358 |     1.3
   50 |   1.0453 |     35.604 |   1.1608 |     38.327 |     1.3
   51 |   1.0415 |     35.555 |   1.1331 |     37.194 |     1.3
   52 |   1.0223 |     34.710 |   1.1261 |     36.826 |     1.4
   53 |   1.0086 |     33.821 |   1.1315 |     37.071 |     1.4
   54 |   1.0085 |     34.314 |   1.1301 |     36.949 |     1.4
   55 |   0.9982 |     33.881 |   1.1235 |     36.244 |     1.4
   56 |   0.9881 |     33.631 |   1.1216 |     36.765 |     1.5
   57 |   0.9854 |     33.274 |   1.1190 |     37.040 |     1.5
   58 |   0.9743 |     33.192 |   1.1125 |     36.397 |     1.5
   59 |   0.9692 |     32.943 |   1.1524 |     37.592 |     1.5
   60 |   0.9619 |     32.391 |   1.1126 |     36.121 |     1.6
   61 |   0.9495 |     32.331 |   1.1324 |     36.795 |     1.6
   62 |   0.9386 |     31.843 |   1.1077 |     36.550 |     1.6
   63 |   0.9258 |     31.637 |   1.1034 |     36.918 |     1.7
   64 |   0.9242 |     31.442 |   1.1068 |     35.999 |     1.7
   65 |   0.9128 |     31.128 |   1.0810 |     35.080 |     1.7
   66 |   0.8998 |     30.483 |   1.0957 |     35.386 |     1.7
   67 |   0.8983 |     30.267 |   1.0873 |     35.539 |     1.8
   68 |   0.8887 |     30.061 |   1.0896 |     35.570 |     1.8
   69 |   0.8866 |     29.996 |   1.0756 |     35.876 |     1.8
   70 |   0.8785 |     29.627 |   1.0778 |     34.436 |     1.8
   71 |   0.8703 |     29.568 |   1.0829 |     35.110 |     1.9
   72 |   0.8563 |     29.123 |   1.0846 |     34.681 |     1.9
   73 |   0.8440 |     28.749 |   1.0769 |     34.804 |     1.9
   74 |   0.8447 |     28.581 |   1.0686 |     34.620 |     1.9
   75 |   0.8429 |     28.885 |   1.0849 |     34.865 |     2.0
   76 |   0.8342 |     28.256 |   1.0718 |     34.589 |     2.0
   77 |   0.8298 |     28.061 |   1.0614 |     34.222 |     2.0
   78 |   0.8283 |     28.273 |   1.0564 |     33.487 |     2.0
   79 |   0.8293 |     27.861 |   1.0549 |     33.548 |     2.1
   80 |   0.8083 |     27.498 |   1.0443 |     33.303 |     2.1
   81 |   0.7967 |     27.140 |   1.0756 |     34.283 |     2.1
   82 |   0.7905 |     27.352 |   1.0471 |     33.058 |     2.1
   83 |   0.7912 |     26.707 |   1.0512 |     33.915 |     2.2
   84 |   0.7881 |     26.896 |   1.0496 |     33.548 |     2.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 1,391,138

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1696 |     59.655 |   1.5741 |     45.650 |     0.0
    2 |   1.4654 |     45.855 |   1.3951 |     44.424 |     0.1
    3 |   1.3706 |     44.614 |   1.3293 |     42.892 |     0.1
    4 |   1.3144 |     43.390 |   1.2779 |     42.034 |     0.2
    5 |   1.2661 |     41.883 |   1.2583 |     40.901 |     0.2
    6 |   1.2220 |     40.616 |   1.2157 |     38.603 |     0.3
    7 |   1.1778 |     39.125 |   1.1762 |     37.592 |     0.3
    8 |   1.1316 |     37.289 |   1.1400 |     37.316 |     0.3
    9 |   1.0943 |     36.227 |   1.1168 |     36.275 |     0.4
   10 |   1.0551 |     35.246 |   1.0712 |     35.202 |     0.4
   11 |   1.0087 |     33.474 |   1.0436 |     34.222 |     0.5
   12 |   0.9679 |     31.822 |   1.0242 |     33.456 |     0.5
   13 |   0.9327 |     30.841 |   1.0015 |     32.353 |     0.6
   14 |   0.9043 |     30.061 |   0.9860 |     32.904 |     0.6
   15 |   0.8688 |     28.804 |   0.9921 |     32.353 |     0.6
   16 |   0.8419 |     27.807 |   0.9509 |     31.036 |     0.7
   17 |   0.8182 |     27.205 |   0.9455 |     29.626 |     0.7
   18 |   0.7860 |     26.040 |   0.9377 |     30.362 |     0.8
   19 |   0.7873 |     26.479 |   0.9323 |     29.412 |     0.8
   20 |   0.7489 |     25.293 |   0.9245 |     29.963 |     0.9
   21 |   0.7118 |     23.651 |   0.9031 |     28.768 |     0.9
   22 |   0.6836 |     22.762 |   0.8888 |     28.339 |     1.0
   23 |   0.6563 |     22.031 |   0.9035 |     27.849 |     1.0
   24 |   0.6385 |     21.478 |   0.8840 |     27.512 |     1.0
   25 |   0.6147 |     20.638 |   0.9028 |     28.217 |     1.1
   26 |   0.5918 |     19.912 |   0.8884 |     27.022 |     1.1
   27 |   0.5824 |     19.679 |   0.8952 |     27.206 |     1.2
   28 |   0.5598 |     18.818 |   0.8993 |     27.512 |     1.2
   29 |   0.5395 |     18.260 |   0.8723 |     26.532 |     1.3
   30 |   0.5198 |     17.317 |   0.8739 |     26.103 |     1.3
   31 |   0.5105 |     17.333 |   0.8795 |     26.716 |     1.3
   32 |   0.5152 |     17.561 |   0.8861 |     26.930 |     1.4
   33 |   0.4838 |     16.260 |   0.8960 |     27.267 |     1.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 396,674

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4890 |     93.466 |   3.2810 |     83.303 |     0.0
    2 |   2.8747 |     81.469 |   2.6428 |     81.893 |     0.0
    3 |   2.4997 |     64.006 |   2.3675 |     58.824 |     0.1
    4 |   2.2806 |     57.857 |   2.1912 |     58.824 |     0.1
    5 |   2.1426 |     57.168 |   2.0773 |     58.824 |     0.1
    6 |   2.0456 |     56.345 |   1.9867 |     55.545 |     0.1
    7 |   1.9648 |     55.646 |   1.9105 |     53.768 |     0.1
    8 |   1.8929 |     53.191 |   1.8454 |     48.346 |     0.2
    9 |   1.8299 |     49.875 |   1.7860 |     48.346 |     0.2
   10 |   1.7715 |     49.084 |   1.7303 |     48.346 |     0.2
   11 |   1.7182 |     48.803 |   1.6818 |     48.346 |     0.2
   12 |   1.6756 |     48.678 |   1.6391 |     48.346 |     0.2
   13 |   1.6327 |     48.618 |   1.6022 |     48.346 |     0.2
   14 |   1.6005 |     47.340 |   1.5718 |     45.466 |     0.3
   15 |   1.5736 |     46.278 |   1.5460 |     45.466 |     0.3
   16 |   1.5495 |     46.213 |   1.5253 |     45.466 |     0.3
   17 |   1.5284 |     46.202 |   1.5080 |     45.466 |     0.3
   18 |   1.5113 |     46.213 |   1.4936 |     45.466 |     0.3
   19 |   1.4968 |     46.180 |   1.4813 |     45.466 |     0.4
   20 |   1.4878 |     46.251 |   1.4708 |     45.466 |     0.4
   21 |   1.4751 |     46.278 |   1.4620 |     45.466 |     0.4
   22 |   1.4669 |     46.261 |   1.4543 |     45.466 |     0.4
   23 |   1.4574 |     46.234 |   1.4468 |     45.466 |     0.4
   24 |   1.4518 |     46.223 |   1.4408 |     45.466 |     0.5
   25 |   1.4438 |     46.196 |   1.4346 |     45.466 |     0.5
   26 |   1.4399 |     46.202 |   1.4294 |     45.466 |     0.5
   27 |   1.4329 |     46.267 |   1.4233 |     45.466 |     0.5
   28 |   1.4264 |     46.484 |   1.4188 |     45.466 |     0.5
   29 |   1.4231 |     46.283 |   1.4130 |     45.466 |     0.6
   30 |   1.4173 |     46.278 |   1.4085 |     45.466 |     0.6
   31 |   1.4102 |     46.207 |   1.4033 |     45.466 |     0.6
   32 |   1.4058 |     46.294 |   1.3982 |     45.466 |     0.6
   33 |   1.4017 |     46.175 |   1.3920 |     45.190 |     0.6
   34 |   1.3937 |     45.920 |   1.3878 |     44.853 |     0.7
   35 |   1.3857 |     45.611 |   1.3786 |     44.884 |     0.7
   36 |   1.3764 |     45.362 |   1.3706 |     45.159 |     0.7
   37 |   1.3656 |     45.216 |   1.3622 |     44.669 |     0.7
   38 |   1.3559 |     45.308 |   1.3535 |     44.547 |     0.7
   39 |   1.3439 |     44.804 |   1.3419 |     44.455 |     0.7
   40 |   1.3322 |     44.441 |   1.3345 |     43.995 |     0.8
   41 |   1.3212 |     44.099 |   1.3280 |     43.873 |     0.8
   42 |   1.3151 |     43.937 |   1.3169 |     43.627 |     0.8
   43 |   1.3032 |     43.422 |   1.3125 |     43.719 |     0.8
   44 |   1.2937 |     43.281 |   1.3098 |     43.536 |     0.8
   45 |   1.2899 |     43.298 |   1.3047 |     43.352 |     0.9
   46 |   1.2802 |     42.832 |   1.2980 |     43.505 |     0.9
   47 |   1.2746 |     42.745 |   1.2986 |     43.168 |     0.9
   48 |   1.2694 |     42.452 |   1.2864 |     42.616 |     0.9
   49 |   1.2667 |     42.008 |   1.2864 |     42.249 |     0.9
   50 |   1.2554 |     41.748 |   1.2816 |     42.831 |     1.0
   51 |   1.2483 |     41.618 |   1.2759 |     42.555 |     1.0
   52 |   1.2451 |     41.753 |   1.2707 |     42.555 |     1.0
   53 |   1.2414 |     41.764 |   1.2681 |     42.525 |     1.0
   54 |   1.2366 |     41.396 |   1.2633 |     41.942 |     1.0
   55 |   1.2258 |     41.239 |   1.2646 |     42.402 |     1.0
   56 |   1.2246 |     41.044 |   1.2603 |     42.218 |     1.1
   57 |   1.2170 |     40.892 |   1.2551 |     41.820 |     1.1
   58 |   1.2141 |     40.849 |   1.2565 |     42.402 |     1.1
   59 |   1.2097 |     40.762 |   1.2561 |     41.789 |     1.1
   60 |   1.2051 |     40.767 |   1.2463 |     41.422 |     1.1
   61 |   1.1988 |     40.366 |   1.2481 |     41.238 |     1.2
   62 |   1.1936 |     40.345 |   1.2434 |     41.391 |     1.2
   63 |   1.1882 |     40.290 |   1.2401 |     41.268 |     1.2
   64 |   1.1866 |     40.318 |   1.2410 |     41.330 |     1.2
   65 |   1.1808 |     40.117 |   1.2382 |     41.330 |     1.2
   66 |   1.1804 |     40.030 |   1.2387 |     40.931 |     1.2
   67 |   1.1724 |     39.776 |   1.2384 |     40.993 |     1.3
   68 |   1.1716 |     39.602 |   1.2307 |     40.870 |     1.3
   69 |   1.1657 |     39.375 |   1.2324 |     40.962 |     1.3
   70 |   1.1617 |     39.429 |   1.2254 |     40.502 |     1.3
   71 |   1.1574 |     39.088 |   1.2253 |     41.360 |     1.3
   72 |   1.1622 |     39.180 |   1.2254 |     40.686 |     1.4
   73 |   1.1494 |     39.017 |   1.2215 |     40.257 |     1.4
   74 |   1.1492 |     39.196 |   1.2206 |     39.614 |     1.4
   75 |   1.1440 |     38.730 |   1.2147 |     39.982 |     1.4
   76 |   1.1389 |     38.562 |   1.2105 |     40.074 |     1.4
   77 |   1.1342 |     38.584 |   1.2118 |     40.165 |     1.4
   78 |   1.1294 |     38.253 |   1.2139 |     40.012 |     1.5
   79 |   1.1280 |     38.410 |   1.2103 |     39.277 |     1.5
   80 |   1.1248 |     38.194 |   1.2072 |     40.227 |     1.5
   81 |   1.1230 |     38.291 |   1.2086 |     40.196 |     1.5
   82 |   1.1192 |     38.123 |   1.2044 |     39.491 |     1.5
   83 |   1.1177 |     37.901 |   1.2029 |     40.104 |     1.6
   84 |   1.1143 |     37.663 |   1.2012 |     39.614 |     1.6
   85 |   1.1048 |     37.652 |   1.2025 |     39.706 |     1.6
   86 |   1.1098 |     37.885 |   1.1981 |     39.246 |     1.6
   87 |   1.1010 |     37.397 |   1.1966 |     39.491 |     1.6
   88 |   1.1020 |     37.397 |   1.1945 |     40.074 |     1.6
   89 |   1.0993 |     37.289 |   1.1914 |     39.645 |     1.7
   90 |   1.0939 |     37.115 |   1.1943 |     39.522 |     1.7
   91 |   1.0877 |     37.245 |   1.1914 |     39.491 |     1.7
   92 |   1.0861 |     36.980 |   1.1949 |     39.246 |     1.7
   93 |   1.0859 |     37.007 |   1.1940 |     39.706 |     1.7
   94 |   1.0777 |     36.644 |   1.1933 |     39.890 |     1.8
   95 |   1.0749 |     36.676 |   1.1906 |     39.338 |     1.8
   96 |   1.0753 |     36.617 |   1.1903 |     39.491 |     1.8
   97 |   1.0724 |     36.769 |   1.1908 |     39.583 |     1.8
   98 |   1.0708 |     36.492 |   1.1822 |     39.246 |     1.8
   99 |   1.0638 |     36.433 |   1.1794 |     39.400 |     1.8
  100 |   1.0604 |     36.265 |   1.1772 |     38.787 |     1.9
  101 |   1.0547 |     36.200 |   1.1856 |     39.583 |     1.9
  102 |   1.0530 |     36.080 |   1.1747 |     39.338 |     1.9
  103 |   1.0496 |     36.064 |   1.1741 |     39.338 |     1.9
  104 |   1.0432 |     35.690 |   1.1757 |     38.817 |     1.9
  105 |   1.0453 |     35.772 |   1.1746 |     38.725 |     2.0
  106 |   1.0420 |     35.609 |   1.1788 |     38.725 |     2.0
  107 |   1.0401 |     35.338 |   1.1726 |     38.542 |     2.0
  108 |   1.0343 |     35.371 |   1.1703 |     38.756 |     2.0
  109 |   1.0336 |     34.964 |   1.1689 |     38.511 |     2.0
  110 |   1.0292 |     35.046 |   1.1639 |     38.450 |     2.1
  111 |   1.0266 |     34.867 |   1.1634 |     38.419 |     2.1
  112 |   1.0241 |     34.818 |   1.1712 |     38.419 |     2.1
  113 |   1.0193 |     34.693 |   1.1737 |     39.124 |     2.1
  114 |   1.0149 |     34.319 |   1.1590 |     37.990 |     2.1
  115 |   1.0144 |     34.384 |   1.1624 |     38.511 |     2.1
  116 |   1.0097 |     34.428 |   1.1628 |     38.419 |     2.2
  117 |   1.0054 |     34.103 |   1.1580 |     38.082 |     2.2
  118 |   1.0051 |     34.336 |   1.1625 |     38.297 |     2.2
  119 |   1.0030 |     34.011 |   1.1605 |     38.297 |     2.2
  120 |   0.9981 |     33.946 |   1.1521 |     38.051 |     2.2
  121 |   0.9965 |     33.610 |   1.1582 |     38.082 |     2.3
  122 |   0.9916 |     33.886 |   1.1509 |     38.358 |     2.3
  123 |   0.9860 |     33.485 |   1.1550 |     38.113 |     2.3
  124 |   0.9880 |     33.713 |   1.1481 |     37.929 |     2.3
  125 |   0.9798 |     33.453 |   1.1514 |     37.898 |     2.3
  126 |   0.9817 |     33.371 |   1.1478 |     38.450 |     2.3
  127 |   0.9724 |     33.247 |   1.1467 |     37.929 |     2.4
  128 |   0.9766 |     33.285 |   1.1465 |     37.806 |     2.4
  129 |   0.9736 |     33.333 |   1.1464 |     37.837 |     2.4
  130 |   0.9710 |     33.149 |   1.1538 |     38.021 |     2.4
  131 |   0.9665 |     32.873 |   1.1457 |     37.347 |     2.4
  132 |   0.9670 |     32.781 |   1.1434 |     37.837 |     2.5
  133 |   0.9589 |     32.748 |   1.1445 |     37.623 |     2.5
  134 |   0.9568 |     32.634 |   1.1457 |     37.868 |     2.5
  135 |   0.9541 |     32.439 |   1.1449 |     37.929 |     2.5
  136 |   0.9499 |     32.174 |   1.1451 |     37.623 |     2.5
  137 |   0.9546 |     32.401 |   1.1408 |     37.439 |     2.5
  138 |   0.9454 |     32.223 |   1.1419 |     37.439 |     2.6
  139 |   0.9447 |     32.060 |   1.1355 |     38.021 |     2.6
  140 |   0.9421 |     32.174 |   1.1364 |     37.561 |     2.6
  141 |   0.9405 |     31.957 |   1.1318 |     37.194 |     2.6
  142 |   0.9395 |     31.979 |   1.1281 |     37.286 |     2.6
  143 |   0.9372 |     31.648 |   1.1396 |     37.040 |     2.7
  144 |   0.9399 |     32.098 |   1.1327 |     37.286 |     2.7
  145 |   0.9270 |     31.475 |   1.1350 |     37.194 |     2.7
  146 |   0.9315 |     31.437 |   1.1325 |     37.377 |     2.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 783,490

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.1573 |     58.518 |   1.5410 |     45.466 |     0.0
    2 |   1.4535 |     46.251 |   1.4047 |     45.466 |     0.0
    3 |   1.3876 |     46.229 |   1.3613 |     44.822 |     0.1
    4 |   1.3343 |     44.950 |   1.3104 |     43.137 |     0.1
    5 |   1.2776 |     42.902 |   1.2636 |     41.820 |     0.1
    6 |   1.2324 |     41.504 |   1.2269 |     41.299 |     0.1
    7 |   1.1909 |     39.721 |   1.2005 |     39.522 |     0.1
    8 |   1.1762 |     39.564 |   1.1696 |     38.419 |     0.1
    9 |   1.1194 |     37.728 |   1.1340 |     36.949 |     0.2
   10 |   1.0730 |     36.752 |   1.1093 |     36.857 |     0.2
   11 |   1.0372 |     35.208 |   1.0712 |     35.968 |     0.2
   12 |   1.0053 |     33.848 |   1.0406 |     34.467 |     0.2
   13 |   0.9583 |     32.385 |   1.0198 |     33.670 |     0.2
   14 |   0.9284 |     31.426 |   0.9962 |     33.272 |     0.2
   15 |   0.8805 |     29.606 |   0.9725 |     31.250 |     0.3
   16 |   0.8442 |     27.883 |   0.9711 |     32.016 |     0.3
   17 |   0.8120 |     26.810 |   0.9427 |     30.607 |     0.3
   18 |   0.7714 |     25.228 |   0.9365 |     29.871 |     0.3
   19 |   0.7468 |     24.393 |   0.9144 |     29.013 |     0.3
   20 |   0.7054 |     22.871 |   0.9267 |     29.565 |     0.4
   21 |   0.6892 |     22.421 |   0.9097 |     28.585 |     0.4
   22 |   0.6527 |     21.028 |   0.9129 |     28.186 |     0.4
   23 |   0.6254 |     20.075 |   0.9133 |     28.156 |     0.4
   24 |   0.6080 |     19.739 |   0.8908 |     26.930 |     0.4
   25 |   0.5768 |     18.866 |   0.8949 |     26.900 |     0.4
   26 |   0.5382 |     17.301 |   0.8911 |     26.991 |     0.5
   27 |   0.5238 |     17.181 |   0.9090 |     25.582 |     0.5
   28 |   0.5020 |     16.331 |   0.9020 |     26.685 |     0.5
   29 |   0.4926 |     15.962 |   0.8878 |     26.409 |     0.5
   30 |   0.4794 |     15.708 |   0.8981 |     25.888 |     0.5
   31 |   0.4516 |     14.721 |   0.9103 |     25.797 |     0.5
   32 |   0.4460 |     14.749 |   0.8918 |     25.398 |     0.6
   33 |   0.4302 |     14.120 |   0.8919 |     25.490 |     0.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,423,490

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3718 |     84.287 |   3.0328 |     81.893 |     0.1
    2 |   2.7816 |     81.339 |   2.6189 |     73.805 |     0.2
    3 |   2.4765 |     61.189 |   2.3808 |     58.824 |     0.2
    4 |   2.2851 |     58.924 |   2.2317 |     58.456 |     0.3
    5 |   2.1587 |     57.488 |   2.1200 |     57.169 |     0.4
    6 |   2.0648 |     55.088 |   2.0301 |     53.891 |     0.5
    7 |   1.9790 |     54.156 |   1.9488 |     53.707 |     0.6
    8 |   1.9084 |     50.358 |   1.8792 |     48.346 |     0.6
    9 |   1.8441 |     48.613 |   1.8178 |     48.346 |     0.7
   10 |   1.7876 |     48.613 |   1.7617 |     48.346 |     0.8
   11 |   1.7364 |     48.607 |   1.7100 |     48.346 |     0.9
   12 |   1.6883 |     48.607 |   1.6608 |     48.346 |     1.0
   13 |   1.6422 |     48.607 |   1.6170 |     48.346 |     1.0
   14 |   1.6043 |     47.340 |   1.5799 |     45.466 |     1.1
   15 |   1.5694 |     46.213 |   1.5492 |     45.466 |     1.2
   16 |   1.5414 |     46.213 |   1.5240 |     45.466 |     1.3
   17 |   1.5178 |     46.213 |   1.5044 |     45.466 |     1.4
   18 |   1.5004 |     46.213 |   1.4881 |     45.466 |     1.4
   19 |   1.4849 |     46.213 |   1.4747 |     45.466 |     1.5
   20 |   1.4713 |     46.213 |   1.4629 |     45.466 |     1.6
   21 |   1.4618 |     46.213 |   1.4525 |     45.466 |     1.7
   22 |   1.4492 |     46.207 |   1.4439 |     45.466 |     1.8
   23 |   1.4367 |     46.175 |   1.4353 |     45.435 |     1.8
   24 |   1.4293 |     46.050 |   1.4258 |     45.374 |     1.9
   25 |   1.4206 |     45.958 |   1.4185 |     45.190 |     2.0
   26 |   1.4133 |     45.579 |   1.4096 |     45.037 |     2.1
   27 |   1.4007 |     45.470 |   1.4024 |     44.945 |     2.2
   28 |   1.3922 |     45.357 |   1.3966 |     45.129 |     2.2
   29 |   1.3843 |     45.194 |   1.3888 |     44.730 |     2.3
   30 |   1.3724 |     45.037 |   1.3854 |     44.730 |     2.4
   31 |   1.3658 |     45.021 |   1.3787 |     44.700 |     2.5
   32 |   1.3540 |     44.695 |   1.3757 |     44.945 |     2.6
   33 |   1.3484 |     43.926 |   1.3701 |     44.577 |     2.6
   34 |   1.3387 |     43.655 |   1.3666 |     43.505 |     2.7
   35 |   1.3320 |     43.200 |   1.3609 |     44.118 |     2.8
   36 |   1.3216 |     42.615 |   1.3590 |     43.199 |     2.9
   37 |   1.3166 |     42.441 |   1.3544 |     42.616 |     3.0
   38 |   1.3081 |     42.192 |   1.3504 |     43.137 |     3.0
   39 |   1.2982 |     41.948 |   1.3491 |     42.923 |     3.1
   40 |   1.2922 |     41.683 |   1.3478 |     43.137 |     3.2
   41 |   1.2843 |     41.650 |   1.3410 |     42.494 |     3.3
   42 |   1.2761 |     41.222 |   1.3371 |     42.249 |     3.4
   43 |   1.2686 |     40.957 |   1.3309 |     41.636 |     3.4
   44 |   1.2586 |     40.496 |   1.3266 |     41.881 |     3.5
   45 |   1.2522 |     40.307 |   1.3226 |     41.789 |     3.6
   46 |   1.2484 |     39.938 |   1.3168 |     41.605 |     3.7
   47 |   1.2384 |     39.624 |   1.3146 |     41.544 |     3.8
   48 |   1.2304 |     39.250 |   1.3109 |     40.839 |     3.8
   49 |   1.2210 |     38.898 |   1.3067 |     41.023 |     3.9
   50 |   1.2150 |     38.800 |   1.3016 |     41.054 |     4.0
   51 |   1.2050 |     38.622 |   1.3032 |     40.901 |     4.1
   52 |   1.2039 |     38.427 |   1.2921 |     40.778 |     4.2
   53 |   1.1903 |     38.253 |   1.2923 |     40.411 |     4.2
   54 |   1.1803 |     37.879 |   1.2848 |     40.227 |     4.3
   55 |   1.1740 |     37.722 |   1.2864 |     40.533 |     4.4
   56 |   1.1657 |     37.511 |   1.2774 |     40.411 |     4.5
   57 |   1.1596 |     37.495 |   1.2754 |     39.583 |     4.6
   58 |   1.1534 |     37.294 |   1.2733 |     40.165 |     4.6
   59 |   1.1470 |     37.012 |   1.2704 |     39.614 |     4.7
   60 |   1.1367 |     36.763 |   1.2684 |     39.645 |     4.8
   61 |   1.1277 |     36.465 |   1.2647 |     39.553 |     4.9
   62 |   1.1213 |     36.303 |   1.2574 |     39.737 |     5.0
   63 |   1.1154 |     36.248 |   1.2547 |     39.920 |     5.0
   64 |   1.1073 |     35.994 |   1.2522 |     39.216 |     5.1
   65 |   1.0993 |     35.804 |   1.2420 |     39.277 |     5.2
   66 |   1.0930 |     35.587 |   1.2471 |     39.154 |     5.3
   67 |   1.0823 |     35.452 |   1.2436 |     39.491 |     5.4
   68 |   1.0752 |     35.387 |   1.2410 |     39.124 |     5.4
   69 |   1.0673 |     35.170 |   1.2386 |     38.971 |     5.5
   70 |   1.0626 |     34.650 |   1.2348 |     39.062 |     5.6
   71 |   1.0546 |     34.710 |   1.2331 |     38.634 |     5.7
   72 |   1.0449 |     34.628 |   1.2245 |     38.879 |     5.7
   73 |   1.0389 |     34.325 |   1.2254 |     38.971 |     5.8
   74 |   1.0307 |     34.141 |   1.2208 |     38.511 |     5.9
   75 |   1.0250 |     34.076 |   1.2216 |     38.695 |     6.0
   76 |   1.0179 |     33.539 |   1.2211 |     38.143 |     6.1
   77 |   1.0105 |     33.339 |   1.2204 |     37.960 |     6.1
   78 |   1.0069 |     33.420 |   1.2209 |     37.868 |     6.2
   79 |   0.9980 |     32.976 |   1.2115 |     38.235 |     6.3
   80 |   0.9927 |     32.889 |   1.2172 |     37.898 |     6.4
   81 |   0.9844 |     32.764 |   1.2048 |     37.898 |     6.5
   82 |   0.9759 |     32.223 |   1.2110 |     38.511 |     6.5
   83 |   0.9668 |     32.076 |   1.2114 |     37.960 |     6.6
   84 |   0.9632 |     31.838 |   1.2051 |     37.500 |     6.7
   85 |   0.9585 |     31.675 |   1.2066 |     37.439 |     6.8
   86 |   0.9497 |     31.329 |   1.2001 |     37.745 |     6.9
   87 |   0.9423 |     31.318 |   1.1944 |     37.040 |     6.9
   88 |   0.9366 |     30.776 |   1.1925 |     36.949 |     7.0
   89 |   0.9314 |     30.689 |   1.1900 |     36.673 |     7.1
   90 |   0.9247 |     30.386 |   1.1980 |     36.612 |     7.2
   91 |   0.9157 |     29.931 |   1.1884 |     36.275 |     7.3
   92 |   0.9083 |     29.584 |   1.1900 |     36.029 |     7.3
   93 |   0.9078 |     29.530 |   1.1958 |     36.305 |     7.4
   94 |   0.9027 |     29.156 |   1.1898 |     36.091 |     7.5
   95 |   0.8922 |     28.701 |   1.1872 |     36.029 |     7.6
   96 |   0.8853 |     28.603 |   1.1833 |     36.366 |     7.7
   97 |   0.8828 |     28.386 |   1.1849 |     35.723 |     7.7
   98 |   0.8742 |     28.126 |   1.1833 |     35.509 |     7.8
   99 |   0.8664 |     27.839 |   1.1815 |     35.570 |     7.9
  100 |   0.8612 |     27.660 |   1.1830 |     35.202 |     8.0
  101 |   0.8562 |     27.379 |   1.1779 |     35.018 |     8.1
  102 |   0.8504 |     27.232 |   1.1836 |     34.896 |     8.1
  103 |   0.8411 |     26.810 |   1.1808 |     34.743 |     8.2
  104 |   0.8397 |     26.761 |   1.1842 |     35.509 |     8.3
  105 |   0.8336 |     26.300 |   1.1778 |     34.651 |     8.4
  106 |   0.8274 |     26.430 |   1.1792 |     35.294 |     8.5
  107 |   0.8240 |     26.111 |   1.1865 |     35.325 |     8.5
  108 |   0.8140 |     25.813 |   1.1794 |     34.926 |     8.6
  109 |   0.8092 |     25.580 |   1.1785 |     34.252 |     8.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 440,642

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5620 |     68.065 |   2.0475 |     53.922 |     0.0
    2 |   1.8054 |     50.948 |   1.5991 |     45.833 |     0.0
    3 |   1.5164 |     46.180 |   1.4552 |     45.987 |     0.0
    4 |   1.4410 |     46.142 |   1.4188 |     45.466 |     0.1
    5 |   1.4192 |     46.278 |   1.4090 |     45.987 |     0.1
    6 |   1.4099 |     46.305 |   1.3995 |     45.987 |     0.1
    7 |   1.4008 |     46.261 |   1.3946 |     45.987 |     0.1
    8 |   1.3911 |     46.072 |   1.3837 |     44.792 |     0.1
    9 |   1.3734 |     44.918 |   1.3554 |     43.811 |     0.2
   10 |   1.3462 |     44.289 |   1.3297 |     43.076 |     0.2
   11 |   1.3253 |     43.986 |   1.3131 |     43.321 |     0.2
   12 |   1.3069 |     43.677 |   1.3000 |     42.770 |     0.2
   13 |   1.2856 |     43.173 |   1.2845 |     42.218 |     0.2
   14 |   1.2668 |     42.317 |   1.2677 |     41.759 |     0.2
   15 |   1.2459 |     41.840 |   1.2579 |     41.085 |     0.3
   16 |   1.2223 |     41.103 |   1.2392 |     40.472 |     0.3
   17 |   1.2039 |     40.269 |   1.2341 |     40.165 |     0.3
   18 |   1.1861 |     39.554 |   1.2294 |     40.411 |     0.3
   19 |   1.1640 |     38.790 |   1.2047 |     38.634 |     0.3
   20 |   1.1470 |     37.912 |   1.1903 |     38.113 |     0.3
   21 |   1.1365 |     37.565 |   1.2047 |     38.725 |     0.4
   22 |   1.1079 |     36.823 |   1.1684 |     37.714 |     0.4
   23 |   1.0824 |     35.912 |   1.1566 |     37.653 |     0.4
   24 |   1.0659 |     35.165 |   1.1357 |     36.183 |     0.4
   25 |   1.0533 |     34.450 |   1.1449 |     37.071 |     0.4
   26 |   1.0329 |     34.032 |   1.1138 |     36.458 |     0.4
   27 |   1.0096 |     33.718 |   1.1217 |     36.121 |     0.5
   28 |   0.9899 |     32.575 |   1.1016 |     35.631 |     0.5
   29 |   0.9662 |     31.648 |   1.0931 |     36.275 |     0.5
   30 |   0.9500 |     31.388 |   1.1002 |     35.141 |     0.5
   31 |   0.9480 |     31.394 |   1.0979 |     35.631 |     0.5
   32 |   0.9262 |     30.575 |   1.0836 |     35.263 |     0.5
   33 |   0.9126 |     30.012 |   1.0797 |     34.957 |     0.6
   34 |   0.8892 |     29.264 |   1.0703 |     34.069 |     0.6
   35 |   0.8761 |     28.907 |   1.0699 |     34.651 |     0.6
   36 |   0.8660 |     28.647 |   1.0786 |     34.896 |     0.6
   37 |   0.8405 |     27.715 |   1.0686 |     34.069 |     0.6
   38 |   0.8185 |     26.723 |   1.0650 |     33.854 |     0.6
   39 |   0.8130 |     26.858 |   1.0401 |     33.364 |     0.7
   40 |   0.7982 |     26.393 |   1.0495 |     33.241 |     0.7
   41 |   0.7882 |     26.100 |   1.0611 |     33.517 |     0.7
   42 |   0.7715 |     25.612 |   1.0493 |     32.996 |     0.7
   43 |   0.7614 |     25.141 |   1.0499 |     32.904 |     0.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 288,610

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5439 |     67.761 |   1.9694 |     53.278 |     0.0
    2 |   1.7381 |     49.518 |   1.5441 |     44.945 |     0.0
    3 |   1.4900 |     46.207 |   1.4362 |     45.987 |     0.0
    4 |   1.4226 |     46.229 |   1.3985 |     45.680 |     0.1
    5 |   1.3934 |     45.866 |   1.3754 |     44.577 |     0.1
    6 |   1.3654 |     45.562 |   1.3439 |     44.822 |     0.1
    7 |   1.3387 |     45.042 |   1.3186 |     43.199 |     0.1
    8 |   1.3160 |     44.717 |   1.3061 |     44.179 |     0.1
    9 |   1.2893 |     44.127 |   1.2823 |     43.260 |     0.1
   10 |   1.2659 |     43.634 |   1.2591 |     42.065 |     0.1
   11 |   1.2465 |     42.485 |   1.2372 |     40.962 |     0.2
   12 |   1.2173 |     41.407 |   1.2160 |     40.533 |     0.2
   13 |   1.1968 |     40.713 |   1.1998 |     39.675 |     0.2
   14 |   1.1758 |     40.231 |   1.1770 |     39.614 |     0.2
   15 |   1.1540 |     39.694 |   1.1567 |     39.400 |     0.2
   16 |   1.1290 |     38.676 |   1.1420 |     37.868 |     0.2
   17 |   1.1049 |     37.890 |   1.1192 |     38.143 |     0.2
   18 |   1.0856 |     37.142 |   1.1034 |     37.623 |     0.3
   19 |   1.0628 |     36.091 |   1.0837 |     36.336 |     0.3
   20 |   1.0373 |     35.430 |   1.0579 |     36.336 |     0.3
   21 |   1.0297 |     34.704 |   1.0508 |     35.784 |     0.3
   22 |   1.0047 |     34.032 |   1.0441 |     34.589 |     0.3
   23 |   0.9742 |     33.149 |   1.0427 |     33.946 |     0.3
   24 |   0.9562 |     32.266 |   1.0213 |     33.793 |     0.3
   25 |   0.9438 |     31.816 |   1.0148 |     33.793 |     0.4
   26 |   0.9215 |     30.955 |   0.9864 |     32.567 |     0.4
   27 |   0.9095 |     30.836 |   0.9892 |     32.445 |     0.4
   28 |   0.8909 |     29.746 |   0.9851 |     32.445 |     0.4
   29 |   0.8752 |     29.562 |   0.9890 |     32.843 |     0.4
   30 |   0.8653 |     29.378 |   0.9726 |     32.077 |     0.4
   31 |   0.8472 |     28.229 |   0.9691 |     32.077 |     0.4
   32 |   0.8325 |     27.785 |   0.9622 |     30.944 |     0.5
   33 |   0.8152 |     27.108 |   0.9684 |     31.740 |     0.5
   34 |   0.7998 |     26.615 |   0.9476 |     30.239 |     0.5
   35 |   0.7878 |     26.414 |   0.9706 |     30.944 |     0.5
   36 |   0.7667 |     25.412 |   0.9571 |     30.086 |     0.5
   37 |   0.7625 |     25.244 |   0.9576 |     30.208 |     0.5
   38 |   0.7491 |     24.810 |   0.9406 |     29.749 |     0.5
   39 |   0.7339 |     23.981 |   0.9222 |     28.799 |     0.6
   40 |   0.7240 |     24.084 |   0.9391 |     29.504 |     0.6
   41 |   0.7136 |     23.494 |   0.9179 |     28.217 |     0.6
   42 |   0.7013 |     23.011 |   0.9210 |     28.676 |     0.6
   43 |   0.6955 |     22.367 |   0.9080 |     27.574 |     0.6
   44 |   0.6883 |     22.648 |   0.9221 |     27.911 |     0.6
   45 |   0.6849 |     22.480 |   0.9113 |     27.696 |     0.6
   46 |   0.6609 |     21.478 |   0.9125 |     28.186 |     0.7
   47 |   0.6621 |     21.500 |   0.9026 |     28.064 |     0.7
   48 |   0.6377 |     20.319 |   0.9140 |     28.094 |     0.7
   49 |   0.6357 |     20.763 |   0.8996 |     27.574 |     0.7
   50 |   0.6190 |     20.075 |   0.9094 |     27.911 |     0.7
   51 |   0.6153 |     20.037 |   0.9077 |     26.961 |     0.7
   52 |   0.6088 |     19.826 |   0.9132 |     27.237 |     0.7
   53 |   0.6053 |     19.430 |   0.8974 |     27.696 |     0.8
   54 |   0.5876 |     19.110 |   0.8950 |     26.409 |     0.8
   55 |   0.5842 |     19.078 |   0.8876 |     26.317 |     0.8
   56 |   0.5752 |     18.834 |   0.8919 |     27.083 |     0.8
   57 |   0.5673 |     18.411 |   0.8650 |     26.317 |     0.8
   58 |   0.5625 |     18.498 |   0.8754 |     26.869 |     0.8
   59 |   0.5576 |     18.157 |   0.8780 |     26.808 |     0.8
   60 |   0.5520 |     18.103 |   0.8706 |     26.317 |     0.9
   61 |   0.5464 |     17.907 |   0.8816 |     25.674 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 189,794

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.9026 |     75.845 |   2.4076 |     58.824 |     0.0
    2 |   2.1641 |     57.802 |   1.9561 |     56.311 |     0.0
    3 |   1.8097 |     51.246 |   1.6664 |     48.346 |     0.0
    4 |   1.5931 |     46.484 |   1.5253 |     45.466 |     0.1
    5 |   1.4990 |     46.288 |   1.4656 |     45.466 |     0.1
    6 |   1.4566 |     46.256 |   1.4384 |     45.987 |     0.1
    7 |   1.4342 |     46.316 |   1.4195 |     45.466 |     0.1
    8 |   1.4204 |     46.251 |   1.4046 |     45.466 |     0.1
    9 |   1.4036 |     46.240 |   1.3939 |     45.466 |     0.1
   10 |   1.3920 |     46.196 |   1.3785 |     45.374 |     0.1
   11 |   1.3760 |     46.267 |   1.3649 |     45.987 |     0.2
   12 |   1.3606 |     45.925 |   1.3490 |     44.210 |     0.2
   13 |   1.3504 |     45.481 |   1.3502 |     44.516 |     0.2
   14 |   1.3404 |     45.172 |   1.3320 |     44.424 |     0.2
   15 |   1.3227 |     45.026 |   1.3251 |     44.056 |     0.2
   16 |   1.3184 |     44.706 |   1.3174 |     43.352 |     0.2
   17 |   1.3072 |     44.284 |   1.3093 |     43.321 |     0.2
   18 |   1.3005 |     44.230 |   1.3039 |     42.708 |     0.3
   19 |   1.2852 |     43.617 |   1.2931 |     43.076 |     0.3
   20 |   1.2738 |     43.211 |   1.2827 |     42.126 |     0.3
   21 |   1.2632 |     42.962 |   1.2644 |     42.218 |     0.3
   22 |   1.2501 |     42.837 |   1.2541 |     42.249 |     0.3
   23 |   1.2382 |     42.371 |   1.2553 |     42.586 |     0.3
   24 |   1.2248 |     41.916 |   1.2375 |     41.942 |     0.3
   25 |   1.2157 |     41.889 |   1.2311 |     41.544 |     0.4
   26 |   1.2137 |     41.862 |   1.2202 |     41.513 |     0.4
   27 |   1.1949 |     41.325 |   1.2222 |     41.483 |     0.4
   28 |   1.1867 |     40.783 |   1.2144 |     41.513 |     0.4
   29 |   1.1818 |     40.659 |   1.2042 |     41.391 |     0.4
   30 |   1.1723 |     40.068 |   1.2079 |     41.238 |     0.4
   31 |   1.1606 |     39.873 |   1.1958 |     40.104 |     0.4
   32 |   1.1477 |     39.212 |   1.1871 |     40.319 |     0.4
   33 |   1.1347 |     38.855 |   1.1884 |     39.920 |     0.5
   34 |   1.1306 |     38.692 |   1.1790 |     40.380 |     0.5
   35 |   1.1216 |     38.540 |   1.1756 |     40.135 |     0.5
   36 |   1.1117 |     38.150 |   1.1809 |     40.748 |     0.5
   37 |   1.1042 |     37.917 |   1.1582 |     38.695 |     0.5
   38 |   1.0958 |     37.522 |   1.1669 |     40.043 |     0.5
   39 |   1.0816 |     37.278 |   1.1635 |     39.706 |     0.5
   40 |   1.0800 |     37.045 |   1.1538 |     39.124 |     0.6
   41 |   1.0721 |     36.855 |   1.1475 |     38.664 |     0.6
   42 |   1.0598 |     36.541 |   1.1465 |     38.603 |     0.6
   43 |   1.0532 |     36.183 |   1.1488 |     39.093 |     0.6
   44 |   1.0540 |     36.189 |   1.1456 |     38.266 |     0.6
   45 |   1.0429 |     35.804 |   1.1494 |     38.450 |     0.6
   46 |   1.0324 |     35.652 |   1.1469 |     38.848 |     0.6
   47 |   1.0256 |     35.436 |   1.1445 |     38.879 |     0.7
   48 |   1.0128 |     34.775 |   1.1254 |     37.347 |     0.7
   49 |   1.0095 |     34.607 |   1.1206 |     37.439 |     0.7
   50 |   1.0029 |     34.504 |   1.1224 |     37.500 |     0.7
   51 |   0.9978 |     34.422 |   1.1229 |     38.235 |     0.7
   52 |   0.9924 |     34.043 |   1.1211 |     37.010 |     0.7
   53 |   0.9924 |     34.059 |   1.1221 |     38.388 |     0.7
   54 |   0.9874 |     34.146 |   1.1130 |     36.857 |     0.8
   55 |   0.9673 |     33.052 |   1.1104 |     37.347 |     0.8
   56 |   0.9580 |     32.938 |   1.1109 |     36.673 |     0.8
   57 |   0.9592 |     32.618 |   1.1144 |     36.612 |     0.8
   58 |   0.9524 |     32.710 |   1.1064 |     36.458 |     0.8
   59 |   0.9462 |     32.174 |   1.0989 |     36.305 |     0.8
   60 |   0.9376 |     32.044 |   1.1096 |     35.539 |     0.8
   61 |   0.9262 |     31.464 |   1.1006 |     36.397 |     0.8
   62 |   0.9324 |     31.394 |   1.0873 |     35.570 |     0.9
   63 |   0.9176 |     31.193 |   1.1027 |     35.999 |     0.9
   64 |   0.9146 |     30.781 |   1.1102 |     36.244 |     0.9
   65 |   0.9114 |     31.264 |   1.1076 |     35.570 |     0.9
   66 |   0.9029 |     30.543 |   1.0976 |     35.233 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 505,730

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2796 |     87.598 |   2.7615 |     83.333 |     0.0
    2 |   2.4882 |     66.119 |   2.2597 |     58.824 |     0.0
    3 |   2.1045 |     57.130 |   2.0125 |     54.197 |     0.0
    4 |   1.9292 |     52.899 |   1.8692 |     48.407 |     0.1
    5 |   1.8091 |     48.629 |   1.7592 |     48.346 |     0.1
    6 |   1.7145 |     48.613 |   1.6679 |     48.346 |     0.1
    7 |   1.6332 |     48.607 |   1.5885 |     48.346 |     0.1
    8 |   1.5637 |     46.771 |   1.5304 |     45.466 |     0.1
    9 |   1.5164 |     46.213 |   1.4926 |     45.466 |     0.1
   10 |   1.4836 |     46.213 |   1.4679 |     45.466 |     0.1
   11 |   1.4620 |     46.207 |   1.4506 |     45.987 |     0.1
   12 |   1.4468 |     46.245 |   1.4362 |     45.466 |     0.2
   13 |   1.4355 |     46.299 |   1.4239 |     45.466 |     0.2
   14 |   1.4218 |     46.305 |   1.4128 |     45.466 |     0.2
   15 |   1.4097 |     46.213 |   1.4037 |     45.466 |     0.2
   16 |   1.3984 |     46.213 |   1.3939 |     45.466 |     0.2
   17 |   1.3856 |     46.299 |   1.3841 |     45.588 |     0.2
   18 |   1.3774 |     46.359 |   1.3834 |     45.558 |     0.2
   19 |   1.3679 |     45.888 |   1.3734 |     45.374 |     0.3
   20 |   1.3611 |     45.590 |   1.3656 |     45.650 |     0.3
   21 |   1.3519 |     45.470 |   1.3605 |     45.098 |     0.3
   22 |   1.3460 |     45.438 |   1.3546 |     44.853 |     0.3
   23 |   1.3399 |     45.183 |   1.3512 |     44.822 |     0.3
   24 |   1.3337 |     45.161 |   1.3414 |     44.608 |     0.3
   25 |   1.3263 |     45.004 |   1.3381 |     44.455 |     0.3
   26 |   1.3250 |     44.956 |   1.3343 |     44.638 |     0.3
   27 |   1.3189 |     44.874 |   1.3293 |     44.056 |     0.4
   28 |   1.3133 |     44.332 |   1.3239 |     43.199 |     0.4
   29 |   1.3041 |     43.921 |   1.3193 |     43.199 |     0.4
   30 |   1.2934 |     43.780 |   1.3114 |     42.678 |     0.4
   31 |   1.2874 |     43.205 |   1.3056 |     42.371 |     0.4
   32 |   1.2806 |     42.924 |   1.2976 |     42.494 |     0.4
   33 |   1.2759 |     42.935 |   1.2973 |     42.586 |     0.4
   34 |   1.2672 |     42.528 |   1.2880 |     41.973 |     0.5
   35 |   1.2586 |     42.284 |   1.2831 |     42.065 |     0.5
   36 |   1.2508 |     42.111 |   1.2784 |     42.126 |     0.5
   37 |   1.2443 |     42.024 |   1.2735 |     41.697 |     0.5
   38 |   1.2401 |     41.851 |   1.2708 |     41.789 |     0.5
   39 |   1.2347 |     41.347 |   1.2640 |     41.667 |     0.5
   40 |   1.2276 |     41.287 |   1.2646 |     41.789 |     0.5
   41 |   1.2209 |     41.049 |   1.2589 |     41.881 |     0.5
   42 |   1.2131 |     40.789 |   1.2554 |     42.004 |     0.6
   43 |   1.2071 |     40.800 |   1.2495 |     41.605 |     0.6
   44 |   1.1964 |     40.475 |   1.2481 |     41.759 |     0.6
   45 |   1.1891 |     40.426 |   1.2442 |     41.238 |     0.6
   46 |   1.1864 |     40.215 |   1.2372 |     41.422 |     0.6
   47 |   1.1764 |     39.992 |   1.2326 |     41.146 |     0.6
   48 |   1.1711 |     40.074 |   1.2320 |     41.391 |     0.6
   49 |   1.1630 |     39.494 |   1.2244 |     40.962 |     0.7
   50 |   1.1532 |     39.023 |   1.2220 |     40.993 |     0.7
   51 |   1.1464 |     38.708 |   1.2171 |     40.656 |     0.7
   52 |   1.1378 |     38.448 |   1.2092 |     40.380 |     0.7
   53 |   1.1311 |     38.221 |   1.2040 |     40.502 |     0.7
   54 |   1.1202 |     37.847 |   1.2008 |     40.043 |     0.7
   55 |   1.1137 |     37.825 |   1.1935 |     40.135 |     0.7
   56 |   1.1069 |     37.327 |   1.1915 |     39.951 |     0.8
   57 |   1.1031 |     37.067 |   1.1817 |     39.583 |     0.8
   58 |   1.0906 |     36.644 |   1.1779 |     39.032 |     0.8
   59 |   1.0812 |     36.406 |   1.1759 |     39.185 |     0.8
   60 |   1.0751 |     36.124 |   1.1692 |     38.450 |     0.8
   61 |   1.0632 |     35.642 |   1.1632 |     38.817 |     0.8
   62 |   1.0565 |     35.549 |   1.1662 |     38.450 |     0.8
   63 |   1.0496 |     35.132 |   1.1607 |     37.929 |     0.8
   64 |   1.0394 |     34.704 |   1.1565 |     38.174 |     0.9
   65 |   1.0352 |     34.536 |   1.1524 |     37.561 |     0.9
   66 |   1.0256 |     34.086 |   1.1491 |     37.561 |     0.9
   67 |   1.0163 |     33.929 |   1.1442 |     37.714 |     0.9
   68 |   1.0082 |     33.539 |   1.1398 |     37.347 |     0.9
   69 |   0.9993 |     33.431 |   1.1389 |     37.347 |     0.9
   70 |   0.9892 |     32.857 |   1.1299 |     36.918 |     0.9
   71 |   0.9860 |     32.526 |   1.1307 |     37.010 |     1.0
   72 |   0.9750 |     32.082 |   1.1322 |     37.132 |     1.0
   73 |   0.9652 |     31.789 |   1.1292 |     37.439 |     1.0
   74 |   0.9617 |     31.746 |   1.1288 |     37.010 |     1.0
   75 |   0.9531 |     31.448 |   1.1226 |     36.734 |     1.0
   76 |   0.9433 |     30.684 |   1.1240 |     36.550 |     1.0
   77 |   0.9366 |     30.754 |   1.1095 |     36.336 |     1.0
   78 |   0.9267 |     30.261 |   1.1166 |     36.673 |     1.0
   79 |   0.9177 |     29.920 |   1.1257 |     36.397 |     1.1
   80 |   0.9098 |     29.470 |   1.1119 |     36.397 |     1.1
   81 |   0.9005 |     29.291 |   1.1084 |     35.784 |     1.1
   82 |   0.8955 |     29.010 |   1.1073 |     35.570 |     1.1
   83 |   0.8854 |     28.852 |   1.1029 |     36.336 |     1.1
   84 |   0.8776 |     28.571 |   1.0973 |     35.846 |     1.1
   85 |   0.8695 |     28.007 |   1.0996 |     35.080 |     1.1
   86 |   0.8618 |     27.823 |   1.0987 |     35.907 |     1.2
   87 |   0.8514 |     27.379 |   1.0930 |     35.080 |     1.2
   88 |   0.8464 |     27.406 |   1.0941 |     35.723 |     1.2
   89 |   0.8382 |     26.756 |   1.0916 |     35.447 |     1.2
   90 |   0.8284 |     26.317 |   1.0932 |     35.049 |     1.2
   91 |   0.8217 |     26.371 |   1.0938 |     34.896 |     1.2
   92 |   0.8143 |     25.813 |   1.0934 |     34.926 |     1.2
   93 |   0.8065 |     25.769 |   1.0855 |     34.559 |     1.2
   94 |   0.7989 |     25.406 |   1.0937 |     34.681 |     1.3
   95 |   0.7932 |     25.206 |   1.0857 |     34.681 |     1.3
   96 |   0.7829 |     24.881 |   1.0907 |     34.528 |     1.3
   97 |   0.7736 |     24.545 |   1.0881 |     34.528 |     1.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 398,754

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8897 |     77.845 |   2.4482 |     58.824 |     0.0
    2 |   2.2331 |     58.425 |   2.0480 |     58.824 |     0.0
    3 |   1.9258 |     53.354 |   1.7742 |     48.775 |     0.1
    4 |   1.6834 |     48.104 |   1.5791 |     45.466 |     0.1
    5 |   1.5430 |     46.419 |   1.4926 |     46.017 |     0.1
    6 |   1.4763 |     46.321 |   1.4447 |     45.466 |     0.1
    7 |   1.4425 |     46.245 |   1.4249 |     45.466 |     0.1
    8 |   1.4279 |     46.131 |   1.4145 |     45.987 |     0.1
    9 |   1.4188 |     46.299 |   1.4064 |     45.466 |     0.2
   10 |   1.4076 |     46.148 |   1.3940 |     45.987 |     0.2
   11 |   1.3913 |     46.131 |   1.3825 |     45.404 |     0.2
   12 |   1.3828 |     46.093 |   1.3731 |     45.343 |     0.2
   13 |   1.3695 |     46.077 |   1.3660 |     45.190 |     0.2
   14 |   1.3588 |     45.779 |   1.3545 |     45.037 |     0.3
   15 |   1.3437 |     45.606 |   1.3392 |     45.282 |     0.3
   16 |   1.3298 |     45.497 |   1.3159 |     44.363 |     0.3
   17 |   1.3082 |     44.744 |   1.3029 |     44.516 |     0.3
   18 |   1.3036 |     44.609 |   1.2998 |     44.210 |     0.3
   19 |   1.2903 |     44.219 |   1.2875 |     43.566 |     0.4
   20 |   1.2814 |     44.024 |   1.2797 |     43.260 |     0.4
   21 |   1.2765 |     43.747 |   1.2842 |     43.597 |     0.4
   22 |   1.2700 |     43.601 |   1.2786 |     43.413 |     0.4
   23 |   1.2586 |     43.205 |   1.2743 |     43.811 |     0.4
   24 |   1.2474 |     42.978 |   1.2638 |     43.260 |     0.4
   25 |   1.2458 |     43.130 |   1.2646 |     43.107 |     0.5
   26 |   1.2387 |     42.479 |   1.2587 |     42.708 |     0.5
   27 |   1.2342 |     42.436 |   1.2452 |     42.188 |     0.5
   28 |   1.2262 |     42.192 |   1.2453 |     42.494 |     0.5
   29 |   1.2206 |     41.938 |   1.2432 |     41.605 |     0.5
   30 |   1.2175 |     41.683 |   1.2364 |     41.575 |     0.5
   31 |   1.2091 |     41.369 |   1.2349 |     41.820 |     0.6
   32 |   1.2028 |     41.520 |   1.2223 |     40.839 |     0.6
   33 |   1.2018 |     41.217 |   1.2211 |     41.023 |     0.6
   34 |   1.1902 |     40.870 |   1.2192 |     41.023 |     0.6
   35 |   1.1905 |     40.843 |   1.2098 |     40.564 |     0.6
   36 |   1.1838 |     40.469 |   1.2087 |     40.380 |     0.6
   37 |   1.1778 |     40.187 |   1.1976 |     39.522 |     0.7
   38 |   1.1789 |     40.355 |   1.2040 |     39.951 |     0.7
   39 |   1.1711 |     39.667 |   1.1955 |     39.828 |     0.7
   40 |   1.1630 |     39.505 |   1.1914 |     39.798 |     0.7
   41 |   1.1606 |     39.651 |   1.1876 |     39.767 |     0.7
   42 |   1.1537 |     38.979 |   1.1870 |     39.614 |     0.8
   43 |   1.1608 |     39.342 |   1.1858 |     39.675 |     0.8
   44 |   1.1478 |     38.920 |   1.1827 |     39.246 |     0.8
   45 |   1.1403 |     38.318 |   1.1679 |     39.154 |     0.8
   46 |   1.1298 |     38.329 |   1.1707 |     38.787 |     0.8
   47 |   1.1211 |     37.365 |   1.1683 |     38.971 |     0.8
   48 |   1.1217 |     38.150 |   1.1692 |     39.277 |     0.9
   49 |   1.1150 |     37.744 |   1.1608 |     38.511 |     0.9
   50 |   1.1084 |     37.191 |   1.1483 |     38.603 |     0.9
   51 |   1.1018 |     37.213 |   1.1429 |     38.664 |     0.9
   52 |   1.0983 |     37.110 |   1.1476 |     38.971 |     0.9
   53 |   1.0900 |     36.915 |   1.1364 |     38.174 |     0.9
   54 |   1.0882 |     37.012 |   1.1475 |     39.308 |     1.0
   55 |   1.0866 |     36.481 |   1.1412 |     38.235 |     1.0
   56 |   1.0824 |     36.541 |   1.1468 |     38.450 |     1.0
   57 |   1.0763 |     36.340 |   1.1325 |     38.756 |     1.0
   58 |   1.0687 |     36.183 |   1.1340 |     38.664 |     1.0
   59 |   1.0680 |     35.983 |   1.1365 |     37.684 |     1.1
   60 |   1.0626 |     35.739 |   1.1348 |     38.174 |     1.1
   61 |   1.0546 |     35.484 |   1.1312 |     38.051 |     1.1
   62 |   1.0659 |     35.853 |   1.1291 |     37.868 |     1.1
   63 |   1.0465 |     35.148 |   1.1302 |     38.235 |     1.1
   64 |   1.0463 |     35.528 |   1.1203 |     38.450 |     1.1
   65 |   1.0401 |     35.148 |   1.1116 |     37.898 |     1.2
   66 |   1.0395 |     34.948 |   1.1024 |     37.102 |     1.2
   67 |   1.0385 |     34.872 |   1.1044 |     37.561 |     1.2
   68 |   1.0336 |     34.661 |   1.1087 |     38.174 |     1.2
   69 |   1.0201 |     34.531 |   1.1058 |     37.163 |     1.2
   70 |   1.0317 |     34.726 |   1.1052 |     37.102 |     1.2
   71 |   1.0185 |     34.390 |   1.0961 |     37.163 |     1.3
   72 |   1.0139 |     34.363 |   1.1049 |     37.653 |     1.3
   73 |   1.0032 |     33.821 |   1.0995 |     37.040 |     1.3
   74 |   0.9961 |     33.891 |   1.0981 |     36.673 |     1.3
   75 |   1.0017 |     33.848 |   1.1003 |     37.806 |     1.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,432,770

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.4952 |     66.672 |   1.9784 |     58.824 |     0.1
    2 |   1.7432 |     48.705 |   1.5556 |     45.466 |     0.2
    3 |   1.4944 |     46.213 |   1.4497 |     45.466 |     0.2
    4 |   1.4391 |     46.175 |   1.4204 |     45.987 |     0.3
    5 |   1.4166 |     46.381 |   1.4067 |     45.466 |     0.4
    6 |   1.4065 |     46.240 |   1.3972 |     45.466 |     0.5
    7 |   1.3967 |     45.828 |   1.3765 |     44.301 |     0.6
    8 |   1.3742 |     44.912 |   1.3575 |     44.240 |     0.6
    9 |   1.3592 |     44.706 |   1.3425 |     43.964 |     0.7
   10 |   1.3475 |     44.414 |   1.3442 |     43.781 |     0.8
   11 |   1.3399 |     44.267 |   1.3349 |     43.413 |     0.9
   12 |   1.3337 |     44.062 |   1.3195 |     43.352 |     1.0
   13 |   1.3214 |     44.002 |   1.3168 |     43.229 |     1.0
   14 |   1.3110 |     43.736 |   1.3118 |     43.015 |     1.1
   15 |   1.3033 |     43.590 |   1.3031 |     43.137 |     1.2
   16 |   1.2970 |     43.623 |   1.3002 |     42.402 |     1.3
   17 |   1.2964 |     43.325 |   1.2953 |     42.004 |     1.4
   18 |   1.2767 |     43.000 |   1.2888 |     42.923 |     1.4
   19 |   1.2783 |     43.195 |   1.2820 |     42.279 |     1.5
   20 |   1.2727 |     42.799 |   1.2669 |     41.881 |     1.6
   21 |   1.2630 |     42.707 |   1.2614 |     41.544 |     1.7
   22 |   1.2608 |     42.263 |   1.2635 |     41.820 |     1.8
   23 |   1.2509 |     42.181 |   1.2563 |     41.697 |     1.8
   24 |   1.2432 |     41.948 |   1.2504 |     41.483 |     1.9
   25 |   1.2394 |     41.656 |   1.2523 |     41.146 |     2.0
   26 |   1.2331 |     41.309 |   1.2393 |     40.962 |     2.1
   27 |   1.2268 |     41.249 |   1.2334 |     41.207 |     2.2
   28 |   1.2245 |     41.336 |   1.2375 |     41.176 |     2.2
   29 |   1.2122 |     41.000 |   1.2267 |     40.319 |     2.3
   30 |   1.2092 |     40.388 |   1.2272 |     40.411 |     2.4
   31 |   1.2057 |     40.513 |   1.2336 |     40.993 |     2.5
   32 |   1.1978 |     40.334 |   1.2259 |     40.288 |     2.6
   33 |   1.1997 |     40.426 |   1.2034 |     39.553 |     2.6
   34 |   1.1863 |     39.581 |   1.2154 |     39.951 |     2.7
   35 |   1.1764 |     39.304 |   1.2031 |     38.756 |     2.8
   36 |   1.1670 |     39.228 |   1.2009 |     39.216 |     2.9
   37 |   1.1615 |     39.055 |   1.1993 |     39.246 |     3.0
   38 |   1.1732 |     39.288 |   1.1896 |     38.848 |     3.1
   39 |   1.1451 |     38.578 |   1.1947 |     39.338 |     3.1
   40 |   1.1513 |     38.622 |   1.2058 |     38.940 |     3.2
   41 |   1.1435 |     38.324 |   1.2017 |     38.817 |     3.3
   42 |   1.1336 |     38.085 |   1.1924 |     39.369 |     3.4
   43 |   1.1216 |     37.619 |   1.1832 |     38.511 |     3.5
   44 |   1.1188 |     37.522 |   1.1765 |     38.787 |     3.5
   45 |   1.1168 |     37.305 |   1.1816 |     38.879 |     3.6
   46 |   1.1092 |     36.996 |   1.1685 |     38.787 |     3.7
   47 |   1.0975 |     36.915 |   1.1567 |     38.327 |     3.8
   48 |   1.1028 |     36.709 |   1.1606 |     37.929 |     3.9
   49 |   1.0906 |     36.476 |   1.1599 |     38.021 |     3.9
   50 |   1.0878 |     36.850 |   1.1517 |     37.561 |     4.0
   51 |   1.0844 |     36.471 |   1.1578 |     37.898 |     4.1
   52 |   1.0756 |     36.492 |   1.1590 |     38.603 |     4.2
   53 |   1.0822 |     36.725 |   1.1662 |     37.990 |     4.3
   54 |   1.0706 |     36.118 |   1.1627 |     37.990 |     4.3
   55 |   1.0724 |     36.270 |   1.1491 |     37.806 |     4.4
   56 |   1.0641 |     36.162 |   1.1530 |     38.787 |     4.5
   57 |   1.0783 |     36.357 |   1.1760 |     38.725 |     4.6
   58 |   1.0665 |     36.010 |   1.1466 |     37.531 |     4.7
   59 |   1.0560 |     35.273 |   1.1368 |     37.071 |     4.7
   60 |   1.0408 |     35.273 |   1.1396 |     37.960 |     4.8
   61 |   1.0395 |     34.932 |   1.1279 |     37.040 |     4.9
   62 |   1.0316 |     34.650 |   1.1386 |     36.918 |     5.0
   63 |   1.0348 |     35.100 |   1.1479 |     37.224 |     5.1
   64 |   1.0280 |     34.813 |   1.1445 |     37.439 |     5.1
   65 |   1.0390 |     35.241 |   1.1348 |     37.623 |     5.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 2,316,386

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8218 |     75.217 |   2.4066 |     59.436 |     0.1
    2 |   2.1862 |     58.192 |   2.0042 |     53.799 |     0.2
    3 |   1.8872 |     52.568 |   1.7612 |     48.376 |     0.3
    4 |   1.6892 |     47.269 |   1.6030 |     45.558 |     0.3
    5 |   1.5649 |     46.240 |   1.5142 |     45.496 |     0.4
    6 |   1.4942 |     46.283 |   1.4587 |     45.496 |     0.5
    7 |   1.4518 |     46.256 |   1.4333 |     45.987 |     0.6
    8 |   1.4349 |     46.153 |   1.4289 |     45.404 |     0.7
    9 |   1.4237 |     46.137 |   1.4095 |     45.925 |     0.8
   10 |   1.4115 |     45.969 |   1.4005 |     45.466 |     0.8
   11 |   1.4061 |     45.736 |   1.3951 |     44.853 |     0.9
   12 |   1.3984 |     45.524 |   1.3943 |     45.435 |     1.0
   13 |   1.3965 |     45.465 |   1.3813 |     44.792 |     1.1
   14 |   1.3856 |     45.010 |   1.3722 |     44.179 |     1.2
   15 |   1.3715 |     44.668 |   1.3581 |     43.903 |     1.3
   16 |   1.3608 |     44.441 |   1.3445 |     44.056 |     1.4
   17 |   1.3529 |     44.414 |   1.3428 |     43.873 |     1.4
   18 |   1.3517 |     44.262 |   1.3325 |     43.444 |     1.5
   19 |   1.3405 |     44.278 |   1.3265 |     43.352 |     1.6
   20 |   1.3270 |     43.829 |   1.3193 |     43.321 |     1.7
   21 |   1.3251 |     43.655 |   1.3145 |     42.494 |     1.8
   22 |   1.3153 |     43.276 |   1.3084 |     42.218 |     1.9
   23 |   1.3129 |     43.238 |   1.3032 |     42.096 |     2.0
   24 |   1.3019 |     43.070 |   1.2933 |     42.065 |     2.0
   25 |   1.2972 |     43.184 |   1.3021 |     42.249 |     2.1
   26 |   1.2915 |     42.870 |   1.2872 |     41.605 |     2.2
   27 |   1.2910 |     42.972 |   1.2795 |     41.820 |     2.3
   28 |   1.2841 |     42.485 |   1.2787 |     42.034 |     2.4
   29 |   1.2783 |     42.669 |   1.2697 |     41.544 |     2.5
   30 |   1.2725 |     42.566 |   1.2762 |     41.544 |     2.5
   31 |   1.2732 |     42.534 |   1.2647 |     41.575 |     2.6
   32 |   1.2692 |     42.534 |   1.2653 |     41.759 |     2.7
   33 |   1.2713 |     42.609 |   1.2591 |     41.176 |     2.8
   34 |   1.2560 |     42.295 |   1.2541 |     40.778 |     2.9
   35 |   1.2536 |     42.041 |   1.2485 |     41.391 |     3.0
   36 |   1.2538 |     42.349 |   1.2461 |     41.238 |     3.0
   37 |   1.2475 |     42.328 |   1.2488 |     41.268 |     3.1
   38 |   1.2435 |     41.889 |   1.2438 |     41.544 |     3.2
   39 |   1.2418 |     41.954 |   1.2389 |     40.502 |     3.3
   40 |   1.2436 |     42.057 |   1.2487 |     41.912 |     3.4
   41 |   1.2421 |     42.279 |   1.2400 |     41.238 |     3.5
   42 |   1.2302 |     41.829 |   1.2335 |     41.452 |     3.6
   43 |   1.2227 |     41.293 |   1.2436 |     40.686 |     3.6
   44 |   1.2325 |     41.775 |   1.2307 |     40.349 |     3.7
   45 |   1.2438 |     42.095 |   1.2477 |     41.636 |     3.8
   46 |   1.2291 |     41.883 |   1.2343 |     41.207 |     3.9
   47 |   1.2205 |     41.580 |   1.2181 |     39.675 |     4.0
   48 |   1.2071 |     41.244 |   1.2183 |     40.472 |     4.1
   49 |   1.2006 |     40.794 |   1.2169 |     40.502 |     4.1
   50 |   1.1960 |     40.903 |   1.2105 |     39.890 |     4.2
   51 |   1.1966 |     40.849 |   1.2251 |     40.380 |     4.3
   52 |   1.2005 |     40.578 |   1.2040 |     39.430 |     4.4
   53 |   1.1934 |     40.431 |   1.2016 |     39.645 |     4.5
   54 |   1.1894 |     40.350 |   1.1994 |     38.909 |     4.6
   55 |   1.1868 |     40.420 |   1.2006 |     39.522 |     4.6
   56 |   1.1850 |     40.025 |   1.1995 |     39.216 |     4.7
   57 |   1.1798 |     40.020 |   1.2031 |     39.767 |     4.8
   58 |   1.1710 |     39.808 |   1.1901 |     38.817 |     4.9
   59 |   1.1678 |     39.667 |   1.1857 |     38.603 |     5.0
   60 |   1.1651 |     39.407 |   1.1849 |     38.297 |     5.1
   61 |   1.1568 |     39.223 |   1.1968 |     39.308 |     5.1
   62 |   1.1620 |     39.250 |   1.1752 |     38.266 |     5.2
   63 |   1.1571 |     38.898 |   1.1869 |     38.297 |     5.3
   64 |   1.1512 |     38.676 |   1.1946 |     39.890 |     5.4
   65 |   1.1593 |     38.952 |   1.1854 |     38.603 |     5.5
   66 |   1.1571 |     38.784 |   1.1827 |     38.572 |     5.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,586,786

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2155 |     84.932 |   2.7177 |     81.893 |     0.0
    2 |   2.4603 |     65.177 |   2.2368 |     58.824 |     0.1
    3 |   2.1159 |     57.065 |   1.9992 |     53.768 |     0.1
    4 |   1.9360 |     54.178 |   1.8569 |     48.376 |     0.2
    5 |   1.8176 |     49.436 |   1.7494 |     48.346 |     0.2
    6 |   1.7221 |     48.748 |   1.6636 |     48.346 |     0.3
    7 |   1.6396 |     48.629 |   1.5916 |     48.346 |     0.3
    8 |   1.5758 |     47.611 |   1.5345 |     45.466 |     0.3
    9 |   1.5254 |     46.218 |   1.4950 |     45.466 |     0.4
   10 |   1.4914 |     46.218 |   1.4687 |     45.466 |     0.4
   11 |   1.4684 |     46.218 |   1.4500 |     45.466 |     0.5
   12 |   1.4540 |     46.223 |   1.4371 |     45.466 |     0.5
   13 |   1.4398 |     46.278 |   1.4266 |     45.466 |     0.6
   14 |   1.4317 |     46.218 |   1.4194 |     45.466 |     0.6
   15 |   1.4199 |     46.316 |   1.4114 |     45.466 |     0.6
   16 |   1.4120 |     46.305 |   1.4007 |     45.466 |     0.7
   17 |   1.4031 |     46.196 |   1.3885 |     45.466 |     0.7
   18 |   1.3923 |     46.240 |   1.3802 |     45.466 |     0.8
   19 |   1.3794 |     46.061 |   1.3708 |     45.404 |     0.8
   20 |   1.3686 |     45.785 |   1.3665 |     44.393 |     0.9
   21 |   1.3577 |     45.357 |   1.3550 |     44.455 |     0.9
   22 |   1.3509 |     45.281 |   1.3511 |     44.547 |     0.9
   23 |   1.3398 |     45.210 |   1.3405 |     44.301 |     1.0
   24 |   1.3315 |     45.221 |   1.3342 |     44.363 |     1.0
   25 |   1.3220 |     45.232 |   1.3280 |     44.148 |     1.1
   26 |   1.3129 |     44.923 |   1.3201 |     44.118 |     1.1
   27 |   1.3036 |     44.712 |   1.3136 |     44.179 |     1.2
   28 |   1.2950 |     44.397 |   1.3073 |     43.934 |     1.2
   29 |   1.2848 |     43.991 |   1.2971 |     43.229 |     1.2
   30 |   1.2753 |     43.455 |   1.2905 |     43.750 |     1.3
   31 |   1.2648 |     43.081 |   1.2852 |     43.290 |     1.3
   32 |   1.2533 |     42.702 |   1.2791 |     42.463 |     1.4
   33 |   1.2496 |     42.360 |   1.2695 |     42.096 |     1.4
   34 |   1.2400 |     42.041 |   1.2623 |     41.667 |     1.5
   35 |   1.2335 |     41.851 |   1.2594 |     42.004 |     1.5
   36 |   1.2244 |     41.596 |   1.2482 |     41.452 |     1.5
   37 |   1.2153 |     41.184 |   1.2482 |     41.391 |     1.6
   38 |   1.2077 |     40.979 |   1.2417 |     41.422 |     1.6
   39 |   1.2032 |     41.060 |   1.2362 |     40.839 |     1.7
   40 |   1.1920 |     40.502 |   1.2360 |     41.238 |     1.7
   41 |   1.1898 |     40.122 |   1.2279 |     41.513 |     1.8
   42 |   1.1834 |     40.090 |   1.2250 |     40.349 |     1.8
   43 |   1.1721 |     39.570 |   1.2181 |     40.043 |     1.8
   44 |   1.1664 |     39.191 |   1.2237 |     40.748 |     1.9
   45 |   1.1635 |     39.120 |   1.2125 |     40.043 |     1.9
   46 |   1.1536 |     38.730 |   1.2088 |     39.859 |     2.0
   47 |   1.1469 |     38.817 |   1.2003 |     40.104 |     2.0
   48 |   1.1419 |     38.443 |   1.1965 |     39.890 |     2.1
   49 |   1.1360 |     38.096 |   1.1890 |     39.216 |     2.1
   50 |   1.1281 |     38.313 |   1.1882 |     39.277 |     2.1
   51 |   1.1261 |     37.717 |   1.1895 |     39.461 |     2.2
   52 |   1.1169 |     37.652 |   1.1851 |     39.185 |     2.2
   53 |   1.1117 |     37.262 |   1.1916 |     39.675 |     2.3
   54 |   1.1069 |     37.153 |   1.1796 |     39.062 |     2.3
   55 |   1.1004 |     37.029 |   1.1721 |     38.664 |     2.3
   56 |   1.0947 |     36.460 |   1.1714 |     38.971 |     2.4
   57 |   1.0850 |     36.525 |   1.1639 |     38.419 |     2.4
   58 |   1.0809 |     36.275 |   1.1608 |     38.542 |     2.5
   59 |   1.0805 |     36.075 |   1.1608 |     38.817 |     2.5
   60 |   1.0681 |     35.875 |   1.1572 |     38.205 |     2.6
   61 |   1.0638 |     35.755 |   1.1560 |     38.664 |     2.6
   62 |   1.0559 |     35.398 |   1.1471 |     37.868 |     2.6
   63 |   1.0503 |     35.170 |   1.1545 |     38.787 |     2.7
   64 |   1.0497 |     35.176 |   1.1495 |     38.388 |     2.7
   65 |   1.0408 |     34.829 |   1.1459 |     38.143 |     2.8
   66 |   1.0404 |     34.726 |   1.1450 |     38.113 |     2.8
   67 |   1.0319 |     34.390 |   1.1357 |     37.500 |     2.9
   68 |   1.0231 |     34.390 |   1.1449 |     37.929 |     2.9
   69 |   1.0196 |     34.189 |   1.1418 |     38.327 |     2.9
   70 |   1.0171 |     34.032 |   1.1391 |     38.143 |     3.0
   71 |   1.0052 |     33.626 |   1.1346 |     37.837 |     3.0
   72 |   1.0027 |     33.534 |   1.1346 |     38.113 |     3.1
   73 |   0.9983 |     33.312 |   1.1277 |     37.255 |     3.1
   74 |   0.9963 |     33.117 |   1.1343 |     37.745 |     3.2
   75 |   0.9883 |     32.938 |   1.1345 |     37.255 |     3.2
   76 |   0.9819 |     32.591 |   1.1216 |     36.979 |     3.2
   77 |   0.9792 |     32.477 |   1.1236 |     36.673 |     3.3
   78 |   0.9788 |     32.136 |   1.1246 |     36.979 |     3.3
   79 |   0.9699 |     32.553 |   1.1307 |     36.887 |     3.4
   80 |   0.9645 |     32.044 |   1.1265 |     36.703 |     3.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 407,266

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7428 |     71.635 |   2.2530 |     58.885 |     0.0
    2 |   2.0601 |     55.792 |   1.8822 |     48.284 |     0.0
    3 |   1.7574 |     48.954 |   1.6248 |     48.100 |     0.1
    4 |   1.5635 |     46.597 |   1.4988 |     45.129 |     0.1
    5 |   1.4749 |     45.779 |   1.4416 |     45.374 |     0.1
    6 |   1.4311 |     45.617 |   1.4088 |     44.608 |     0.1
    7 |   1.4038 |     45.134 |   1.3904 |     44.547 |     0.1
    8 |   1.3782 |     44.620 |   1.3697 |     43.995 |     0.2
    9 |   1.3599 |     44.327 |   1.3503 |     44.301 |     0.2
   10 |   1.3338 |     43.390 |   1.3283 |     42.555 |     0.2
   11 |   1.3096 |     42.669 |   1.3111 |     42.371 |     0.2
   12 |   1.2869 |     41.564 |   1.2839 |     41.299 |     0.2
   13 |   1.2640 |     41.174 |   1.2723 |     40.870 |     0.3
   14 |   1.2390 |     40.718 |   1.2450 |     40.196 |     0.3
   15 |   1.2136 |     39.808 |   1.2321 |     39.920 |     0.3
   16 |   1.1908 |     39.510 |   1.2152 |     39.461 |     0.3
   17 |   1.1721 |     38.573 |   1.2018 |     39.001 |     0.3
   18 |   1.1544 |     38.291 |   1.1822 |     38.787 |     0.4
   19 |   1.1312 |     37.305 |   1.1753 |     38.572 |     0.4
   20 |   1.1141 |     37.067 |   1.1548 |     37.561 |     0.4
   21 |   1.0901 |     36.443 |   1.1558 |     37.868 |     0.4
   22 |   1.0664 |     35.268 |   1.1378 |     37.531 |     0.4
   23 |   1.0518 |     34.650 |   1.1290 |     36.642 |     0.5
   24 |   1.0420 |     34.092 |   1.1170 |     36.275 |     0.5
   25 |   1.0170 |     33.339 |   1.1124 |     35.325 |     0.5
   26 |   0.9962 |     32.488 |   1.1049 |     34.988 |     0.5
   27 |   0.9787 |     31.627 |   1.1030 |     35.846 |     0.5
   28 |   0.9638 |     31.171 |   1.0949 |     35.294 |     0.6
   29 |   0.9602 |     31.063 |   1.0812 |     34.773 |     0.6
   30 |   0.9462 |     30.342 |   1.0734 |     34.712 |     0.6
   31 |   0.9237 |     29.427 |   1.0833 |     35.263 |     0.6
   32 |   0.9030 |     28.999 |   1.0635 |     33.915 |     0.6
   33 |   0.8950 |     28.527 |   1.0553 |     33.732 |     0.7
   34 |   0.8871 |     28.484 |   1.0683 |     33.456 |     0.7
   35 |   0.8882 |     28.544 |   1.0407 |     32.751 |     0.7
   36 |   0.8667 |     27.818 |   1.0350 |     32.567 |     0.7
   37 |   0.8446 |     27.162 |   1.0300 |     32.629 |     0.7
   38 |   0.8344 |     26.598 |   1.0200 |     32.200 |     0.8
   39 |   0.8240 |     26.398 |   1.0268 |     32.353 |     0.8
   40 |   0.8156 |     26.040 |   1.0259 |     32.414 |     0.8
   41 |   0.8036 |     25.694 |   1.0213 |     31.526 |     0.8
   42 |   0.7907 |     25.265 |   1.0216 |     32.016 |     0.8
   43 |   0.7731 |     25.000 |   1.0108 |     31.924 |     0.8
   44 |   0.7732 |     24.778 |   1.0190 |     32.353 |     0.9
   45 |   0.7580 |     24.160 |   1.0100 |     31.924 |     0.9
   46 |   0.7521 |     23.949 |   1.0005 |     31.924 |     0.9
   47 |   0.7484 |     23.846 |   1.0011 |     31.801 |     0.9
   48 |   0.7309 |     23.364 |   1.0062 |     31.434 |     0.9
   49 |   0.7256 |     23.272 |   0.9952 |     31.158 |     1.0
   50 |   0.7187 |     22.968 |   1.0066 |     31.311 |     1.0
   51 |   0.7048 |     22.567 |   1.0062 |     30.147 |     1.0
   52 |   0.7053 |     22.686 |   0.9984 |     31.526 |     1.0
   53 |   0.6938 |     22.415 |   0.9974 |     31.434 |     1.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 503,586

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6096 |     69.652 |   2.1140 |     54.289 |     0.0
    2 |   1.8629 |     52.335 |   1.6267 |     45.466 |     0.0
    3 |   1.5364 |     46.283 |   1.4654 |     45.466 |     0.1
    4 |   1.4493 |     46.299 |   1.4228 |     45.466 |     0.1
    5 |   1.4160 |     46.169 |   1.3981 |     45.987 |     0.1
    6 |   1.3879 |     46.137 |   1.3651 |     45.312 |     0.1
    7 |   1.3542 |     45.308 |   1.3386 |     42.770 |     0.1
    8 |   1.3176 |     43.780 |   1.3038 |     42.249 |     0.2
    9 |   1.2800 |     42.707 |   1.2701 |     42.034 |     0.2
   10 |   1.2479 |     41.743 |   1.2465 |     40.870 |     0.2
   11 |   1.2220 |     41.271 |   1.2167 |     40.502 |     0.2
   12 |   1.1959 |     40.448 |   1.2032 |     40.686 |     0.2
   13 |   1.1757 |     39.846 |   1.1796 |     39.583 |     0.3
   14 |   1.1553 |     39.174 |   1.1561 |     38.113 |     0.3
   15 |   1.1315 |     38.351 |   1.1411 |     37.469 |     0.3
   16 |   1.1079 |     37.392 |   1.1288 |     37.316 |     0.3
   17 |   1.0855 |     36.536 |   1.1274 |     38.082 |     0.3
   18 |   1.0684 |     36.075 |   1.1237 |     37.653 |     0.4
   19 |   1.0492 |     35.333 |   1.0938 |     36.305 |     0.4
   20 |   1.0338 |     34.964 |   1.0738 |     35.662 |     0.4
   21 |   1.0161 |     34.227 |   1.0789 |     35.999 |     0.4
   22 |   0.9922 |     33.593 |   1.0561 |     34.804 |     0.4
   23 |   0.9718 |     32.678 |   1.0455 |     34.559 |     0.5
   24 |   0.9545 |     31.979 |   1.0189 |     33.640 |     0.5
   25 |   0.9365 |     31.627 |   1.0214 |     33.425 |     0.5
   26 |   0.9232 |     31.139 |   1.0097 |     33.701 |     0.5
   27 |   0.9009 |     29.893 |   1.0048 |     33.027 |     0.5
   28 |   0.8822 |     29.519 |   1.0076 |     32.843 |     0.6
   29 |   0.8718 |     29.096 |   0.9802 |     31.893 |     0.6
   30 |   0.8518 |     28.430 |   0.9789 |     31.740 |     0.6
   31 |   0.8438 |     27.530 |   0.9693 |     31.097 |     0.6
   32 |   0.8260 |     27.221 |   0.9737 |     31.679 |     0.6
   33 |   0.8022 |     26.560 |   0.9692 |     31.740 |     0.7
   34 |   0.7887 |     25.791 |   0.9703 |     31.036 |     0.7
   35 |   0.7697 |     25.217 |   0.9561 |     30.882 |     0.7
   36 |   0.7565 |     24.995 |   0.9483 |     31.189 |     0.7
   37 |   0.7522 |     24.827 |   0.9553 |     30.760 |     0.7
   38 |   0.7410 |     24.377 |   0.9291 |     30.331 |     0.8
   39 |   0.7095 |     23.201 |   0.9387 |     29.810 |     0.8
   40 |   0.7081 |     23.185 |   0.9350 |     30.116 |     0.8
   41 |   0.7124 |     23.212 |   0.9367 |     29.657 |     0.8
   42 |   0.6960 |     22.903 |   0.9512 |     29.688 |     0.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,745,506

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2504 |     79.736 |   2.8578 |     67.341 |     0.1
    2 |   2.6568 |     66.206 |   2.5194 |     62.194 |     0.1
    3 |   2.4066 |     59.596 |   2.3327 |     58.456 |     0.2
    4 |   2.2531 |     58.225 |   2.1998 |     57.353 |     0.3
    5 |   2.1435 |     57.000 |   2.0996 |     56.801 |     0.3
    6 |   2.0543 |     55.987 |   2.0130 |     54.534 |     0.4
    7 |   1.9758 |     55.185 |   1.9353 |     54.013 |     0.4
    8 |   1.8995 |     52.774 |   1.8576 |     48.315 |     0.5
    9 |   1.8258 |     49.019 |   1.7883 |     48.499 |     0.6
   10 |   1.7595 |     48.662 |   1.7248 |     48.315 |     0.6
   11 |   1.7011 |     48.705 |   1.6698 |     48.499 |     0.7
   12 |   1.6472 |     47.166 |   1.6209 |     45.588 |     0.8
   13 |   1.6040 |     46.283 |   1.5852 |     45.558 |     0.8
   14 |   1.5673 |     46.191 |   1.5475 |     45.466 |     0.9
   15 |   1.5362 |     46.169 |   1.5189 |     45.558 |     1.0
   16 |   1.5100 |     46.169 |   1.4955 |     45.466 |     1.0
   17 |   1.4891 |     46.104 |   1.4757 |     45.496 |     1.1
   18 |   1.4663 |     46.083 |   1.4574 |     45.527 |     1.1
   19 |   1.4457 |     45.860 |   1.4413 |     45.343 |     1.2
   20 |   1.4300 |     45.357 |   1.4305 |     45.006 |     1.3
   21 |   1.4146 |     45.015 |   1.4147 |     44.547 |     1.3
   22 |   1.3977 |     44.739 |   1.4035 |     44.026 |     1.4
   23 |   1.3869 |     44.452 |   1.3913 |     43.964 |     1.5
   24 |   1.3744 |     44.121 |   1.3814 |     43.505 |     1.5
   25 |   1.3626 |     43.839 |   1.3704 |     43.382 |     1.6
   26 |   1.3467 |     43.634 |   1.3587 |     43.015 |     1.6
   27 |   1.3326 |     43.336 |   1.3501 |     43.107 |     1.7
   28 |   1.3233 |     43.173 |   1.3396 |     42.800 |     1.8
   29 |   1.3088 |     42.696 |   1.3261 |     42.341 |     1.8
   30 |   1.2944 |     42.339 |   1.3215 |     42.371 |     1.9
   31 |   1.2808 |     41.661 |   1.3071 |     41.360 |     2.0
   32 |   1.2699 |     40.946 |   1.2962 |     41.391 |     2.0
   33 |   1.2565 |     40.290 |   1.2855 |     40.411 |     2.1
   34 |   1.2436 |     39.440 |   1.2754 |     40.074 |     2.1
   35 |   1.2296 |     38.941 |   1.2632 |     39.277 |     2.2
   36 |   1.2162 |     38.367 |   1.2526 |     38.909 |     2.3
   37 |   1.2015 |     37.896 |   1.2465 |     38.664 |     2.3
   38 |   1.1861 |     37.186 |   1.2328 |     37.531 |     2.4
   39 |   1.1738 |     36.622 |   1.2246 |     37.653 |     2.5
   40 |   1.1627 |     36.357 |   1.2125 |     36.949 |     2.5
   41 |   1.1491 |     35.566 |   1.2072 |     37.071 |     2.6
   42 |   1.1354 |     34.813 |   1.1951 |     36.121 |     2.6
   43 |   1.1227 |     34.455 |   1.1867 |     36.366 |     2.7
   44 |   1.1109 |     34.254 |   1.1800 |     35.692 |     2.8
   45 |   1.0980 |     33.848 |   1.1693 |     35.723 |     2.8
   46 |   1.0851 |     33.155 |   1.1626 |     35.172 |     2.9
   47 |   1.0718 |     32.754 |   1.1514 |     34.988 |     3.0
   48 |   1.0600 |     32.407 |   1.1524 |     35.447 |     3.0
   49 |   1.0502 |     31.876 |   1.1415 |     34.835 |     3.1
   50 |   1.0429 |     31.605 |   1.1343 |     34.835 |     3.1
   51 |   1.0276 |     31.329 |   1.1273 |     34.589 |     3.2
   52 |   1.0160 |     30.993 |   1.1235 |     34.957 |     3.3
   53 |   1.0030 |     30.380 |   1.1126 |     34.743 |     3.3
   54 |   0.9936 |     30.256 |   1.1078 |     34.099 |     3.4
   55 |   0.9835 |     29.806 |   1.1056 |     34.222 |     3.5
   56 |   0.9728 |     29.486 |   1.1003 |     34.191 |     3.5
   57 |   0.9641 |     29.280 |   1.0907 |     33.303 |     3.6
   58 |   0.9519 |     28.647 |   1.0874 |     34.069 |     3.6
   59 |   0.9418 |     28.744 |   1.0752 |     33.762 |     3.7
   60 |   0.9345 |     28.343 |   1.0752 |     33.333 |     3.8
   61 |   0.9247 |     27.991 |   1.0732 |     33.456 |     3.8
   62 |   0.9199 |     28.007 |   1.0639 |     33.609 |     3.9
   63 |   0.9055 |     27.742 |   1.0600 |     33.027 |     4.0
   64 |   0.8940 |     27.525 |   1.0564 |     33.241 |     4.0
   65 |   0.8864 |     26.940 |   1.0526 |     32.782 |     4.1
   66 |   0.8807 |     26.577 |   1.0496 |     32.874 |     4.1
   67 |   0.8707 |     26.360 |   1.0451 |     32.751 |     4.2
   68 |   0.8639 |     26.252 |   1.0408 |     33.211 |     4.3
   69 |   0.8528 |     25.970 |   1.0401 |     32.537 |     4.3
   70 |   0.8460 |     25.498 |   1.0304 |     32.629 |     4.4
   71 |   0.8353 |     25.271 |   1.0296 |     32.292 |     4.5
   72 |   0.8288 |     25.222 |   1.0316 |     32.843 |     4.5
   73 |   0.8212 |     24.951 |   1.0312 |     32.384 |     4.6
   74 |   0.8130 |     24.632 |   1.0218 |     31.924 |     4.6
   75 |   0.8039 |     24.512 |   1.0177 |     32.261 |     4.7
   76 |   0.7972 |     24.176 |   1.0150 |     31.832 |     4.8
   77 |   0.7949 |     23.949 |   1.0139 |     31.434 |     4.8
   78 |   0.7910 |     23.515 |   1.0110 |     31.679 |     4.9
   79 |   0.7759 |     23.337 |   1.0101 |     31.556 |     5.0
   80 |   0.7653 |     23.142 |   1.0141 |     31.311 |     5.0
   81 |   0.7592 |     22.974 |   1.0110 |     31.587 |     5.1
   82 |   0.7560 |     22.681 |   1.0056 |     31.342 |     5.1
   83 |   0.7449 |     22.356 |   1.0056 |     31.066 |     5.2
   84 |   0.7358 |     22.145 |   0.9978 |     30.974 |     5.3
   85 |   0.7314 |     21.998 |   0.9968 |     30.270 |     5.3
   86 |   0.7279 |     21.603 |   0.9987 |     30.944 |     5.4
   87 |   0.7218 |     21.749 |   0.9958 |     30.331 |     5.5
   88 |   0.7156 |     21.549 |   0.9930 |     30.852 |     5.5
   89 |   0.7085 |     20.980 |   0.9878 |     30.392 |     5.6
   90 |   0.7010 |     20.990 |   0.9855 |     30.392 |     5.7
   91 |   0.6934 |     20.709 |   0.9872 |     30.270 |     5.7
   92 |   0.6912 |     20.600 |   0.9856 |     30.270 |     5.8
   93 |   0.6855 |     20.562 |   0.9968 |     30.300 |     5.8
   94 |   0.6749 |     19.923 |   0.9919 |     30.116 |     5.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 726,146

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2268 |     84.666 |   2.7219 |     82.108 |     0.0
    2 |   2.4382 |     63.470 |   2.2054 |     58.517 |     0.0
    3 |   2.0817 |     56.144 |   1.9759 |     53.891 |     0.1
    4 |   1.9068 |     51.490 |   1.8262 |     48.346 |     0.1
    5 |   1.7743 |     48.813 |   1.7051 |     48.346 |     0.1
    6 |   1.6676 |     48.618 |   1.6132 |     48.346 |     0.1
    7 |   1.5918 |     47.004 |   1.5462 |     45.466 |     0.2
    8 |   1.5321 |     46.207 |   1.5029 |     45.466 |     0.2
    9 |   1.4979 |     46.213 |   1.4723 |     45.466 |     0.2
   10 |   1.4689 |     46.213 |   1.4477 |     45.466 |     0.2
   11 |   1.4453 |     46.218 |   1.4254 |     45.466 |     0.3
   12 |   1.4236 |     46.223 |   1.4079 |     45.466 |     0.3
   13 |   1.4067 |     46.218 |   1.3902 |     44.516 |     0.3
   14 |   1.3875 |     45.665 |   1.3765 |     44.148 |     0.3
   15 |   1.3712 |     45.124 |   1.3650 |     44.210 |     0.4
   16 |   1.3594 |     45.194 |   1.3559 |     44.087 |     0.4
   17 |   1.3465 |     45.091 |   1.3480 |     44.240 |     0.4
   18 |   1.3345 |     44.609 |   1.3351 |     43.290 |     0.4
   19 |   1.3238 |     44.240 |   1.3275 |     43.137 |     0.4
   20 |   1.3151 |     43.839 |   1.3195 |     42.984 |     0.5
   21 |   1.3045 |     43.829 |   1.3116 |     42.953 |     0.5
   22 |   1.2942 |     43.130 |   1.3098 |     42.616 |     0.5
   23 |   1.2821 |     42.853 |   1.2955 |     42.739 |     0.5
   24 |   1.2735 |     42.544 |   1.2906 |     42.218 |     0.6
   25 |   1.2630 |     42.230 |   1.2852 |     42.034 |     0.6
   26 |   1.2500 |     41.705 |   1.2764 |     42.034 |     0.6
   27 |   1.2414 |     41.612 |   1.2625 |     41.544 |     0.6
   28 |   1.2336 |     41.314 |   1.2577 |     41.942 |     0.7
   29 |   1.2156 |     40.930 |   1.2480 |     41.422 |     0.7
   30 |   1.2020 |     40.523 |   1.2375 |     40.380 |     0.7
   31 |   1.1899 |     40.301 |   1.2331 |     40.594 |     0.7
   32 |   1.1812 |     39.960 |   1.2294 |     40.717 |     0.7
   33 |   1.1697 |     39.608 |   1.2137 |     40.441 |     0.8
   34 |   1.1589 |     39.185 |   1.2104 |     40.686 |     0.8
   35 |   1.1488 |     39.039 |   1.2016 |     40.472 |     0.8
   36 |   1.1336 |     38.492 |   1.1926 |     39.553 |     0.8
   37 |   1.1217 |     38.410 |   1.1894 |     40.962 |     0.9
   38 |   1.1130 |     37.917 |   1.1787 |     39.277 |     0.9
   39 |   1.0980 |     37.332 |   1.1725 |     39.032 |     0.9
   40 |   1.0881 |     36.855 |   1.1636 |     38.143 |     0.9
   41 |   1.0751 |     36.389 |   1.1480 |     37.653 |     1.0
   42 |   1.0599 |     35.192 |   1.1432 |     37.469 |     1.0
   43 |   1.0485 |     34.878 |   1.1342 |     37.102 |     1.0
   44 |   1.0373 |     34.422 |   1.1322 |     37.132 |     1.0
   45 |   1.0253 |     33.886 |   1.1258 |     36.642 |     1.0
   46 |   1.0158 |     33.382 |   1.1131 |     36.520 |     1.1
   47 |   1.0011 |     33.214 |   1.1073 |     35.999 |     1.1
   48 |   0.9905 |     32.580 |   1.0937 |     35.447 |     1.1
   49 |   0.9771 |     32.228 |   1.0876 |     35.263 |     1.1
   50 |   0.9646 |     31.659 |   1.0916 |     35.570 |     1.2
   51 |   0.9556 |     31.139 |   1.0754 |     34.988 |     1.2
   52 |   0.9424 |     30.944 |   1.0765 |     35.294 |     1.2
   53 |   0.9318 |     30.239 |   1.0641 |     35.049 |     1.2
   54 |   0.9197 |     30.007 |   1.0557 |     34.804 |     1.3
   55 |   0.9103 |     29.291 |   1.0541 |     34.559 |     1.3
   56 |   0.9001 |     29.075 |   1.0444 |     34.069 |     1.3
   57 |   0.8892 |     28.777 |   1.0362 |     33.732 |     1.3
   58 |   0.8881 |     28.674 |   1.0315 |     33.915 |     1.3
   59 |   0.8733 |     28.061 |   1.0299 |     33.885 |     1.4
   60 |   0.8654 |     27.671 |   1.0192 |     33.395 |     1.4
   61 |   0.8532 |     27.249 |   1.0237 |     33.732 |     1.4
   62 |   0.8448 |     27.043 |   1.0245 |     33.670 |     1.4
   63 |   0.8345 |     26.680 |   1.0176 |     33.303 |     1.5
   64 |   0.8267 |     26.409 |   1.0003 |     32.874 |     1.5
   65 |   0.8181 |     25.997 |   1.0044 |     33.088 |     1.5
   66 |   0.8066 |     25.536 |   1.0016 |     32.843 |     1.5
   67 |   0.7992 |     25.244 |   0.9947 |     32.322 |     1.6
   68 |   0.7960 |     25.103 |   0.9971 |     32.292 |     1.6
   69 |   0.7855 |     25.141 |   0.9984 |     32.108 |     1.6
   70 |   0.7797 |     24.957 |   0.9831 |     31.587 |     1.6
   71 |   0.7702 |     24.567 |   0.9911 |     31.771 |     1.6
   72 |   0.7612 |     24.252 |   0.9843 |     31.893 |     1.7
   73 |   0.7538 |     23.678 |   0.9738 |     31.311 |     1.7
   74 |   0.7448 |     23.732 |   0.9886 |     32.077 |     1.7
   75 |   0.7397 |     23.521 |   0.9952 |     31.495 |     1.7
   76 |   0.7319 |     23.120 |   0.9725 |     31.036 |     1.8
   77 |   0.7197 |     23.098 |   0.9751 |     31.587 |     1.8
   78 |   0.7153 |     22.643 |   0.9691 |     31.066 |     1.8
   79 |   0.7104 |     22.491 |   0.9765 |     30.668 |     1.8
   80 |   0.7030 |     22.291 |   0.9705 |     30.729 |     1.9
   81 |   0.6964 |     22.155 |   0.9691 |     30.790 |     1.9
   82 |   0.6903 |     22.145 |   0.9614 |     30.699 |     1.9
   83 |   0.6829 |     21.646 |   0.9773 |     30.729 |     1.9
   84 |   0.6746 |     21.608 |   0.9694 |     30.453 |     1.9
   85 |   0.6733 |     21.359 |   0.9742 |     30.453 |     2.0
   86 |   0.6629 |     20.969 |   0.9608 |     30.576 |     2.0
   87 |   0.6551 |     20.573 |   0.9634 |     30.576 |     2.0
   88 |   0.6544 |     20.779 |   0.9586 |     30.239 |     2.0
   89 |   0.6474 |     20.400 |   0.9499 |     29.412 |     2.1
   90 |   0.6448 |     20.557 |   0.9612 |     29.534 |     2.1
   91 |   0.6313 |     20.053 |   0.9550 |     29.473 |     2.1
   92 |   0.6298 |     19.956 |   0.9687 |     29.871 |     2.1
   93 |   0.6253 |     19.766 |   0.9532 |     29.688 |     2.1
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 538,114

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.0610 |     56.583 |   1.4749 |     45.374 |     0.0
    2 |   1.3830 |     44.836 |   1.3146 |     43.964 |     0.0
    3 |   1.2546 |     41.130 |   1.1959 |     38.266 |     0.1
    4 |   1.1709 |     38.486 |   1.1406 |     37.347 |     0.1
    5 |   1.0859 |     35.609 |   1.0841 |     35.999 |     0.1
    6 |   1.0168 |     33.371 |   1.0394 |     33.885 |     0.1
    7 |   0.9610 |     31.448 |   1.0105 |     33.885 |     0.1
    8 |   0.8974 |     29.156 |   0.9553 |     31.495 |     0.1
    9 |   0.8439 |     27.211 |   0.9342 |     30.668 |     0.2
   10 |   0.7887 |     25.379 |   0.9088 |     29.473 |     0.2
   11 |   0.7390 |     23.971 |   0.8745 |     28.462 |     0.2
   12 |   0.6963 |     21.944 |   0.8740 |     28.248 |     0.2
   13 |   0.6538 |     20.974 |   0.8539 |     26.716 |     0.2
   14 |   0.6276 |     20.010 |   0.8439 |     26.746 |     0.3
   15 |   0.5883 |     18.568 |   0.8391 |     26.317 |     0.3
   16 |   0.5683 |     18.211 |   0.8113 |     25.521 |     0.3
   17 |   0.5332 |     17.116 |   0.8470 |     26.225 |     0.3
   18 |   0.5045 |     16.331 |   0.8115 |     24.969 |     0.3
   19 |   0.4779 |     15.383 |   0.8284 |     25.245 |     0.3
   20 |   0.4599 |     14.906 |   0.8035 |     24.173 |     0.4
   21 |   0.4322 |     14.293 |   0.8115 |     24.295 |     0.4
   22 |   0.4100 |     13.410 |   0.8145 |     24.908 |     0.4
   23 |   0.3974 |     12.933 |   0.8231 |     24.449 |     0.4
   24 |   0.3797 |     12.543 |   0.8266 |     24.173 |     0.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 483,202

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6079 |     69.343 |   2.0426 |     58.824 |     0.0
    2 |   1.7896 |     50.683 |   1.5969 |     45.466 |     0.0
    3 |   1.5239 |     46.240 |   1.4675 |     45.435 |     0.0
    4 |   1.4505 |     46.251 |   1.4299 |     45.466 |     0.1
    5 |   1.4264 |     46.402 |   1.4108 |     45.987 |     0.1
    6 |   1.4131 |     46.326 |   1.4011 |     45.987 |     0.1
    7 |   1.4038 |     46.288 |   1.3969 |     45.466 |     0.1
    8 |   1.3984 |     46.115 |   1.3897 |     45.435 |     0.1
    9 |   1.3900 |     46.104 |   1.3778 |     45.404 |     0.2
   10 |   1.3756 |     45.459 |   1.3617 |     44.179 |     0.2
   11 |   1.3601 |     44.918 |   1.3435 |     44.118 |     0.2
   12 |   1.3469 |     44.853 |   1.3404 |     44.669 |     0.2
   13 |   1.3360 |     44.831 |   1.3258 |     44.026 |     0.2
   14 |   1.3212 |     44.544 |   1.3247 |     44.485 |     0.2
   15 |   1.3160 |     44.105 |   1.3105 |     43.597 |     0.2
   16 |   1.3028 |     43.969 |   1.2951 |     42.984 |     0.3
   17 |   1.2885 |     43.411 |   1.2990 |     43.076 |     0.3
   18 |   1.2799 |     43.103 |   1.2774 |     41.452 |     0.3
   19 |   1.2662 |     42.626 |   1.2665 |     42.279 |     0.3
   20 |   1.2525 |     42.127 |   1.2561 |     41.789 |     0.3
   21 |   1.2363 |     41.878 |   1.2531 |     42.096 |     0.3
   22 |   1.2228 |     41.851 |   1.2296 |     41.452 |     0.4
   23 |   1.2030 |     41.271 |   1.2235 |     40.564 |     0.4
   24 |   1.1783 |     40.339 |   1.2041 |     40.104 |     0.4
   25 |   1.1708 |     39.960 |   1.2021 |     40.043 |     0.4
   26 |   1.1480 |     38.855 |   1.1732 |     38.756 |     0.4
   27 |   1.1225 |     37.988 |   1.1619 |     38.971 |     0.5
   28 |   1.0993 |     37.473 |   1.1524 |     38.388 |     0.5
   29 |   1.0831 |     36.704 |   1.1492 |     37.377 |     0.5
   30 |   1.0717 |     35.940 |   1.1435 |     37.531 |     0.5
   31 |   1.0519 |     35.539 |   1.1111 |     36.121 |     0.5
   32 |   1.0375 |     34.894 |   1.1145 |     35.938 |     0.5
   33 |   1.0174 |     34.162 |   1.0937 |     35.754 |     0.6
   34 |   1.0033 |     33.555 |   1.0991 |     36.336 |     0.6
   35 |   0.9836 |     33.144 |   1.1059 |     36.428 |     0.6
   36 |   0.9603 |     32.472 |   1.0892 |     35.110 |     0.6
   37 |   0.9644 |     32.472 |   1.0801 |     35.600 |     0.6
   38 |   0.9471 |     31.724 |   1.0782 |     35.110 |     0.6
   39 |   0.9341 |     31.562 |   1.0810 |     35.294 |     0.7
   40 |   0.9228 |     31.285 |   1.0746 |     35.141 |     0.7
   41 |   0.8991 |     30.267 |   1.0711 |     34.620 |     0.7
   42 |   0.8783 |     29.470 |   1.0462 |     34.314 |     0.7
   43 |   0.8766 |     29.020 |   1.0553 |     34.467 |     0.7
   44 |   0.8491 |     28.002 |   1.0439 |     33.333 |     0.7
   45 |   0.8396 |     28.040 |   1.0314 |     33.241 |     0.8
   46 |   0.8205 |     27.536 |   1.0491 |     33.364 |     0.8
   47 |   0.8088 |     26.783 |   1.0444 |     33.793 |     0.8
   48 |   0.7976 |     26.436 |   1.0488 |     33.640 |     0.8
   49 |   0.7796 |     25.948 |   1.0407 |     32.904 |     0.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 410,530

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4612 |     83.463 |   3.3585 |     81.863 |     0.0
    2 |   3.1849 |     81.491 |   3.0084 |     81.893 |     0.0
    3 |   2.8815 |     81.491 |   2.7806 |     81.893 |     0.1
    4 |   2.7002 |     79.378 |   2.6351 |     65.901 |     0.1
    5 |   2.5770 |     65.057 |   2.5276 |     66.912 |     0.1
    6 |   2.4869 |     62.782 |   2.4477 |     58.824 |     0.1
    7 |   2.4157 |     61.638 |   2.3853 |     58.824 |     0.1
    8 |   2.3613 |     60.511 |   2.3321 |     58.824 |     0.2
    9 |   2.3141 |     59.980 |   2.2865 |     58.824 |     0.2
   10 |   2.2696 |     59.319 |   2.2461 |     58.824 |     0.2
   11 |   2.2284 |     58.924 |   2.1974 |     58.824 |     0.2
   12 |   2.1823 |     58.382 |   2.1553 |     58.824 |     0.2
   13 |   2.1441 |     57.878 |   2.1168 |     58.824 |     0.3
   14 |   2.1096 |     57.483 |   2.0790 |     58.824 |     0.3
   15 |   2.0713 |     57.261 |   2.0399 |     58.824 |     0.3
   16 |   2.0288 |     56.475 |   1.9972 |     58.824 |     0.3
   17 |   1.9950 |     55.803 |   1.9608 |     58.241 |     0.3
   18 |   1.9611 |     54.470 |   1.9273 |     51.317 |     0.4
   19 |   1.9287 |     53.414 |   1.8966 |     48.407 |     0.4
   20 |   1.9008 |     52.086 |   1.8682 |     48.744 |     0.4
   21 |   1.8723 |     50.970 |   1.8406 |     48.346 |     0.4
   22 |   1.8423 |     50.141 |   1.8135 |     48.346 |     0.4
   23 |   1.8156 |     49.762 |   1.7868 |     48.346 |     0.5
   24 |   1.7889 |     49.502 |   1.7606 |     48.346 |     0.5
   25 |   1.7659 |     49.285 |   1.7364 |     48.346 |     0.5
   26 |   1.7419 |     48.927 |   1.7143 |     48.346 |     0.5
   27 |   1.7206 |     48.212 |   1.6943 |     46.140 |     0.5
   28 |   1.6997 |     47.600 |   1.6751 |     45.466 |     0.6
   29 |   1.6801 |     46.722 |   1.6572 |     45.466 |     0.6
   30 |   1.6621 |     46.608 |   1.6402 |     45.466 |     0.6
   31 |   1.6486 |     46.419 |   1.6241 |     45.466 |     0.6
   32 |   1.6287 |     46.408 |   1.6088 |     45.466 |     0.6
   33 |   1.6146 |     46.375 |   1.5943 |     45.466 |     0.7
   34 |   1.6018 |     46.305 |   1.5794 |     45.466 |     0.7
   35 |   1.5879 |     46.316 |   1.5644 |     45.466 |     0.7
   36 |   1.5747 |     46.283 |   1.5513 |     45.466 |     0.7
   37 |   1.5613 |     46.261 |   1.5397 |     45.466 |     0.7
   38 |   1.5447 |     46.267 |   1.5286 |     45.466 |     0.8
   39 |   1.5357 |     46.272 |   1.5178 |     45.466 |     0.8
   40 |   1.5276 |     46.245 |   1.5077 |     45.466 |     0.8
   41 |   1.5173 |     46.240 |   1.4990 |     45.466 |     0.8
   42 |   1.5078 |     46.240 |   1.4912 |     45.466 |     0.8
   43 |   1.4989 |     46.229 |   1.4843 |     45.466 |     0.9
   44 |   1.4921 |     46.213 |   1.4782 |     45.466 |     0.9
   45 |   1.4869 |     46.202 |   1.4721 |     45.466 |     0.9
   46 |   1.4806 |     46.218 |   1.4665 |     45.466 |     0.9
   47 |   1.4728 |     46.234 |   1.4615 |     45.466 |     0.9
   48 |   1.4660 |     46.191 |   1.4567 |     45.466 |     1.0
   49 |   1.4624 |     46.164 |   1.4523 |     45.466 |     1.0
   50 |   1.4569 |     46.251 |   1.4478 |     45.466 |     1.0
   51 |   1.4550 |     46.126 |   1.4437 |     45.466 |     1.0
   52 |   1.4481 |     46.196 |   1.4401 |     45.466 |     1.0
   53 |   1.4435 |     46.251 |   1.4359 |     45.466 |     1.1
   54 |   1.4374 |     46.191 |   1.4329 |     45.466 |     1.1
   55 |   1.4363 |     46.267 |   1.4296 |     45.466 |     1.1
   56 |   1.4299 |     46.191 |   1.4267 |     45.466 |     1.1
   57 |   1.4260 |     46.213 |   1.4231 |     45.466 |     1.1
   58 |   1.4208 |     46.180 |   1.4203 |     45.466 |     1.2
   59 |   1.4167 |     46.142 |   1.4180 |     45.466 |     1.2
   60 |   1.4176 |     46.072 |   1.4157 |     45.466 |     1.2
   61 |   1.4115 |     45.974 |   1.4124 |     45.312 |     1.2
   62 |   1.4098 |     45.860 |   1.4099 |     45.251 |     1.2
   63 |   1.4042 |     45.730 |   1.4072 |     45.251 |     1.3
   64 |   1.3989 |     45.687 |   1.4049 |     45.221 |     1.3
   65 |   1.3932 |     45.535 |   1.4041 |     45.129 |     1.3
   66 |   1.3896 |     45.389 |   1.4004 |     45.190 |     1.3
   67 |   1.3883 |     45.292 |   1.3958 |     45.159 |     1.3
   68 |   1.3830 |     45.248 |   1.3950 |     45.098 |     1.4
   69 |   1.3782 |     45.048 |   1.3931 |     45.190 |     1.4
   70 |   1.3776 |     45.037 |   1.3911 |     45.129 |     1.4
   71 |   1.3700 |     44.918 |   1.3872 |     44.853 |     1.4
   72 |   1.3669 |     44.798 |   1.3879 |     45.251 |     1.4
   73 |   1.3643 |     44.798 |   1.3830 |     45.159 |     1.4
   74 |   1.3611 |     44.473 |   1.3817 |     45.037 |     1.5
   75 |   1.3572 |     44.446 |   1.3802 |     44.945 |     1.5
   76 |   1.3561 |     44.528 |   1.3780 |     45.221 |     1.5
   77 |   1.3508 |     44.468 |   1.3773 |     45.312 |     1.5
   78 |   1.3483 |     44.522 |   1.3742 |     44.638 |     1.5
   79 |   1.3420 |     44.311 |   1.3782 |     45.404 |     1.6
   80 |   1.3388 |     44.322 |   1.3747 |     44.822 |     1.6
   81 |   1.3344 |     44.257 |   1.3731 |     45.282 |     1.6
   82 |   1.3330 |     44.175 |   1.3686 |     45.190 |     1.6
   83 |   1.3326 |     44.430 |   1.3698 |     44.516 |     1.6
   84 |   1.3261 |     44.267 |   1.3677 |     44.761 |     1.7
   85 |   1.3227 |     44.202 |   1.3657 |     44.700 |     1.7
   86 |   1.3220 |     44.045 |   1.3627 |     44.577 |     1.7
   87 |   1.3157 |     44.045 |   1.3618 |     45.129 |     1.7
   88 |   1.3158 |     44.034 |   1.3634 |     44.547 |     1.7
   89 |   1.3142 |     44.089 |   1.3604 |     44.516 |     1.8
   90 |   1.3111 |     43.899 |   1.3605 |     44.547 |     1.8
   91 |   1.3073 |     43.796 |   1.3575 |     44.638 |     1.8
   92 |   1.3032 |     43.704 |   1.3580 |     44.669 |     1.8
   93 |   1.3010 |     43.801 |   1.3565 |     44.638 |     1.8
   94 |   1.2971 |     43.720 |   1.3559 |     44.608 |     1.9
   95 |   1.2940 |     43.677 |   1.3548 |     44.455 |     1.9
   96 |   1.2940 |     43.606 |   1.3550 |     44.577 |     1.9
   97 |   1.2909 |     43.520 |   1.3556 |     44.485 |     1.9
   98 |   1.2895 |     43.541 |   1.3509 |     44.485 |     1.9
   99 |   1.2835 |     43.200 |   1.3530 |     44.118 |     2.0
  100 |   1.2835 |     43.363 |   1.3491 |     44.118 |     2.0
  101 |   1.2801 |     43.070 |   1.3502 |     44.332 |     2.0
  102 |   1.2771 |     43.000 |   1.3488 |     44.455 |     2.0
  103 |   1.2753 |     42.929 |   1.3495 |     44.485 |     2.0
  104 |   1.2736 |     43.140 |   1.3464 |     44.179 |     2.1
  105 |   1.2697 |     42.810 |   1.3456 |     44.271 |     2.1
  106 |   1.2686 |     42.631 |   1.3444 |     44.424 |     2.1
  107 |   1.2704 |     42.745 |   1.3409 |     44.271 |     2.1
  108 |   1.2611 |     42.582 |   1.3387 |     43.934 |     2.1
  109 |   1.2618 |     42.311 |   1.3423 |     44.026 |     2.2
  110 |   1.2577 |     42.274 |   1.3449 |     44.056 |     2.2
  111 |   1.2581 |     42.555 |   1.3433 |     43.903 |     2.2
  112 |   1.2544 |     42.100 |   1.3439 |     43.566 |     2.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 879,202

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2394 |     73.461 |   2.8601 |     67.096 |     0.0
    2 |   2.6653 |     66.856 |   2.5131 |     64.737 |     0.1
    3 |   2.3977 |     60.398 |   2.3019 |     58.517 |     0.1
    4 |   2.2300 |     57.911 |   2.1637 |     57.966 |     0.1
    5 |   2.1056 |     57.206 |   2.0436 |     55.515 |     0.2
    6 |   1.9988 |     53.554 |   1.9375 |     49.969 |     0.2
    7 |   1.8951 |     49.995 |   1.8365 |     48.192 |     0.2
    8 |   1.8025 |     49.236 |   1.7507 |     48.162 |     0.3
    9 |   1.7256 |     48.743 |   1.6793 |     48.192 |     0.3
   10 |   1.6625 |     48.564 |   1.6203 |     48.162 |     0.4
   11 |   1.6087 |     48.347 |   1.5735 |     48.070 |     0.4
   12 |   1.5672 |     47.529 |   1.5377 |     45.312 |     0.4
   13 |   1.5279 |     46.169 |   1.5025 |     45.129 |     0.5
   14 |   1.4973 |     45.871 |   1.4761 |     45.006 |     0.5
   15 |   1.4735 |     45.600 |   1.4544 |     45.006 |     0.5
   16 |   1.4493 |     45.384 |   1.4355 |     44.884 |     0.6
   17 |   1.4251 |     45.118 |   1.4146 |     44.730 |     0.6
   18 |   1.4082 |     44.652 |   1.4017 |     44.638 |     0.6
   19 |   1.3926 |     44.522 |   1.3849 |     44.301 |     0.7
   20 |   1.3786 |     44.159 |   1.3740 |     43.474 |     0.7
   21 |   1.3588 |     43.498 |   1.3606 |     43.781 |     0.7
   22 |   1.3437 |     43.027 |   1.3468 |     43.015 |     0.8
   23 |   1.3307 |     42.566 |   1.3361 |     42.862 |     0.8
   24 |   1.3174 |     42.339 |   1.3267 |     42.739 |     0.8
   25 |   1.3089 |     42.084 |   1.3132 |     41.605 |     0.9
   26 |   1.2904 |     41.537 |   1.3061 |     42.218 |     0.9
   27 |   1.2791 |     41.071 |   1.2906 |     41.360 |     0.9
   28 |   1.2679 |     40.637 |   1.2799 |     40.564 |     1.0
   29 |   1.2578 |     40.442 |   1.2735 |     40.809 |     1.0
   30 |   1.2433 |     40.388 |   1.2647 |     40.165 |     1.1
   31 |   1.2309 |     39.759 |   1.2552 |     40.043 |     1.1
   32 |   1.2177 |     39.461 |   1.2446 |     39.675 |     1.1
   33 |   1.2070 |     39.109 |   1.2435 |     39.951 |     1.2
   34 |   1.1921 |     38.627 |   1.2273 |     39.185 |     1.2
   35 |   1.1827 |     38.562 |   1.2195 |     39.124 |     1.2
   36 |   1.1690 |     38.026 |   1.2111 |     39.093 |     1.3
   37 |   1.1573 |     37.451 |   1.2052 |     38.603 |     1.3
   38 |   1.1484 |     37.392 |   1.1975 |     38.358 |     1.3
   39 |   1.1366 |     37.240 |   1.1891 |     38.787 |     1.4
   40 |   1.1266 |     36.563 |   1.1793 |     37.929 |     1.4
   41 |   1.1157 |     36.438 |   1.1708 |     37.837 |     1.4
   42 |   1.1041 |     35.560 |   1.1650 |     37.776 |     1.5
   43 |   1.0902 |     35.354 |   1.1548 |     36.979 |     1.5
   44 |   1.0822 |     34.829 |   1.1459 |     36.765 |     1.5
   45 |   1.0732 |     34.552 |   1.1398 |     36.581 |     1.6
   46 |   1.0593 |     34.428 |   1.1359 |     36.857 |     1.6
   47 |   1.0484 |     33.897 |   1.1292 |     36.458 |     1.6
   48 |   1.0369 |     33.442 |   1.1217 |     35.600 |     1.7
   49 |   1.0281 |     32.965 |   1.1159 |     36.121 |     1.7
   50 |   1.0174 |     32.147 |   1.1064 |     35.723 |     1.7
   51 |   1.0062 |     31.963 |   1.0996 |     34.681 |     1.8
   52 |   0.9977 |     31.399 |   1.0929 |     35.018 |     1.8
   53 |   0.9835 |     30.695 |   1.0840 |     34.191 |     1.9
   54 |   0.9759 |     30.581 |   1.0854 |     34.406 |     1.9
   55 |   0.9639 |     30.028 |   1.0690 |     33.824 |     1.9
   56 |   0.9552 |     29.763 |   1.0669 |     33.119 |     2.0
   57 |   0.9429 |     29.188 |   1.0510 |     32.966 |     2.0
   58 |   0.9364 |     28.777 |   1.0540 |     33.027 |     2.0
   59 |   0.9246 |     28.782 |   1.0425 |     32.659 |     2.1
   60 |   0.9142 |     28.105 |   1.0417 |     32.598 |     2.1
   61 |   0.9050 |     27.834 |   1.0384 |     32.322 |     2.1
   62 |   0.8967 |     27.601 |   1.0287 |     32.353 |     2.2
   63 |   0.8881 |     27.314 |   1.0194 |     32.261 |     2.2
   64 |   0.8795 |     26.896 |   1.0213 |     31.648 |     2.2
   65 |   0.8756 |     26.907 |   1.0160 |     31.955 |     2.3
   66 |   0.8606 |     26.219 |   1.0104 |     31.403 |     2.3
   67 |   0.8531 |     26.165 |   1.0045 |     31.587 |     2.3
   68 |   0.8463 |     25.948 |   1.0018 |     30.821 |     2.4
   69 |   0.8364 |     25.471 |   0.9981 |     31.189 |     2.4
   70 |   0.8283 |     25.222 |   0.9855 |     30.331 |     2.4
   71 |   0.8175 |     24.973 |   0.9824 |     30.423 |     2.5
   72 |   0.8163 |     24.924 |   0.9890 |     30.852 |     2.5
   73 |   0.8032 |     24.664 |   0.9748 |     30.147 |     2.5
   74 |   0.7981 |     24.420 |   0.9734 |     30.116 |     2.6
   75 |   0.7926 |     24.025 |   0.9754 |     30.362 |     2.6
   76 |   0.7835 |     23.835 |   0.9753 |     30.453 |     2.7
   77 |   0.7754 |     23.759 |   0.9673 |     30.025 |     2.7
   78 |   0.7702 |     23.440 |   0.9633 |     29.779 |     2.7
   79 |   0.7587 |     22.822 |   0.9594 |     30.086 |     2.8
   80 |   0.7512 |     22.638 |   0.9651 |     30.300 |     2.8
   81 |   0.7497 |     22.735 |   0.9568 |     29.749 |     2.8
   82 |   0.7392 |     22.638 |   0.9541 |     29.902 |     2.9
   83 |   0.7377 |     22.405 |   0.9597 |     29.718 |     2.9
   84 |   0.7249 |     22.253 |   0.9525 |     29.657 |     2.9
   85 |   0.7202 |     22.085 |   0.9505 |     29.565 |     3.0
   86 |   0.7126 |     21.624 |   0.9437 |     29.075 |     3.0
   87 |   0.7052 |     21.597 |   0.9443 |     28.860 |     3.0
   88 |   0.7009 |     21.310 |   0.9350 |     28.768 |     3.1
   89 |   0.6993 |     21.353 |   0.9378 |     29.044 |     3.1
   90 |   0.6904 |     20.682 |   0.9380 |     28.891 |     3.1
   91 |   0.6807 |     20.541 |   0.9397 |     28.983 |     3.2
   92 |   0.6742 |     20.411 |   0.9360 |     28.891 |     3.2
   93 |   0.6780 |     20.584 |   0.9293 |     28.830 |     3.2
   94 |   0.6614 |     19.880 |   0.9307 |     28.585 |     3.3
   95 |   0.6639 |     19.950 |   0.9273 |     28.646 |     3.3
   96 |   0.6563 |     19.847 |   0.9301 |     28.799 |     3.3
   97 |   0.6489 |     19.620 |   0.9343 |     29.044 |     3.4
   98 |   0.6453 |     19.690 |   0.9288 |     28.309 |     3.4
   99 |   0.6400 |     19.240 |   0.9242 |     28.462 |     3.4
  100 |   0.6336 |     19.278 |   0.9196 |     28.156 |     3.5
  101 |   0.6260 |     19.024 |   0.9198 |     28.002 |     3.5
  102 |   0.6219 |     18.715 |   0.9215 |     27.911 |     3.6
  103 |   0.6194 |     18.498 |   0.9233 |     28.309 |     3.6
  104 |   0.6161 |     18.460 |   0.9158 |     28.033 |     3.6
  105 |   0.6112 |     18.357 |   0.9180 |     27.665 |     3.7
  106 |   0.6044 |     18.336 |   0.9204 |     27.788 |     3.7
  107 |   0.6003 |     18.097 |   0.9148 |     27.819 |     3.7
  108 |   0.5982 |     18.000 |   0.9216 |     28.248 |     3.8
  109 |   0.5860 |     17.474 |   0.9153 |     27.696 |     3.8
  110 |   0.5849 |     17.783 |   0.9175 |     28.186 |     3.8
  111 |   0.5827 |     17.414 |   0.9182 |     27.727 |     3.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 1,625,794

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4137 |     86.563 |   3.2565 |     85.417 |     0.1
    2 |   3.0889 |     76.458 |   2.9273 |     67.892 |     0.1
    3 |   2.8033 |     61.519 |   2.6975 |     56.556 |     0.2
    4 |   2.6113 |     57.764 |   2.5425 |     57.782 |     0.3
    5 |   2.4821 |     57.829 |   2.4358 |     58.303 |     0.3
    6 |   2.3862 |     57.862 |   2.3502 |     57.996 |     0.4
    7 |   2.3076 |     57.369 |   2.2794 |     55.974 |     0.4
    8 |   2.2428 |     57.006 |   2.2209 |     58.058 |     0.5
    9 |   2.1839 |     56.735 |   2.1567 |     56.066 |     0.6
   10 |   2.1310 |     56.123 |   2.1090 |     56.189 |     0.6
   11 |   2.0856 |     55.733 |   2.0646 |     55.699 |     0.7
   12 |   2.0414 |     55.467 |   2.0253 |     54.963 |     0.8
   13 |   1.9987 |     54.833 |   1.9782 |     54.167 |     0.8
   14 |   1.9489 |     50.677 |   1.9259 |     48.775 |     0.9
   15 |   1.9051 |     49.149 |   1.8818 |     48.438 |     0.9
   16 |   1.8648 |     48.819 |   1.8418 |     48.438 |     1.0
   17 |   1.8235 |     48.727 |   1.8008 |     48.468 |     1.1
   18 |   1.7823 |     48.564 |   1.7620 |     48.376 |     1.1
   19 |   1.7438 |     48.477 |   1.7241 |     48.315 |     1.2
   20 |   1.7077 |     48.440 |   1.6904 |     48.223 |     1.3
   21 |   1.6741 |     48.326 |   1.6575 |     48.315 |     1.3
   22 |   1.6448 |     48.461 |   1.6271 |     48.039 |     1.4
   23 |   1.6173 |     48.396 |   1.6003 |     48.100 |     1.5
   24 |   1.5923 |     47.990 |   1.5743 |     46.507 |     1.5
   25 |   1.5659 |     46.500 |   1.5511 |     45.558 |     1.6
   26 |   1.5393 |     46.077 |   1.5292 |     45.312 |     1.6
   27 |   1.5226 |     46.066 |   1.5127 |     45.466 |     1.7
   28 |   1.5029 |     46.039 |   1.4942 |     45.343 |     1.8
   29 |   1.4852 |     46.083 |   1.4779 |     45.251 |     1.8
   30 |   1.4692 |     45.990 |   1.4646 |     45.343 |     1.9
   31 |   1.4533 |     45.996 |   1.4508 |     45.374 |     2.0
   32 |   1.4396 |     45.866 |   1.4377 |     45.343 |     2.0
   33 |   1.4277 |     45.779 |   1.4293 |     45.251 |     2.1
   34 |   1.4153 |     45.541 |   1.4185 |     45.312 |     2.1
   35 |   1.4033 |     45.508 |   1.4070 |     45.098 |     2.2
   36 |   1.3927 |     45.205 |   1.3987 |     45.037 |     2.3
   37 |   1.3805 |     45.129 |   1.3894 |     44.792 |     2.3
   38 |   1.3683 |     44.820 |   1.3814 |     44.638 |     2.4
   39 |   1.3607 |     44.414 |   1.3714 |     44.455 |     2.5
   40 |   1.3487 |     44.457 |   1.3612 |     44.301 |     2.5
   41 |   1.3376 |     44.056 |   1.3554 |     44.148 |     2.6
   42 |   1.3320 |     43.796 |   1.3473 |     43.995 |     2.6
   43 |   1.3231 |     43.466 |   1.3389 |     43.444 |     2.7
   44 |   1.3120 |     43.097 |   1.3338 |     43.260 |     2.8
   45 |   1.3018 |     42.935 |   1.3264 |     43.321 |     2.8
   46 |   1.2953 |     42.555 |   1.3217 |     43.444 |     2.9
   47 |   1.2877 |     42.236 |   1.3165 |     43.168 |     3.0
   48 |   1.2763 |     41.818 |   1.3106 |     42.096 |     3.0
   49 |   1.2681 |     41.504 |   1.3023 |     42.034 |     3.1
   50 |   1.2613 |     41.233 |   1.2974 |     41.789 |     3.1
   51 |   1.2543 |     40.886 |   1.2908 |     41.299 |     3.2
   52 |   1.2449 |     40.448 |   1.2843 |     41.085 |     3.3
   53 |   1.2361 |     40.453 |   1.2774 |     40.901 |     3.3
   54 |   1.2330 |     40.209 |   1.2741 |     41.728 |     3.4
   55 |   1.2239 |     39.900 |   1.2660 |     40.748 |     3.5
   56 |   1.2164 |     39.467 |   1.2653 |     40.625 |     3.5
   57 |   1.2108 |     39.423 |   1.2608 |     40.441 |     3.6
   58 |   1.2056 |     39.326 |   1.2545 |     39.920 |     3.6
   59 |   1.1939 |     38.822 |   1.2482 |     40.196 |     3.7
   60 |   1.1881 |     38.508 |   1.2518 |     40.472 |     3.8
   61 |   1.1803 |     38.546 |   1.2397 |     39.859 |     3.8
   62 |   1.1729 |     38.280 |   1.2352 |     40.165 |     3.9
   63 |   1.1671 |     37.944 |   1.2296 |     39.308 |     4.0
   64 |   1.1567 |     37.896 |   1.2246 |     39.522 |     4.0
   65 |   1.1495 |     37.424 |   1.2213 |     39.400 |     4.1
   66 |   1.1477 |     37.137 |   1.2153 |     38.879 |     4.1
   67 |   1.1407 |     37.083 |   1.2204 |     39.461 |     4.2
   68 |   1.1361 |     37.056 |   1.2118 |     39.154 |     4.3
   69 |   1.1283 |     36.443 |   1.2036 |     39.032 |     4.3
   70 |   1.1202 |     36.492 |   1.2043 |     39.277 |     4.4
   71 |   1.1146 |     36.189 |   1.2048 |     39.185 |     4.5
   72 |   1.1099 |     36.178 |   1.1970 |     39.001 |     4.5
   73 |   1.1035 |     35.858 |   1.1971 |     39.216 |     4.6
   74 |   1.0982 |     35.847 |   1.1895 |     39.032 |     4.6
   75 |   1.0949 |     35.582 |   1.1927 |     38.787 |     4.7
   76 |   1.0946 |     35.544 |   1.1827 |     38.572 |     4.8
   77 |   1.0827 |     35.111 |   1.1802 |     38.572 |     4.8
   78 |   1.0811 |     35.073 |   1.1826 |     38.725 |     4.9
   79 |   1.0707 |     34.894 |   1.1770 |     38.297 |     5.0
   80 |   1.0631 |     34.677 |   1.1703 |     38.358 |     5.0
   81 |   1.0610 |     34.450 |   1.1687 |     38.082 |     5.1
   82 |   1.0547 |     34.233 |   1.1678 |     37.990 |     5.1
   83 |   1.0478 |     34.157 |   1.1658 |     37.929 |     5.2
   84 |   1.0438 |     34.059 |   1.1556 |     37.806 |     5.3
   85 |   1.0364 |     33.610 |   1.1566 |     37.592 |     5.3
   86 |   1.0363 |     33.837 |   1.1524 |     37.806 |     5.4
   87 |   1.0365 |     33.864 |   1.1589 |     37.776 |     5.5
   88 |   1.0234 |     33.604 |   1.1557 |     37.439 |     5.5
   89 |   1.0192 |     33.057 |   1.1494 |     37.500 |     5.6
   90 |   1.0109 |     32.672 |   1.1439 |     37.653 |     5.6
   91 |   1.0090 |     32.786 |   1.1433 |     37.347 |     5.7
   92 |   1.0029 |     32.721 |   1.1400 |     37.408 |     5.8
   93 |   0.9961 |     32.418 |   1.1371 |     37.377 |     5.8
   94 |   0.9934 |     32.093 |   1.1371 |     36.765 |     5.9
   95 |   0.9901 |     32.130 |   1.1343 |     37.316 |     6.0
   96 |   0.9852 |     32.006 |   1.1377 |     37.071 |     6.0
   97 |   0.9768 |     31.361 |   1.1308 |     37.010 |     6.1
   98 |   0.9713 |     31.421 |   1.1256 |     36.703 |     6.1
   99 |   0.9686 |     31.366 |   1.1262 |     36.949 |     6.2
  100 |   0.9642 |     30.933 |   1.1269 |     36.458 |     6.3
  101 |   0.9571 |     30.993 |   1.1277 |     36.826 |     6.3
  102 |   0.9554 |     30.570 |   1.1226 |     36.703 |     6.4
  103 |   0.9531 |     30.537 |   1.1176 |     36.581 |     6.5
  104 |   0.9459 |     30.516 |   1.1153 |     36.091 |     6.5
  105 |   0.9407 |     30.250 |   1.1106 |     36.428 |     6.6
  106 |   0.9374 |     30.196 |   1.1095 |     36.029 |     6.6
  107 |   0.9318 |     29.746 |   1.1170 |     36.275 |     6.7
  108 |   0.9253 |     29.687 |   1.1097 |     35.938 |     6.8
  109 |   0.9235 |     29.524 |   1.1065 |     35.846 |     6.8
  110 |   0.9195 |     29.123 |   1.1004 |     35.815 |     6.9
  111 |   0.9143 |     29.237 |   1.0952 |     35.631 |     7.0
  112 |   0.9117 |     29.340 |   1.0975 |     35.539 |     7.0
  113 |   0.9090 |     29.069 |   1.0959 |     35.631 |     7.1
  114 |   0.9056 |     28.771 |   1.0971 |     35.478 |     7.1
  115 |   0.8963 |     28.636 |   1.0953 |     35.263 |     7.2
  116 |   0.8914 |     28.316 |   1.0915 |     35.141 |     7.3
  117 |   0.8919 |     28.311 |   1.0949 |     35.049 |     7.3
  118 |   0.8852 |     27.920 |   1.0916 |     35.263 |     7.4
  119 |   0.8837 |     27.752 |   1.0858 |     35.080 |     7.5
  120 |   0.8749 |     27.379 |   1.0806 |     34.988 |     7.5
  121 |   0.8755 |     27.606 |   1.0830 |     34.191 |     7.6
  122 |   0.8706 |     27.422 |   1.0856 |     34.681 |     7.6
  123 |   0.8634 |     26.983 |   1.0817 |     34.559 |     7.7
  124 |   0.8581 |     26.810 |   1.0757 |     33.762 |     7.8
  125 |   0.8539 |     26.691 |   1.0825 |     34.559 |     7.8
  126 |   0.8497 |     26.441 |   1.0795 |     34.252 |     7.9
  127 |   0.8427 |     26.165 |   1.0737 |     34.099 |     8.0
  128 |   0.8421 |     26.290 |   1.0721 |     34.191 |     8.0
  129 |   0.8354 |     25.883 |   1.0743 |     33.824 |     8.1
  130 |   0.8380 |     26.051 |   1.0738 |     34.252 |     8.1
  131 |   0.8334 |     26.084 |   1.0661 |     34.130 |     8.2
  132 |   0.8270 |     25.840 |   1.0644 |     33.946 |     8.3
  133 |   0.8213 |     25.710 |   1.0734 |     34.007 |     8.3
  134 |   0.8193 |     25.293 |   1.0657 |     33.793 |     8.4
  135 |   0.8244 |     25.694 |   1.0569 |     33.425 |     8.5
  136 |   0.8132 |     25.331 |   1.0574 |     32.996 |     8.5
  137 |   0.8085 |     25.022 |   1.0601 |     34.191 |     8.6
  138 |   0.8078 |     24.794 |   1.0609 |     33.670 |     8.6
  139 |   0.8076 |     24.886 |   1.0589 |     33.211 |     8.7
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 2,399,490

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5633 |     65.811 |   2.0675 |     58.824 |     0.1
    2 |   1.8508 |     52.048 |   1.6222 |     45.588 |     0.2
    3 |   1.5295 |     46.169 |   1.4554 |     46.017 |     0.3
    4 |   1.4391 |     46.251 |   1.4127 |     45.435 |     0.3
    5 |   1.4109 |     45.817 |   1.3901 |     44.393 |     0.4
    6 |   1.3775 |     44.750 |   1.3543 |     43.658 |     0.5
    7 |   1.3525 |     44.197 |   1.3362 |     43.413 |     0.6
    8 |   1.3336 |     43.883 |   1.3237 |     43.352 |     0.7
    9 |   1.3227 |     43.699 |   1.3100 |     42.555 |     0.8
   10 |   1.3075 |     43.308 |   1.2995 |     42.126 |     0.8
   11 |   1.2958 |     43.254 |   1.2836 |     42.126 |     0.9
   12 |   1.2881 |     42.431 |   1.2738 |     40.686 |     1.0
   13 |   1.2741 |     42.041 |   1.2705 |     41.452 |     1.1
   14 |   1.2586 |     41.932 |   1.2526 |     40.870 |     1.2
   15 |   1.2549 |     41.298 |   1.2650 |     40.901 |     1.3
   16 |   1.2466 |     41.569 |   1.2525 |     41.544 |     1.3
   17 |   1.2359 |     41.298 |   1.2467 |     41.544 |     1.4
   18 |   1.2244 |     41.016 |   1.2308 |     40.564 |     1.5
   19 |   1.2157 |     40.388 |   1.2194 |     40.319 |     1.6
   20 |   1.2080 |     40.155 |   1.2155 |     39.982 |     1.7
   21 |   1.1914 |     40.009 |   1.2204 |     39.614 |     1.8
   22 |   1.1861 |     39.554 |   1.2076 |     39.890 |     1.9
   23 |   1.1786 |     39.082 |   1.1938 |     38.358 |     1.9
   24 |   1.1695 |     38.968 |   1.1858 |     39.185 |     2.0
   25 |   1.1617 |     38.421 |   1.1744 |     38.480 |     2.1
   26 |   1.1439 |     38.150 |   1.1757 |     38.082 |     2.2
   27 |   1.1447 |     37.912 |   1.1657 |     38.113 |     2.3
   28 |   1.1324 |     37.825 |   1.1591 |     37.469 |     2.4
   29 |   1.1445 |     38.324 |   1.1614 |     38.174 |     2.4
   30 |   1.1207 |     37.505 |   1.1570 |     38.021 |     2.5
   31 |   1.1287 |     37.771 |   1.1646 |     37.929 |     2.6
   32 |   1.1207 |     37.408 |   1.1427 |     37.347 |     2.7
   33 |   1.1017 |     36.714 |   1.1432 |     36.949 |     2.8
   34 |   1.0963 |     36.552 |   1.1487 |     37.377 |     2.9
   35 |   1.0962 |     36.503 |   1.1180 |     36.734 |     2.9
   36 |   1.0913 |     36.324 |   1.1516 |     37.255 |     3.0
   37 |   1.0872 |     36.487 |   1.1362 |     36.887 |     3.1
   38 |   1.0799 |     35.782 |   1.1099 |     36.183 |     3.2
   39 |   1.0696 |     35.598 |   1.1160 |     36.336 |     3.3
   40 |   1.0570 |     35.051 |   1.1078 |     35.692 |     3.4
   41 |   1.0438 |     35.073 |   1.1101 |     36.397 |     3.4
   42 |   1.0394 |     34.975 |   1.1325 |     37.163 |     3.5
   43 |   1.0367 |     34.850 |   1.1074 |     36.428 |     3.6
   44 |   1.0389 |     34.845 |   1.1171 |     36.458 |     3.7
   45 |   1.0293 |     34.775 |   1.1117 |     37.102 |     3.8
   46 |   1.0214 |     34.319 |   1.1055 |     35.570 |     3.9
   47 |   1.0343 |     34.932 |   1.1115 |     36.336 |     4.0
   48 |   1.0206 |     34.081 |   1.1193 |     36.581 |     4.0
   49 |   1.0244 |     34.428 |   1.1100 |     36.397 |     4.1
   50 |   1.0128 |     34.076 |   1.1074 |     35.815 |     4.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 2,125,474

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1290 |     83.003 |   2.6374 |     67.341 |     0.1
    2 |   2.3934 |     59.818 |   2.2222 |     58.824 |     0.1
    3 |   2.0913 |     57.109 |   2.0005 |     57.016 |     0.2
    4 |   1.9156 |     52.238 |   1.8512 |     48.376 |     0.2
    5 |   1.7900 |     48.629 |   1.7374 |     48.346 |     0.3
    6 |   1.6910 |     48.602 |   1.6418 |     48.346 |     0.4
    7 |   1.6096 |     47.551 |   1.5703 |     45.466 |     0.4
    8 |   1.5495 |     46.202 |   1.5212 |     45.466 |     0.5
    9 |   1.5093 |     46.213 |   1.4881 |     45.496 |     0.5
   10 |   1.4791 |     46.213 |   1.4641 |     45.466 |     0.6
   11 |   1.4574 |     46.213 |   1.4453 |     45.466 |     0.7
   12 |   1.4373 |     46.207 |   1.4290 |     45.466 |     0.7
   13 |   1.4262 |     46.213 |   1.4147 |     45.466 |     0.8
   14 |   1.4079 |     46.218 |   1.4014 |     45.466 |     0.8
   15 |   1.3925 |     46.137 |   1.3921 |     45.435 |     0.9
   16 |   1.3824 |     45.996 |   1.3802 |     44.853 |     1.0
   17 |   1.3700 |     45.432 |   1.3752 |     44.669 |     1.0
   18 |   1.3572 |     45.075 |   1.3602 |     44.118 |     1.1
   19 |   1.3448 |     44.674 |   1.3530 |     44.026 |     1.1
   20 |   1.3366 |     44.365 |   1.3466 |     43.505 |     1.2
   21 |   1.3249 |     44.186 |   1.3372 |     43.750 |     1.3
   22 |   1.3162 |     43.937 |   1.3331 |     43.382 |     1.3
   23 |   1.3073 |     43.547 |   1.3303 |     43.168 |     1.4
   24 |   1.2943 |     42.859 |   1.3161 |     43.658 |     1.4
   25 |   1.2846 |     42.615 |   1.3110 |     42.249 |     1.5
   26 |   1.2740 |     41.981 |   1.3051 |     42.188 |     1.6
   27 |   1.2619 |     41.233 |   1.2964 |     41.422 |     1.6
   28 |   1.2510 |     40.583 |   1.2873 |     41.483 |     1.7
   29 |   1.2347 |     40.085 |   1.2821 |     40.686 |     1.7
   30 |   1.2209 |     39.277 |   1.2745 |     40.778 |     1.8
   31 |   1.2075 |     39.120 |   1.2666 |     40.778 |     1.9
   32 |   1.1962 |     38.529 |   1.2562 |     40.012 |     1.9
   33 |   1.1807 |     38.237 |   1.2431 |     40.472 |     2.0
   34 |   1.1676 |     37.863 |   1.2355 |     39.859 |     2.0
   35 |   1.1540 |     37.207 |   1.2281 |     39.890 |     2.1
   36 |   1.1421 |     37.072 |   1.2248 |     39.277 |     2.2
   37 |   1.1289 |     36.714 |   1.2173 |     39.522 |     2.2
   38 |   1.1152 |     36.557 |   1.2056 |     38.909 |     2.3
   39 |   1.1026 |     36.113 |   1.2018 |     38.572 |     2.3
   40 |   1.0925 |     35.950 |   1.1912 |     38.879 |     2.4
   41 |   1.0791 |     35.284 |   1.1887 |     38.971 |     2.5
   42 |   1.0713 |     35.311 |   1.1800 |     38.480 |     2.5
   43 |   1.0553 |     34.444 |   1.1823 |     38.266 |     2.6
   44 |   1.0358 |     33.810 |   1.1766 |     38.051 |     2.6
   45 |   1.0274 |     33.187 |   1.1643 |     37.806 |     2.7
   46 |   1.0185 |     32.981 |   1.1649 |     37.377 |     2.8
   47 |   0.9963 |     32.266 |   1.1585 |     37.377 |     2.8
   48 |   0.9866 |     31.822 |   1.1558 |     37.500 |     2.9
   49 |   0.9748 |     31.692 |   1.1470 |     36.458 |     2.9
   50 |   0.9590 |     30.830 |   1.1529 |     36.581 |     3.0
   51 |   0.9501 |     30.402 |   1.1499 |     36.795 |     3.1
   52 |   0.9354 |     29.741 |   1.1471 |     37.010 |     3.1
   53 |   0.9250 |     29.600 |   1.1318 |     36.152 |     3.2
   54 |   0.9096 |     28.766 |   1.1457 |     36.826 |     3.2
   55 |   0.9015 |     28.863 |   1.1288 |     35.570 |     3.3
   56 |   0.8883 |     28.132 |   1.1315 |     36.152 |     3.4
   57 |   0.8755 |     27.411 |   1.1368 |     35.600 |     3.4
   58 |   0.8627 |     27.276 |   1.1155 |     35.233 |     3.5
   59 |   0.8487 |     26.647 |   1.1205 |     34.804 |     3.5
   60 |   0.8432 |     26.479 |   1.1254 |     35.172 |     3.6
   61 |   0.8298 |     26.089 |   1.1099 |     34.344 |     3.7
   62 |   0.8222 |     25.645 |   1.1162 |     34.681 |     3.7
   63 |   0.8087 |     25.515 |   1.1157 |     34.957 |     3.8
   64 |   0.7982 |     24.924 |   1.1134 |     34.620 |     3.8
   65 |   0.7902 |     24.675 |   1.1160 |     34.651 |     3.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 801,378

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2303 |     86.395 |   2.7241 |     83.333 |     0.0
    2 |   2.4458 |     61.958 |   2.2365 |     54.105 |     0.0
    3 |   2.0935 |     55.478 |   1.9968 |     53.799 |     0.1
    4 |   1.9032 |     52.010 |   1.8340 |     48.438 |     0.1
    5 |   1.7637 |     48.607 |   1.7022 |     48.346 |     0.1
    6 |   1.6509 |     48.613 |   1.6077 |     48.346 |     0.1
    7 |   1.5751 |     46.321 |   1.5421 |     45.466 |     0.1
    8 |   1.5233 |     46.213 |   1.4994 |     45.466 |     0.1
    9 |   1.4893 |     46.272 |   1.4713 |     45.466 |     0.2
   10 |   1.4682 |     46.213 |   1.4520 |     45.466 |     0.2
   11 |   1.4501 |     46.207 |   1.4382 |     45.466 |     0.2
   12 |   1.4337 |     46.164 |   1.4248 |     46.017 |     0.2
   13 |   1.4198 |     46.256 |   1.4118 |     45.282 |     0.2
   14 |   1.4075 |     46.158 |   1.3991 |     45.190 |     0.2
   15 |   1.3915 |     45.920 |   1.3847 |     45.343 |     0.3
   16 |   1.3754 |     45.638 |   1.3699 |     45.098 |     0.3
   17 |   1.3616 |     45.655 |   1.3568 |     44.945 |     0.3
   18 |   1.3467 |     45.145 |   1.3484 |     44.424 |     0.3
   19 |   1.3366 |     44.679 |   1.3374 |     44.118 |     0.3
   20 |   1.3204 |     44.316 |   1.3271 |     43.811 |     0.4
   21 |   1.3131 |     43.677 |   1.3176 |     42.984 |     0.4
   22 |   1.3006 |     43.352 |   1.3160 |     43.321 |     0.4
   23 |   1.2921 |     43.108 |   1.3074 |     43.321 |     0.4
   24 |   1.2805 |     42.593 |   1.2981 |     42.586 |     0.4
   25 |   1.2715 |     42.463 |   1.2947 |     42.525 |     0.4
   26 |   1.2623 |     42.127 |   1.2858 |     42.341 |     0.5
   27 |   1.2505 |     41.661 |   1.2822 |     42.678 |     0.5
   28 |   1.2394 |     41.629 |   1.2777 |     42.341 |     0.5
   29 |   1.2296 |     41.277 |   1.2674 |     42.433 |     0.5
   30 |   1.2211 |     40.870 |   1.2616 |     42.249 |     0.5
   31 |   1.2105 |     40.632 |   1.2623 |     41.299 |     0.5
   32 |   1.1984 |     39.700 |   1.2526 |     41.207 |     0.6
   33 |   1.1872 |     39.288 |   1.2464 |     40.594 |     0.6
   34 |   1.1752 |     38.741 |   1.2378 |     40.227 |     0.6
   35 |   1.1623 |     38.454 |   1.2298 |     40.196 |     0.6
   36 |   1.1518 |     38.074 |   1.2204 |     39.522 |     0.6
   37 |   1.1381 |     37.690 |   1.2116 |     39.859 |     0.7
   38 |   1.1282 |     37.039 |   1.2048 |     39.062 |     0.7
   39 |   1.1171 |     36.904 |   1.1997 |     38.542 |     0.7
   40 |   1.1058 |     36.335 |   1.1910 |     38.634 |     0.7
   41 |   1.0897 |     35.625 |   1.1817 |     38.174 |     0.7
   42 |   1.0751 |     35.528 |   1.1746 |     37.868 |     0.7
   43 |   1.0675 |     34.384 |   1.1844 |     38.205 |     0.8
   44 |   1.0521 |     34.260 |   1.1724 |     37.868 |     0.8
   45 |   1.0386 |     33.756 |   1.1601 |     37.347 |     0.8
   46 |   1.0279 |     33.588 |   1.1536 |     36.857 |     0.8
   47 |   1.0147 |     32.873 |   1.1549 |     36.673 |     0.8
   48 |   1.0021 |     32.358 |   1.1382 |     35.938 |     0.8
   49 |   0.9905 |     31.946 |   1.1379 |     35.539 |     0.9
   50 |   0.9796 |     31.556 |   1.1321 |     35.233 |     0.9
   51 |   0.9635 |     31.372 |   1.1227 |     34.865 |     0.9
   52 |   0.9533 |     30.397 |   1.1168 |     34.804 |     0.9
   53 |   0.9409 |     30.288 |   1.1167 |     34.651 |     0.9
   54 |   0.9297 |     29.622 |   1.1031 |     34.681 |     1.0
   55 |   0.9193 |     29.443 |   1.1028 |     34.712 |     1.0
   56 |   0.9081 |     29.004 |   1.0979 |     34.130 |     1.0
   57 |   0.8932 |     28.527 |   1.0935 |     34.406 |     1.0
   58 |   0.8800 |     28.110 |   1.0854 |     33.977 |     1.0
   59 |   0.8688 |     27.742 |   1.0814 |     34.069 |     1.0
   60 |   0.8561 |     27.178 |   1.0870 |     33.824 |     1.1
   61 |   0.8408 |     26.761 |   1.0675 |     33.456 |     1.1
   62 |   0.8326 |     26.474 |   1.0753 |     33.762 |     1.1
   63 |   0.8192 |     26.154 |   1.0696 |     33.241 |     1.1
   64 |   0.8076 |     25.396 |   1.0623 |     33.272 |     1.1
   65 |   0.7968 |     25.146 |   1.0722 |     33.364 |     1.2
   66 |   0.7874 |     24.767 |   1.0536 |     32.169 |     1.2
   67 |   0.7752 |     24.350 |   1.0686 |     32.567 |     1.2
   68 |   0.7658 |     24.155 |   1.0548 |     32.353 |     1.2
   69 |   0.7583 |     23.732 |   1.0563 |     32.629 |     1.2
   70 |   0.7482 |     23.244 |   1.0525 |     32.414 |     1.2
   71 |   0.7369 |     23.028 |   1.0640 |     32.353 |     1.3
   72 |   0.7267 |     22.551 |   1.0617 |     32.322 |     1.3
   73 |   0.7169 |     22.128 |   1.0621 |     32.261 |     1.3
   74 |   0.7103 |     22.042 |   1.0655 |     32.384 |     1.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 1,621,730

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2735 |     62.619 |   1.6311 |     45.466 |     0.0
    2 |   1.4874 |     46.348 |   1.4232 |     45.987 |     0.1
    3 |   1.4155 |     46.207 |   1.3984 |     45.987 |     0.1
    4 |   1.4001 |     46.267 |   1.3904 |     45.466 |     0.2
    5 |   1.3960 |     46.256 |   1.3891 |     45.987 |     0.2
    6 |   1.3920 |     46.505 |   1.3873 |     46.017 |     0.2
    7 |   1.3857 |     46.332 |   1.3718 |     45.987 |     0.3
    8 |   1.3720 |     45.562 |   1.3478 |     44.148 |     0.3
    9 |   1.3515 |     44.961 |   1.3431 |     44.056 |     0.4
   10 |   1.3372 |     44.901 |   1.3267 |     44.026 |     0.4
   11 |   1.3287 |     44.679 |   1.3228 |     43.597 |     0.5
   12 |   1.3273 |     44.620 |   1.3227 |     43.873 |     0.5
   13 |   1.3151 |     44.121 |   1.3163 |     44.271 |     0.5
   14 |   1.3100 |     43.986 |   1.3085 |     43.781 |     0.6
   15 |   1.3010 |     43.812 |   1.2940 |     43.352 |     0.6
   16 |   1.2920 |     43.574 |   1.3034 |     43.382 |     0.7
   17 |   1.2852 |     43.151 |   1.2792 |     42.770 |     0.7
   18 |   1.2715 |     42.924 |   1.2891 |     42.770 |     0.7
   19 |   1.2697 |     42.994 |   1.2680 |     42.096 |     0.8
   20 |   1.2518 |     42.284 |   1.2765 |     42.126 |     0.8
   21 |   1.2385 |     42.219 |   1.2454 |     40.717 |     0.9
   22 |   1.2247 |     41.640 |   1.2381 |     40.625 |     0.9
   23 |   1.2083 |     40.778 |   1.2288 |     40.288 |     1.0
   24 |   1.1892 |     40.003 |   1.2181 |     40.349 |     1.0
   25 |   1.1837 |     39.917 |   1.2018 |     39.583 |     1.0
   26 |   1.1607 |     39.060 |   1.2154 |     40.104 |     1.1
   27 |   1.1413 |     38.589 |   1.1886 |     39.246 |     1.1
   28 |   1.1317 |     38.150 |   1.1813 |     38.603 |     1.2
   29 |   1.1068 |     37.343 |   1.1778 |     39.491 |     1.2
   30 |   1.0990 |     36.590 |   1.1646 |     38.909 |     1.2
   31 |   1.0849 |     36.048 |   1.1581 |     38.542 |     1.3
   32 |   1.0575 |     35.549 |   1.1459 |     37.990 |     1.3
   33 |   1.0437 |     34.699 |   1.1458 |     38.205 |     1.4
   34 |   1.0240 |     34.086 |   1.1385 |     37.347 |     1.4
   35 |   1.0084 |     33.994 |   1.1222 |     36.795 |     1.5
   36 |   0.9727 |     32.640 |   1.0968 |     36.366 |     1.5
   37 |   0.9611 |     32.472 |   1.1007 |     36.949 |     1.5
   38 |   0.9364 |     31.556 |   1.0897 |     35.600 |     1.6
   39 |   0.9278 |     31.101 |   1.0807 |     35.141 |     1.6
   40 |   0.9072 |     30.662 |   1.0864 |     35.754 |     1.7
   41 |   0.8814 |     29.801 |   1.0948 |     35.815 |     1.7
   42 |   0.8683 |     29.243 |   1.0644 |     35.631 |     1.7
   43 |   0.8469 |     29.026 |   1.0529 |     34.651 |     1.8
   44 |   0.8284 |     28.359 |   1.0543 |     34.620 |     1.8
   45 |   0.8116 |     27.650 |   1.0555 |     34.559 |     1.9
   46 |   0.7990 |     27.081 |   1.0391 |     33.517 |     1.9
   47 |   0.7609 |     26.170 |   1.0758 |     33.885 |     2.0
   48 |   0.7598 |     25.992 |   1.0270 |     32.292 |     2.0
   49 |   0.7419 |     25.347 |   1.0402 |     33.027 |     2.0
   50 |   0.7303 |     24.848 |   1.0338 |     32.904 |     2.1
   51 |   0.7047 |     23.895 |   1.0262 |     31.832 |     2.1
   52 |   0.6816 |     23.207 |   1.0287 |     32.690 |     2.2
   53 |   0.6668 |     23.001 |   1.0129 |     31.495 |     2.2
   54 |   0.6407 |     21.884 |   1.0235 |     31.893 |     2.2
   55 |   0.6338 |     21.971 |   1.0074 |     31.373 |     2.3
   56 |   0.6283 |     21.760 |   1.0152 |     31.587 |     2.3
   57 |   0.6124 |     21.072 |   1.0077 |     31.679 |     2.4
   58 |   0.5925 |     20.340 |   1.0290 |     31.801 |     2.4
   59 |   0.5760 |     19.744 |   0.9964 |     30.913 |     2.5
   60 |   0.5512 |     18.850 |   1.0155 |     30.852 |     2.5
   61 |   0.5391 |     18.780 |   1.0214 |     30.637 |     2.5
   62 |   0.5143 |     17.875 |   1.0191 |     29.994 |     2.6
   63 |   0.5093 |     17.848 |   1.0368 |     30.055 |     2.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,735,234

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5620 |     68.601 |   2.0431 |     58.058 |     0.1
    2 |   1.8036 |     50.461 |   1.6103 |     45.466 |     0.1
    3 |   1.5326 |     46.218 |   1.4728 |     45.987 |     0.2
    4 |   1.4558 |     46.310 |   1.4323 |     45.466 |     0.2
    5 |   1.4293 |     46.234 |   1.4135 |     45.466 |     0.3
    6 |   1.4185 |     46.218 |   1.4047 |     45.466 |     0.4
    7 |   1.4042 |     46.251 |   1.3974 |     45.466 |     0.4
    8 |   1.4023 |     46.099 |   1.3975 |     45.466 |     0.5
    9 |   1.3915 |     45.947 |   1.3859 |     45.987 |     0.5
   10 |   1.3768 |     45.454 |   1.3650 |     44.148 |     0.6
   11 |   1.3575 |     44.988 |   1.3463 |     44.118 |     0.7
   12 |   1.3494 |     44.912 |   1.3395 |     43.719 |     0.7
   13 |   1.3436 |     44.809 |   1.3303 |     43.903 |     0.8
   14 |   1.3264 |     44.522 |   1.3287 |     44.424 |     0.8
   15 |   1.3230 |     44.517 |   1.3174 |     44.271 |     0.9
   16 |   1.3137 |     44.262 |   1.3071 |     44.210 |     0.9
   17 |   1.3069 |     44.267 |   1.3133 |     44.056 |     1.0
   18 |   1.2986 |     43.753 |   1.2914 |     42.984 |     1.1
   19 |   1.2871 |     43.585 |   1.2912 |     42.402 |     1.1
   20 |   1.2824 |     43.146 |   1.2890 |     42.126 |     1.2
   21 |   1.2743 |     42.810 |   1.2787 |     42.494 |     1.2
   22 |   1.2624 |     42.544 |   1.2757 |     41.759 |     1.3
   23 |   1.2534 |     42.035 |   1.2616 |     40.931 |     1.4
   24 |   1.2444 |     41.829 |   1.2738 |     41.330 |     1.4
   25 |   1.2408 |     41.591 |   1.2670 |     41.146 |     1.5
   26 |   1.2290 |     41.287 |   1.2398 |     41.023 |     1.5
   27 |   1.2225 |     41.000 |   1.2473 |     40.778 |     1.6
   28 |   1.2178 |     40.751 |   1.2294 |     39.737 |     1.7
   29 |   1.1962 |     40.193 |   1.2150 |     40.135 |     1.7
   30 |   1.1860 |     39.727 |   1.2171 |     39.277 |     1.8
   31 |   1.1817 |     39.759 |   1.2213 |     39.890 |     1.8
   32 |   1.1815 |     39.402 |   1.2280 |     40.135 |     1.9
   33 |   1.1710 |     39.071 |   1.2013 |     38.542 |     2.0
   34 |   1.1543 |     38.757 |   1.1882 |     38.971 |     2.0
   35 |   1.1473 |     38.535 |   1.1813 |     38.358 |     2.1
   36 |   1.1438 |     38.562 |   1.1950 |     39.124 |     2.1
   37 |   1.1378 |     38.101 |   1.1745 |     38.695 |     2.2
   38 |   1.1182 |     37.619 |   1.1507 |     37.653 |     2.3
   39 |   1.1114 |     37.560 |   1.1667 |     38.572 |     2.3
   40 |   1.1244 |     37.657 |   1.1792 |     38.695 |     2.4
   41 |   1.1097 |     37.359 |   1.1629 |     38.235 |     2.4
   42 |   1.0971 |     37.018 |   1.1695 |     38.848 |     2.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 1,623,874

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4800 |     83.713 |   3.3953 |     83.333 |     0.1
    2 |   3.2072 |     83.333 |   2.9998 |     83.333 |     0.1
    3 |   2.8651 |     83.333 |   2.7700 |     83.333 |     0.2
    4 |   2.6853 |     72.573 |   2.6220 |     66.912 |     0.3
    5 |   2.5543 |     63.578 |   2.5074 |     58.824 |     0.3
    6 |   2.4526 |     59.558 |   2.4179 |     58.824 |     0.4
    7 |   2.3732 |     59.146 |   2.3438 |     58.824 |     0.4
    8 |   2.3065 |     58.583 |   2.2814 |     58.824 |     0.5
    9 |   2.2533 |     58.117 |   2.2275 |     58.793 |     0.6
   10 |   2.2014 |     57.618 |   2.1767 |     58.793 |     0.6
   11 |   2.1532 |     57.331 |   2.1277 |     57.598 |     0.7
   12 |   2.1052 |     56.816 |   2.0833 |     58.058 |     0.8
   13 |   2.0653 |     56.486 |   2.0409 |     56.097 |     0.8
   14 |   2.0250 |     56.253 |   2.0024 |     56.801 |     0.9
   15 |   1.9871 |     54.692 |   1.9650 |     49.786 |     0.9
   16 |   1.9534 |     51.756 |   1.9302 |     48.407 |     1.0
   17 |   1.9186 |     49.978 |   1.8969 |     48.376 |     1.1
   18 |   1.8878 |     49.686 |   1.8653 |     48.376 |     1.1
   19 |   1.8550 |     49.577 |   1.8342 |     48.346 |     1.2
   20 |   1.8274 |     49.469 |   1.8037 |     48.346 |     1.3
   21 |   1.7930 |     49.128 |   1.7741 |     48.407 |     1.3
   22 |   1.7665 |     49.155 |   1.7458 |     48.346 |     1.4
   23 |   1.7397 |     49.041 |   1.7190 |     48.346 |     1.4
   24 |   1.7154 |     48.911 |   1.6938 |     48.346 |     1.5
   25 |   1.6886 |     48.878 |   1.6705 |     48.346 |     1.6
   26 |   1.6684 |     48.754 |   1.6490 |     48.346 |     1.6
   27 |   1.6465 |     48.315 |   1.6287 |     48.131 |     1.7
   28 |   1.6267 |     47.112 |   1.6106 |     45.466 |     1.8
   29 |   1.6097 |     46.364 |   1.5941 |     45.466 |     1.8
   30 |   1.5963 |     46.294 |   1.5792 |     45.466 |     1.9
   31 |   1.5796 |     46.278 |   1.5659 |     45.466 |     1.9
   32 |   1.5672 |     46.234 |   1.5527 |     45.466 |     2.0
   33 |   1.5562 |     46.240 |   1.5410 |     45.466 |     2.1
   34 |   1.5417 |     46.272 |   1.5304 |     45.466 |     2.1
   35 |   1.5325 |     46.245 |   1.5207 |     45.466 |     2.2
   36 |   1.5236 |     46.288 |   1.5118 |     45.466 |     2.3
   37 |   1.5130 |     46.305 |   1.5029 |     45.925 |     2.3
   38 |   1.5054 |     46.316 |   1.4941 |     46.232 |     2.4
   39 |   1.4945 |     46.223 |   1.4850 |     45.895 |     2.4
   40 |   1.4869 |     46.278 |   1.4772 |     45.772 |     2.5
   41 |   1.4750 |     46.321 |   1.4676 |     45.956 |     2.6
   42 |   1.4683 |     46.283 |   1.4601 |     45.925 |     2.6
   43 |   1.4582 |     46.180 |   1.4524 |     45.895 |     2.7
   44 |   1.4512 |     46.272 |   1.4431 |     45.987 |     2.8
   45 |   1.4387 |     46.050 |   1.4350 |     45.925 |     2.8
   46 |   1.4271 |     45.980 |   1.4257 |     45.619 |     2.9
   47 |   1.4199 |     45.676 |   1.4174 |     45.588 |     3.0
   48 |   1.4077 |     45.541 |   1.4070 |     45.588 |     3.0
   49 |   1.3966 |     45.438 |   1.4010 |     45.588 |     3.1
   50 |   1.3897 |     45.644 |   1.3927 |     45.527 |     3.2
   51 |   1.3789 |     45.465 |   1.3839 |     45.282 |     3.2
   52 |   1.3692 |     45.248 |   1.3747 |     45.435 |     3.3
   53 |   1.3600 |     45.086 |   1.3685 |     45.374 |     3.3
   54 |   1.3492 |     44.939 |   1.3641 |     45.190 |     3.4
   55 |   1.3433 |     44.793 |   1.3551 |     45.159 |     3.5
   56 |   1.3331 |     44.712 |   1.3532 |     45.404 |     3.5
   57 |   1.3273 |     44.528 |   1.3450 |     44.730 |     3.6
   58 |   1.3184 |     44.230 |   1.3381 |     44.761 |     3.7
   59 |   1.3114 |     43.975 |   1.3324 |     44.975 |     3.7
   60 |   1.3044 |     43.612 |   1.3269 |     44.148 |     3.8
   61 |   1.2959 |     42.978 |   1.3199 |     43.444 |     3.8
   62 |   1.2851 |     42.534 |   1.3180 |     43.382 |     3.9
   63 |   1.2816 |     42.366 |   1.3131 |     42.555 |     4.0
   64 |   1.2752 |     41.862 |   1.3072 |     42.647 |     4.0
   65 |   1.2698 |     41.818 |   1.3051 |     42.249 |     4.1
   66 |   1.2625 |     41.309 |   1.2998 |     42.157 |     4.2
   67 |   1.2570 |     41.233 |   1.2982 |     42.126 |     4.3
   68 |   1.2522 |     40.746 |   1.2911 |     41.973 |     4.3
   69 |   1.2444 |     40.599 |   1.2892 |     42.157 |     4.4
   70 |   1.2415 |     40.420 |   1.2875 |     41.513 |     4.4
   71 |   1.2330 |     40.009 |   1.2846 |     41.973 |     4.5
   72 |   1.2259 |     40.025 |   1.2804 |     41.330 |     4.6
   73 |   1.2217 |     39.889 |   1.2754 |     41.085 |     4.6
   74 |   1.2149 |     39.369 |   1.2723 |     41.115 |     4.7
   75 |   1.2095 |     39.434 |   1.2684 |     41.176 |     4.8
   76 |   1.2021 |     39.234 |   1.2683 |     40.778 |     4.8
   77 |   1.1959 |     39.082 |   1.2669 |     40.778 |     4.9
   78 |   1.1971 |     39.006 |   1.2589 |     40.349 |     5.0
   79 |   1.1898 |     38.903 |   1.2594 |     40.564 |     5.0
   80 |   1.1865 |     38.817 |   1.2553 |     40.778 |     5.1
   81 |   1.1783 |     38.475 |   1.2505 |     40.686 |     5.2
   82 |   1.1749 |     38.324 |   1.2510 |     40.472 |     5.2
   83 |   1.1691 |     38.053 |   1.2470 |     40.472 |     5.3
   84 |   1.1646 |     38.237 |   1.2483 |     40.656 |     5.4
   85 |   1.1632 |     37.961 |   1.2420 |     40.809 |     5.4
   86 |   1.1589 |     38.150 |   1.2427 |     40.870 |     5.5
   87 |   1.1524 |     37.690 |   1.2423 |     40.870 |     5.6
   88 |   1.1469 |     37.663 |   1.2420 |     40.962 |     5.7
   89 |   1.1406 |     37.755 |   1.2340 |     40.135 |     5.7
   90 |   1.1380 |     37.495 |   1.2409 |     40.778 |     5.8
   91 |   1.1367 |     37.478 |   1.2361 |     40.870 |     5.9
   92 |   1.1308 |     37.202 |   1.2341 |     40.748 |     5.9
   93 |   1.1255 |     36.942 |   1.2293 |     40.441 |     6.0
   94 |   1.1205 |     36.698 |   1.2306 |     40.839 |     6.1
   95 |   1.1214 |     36.996 |   1.2289 |     40.533 |     6.2
   96 |   1.1143 |     36.763 |   1.2226 |     39.890 |     6.2
   97 |   1.1121 |     36.779 |   1.2264 |     40.135 |     6.3
   98 |   1.1060 |     36.525 |   1.2263 |     40.104 |     6.4
   99 |   1.1064 |     36.438 |   1.2284 |     40.319 |     6.4
  100 |   1.0995 |     36.720 |   1.2214 |     40.441 |     6.5
  101 |   1.0958 |     36.221 |   1.2196 |     40.165 |     6.6
  102 |   1.0915 |     36.200 |   1.2203 |     40.564 |     6.6
  103 |   1.0911 |     36.330 |   1.2125 |     39.890 |     6.7
  104 |   1.0873 |     36.118 |   1.2209 |     40.196 |     6.8
  105 |   1.0833 |     35.983 |   1.2105 |     39.706 |     6.9
  106 |   1.0802 |     35.761 |   1.2123 |     39.982 |     6.9
  107 |   1.0760 |     35.826 |   1.2053 |     39.216 |     7.0
  108 |   1.0704 |     35.522 |   1.2097 |     39.308 |     7.1
  109 |   1.0742 |     35.799 |   1.2094 |     39.920 |     7.1
  110 |   1.0652 |     35.306 |   1.2048 |     39.767 |     7.2
  111 |   1.0621 |     35.495 |   1.2087 |     39.828 |     7.3
  112 |   1.0576 |     35.430 |   1.2078 |     39.675 |     7.3
  113 |   1.0580 |     35.089 |   1.2022 |     39.277 |     7.4
  114 |   1.0519 |     35.224 |   1.2024 |     39.338 |     7.5
  115 |   1.0496 |     35.018 |   1.2055 |     39.400 |     7.6
  116 |   1.0461 |     34.948 |   1.1996 |     39.400 |     7.6
  117 |   1.0421 |     34.894 |   1.1994 |     39.154 |     7.7
  118 |   1.0371 |     34.861 |   1.1939 |     39.400 |     7.8
  119 |   1.0330 |     34.639 |   1.1974 |     39.124 |     7.8
  120 |   1.0333 |     34.596 |   1.2035 |     39.369 |     7.9
  121 |   1.0321 |     34.710 |   1.1996 |     38.940 |     8.0
  122 |   1.0306 |     34.601 |   1.2005 |     39.277 |     8.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 421,666

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6041 |     70.069 |   2.0613 |     58.272 |     0.0
    2 |   1.8222 |     52.059 |   1.6029 |     45.466 |     0.1
    3 |   1.5259 |     46.381 |   1.4605 |     45.987 |     0.1
    4 |   1.4446 |     46.364 |   1.4169 |     45.588 |     0.1
    5 |   1.4123 |     46.348 |   1.3947 |     45.466 |     0.1
    6 |   1.3883 |     46.310 |   1.3711 |     45.987 |     0.2
    7 |   1.3691 |     46.066 |   1.3508 |     44.363 |     0.2
    8 |   1.3457 |     45.497 |   1.3332 |     44.884 |     0.2
    9 |   1.3224 |     44.706 |   1.3013 |     42.708 |     0.2
   10 |   1.2924 |     43.520 |   1.2772 |     42.862 |     0.3
   11 |   1.2641 |     43.054 |   1.2598 |     41.820 |     0.3
   12 |   1.2507 |     42.658 |   1.2438 |     42.096 |     0.3
   13 |   1.2367 |     41.927 |   1.2332 |     40.870 |     0.3
   14 |   1.2138 |     41.147 |   1.2176 |     40.594 |     0.4
   15 |   1.1916 |     40.681 |   1.2017 |     41.146 |     0.4
   16 |   1.1803 |     39.927 |   1.1855 |     39.645 |     0.4
   17 |   1.1603 |     39.467 |   1.1736 |     39.308 |     0.4
   18 |   1.1418 |     38.827 |   1.1567 |     38.235 |     0.5
   19 |   1.1244 |     37.868 |   1.1441 |     38.113 |     0.5
   20 |   1.1099 |     37.446 |   1.1227 |     37.316 |     0.5
   21 |   1.0908 |     36.871 |   1.1224 |     37.286 |     0.5
   22 |   1.0703 |     36.259 |   1.1001 |     36.979 |     0.6
   23 |   1.0572 |     35.707 |   1.0901 |     35.784 |     0.6
   24 |   1.0478 |     35.213 |   1.0928 |     35.509 |     0.6
   25 |   1.0337 |     34.878 |   1.0845 |     36.275 |     0.6
   26 |   1.0135 |     34.520 |   1.0848 |     36.918 |     0.7
   27 |   1.0030 |     33.821 |   1.0744 |     35.968 |     0.7
   28 |   0.9860 |     33.295 |   1.0729 |     34.436 |     0.7
   29 |   0.9704 |     32.580 |   1.0517 |     34.436 |     0.8
   30 |   0.9583 |     32.466 |   1.0439 |     33.762 |     0.8
   31 |   0.9457 |     31.930 |   1.0283 |     33.548 |     0.8
   32 |   0.9248 |     30.987 |   1.0275 |     33.272 |     0.8
   33 |   0.9174 |     30.933 |   1.0461 |     33.732 |     0.9
   34 |   0.9012 |     30.007 |   1.0121 |     32.537 |     0.9
   35 |   0.8943 |     29.801 |   0.9981 |     32.445 |     0.9
   36 |   0.8774 |     29.367 |   1.0113 |     33.150 |     0.9
   37 |   0.8596 |     28.668 |   1.0106 |     32.721 |     1.0
   38 |   0.8494 |     27.991 |   1.0033 |     32.567 |     1.0
   39 |   0.8318 |     27.893 |   0.9916 |     32.169 |     1.0
   40 |   0.8205 |     26.994 |   0.9775 |     31.097 |     1.0
   41 |   0.8126 |     27.081 |   0.9817 |     31.618 |     1.1
   42 |   0.8137 |     27.352 |   0.9843 |     31.955 |     1.1
   43 |   0.7881 |     26.252 |   0.9630 |     31.005 |     1.1
   44 |   0.7796 |     25.986 |   0.9596 |     30.453 |     1.1
   45 |   0.7687 |     25.542 |   0.9504 |     29.412 |     1.2
   46 |   0.7549 |     25.011 |   0.9496 |     29.718 |     1.2
   47 |   0.7441 |     25.070 |   0.9566 |     30.086 |     1.2
   48 |   0.7309 |     24.420 |   0.9548 |     30.178 |     1.2
   49 |   0.7311 |     24.036 |   0.9573 |     29.718 |     1.3
   50 |   0.7208 |     23.884 |   0.9513 |     29.718 |     1.3
   51 |   0.7117 |     23.792 |   0.9444 |     30.055 |     1.3
   52 |   0.7046 |     23.461 |   0.9454 |     29.534 |     1.3
   53 |   0.7005 |     23.147 |   0.9660 |     29.749 |     1.4
   54 |   0.6873 |     22.730 |   0.9379 |     28.339 |     1.4
   55 |   0.6731 |     22.253 |   0.9567 |     28.707 |     1.4
   56 |   0.6661 |     22.139 |   0.9508 |     28.615 |     1.4
   57 |   0.6743 |     22.107 |   0.9562 |     28.891 |     1.5
   58 |   0.6792 |     22.567 |   0.9477 |     29.596 |     1.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 306,466

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4564 |     88.334 |   3.2190 |     73.683 |     0.0
    2 |   2.8286 |     80.489 |   2.6235 |     63.051 |     0.0
    3 |   2.4790 |     59.926 |   2.3858 |     58.824 |     0.1
    4 |   2.2896 |     59.049 |   2.2351 |     58.824 |     0.1
    5 |   2.1667 |     58.507 |   2.1258 |     57.537 |     0.1
    6 |   2.0671 |     55.716 |   2.0359 |     53.860 |     0.1
    7 |   1.9893 |     54.026 |   1.9620 |     53.891 |     0.1
    8 |   1.9234 |     53.571 |   1.8974 |     51.134 |     0.2
    9 |   1.8628 |     49.041 |   1.8369 |     48.346 |     0.2
   10 |   1.8086 |     48.607 |   1.7801 |     48.346 |     0.2
   11 |   1.7543 |     48.607 |   1.7263 |     48.346 |     0.2
   12 |   1.7054 |     48.607 |   1.6782 |     48.346 |     0.2
   13 |   1.6591 |     48.385 |   1.6355 |     46.262 |     0.3
   14 |   1.6239 |     46.299 |   1.5997 |     45.466 |     0.3
   15 |   1.5897 |     46.213 |   1.5707 |     45.466 |     0.3
   16 |   1.5624 |     46.213 |   1.5459 |     45.466 |     0.3
   17 |   1.5404 |     46.213 |   1.5239 |     45.466 |     0.3
   18 |   1.5188 |     46.213 |   1.5066 |     45.466 |     0.3
   19 |   1.5058 |     46.213 |   1.4922 |     45.466 |     0.4
   20 |   1.4904 |     46.213 |   1.4793 |     45.466 |     0.4
   21 |   1.4776 |     46.213 |   1.4687 |     45.466 |     0.4
   22 |   1.4685 |     46.213 |   1.4589 |     45.466 |     0.4
   23 |   1.4571 |     46.213 |   1.4505 |     45.466 |     0.4
   24 |   1.4498 |     46.213 |   1.4426 |     45.466 |     0.5
   25 |   1.4424 |     46.213 |   1.4354 |     45.466 |     0.5
   26 |   1.4347 |     46.213 |   1.4297 |     45.466 |     0.5
   27 |   1.4313 |     46.207 |   1.4235 |     45.466 |     0.5
   28 |   1.4200 |     46.169 |   1.4176 |     45.343 |     0.5
   29 |   1.4124 |     46.142 |   1.4132 |     45.374 |     0.6
   30 |   1.4086 |     45.817 |   1.4061 |     45.312 |     0.6
   31 |   1.4051 |     45.530 |   1.4019 |     45.037 |     0.6
   32 |   1.3976 |     45.340 |   1.3976 |     44.638 |     0.6
   33 |   1.3890 |     45.053 |   1.3930 |     45.098 |     0.6
   34 |   1.3850 |     45.075 |   1.3884 |     45.190 |     0.7
   35 |   1.3782 |     45.172 |   1.3840 |     44.914 |     0.7
   36 |   1.3709 |     44.988 |   1.3810 |     44.975 |     0.7
   37 |   1.3638 |     44.744 |   1.3765 |     44.945 |     0.7
   38 |   1.3569 |     44.690 |   1.3727 |     45.159 |     0.7
   39 |   1.3534 |     44.500 |   1.3676 |     44.853 |     0.8
   40 |   1.3446 |     44.533 |   1.3620 |     44.914 |     0.8
   41 |   1.3394 |     44.674 |   1.3580 |     44.853 |     0.8
   42 |   1.3315 |     44.376 |   1.3540 |     45.098 |     0.8
   43 |   1.3290 |     43.953 |   1.3523 |     44.853 |     0.8
   44 |   1.3189 |     43.623 |   1.3478 |     44.761 |     0.8
   45 |   1.3153 |     43.422 |   1.3435 |     44.148 |     0.9
   46 |   1.3130 |     43.151 |   1.3434 |     44.210 |     0.9
   47 |   1.3035 |     42.956 |   1.3352 |     43.781 |     0.9
   48 |   1.2971 |     42.805 |   1.3322 |     43.413 |     0.9
   49 |   1.2887 |     42.783 |   1.3291 |     43.015 |     0.9
   50 |   1.2842 |     42.376 |   1.3250 |     43.076 |     1.0
   51 |   1.2810 |     42.360 |   1.3228 |     43.321 |     1.0
   52 |   1.2754 |     42.236 |   1.3202 |     43.382 |     1.0
   53 |   1.2680 |     42.089 |   1.3174 |     43.229 |     1.0
   54 |   1.2626 |     42.003 |   1.3158 |     42.800 |     1.0
   55 |   1.2630 |     41.997 |   1.3108 |     43.199 |     1.1
   56 |   1.2510 |     41.791 |   1.3095 |     42.892 |     1.1
   57 |   1.2458 |     41.537 |   1.3054 |     42.647 |     1.1
   58 |   1.2432 |     41.049 |   1.3022 |     42.371 |     1.1
   59 |   1.2348 |     41.044 |   1.2979 |     42.433 |     1.1
   60 |   1.2302 |     40.632 |   1.2961 |     41.973 |     1.2
   61 |   1.2227 |     40.323 |   1.2931 |     41.912 |     1.2
   62 |   1.2186 |     40.393 |   1.2914 |     41.820 |     1.2
   63 |   1.2159 |     40.193 |   1.2905 |     41.667 |     1.2
   64 |   1.2089 |     39.727 |   1.2827 |     41.146 |     1.2
   65 |   1.1982 |     39.613 |   1.2813 |     41.176 |     1.3
   66 |   1.1948 |     39.364 |   1.2807 |     41.667 |     1.3
   67 |   1.1861 |     39.071 |   1.2758 |     41.176 |     1.3
   68 |   1.1807 |     38.974 |   1.2713 |     40.962 |     1.3
   69 |   1.1753 |     38.687 |   1.2705 |     41.176 |     1.3
   70 |   1.1708 |     38.481 |   1.2648 |     40.411 |     1.3
   71 |   1.1597 |     38.183 |   1.2615 |     40.349 |     1.4
   72 |   1.1566 |     37.760 |   1.2590 |     40.564 |     1.4
   73 |   1.1470 |     37.820 |   1.2573 |     39.982 |     1.4
   74 |   1.1407 |     37.294 |   1.2519 |     39.890 |     1.4
   75 |   1.1325 |     37.191 |   1.2525 |     39.951 |     1.4
   76 |   1.1278 |     37.039 |   1.2446 |     39.951 |     1.5
   77 |   1.1197 |     36.709 |   1.2512 |     40.165 |     1.5
   78 |   1.1148 |     36.774 |   1.2404 |     39.277 |     1.5
   79 |   1.1065 |     36.221 |   1.2417 |     39.798 |     1.5
   80 |   1.1035 |     36.037 |   1.2428 |     40.012 |     1.5
   81 |   1.0973 |     35.907 |   1.2310 |     39.185 |     1.6
   82 |   1.0867 |     35.642 |   1.2272 |     38.971 |     1.6
   83 |   1.0826 |     35.582 |   1.2332 |     39.246 |     1.6
   84 |   1.0734 |     34.997 |   1.2228 |     39.032 |     1.6
   85 |   1.0673 |     34.899 |   1.2159 |     39.246 |     1.6
   86 |   1.0592 |     34.737 |   1.2141 |     38.603 |     1.7
   87 |   1.0540 |     34.422 |   1.2144 |     38.297 |     1.7
   88 |   1.0460 |     34.108 |   1.2101 |     37.837 |     1.7
   89 |   1.0385 |     33.761 |   1.2114 |     38.143 |     1.7
   90 |   1.0334 |     33.425 |   1.2111 |     38.388 |     1.7
   91 |   1.0296 |     33.366 |   1.2058 |     37.806 |     1.8
   92 |   1.0204 |     32.943 |   1.1983 |     37.377 |     1.8
   93 |   1.0147 |     32.553 |   1.2021 |     37.561 |     1.8
   94 |   1.0070 |     32.239 |   1.1999 |     37.898 |     1.8
   95 |   1.0031 |     32.190 |   1.1956 |     37.377 |     1.8
   96 |   0.9976 |     31.903 |   1.1964 |     36.918 |     1.8
   97 |   0.9927 |     31.621 |   1.1952 |     37.224 |     1.9
   98 |   0.9884 |     31.843 |   1.1920 |     37.377 |     1.9
   99 |   0.9797 |     31.377 |   1.1889 |     37.010 |     1.9
  100 |   0.9743 |     31.112 |   1.1884 |     36.703 |     1.9
  101 |   0.9669 |     31.090 |   1.1856 |     36.734 |     1.9
  102 |   0.9634 |     30.689 |   1.1832 |     36.489 |     2.0
  103 |   0.9589 |     30.586 |   1.1805 |     36.366 |     2.0
  104 |   0.9534 |     30.326 |   1.1863 |     36.397 |     2.0
  105 |   0.9454 |     30.093 |   1.1788 |     36.550 |     2.0
  106 |   0.9448 |     30.142 |   1.1797 |     36.336 |     2.0
  107 |   0.9357 |     30.007 |   1.1824 |     36.581 |     2.1
  108 |   0.9287 |     29.963 |   1.1871 |     36.581 |     2.1
  109 |   0.9236 |     29.671 |   1.1767 |     36.458 |     2.1
  110 |   0.9202 |     29.215 |   1.1777 |     36.213 |     2.1
  111 |   0.9133 |     29.058 |   1.1818 |     35.846 |     2.1
  112 |   0.9080 |     28.787 |   1.1847 |     36.458 |     2.2
  113 |   0.9056 |     28.712 |   1.1764 |     35.815 |     2.2
  114 |   0.8977 |     28.349 |   1.1785 |     35.999 |     2.2
  115 |   0.8971 |     28.289 |   1.1726 |     35.509 |     2.2
  116 |   0.8918 |     28.110 |   1.1778 |     35.662 |     2.2
  117 |   0.8816 |     27.926 |   1.1743 |     35.355 |     2.2
  118 |   0.8799 |     27.628 |   1.1796 |     35.723 |     2.3
  119 |   0.8734 |     27.590 |   1.1752 |     35.355 |     2.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 411,138

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4133 |     88.015 |   3.0970 |     83.027 |     0.0
    2 |   2.7768 |     82.531 |   2.6137 |     68.015 |     0.1
    3 |   2.4835 |     60.696 |   2.3821 |     58.824 |     0.1
    4 |   2.2964 |     58.382 |   2.2251 |     58.824 |     0.1
    5 |   2.1626 |     57.542 |   2.1055 |     58.487 |     0.1
    6 |   2.0633 |     56.686 |   2.0138 |     55.300 |     0.2
    7 |   1.9829 |     55.462 |   1.9389 |     53.156 |     0.2
    8 |   1.9130 |     51.474 |   1.8717 |     48.438 |     0.2
    9 |   1.8485 |     49.382 |   1.8089 |     48.346 |     0.2
   10 |   1.7895 |     49.052 |   1.7536 |     48.376 |     0.3
   11 |   1.7409 |     48.862 |   1.7036 |     48.376 |     0.3
   12 |   1.6906 |     48.662 |   1.6560 |     48.346 |     0.3
   13 |   1.6451 |     48.635 |   1.6146 |     48.346 |     0.3
   14 |   1.6079 |     48.645 |   1.5801 |     48.346 |     0.4
   15 |   1.5738 |     48.250 |   1.5512 |     45.650 |     0.4
   16 |   1.5491 |     46.343 |   1.5277 |     45.466 |     0.4
   17 |   1.5260 |     46.148 |   1.5071 |     45.466 |     0.4
   18 |   1.5067 |     46.137 |   1.4900 |     45.466 |     0.5
   19 |   1.4902 |     46.126 |   1.4753 |     45.404 |     0.5
   20 |   1.4743 |     45.980 |   1.4615 |     45.343 |     0.5
   21 |   1.4592 |     46.023 |   1.4498 |     45.129 |     0.6
   22 |   1.4472 |     45.947 |   1.4385 |     45.159 |     0.6
   23 |   1.4385 |     45.958 |   1.4290 |     45.129 |     0.6
   24 |   1.4312 |     45.958 |   1.4238 |     45.159 |     0.6
   25 |   1.4190 |     45.833 |   1.4125 |     45.129 |     0.7
   26 |   1.4096 |     45.590 |   1.4056 |     45.006 |     0.7
   27 |   1.4043 |     45.611 |   1.3986 |     44.792 |     0.7
   28 |   1.3944 |     45.243 |   1.3930 |     44.884 |     0.7
   29 |   1.3904 |     45.048 |   1.3882 |     44.792 |     0.8
   30 |   1.3804 |     45.031 |   1.3817 |     44.424 |     0.8
   31 |   1.3748 |     44.880 |   1.3757 |     44.485 |     0.8
   32 |   1.3687 |     44.847 |   1.3715 |     44.730 |     0.8
   33 |   1.3593 |     44.555 |   1.3658 |     44.393 |     0.9
   34 |   1.3545 |     44.652 |   1.3592 |     44.363 |     0.9
   35 |   1.3453 |     44.419 |   1.3539 |     44.424 |     0.9
   36 |   1.3409 |     44.419 |   1.3516 |     44.148 |     0.9
   37 |   1.3340 |     44.181 |   1.3431 |     44.271 |     1.0
   38 |   1.3245 |     44.175 |   1.3392 |     44.118 |     1.0
   39 |   1.3148 |     44.067 |   1.3295 |     44.118 |     1.0
   40 |   1.3094 |     44.121 |   1.3255 |     43.964 |     1.1
   41 |   1.3006 |     43.829 |   1.3167 |     43.597 |     1.1
   42 |   1.2886 |     43.606 |   1.3120 |     44.118 |     1.1
   43 |   1.2820 |     43.336 |   1.3043 |     43.290 |     1.1
   44 |   1.2684 |     43.135 |   1.2968 |     43.168 |     1.2
   45 |   1.2600 |     42.761 |   1.2914 |     43.290 |     1.2
   46 |   1.2516 |     42.154 |   1.2855 |     42.525 |     1.2
   47 |   1.2459 |     42.084 |   1.2809 |     42.800 |     1.2
   48 |   1.2367 |     41.499 |   1.2758 |     42.249 |     1.3
   49 |   1.2279 |     41.152 |   1.2719 |     41.759 |     1.3
   50 |   1.2225 |     41.049 |   1.2678 |     42.371 |     1.3
   51 |   1.2156 |     40.849 |   1.2654 |     41.912 |     1.3
   52 |   1.2048 |     40.518 |   1.2588 |     42.402 |     1.4
   53 |   1.2004 |     40.231 |   1.2556 |     42.126 |     1.4
   54 |   1.1939 |     40.057 |   1.2495 |     42.157 |     1.4
   55 |   1.1868 |     39.754 |   1.2473 |     41.973 |     1.4
   56 |   1.1803 |     39.933 |   1.2490 |     42.279 |     1.5
   57 |   1.1746 |     39.499 |   1.2378 |     42.004 |     1.5
   58 |   1.1657 |     39.310 |   1.2351 |     42.034 |     1.5
   59 |   1.1598 |     39.169 |   1.2330 |     41.391 |     1.5
   60 |   1.1532 |     38.719 |   1.2328 |     41.942 |     1.6
   61 |   1.1482 |     38.486 |   1.2265 |     41.360 |     1.6
   62 |   1.1418 |     38.432 |   1.2207 |     41.513 |     1.6
   63 |   1.1355 |     38.026 |   1.2146 |     40.931 |     1.7
   64 |   1.1290 |     37.782 |   1.2122 |     41.207 |     1.7
   65 |   1.1221 |     37.424 |   1.2046 |     40.686 |     1.7
   66 |   1.1141 |     37.337 |   1.2033 |     40.533 |     1.7
   67 |   1.1075 |     37.186 |   1.1983 |     41.176 |     1.8
   68 |   1.0984 |     36.796 |   1.1938 |     40.288 |     1.8
   69 |   1.0949 |     36.763 |   1.1919 |     40.227 |     1.8
   70 |   1.0922 |     36.124 |   1.1861 |     39.553 |     1.8
   71 |   1.0830 |     35.782 |   1.1861 |     39.430 |     1.9
   72 |   1.0771 |     35.891 |   1.1818 |     39.645 |     1.9
   73 |   1.0726 |     35.446 |   1.1735 |     38.695 |     1.9
   74 |   1.0642 |     35.436 |   1.1752 |     39.461 |     1.9
   75 |   1.0570 |     34.823 |   1.1717 |     39.062 |     2.0
   76 |   1.0512 |     34.607 |   1.1659 |     38.542 |     2.0
   77 |   1.0453 |     34.639 |   1.1612 |     38.511 |     2.0
   78 |   1.0363 |     34.607 |   1.1612 |     37.929 |     2.1
   79 |   1.0322 |     34.352 |   1.1557 |     38.235 |     2.1
   80 |   1.0256 |     33.583 |   1.1517 |     37.408 |     2.1
   81 |   1.0264 |     33.577 |   1.1454 |     37.806 |     2.1
   82 |   1.0110 |     33.176 |   1.1442 |     37.469 |     2.2
   83 |   1.0091 |     33.333 |   1.1371 |     37.316 |     2.2
   84 |   1.0029 |     33.052 |   1.1392 |     37.500 |     2.2
   85 |   0.9982 |     32.965 |   1.1311 |     36.642 |     2.2
   86 |   0.9929 |     32.575 |   1.1329 |     37.316 |     2.3
   87 |   0.9870 |     32.456 |   1.1252 |     36.029 |     2.3
   88 |   0.9772 |     31.865 |   1.1257 |     35.723 |     2.3
   89 |   0.9752 |     31.979 |   1.1192 |     36.029 |     2.3
   90 |   0.9736 |     31.800 |   1.1170 |     35.692 |     2.4
   91 |   0.9665 |     31.432 |   1.1142 |     35.876 |     2.4
   92 |   0.9543 |     31.334 |   1.1096 |     35.938 |     2.4
   93 |   0.9475 |     31.041 |   1.1050 |     35.325 |     2.4
   94 |   0.9459 |     31.068 |   1.1056 |     35.754 |     2.5
   95 |   0.9389 |     30.700 |   1.1106 |     35.784 |     2.5
   96 |   0.9320 |     30.543 |   1.1016 |     35.417 |     2.5
   97 |   0.9304 |     30.462 |   1.0984 |     34.896 |     2.6
   98 |   0.9260 |     30.283 |   1.1008 |     35.417 |     2.6
   99 |   0.9207 |     30.126 |   1.0912 |     34.773 |     2.6
  100 |   0.9143 |     29.909 |   1.0898 |     35.049 |     2.6
  101 |   0.9079 |     29.871 |   1.0875 |     34.926 |     2.7
  102 |   0.9028 |     29.606 |   1.0880 |     34.161 |     2.7
  103 |   0.9016 |     29.503 |   1.0770 |     34.559 |     2.7
  104 |   0.8961 |     29.465 |   1.0834 |     34.559 |     2.7
  105 |   0.8886 |     29.205 |   1.0818 |     34.620 |     2.8
  106 |   0.8871 |     29.004 |   1.0840 |     34.283 |     2.8
  107 |   0.8795 |     28.847 |   1.0743 |     34.099 |     2.8
  108 |   0.8798 |     28.630 |   1.0734 |     33.670 |     2.8
  109 |   0.8699 |     28.462 |   1.0692 |     33.946 |     2.9
  110 |   0.8683 |     28.560 |   1.0655 |     34.069 |     2.9
  111 |   0.8617 |     28.105 |   1.0724 |     34.130 |     2.9
  112 |   0.8581 |     28.045 |   1.0613 |     34.007 |     2.9
  113 |   0.8520 |     27.742 |   1.0618 |     33.640 |     3.0
  114 |   0.8511 |     27.633 |   1.0600 |     33.885 |     3.0
  115 |   0.8460 |     27.541 |   1.0582 |     33.762 |     3.0
  116 |   0.8448 |     27.530 |   1.0568 |     33.333 |     3.1
  117 |   0.8370 |     27.368 |   1.0590 |     33.456 |     3.1
  118 |   0.8327 |     27.016 |   1.0598 |     33.578 |     3.1
  119 |   0.8293 |     26.712 |   1.0622 |     33.211 |     3.1
  120 |   0.8285 |     27.059 |   1.0544 |     32.996 |     3.2
  121 |   0.8186 |     26.577 |   1.0523 |     33.425 |     3.2
  122 |   0.8154 |     26.566 |   1.0521 |     33.027 |     3.2
  123 |   0.8136 |     26.338 |   1.0476 |     32.904 |     3.2
  124 |   0.8058 |     25.964 |   1.0428 |     32.322 |     3.3
  125 |   0.8107 |     26.414 |   1.0369 |     32.598 |     3.3
  126 |   0.8007 |     25.775 |   1.0401 |     32.261 |     3.3
  127 |   0.7928 |     25.574 |   1.0352 |     32.721 |     3.3
  128 |   0.7871 |     25.396 |   1.0344 |     32.230 |     3.4
  129 |   0.7840 |     25.271 |   1.0431 |     32.384 |     3.4
  130 |   0.7816 |     25.016 |   1.0374 |     32.047 |     3.4
  131 |   0.7867 |     25.417 |   1.0374 |     32.138 |     3.4
  132 |   0.7776 |     24.897 |   1.0315 |     31.464 |     3.5
  133 |   0.7681 |     24.827 |   1.0310 |     31.648 |     3.5
  134 |   0.7701 |     24.686 |   1.0289 |     31.250 |     3.5
  135 |   0.7611 |     24.442 |   1.0259 |     32.108 |     3.5
  136 |   0.7608 |     24.512 |   1.0235 |     31.219 |     3.6
  137 |   0.7555 |     24.057 |   1.0228 |     31.495 |     3.6
  138 |   0.7560 |     24.155 |   1.0247 |     31.342 |     3.6
  139 |   0.7509 |     24.073 |   1.0337 |     31.281 |     3.7
  140 |   0.7399 |     23.575 |   1.0220 |     31.403 |     3.7
  141 |   0.7431 |     23.483 |   1.0286 |     31.771 |     3.7
  142 |   0.7380 |     23.537 |   1.0243 |     31.219 |     3.7
  143 |   0.7329 |     23.483 |   1.0238 |     31.281 |     3.8
  144 |   0.7265 |     23.071 |   1.0195 |     30.944 |     3.8
  145 |   0.7244 |     23.152 |   1.0248 |     31.464 |     3.8
  146 |   0.7184 |     22.941 |   1.0196 |     31.005 |     3.8
  147 |   0.7202 |     22.925 |   1.0239 |     31.158 |     3.9
  148 |   0.7113 |     22.632 |   1.0221 |     30.515 |     3.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 233,890

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7471 |     71.554 |   2.2409 |     58.701 |     0.0
    2 |   2.0598 |     55.868 |   1.8933 |     51.287 |     0.0
    3 |   1.7632 |     49.523 |   1.6369 |     45.650 |     0.1
    4 |   1.5757 |     46.332 |   1.5177 |     45.680 |     0.1
    5 |   1.4941 |     46.229 |   1.4546 |     45.312 |     0.1
    6 |   1.4371 |     46.007 |   1.4161 |     45.098 |     0.1
    7 |   1.4001 |     45.915 |   1.3781 |     45.588 |     0.1
    8 |   1.3620 |     45.248 |   1.3498 |     44.822 |     0.2
    9 |   1.3300 |     44.408 |   1.3230 |     43.597 |     0.2
   10 |   1.2990 |     43.395 |   1.2991 |     42.034 |     0.2
   11 |   1.2685 |     42.203 |   1.2737 |     41.513 |     0.2
   12 |   1.2403 |     41.201 |   1.2481 |     41.054 |     0.2
   13 |   1.2148 |     40.626 |   1.2227 |     40.778 |     0.3
   14 |   1.1916 |     39.835 |   1.2150 |     40.380 |     0.3
   15 |   1.1712 |     39.050 |   1.1932 |     39.369 |     0.3
   16 |   1.1455 |     38.264 |   1.1780 |     38.634 |     0.3
   17 |   1.1280 |     37.424 |   1.1568 |     38.082 |     0.3
   18 |   1.1016 |     36.573 |   1.1320 |     37.194 |     0.4
   19 |   1.0761 |     35.566 |   1.1230 |     37.010 |     0.4
   20 |   1.0568 |     34.650 |   1.1138 |     36.213 |     0.4
   21 |   1.0418 |     34.097 |   1.1075 |     36.489 |     0.4
   22 |   1.0191 |     33.100 |   1.0923 |     34.988 |     0.4
   23 |   0.9997 |     31.984 |   1.0862 |     35.662 |     0.4
   24 |   0.9771 |     31.529 |   1.0669 |     34.773 |     0.5
   25 |   0.9626 |     31.171 |   1.0541 |     34.681 |     0.5
   26 |   0.9415 |     30.169 |   1.0546 |     33.793 |     0.5
   27 |   0.9272 |     29.817 |   1.0370 |     34.773 |     0.5
   28 |   0.9192 |     29.253 |   1.0374 |     34.406 |     0.5
   29 |   0.9064 |     28.988 |   1.0288 |     33.425 |     0.6
   30 |   0.8848 |     28.202 |   1.0141 |     33.303 |     0.6
   31 |   0.8620 |     27.720 |   1.0115 |     32.353 |     0.6
   32 |   0.8526 |     27.476 |   1.0092 |     32.169 |     0.6
   33 |   0.8410 |     26.869 |   0.9891 |     32.047 |     0.6
   34 |   0.8230 |     26.696 |   0.9992 |     31.955 |     0.7
   35 |   0.8144 |     26.523 |   1.0018 |     31.710 |     0.7
   36 |   0.8153 |     26.306 |   0.9932 |     31.556 |     0.7
   37 |   0.7908 |     25.179 |   0.9927 |     31.710 |     0.7
   38 |   0.7768 |     25.049 |   0.9821 |     31.036 |     0.7
   39 |   0.7786 |     25.092 |   0.9662 |     29.718 |     0.8
   40 |   0.7646 |     24.063 |   0.9761 |     30.147 |     0.8
   41 |   0.7524 |     23.943 |   0.9596 |     29.442 |     0.8
   42 |   0.7549 |     23.824 |   0.9829 |     30.852 |     0.8
   43 |   0.7290 |     23.418 |   0.9725 |     30.362 |     0.8
   44 |   0.7186 |     22.871 |   0.9616 |     29.442 |     0.9
   45 |   0.7122 |     22.887 |   0.9686 |     30.882 |     0.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 316,162

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5520 |     66.607 |   1.9947 |     58.395 |     0.0
    2 |   1.7361 |     48.201 |   1.5490 |     45.466 |     0.0
    3 |   1.4923 |     46.158 |   1.4485 |     45.987 |     0.1
    4 |   1.4359 |     46.218 |   1.4205 |     45.466 |     0.1
    5 |   1.4127 |     46.299 |   1.4011 |     45.987 |     0.1
    6 |   1.3957 |     45.969 |   1.3772 |     45.466 |     0.1
    7 |   1.3727 |     44.977 |   1.3527 |     43.873 |     0.1
    8 |   1.3445 |     44.262 |   1.3459 |     44.271 |     0.2
    9 |   1.3273 |     44.056 |   1.3222 |     43.934 |     0.2
   10 |   1.3054 |     43.260 |   1.3019 |     42.586 |     0.2
   11 |   1.2832 |     42.474 |   1.2805 |     42.034 |     0.2
   12 |   1.2591 |     41.726 |   1.2728 |     41.667 |     0.2
   13 |   1.2416 |     40.951 |   1.2570 |     41.207 |     0.2
   14 |   1.2214 |     40.675 |   1.2491 |     41.238 |     0.3
   15 |   1.2077 |     40.030 |   1.2361 |     40.625 |     0.3
   16 |   1.1870 |     39.559 |   1.2379 |     39.828 |     0.3
   17 |   1.1702 |     39.256 |   1.2070 |     39.308 |     0.3
   18 |   1.1559 |     38.844 |   1.2115 |     39.920 |     0.3
   19 |   1.1285 |     38.118 |   1.1878 |     39.614 |     0.4
   20 |   1.1181 |     37.717 |   1.1872 |     39.124 |     0.4
   21 |   1.0947 |     37.175 |   1.1837 |     38.787 |     0.4
   22 |   1.0754 |     36.443 |   1.1665 |     38.572 |     0.4
   23 |   1.0570 |     35.549 |   1.1592 |     38.297 |     0.4
   24 |   1.0426 |     34.802 |   1.1444 |     37.408 |     0.5
   25 |   1.0227 |     34.103 |   1.1369 |     36.336 |     0.5
   26 |   1.0166 |     34.054 |   1.1304 |     36.887 |     0.5
   27 |   0.9911 |     32.732 |   1.1203 |     36.336 |     0.5
   28 |   0.9595 |     31.621 |   1.1141 |     35.846 |     0.5
   29 |   0.9460 |     31.301 |   1.1252 |     36.703 |     0.5
   30 |   0.9218 |     30.250 |   1.0998 |     35.202 |     0.6
   31 |   0.9002 |     29.708 |   1.1150 |     34.743 |     0.6
   32 |   0.8791 |     28.739 |   1.1085 |     35.325 |     0.6
   33 |   0.8694 |     28.495 |   1.0912 |     34.007 |     0.6
   34 |   0.8444 |     27.422 |   1.1011 |     34.743 |     0.6
   35 |   0.8292 |     26.858 |   1.0983 |     34.406 |     0.7
   36 |   0.8124 |     26.317 |   1.0830 |     34.222 |     0.7
   37 |   0.7872 |     25.303 |   1.0892 |     34.038 |     0.7
   38 |   0.7734 |     24.962 |   1.0816 |     33.977 |     0.7
   39 |   0.7558 |     24.193 |   1.0855 |     33.548 |     0.7
   40 |   0.7482 |     24.252 |   1.0715 |     32.751 |     0.8
   41 |   0.7444 |     23.998 |   1.0655 |     32.506 |     0.8
   42 |   0.7174 |     22.952 |   1.0655 |     32.353 |     0.8
   43 |   0.7050 |     22.665 |   1.0656 |     32.904 |     0.8
   44 |   0.6978 |     22.616 |   1.0849 |     32.874 |     0.8
   45 |   0.6810 |     21.895 |   1.1235 |     34.773 |     0.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 534,818

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2795 |     81.280 |   2.7438 |     83.333 |     0.0
    2 |   2.5249 |     66.921 |   2.3163 |     58.824 |     0.0
    3 |   2.1495 |     57.044 |   2.0134 |     54.688 |     0.0
    4 |   1.9400 |     53.040 |   1.8561 |     48.468 |     0.1
    5 |   1.8080 |     49.296 |   1.7400 |     48.346 |     0.1
    6 |   1.7063 |     48.759 |   1.6472 |     48.346 |     0.1
    7 |   1.6232 |     48.429 |   1.5732 |     45.680 |     0.1
    8 |   1.5549 |     46.343 |   1.5190 |     45.466 |     0.1
    9 |   1.5097 |     46.213 |   1.4837 |     45.466 |     0.1
   10 |   1.4796 |     46.213 |   1.4580 |     45.466 |     0.2
   11 |   1.4585 |     46.202 |   1.4397 |     45.466 |     0.2
   12 |   1.4385 |     46.223 |   1.4258 |     45.466 |     0.2
   13 |   1.4260 |     46.240 |   1.4125 |     45.466 |     0.2
   14 |   1.4117 |     46.234 |   1.3999 |     45.466 |     0.2
   15 |   1.4012 |     46.261 |   1.3905 |     45.466 |     0.2
   16 |   1.3877 |     46.148 |   1.3804 |     45.404 |     0.3
   17 |   1.3792 |     45.752 |   1.3747 |     44.516 |     0.3
   18 |   1.3713 |     45.552 |   1.3653 |     44.792 |     0.3
   19 |   1.3630 |     45.394 |   1.3585 |     44.638 |     0.3
   20 |   1.3564 |     45.264 |   1.3537 |     44.210 |     0.3
   21 |   1.3467 |     45.194 |   1.3479 |     44.240 |     0.3
   22 |   1.3411 |     45.134 |   1.3405 |     44.148 |     0.4
   23 |   1.3338 |     44.863 |   1.3351 |     44.087 |     0.4
   24 |   1.3286 |     44.993 |   1.3304 |     43.842 |     0.4
   25 |   1.3235 |     44.685 |   1.3249 |     43.995 |     0.4
   26 |   1.3151 |     44.701 |   1.3207 |     43.352 |     0.4
   27 |   1.3095 |     44.587 |   1.3149 |     43.413 |     0.4
   28 |   1.3045 |     44.181 |   1.3112 |     42.953 |     0.5
   29 |   1.3005 |     44.159 |   1.3056 |     43.137 |     0.5
   30 |   1.2918 |     43.758 |   1.3014 |     42.678 |     0.5
   31 |   1.2903 |     43.612 |   1.2987 |     42.647 |     0.5
   32 |   1.2828 |     43.368 |   1.2948 |     42.831 |     0.5
   33 |   1.2792 |     43.157 |   1.2895 |     41.973 |     0.5
   34 |   1.2695 |     42.799 |   1.2857 |     42.555 |     0.6
   35 |   1.2629 |     42.978 |   1.2827 |     42.647 |     0.6
   36 |   1.2564 |     42.702 |   1.2763 |     41.759 |     0.6
   37 |   1.2476 |     42.349 |   1.2700 |     41.973 |     0.6
   38 |   1.2441 |     41.813 |   1.2638 |     41.636 |     0.6
   39 |   1.2350 |     41.699 |   1.2608 |     41.697 |     0.6
   40 |   1.2327 |     41.423 |   1.2525 |     40.625 |     0.6
   41 |   1.2226 |     40.746 |   1.2499 |     41.299 |     0.7
   42 |   1.2160 |     40.843 |   1.2417 |     40.594 |     0.7
   43 |   1.2099 |     40.458 |   1.2357 |     40.104 |     0.7
   44 |   1.2035 |     39.971 |   1.2315 |     39.890 |     0.7
   45 |   1.1958 |     39.759 |   1.2230 |     39.461 |     0.7
   46 |   1.1910 |     39.808 |   1.2173 |     40.319 |     0.7
   47 |   1.1792 |     39.434 |   1.2138 |     39.982 |     0.8
   48 |   1.1768 |     39.277 |   1.2046 |     40.012 |     0.8
   49 |   1.1625 |     39.218 |   1.2080 |     39.890 |     0.8
   50 |   1.1593 |     39.071 |   1.1946 |     39.706 |     0.8
   51 |   1.1492 |     38.784 |   1.1850 |     39.553 |     0.8
   52 |   1.1418 |     38.692 |   1.1797 |     39.246 |     0.8
   53 |   1.1347 |     38.367 |   1.1721 |     38.817 |     0.9
   54 |   1.1261 |     38.188 |   1.1704 |     38.909 |     0.9
   55 |   1.1195 |     37.825 |   1.1611 |     38.388 |     0.9
   56 |   1.1109 |     37.457 |   1.1555 |     38.051 |     0.9
   57 |   1.1060 |     36.980 |   1.1525 |     38.051 |     0.9
   58 |   1.0951 |     36.785 |   1.1444 |     37.868 |     0.9
   59 |   1.0900 |     36.400 |   1.1425 |     37.347 |     1.0
   60 |   1.0806 |     36.026 |   1.1362 |     37.531 |     1.0
   61 |   1.0758 |     35.950 |   1.1288 |     37.163 |     1.0
   62 |   1.0657 |     35.712 |   1.1201 |     36.642 |     1.0
   63 |   1.0620 |     35.647 |   1.1197 |     36.765 |     1.0
   64 |   1.0527 |     35.241 |   1.1178 |     36.612 |     1.0
   65 |   1.0449 |     35.018 |   1.1068 |     36.734 |     1.1
   66 |   1.0388 |     34.796 |   1.1102 |     36.673 |     1.1
   67 |   1.0312 |     34.260 |   1.1005 |     36.612 |     1.1
   68 |   1.0280 |     34.303 |   1.1024 |     35.938 |     1.1
   69 |   1.0243 |     33.783 |   1.0919 |     36.091 |     1.1
   70 |   1.0154 |     33.761 |   1.0904 |     35.999 |     1.1
   71 |   1.0070 |     33.409 |   1.0858 |     35.692 |     1.2
   72 |   0.9969 |     33.192 |   1.0856 |     36.397 |     1.2
   73 |   0.9963 |     33.214 |   1.0766 |     35.631 |     1.2
   74 |   0.9923 |     32.922 |   1.0708 |     35.478 |     1.2
   75 |   0.9835 |     32.878 |   1.0721 |     35.202 |     1.2
   76 |   0.9790 |     32.358 |   1.0695 |     35.386 |     1.2
   77 |   0.9695 |     32.163 |   1.0704 |     35.202 |     1.3
   78 |   0.9663 |     32.212 |   1.0645 |     34.988 |     1.3
   79 |   0.9637 |     32.065 |   1.0620 |     35.141 |     1.3
   80 |   0.9543 |     31.849 |   1.0556 |     34.375 |     1.3
   81 |   0.9526 |     31.740 |   1.0526 |     34.773 |     1.3
   82 |   0.9439 |     31.150 |   1.0508 |     34.988 |     1.3
   83 |   0.9344 |     30.873 |   1.0519 |     34.222 |     1.4
   84 |   0.9325 |     30.917 |   1.0499 |     34.528 |     1.4
   85 |   0.9266 |     30.684 |   1.0453 |     34.191 |     1.4
   86 |   0.9220 |     30.364 |   1.0374 |     34.130 |     1.4
   87 |   0.9180 |     30.082 |   1.0372 |     34.375 |     1.4
   88 |   0.9090 |     29.958 |   1.0336 |     33.670 |     1.4
   89 |   0.9075 |     30.023 |   1.0334 |     34.252 |     1.4
   90 |   0.9021 |     29.741 |   1.0396 |     33.854 |     1.5
   91 |   0.9008 |     29.828 |   1.0292 |     33.885 |     1.5
   92 |   0.8959 |     29.405 |   1.0277 |     33.456 |     1.5
   93 |   0.8824 |     29.199 |   1.0303 |     33.732 |     1.5
   94 |   0.8795 |     28.988 |   1.0286 |     33.578 |     1.5
   95 |   0.8742 |     28.809 |   1.0303 |     33.609 |     1.5
   96 |   0.8698 |     28.554 |   1.0158 |     33.425 |     1.6
   97 |   0.8670 |     28.338 |   1.0244 |     33.670 |     1.6
   98 |   0.8621 |     28.381 |   1.0142 |     33.272 |     1.6
   99 |   0.8589 |     28.457 |   1.0157 |     33.395 |     1.6
  100 |   0.8561 |     28.153 |   1.0124 |     33.088 |     1.6
  101 |   0.8512 |     27.780 |   1.0145 |     33.272 |     1.6
  102 |   0.8449 |     27.796 |   1.0218 |     33.272 |     1.7
  103 |   0.8403 |     27.476 |   1.0072 |     33.824 |     1.7
  104 |   0.8379 |     27.314 |   1.0088 |     33.303 |     1.7
  105 |   0.8332 |     27.373 |   1.0161 |     33.058 |     1.7
  106 |   0.8252 |     27.259 |   1.0081 |     32.935 |     1.7
  107 |   0.8229 |     26.967 |   1.0118 |     33.088 |     1.7
  108 |   0.8186 |     26.864 |   1.0057 |     32.996 |     1.8
  109 |   0.8130 |     26.571 |   1.0088 |     32.966 |     1.8
  110 |   0.8096 |     26.506 |   1.0029 |     32.904 |     1.8
  111 |   0.8083 |     26.631 |   1.0041 |     33.088 |     1.8
  112 |   0.8062 |     26.306 |   1.0019 |     32.230 |     1.8
  113 |   0.7994 |     26.235 |   0.9967 |     32.721 |     1.8
  114 |   0.7955 |     26.154 |   1.0019 |     33.058 |     1.8
  115 |   0.7945 |     26.084 |   0.9934 |     32.445 |     1.9
  116 |   0.7842 |     25.791 |   1.0008 |     32.537 |     1.9
  117 |   0.7792 |     25.466 |   0.9993 |     32.843 |     1.9
  118 |   0.7774 |     25.569 |   1.0057 |     32.598 |     1.9
  119 |   0.7741 |     25.390 |   1.0038 |     32.904 |     1.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,676,962

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4663 |     94.300 |   3.3773 |     83.333 |     0.1
    2 |   3.1844 |     80.971 |   2.9696 |     82.077 |     0.1
    3 |   2.8517 |     81.925 |   2.7698 |     83.333 |     0.2
    4 |   2.7022 |     70.059 |   2.6479 |     67.341 |     0.3
    5 |   2.5900 |     66.412 |   2.5423 |     66.912 |     0.4
    6 |   2.4946 |     63.096 |   2.4532 |     58.824 |     0.4
    7 |   2.4165 |     60.446 |   2.3811 |     58.824 |     0.5
    8 |   2.3504 |     59.520 |   2.3177 |     58.824 |     0.6
    9 |   2.2943 |     59.038 |   2.2631 |     58.824 |     0.6
   10 |   2.2432 |     58.778 |   2.2142 |     58.824 |     0.7
   11 |   2.1994 |     58.425 |   2.1657 |     58.824 |     0.8
   12 |   2.1490 |     58.171 |   2.1115 |     58.824 |     0.8
   13 |   2.1029 |     57.656 |   2.0662 |     58.824 |     0.9
   14 |   2.0631 |     57.174 |   2.0273 |     58.824 |     1.0
   15 |   2.0247 |     56.914 |   1.9881 |     58.824 |     1.1
   16 |   1.9873 |     56.486 |   1.9508 |     57.598 |     1.1
   17 |   1.9503 |     54.979 |   1.9159 |     50.521 |     1.2
   18 |   1.9165 |     52.747 |   1.8830 |     48.070 |     1.3
   19 |   1.8823 |     50.954 |   1.8529 |     48.284 |     1.3
   20 |   1.8522 |     49.919 |   1.8245 |     48.284 |     1.4
   21 |   1.8274 |     49.507 |   1.7982 |     48.284 |     1.5
   22 |   1.7982 |     49.252 |   1.7733 |     48.376 |     1.6
   23 |   1.7749 |     49.204 |   1.7502 |     48.468 |     1.6
   24 |   1.7513 |     49.117 |   1.7262 |     48.223 |     1.7
   25 |   1.7300 |     48.819 |   1.7036 |     48.223 |     1.8
   26 |   1.7036 |     48.716 |   1.6816 |     48.162 |     1.8
   27 |   1.6839 |     48.694 |   1.6612 |     48.284 |     1.9
   28 |   1.6597 |     48.645 |   1.6377 |     48.192 |     2.0
   29 |   1.6393 |     48.521 |   1.6160 |     48.284 |     2.0
   30 |   1.6148 |     48.450 |   1.5920 |     48.100 |     2.1
   31 |   1.5933 |     47.621 |   1.5724 |     45.343 |     2.2
   32 |   1.5737 |     46.727 |   1.5546 |     45.221 |     2.3
   33 |   1.5572 |     46.148 |   1.5397 |     45.159 |     2.3
   34 |   1.5408 |     45.953 |   1.5235 |     45.190 |     2.4
   35 |   1.5251 |     45.752 |   1.5095 |     44.975 |     2.5
   36 |   1.5115 |     45.627 |   1.4976 |     44.853 |     2.5
   37 |   1.4981 |     45.454 |   1.4855 |     44.700 |     2.6
   38 |   1.4858 |     45.367 |   1.4758 |     44.730 |     2.7
   39 |   1.4739 |     45.199 |   1.4647 |     44.424 |     2.8
   40 |   1.4617 |     45.091 |   1.4545 |     44.301 |     2.8
   41 |   1.4552 |     44.918 |   1.4461 |     44.516 |     2.9
   42 |   1.4452 |     44.901 |   1.4358 |     44.240 |     3.0
   43 |   1.4367 |     44.804 |   1.4294 |     44.271 |     3.0
   44 |   1.4259 |     44.733 |   1.4217 |     44.056 |     3.1
   45 |   1.4165 |     44.668 |   1.4155 |     44.240 |     3.2
   46 |   1.4064 |     44.511 |   1.4090 |     44.424 |     3.3
   47 |   1.3990 |     44.414 |   1.4043 |     44.393 |     3.3
   48 |   1.3924 |     44.511 |   1.3967 |     44.118 |     3.4
   49 |   1.3857 |     44.533 |   1.3902 |     44.271 |     3.5
   50 |   1.3774 |     44.360 |   1.3837 |     44.363 |     3.5
   51 |   1.3696 |     44.311 |   1.3771 |     44.240 |     3.6
   52 |   1.3639 |     44.246 |   1.3739 |     44.271 |     3.7
   53 |   1.3579 |     44.186 |   1.3682 |     44.240 |     3.8
   54 |   1.3508 |     44.192 |   1.3604 |     44.118 |     3.8
   55 |   1.3415 |     44.175 |   1.3566 |     44.240 |     3.9
   56 |   1.3363 |     44.159 |   1.3551 |     44.393 |     4.0
   57 |   1.3310 |     44.029 |   1.3470 |     44.363 |     4.0
   58 |   1.3260 |     43.866 |   1.3452 |     44.332 |     4.1
   59 |   1.3185 |     43.769 |   1.3369 |     44.301 |     4.2
   60 |   1.3148 |     43.650 |   1.3338 |     43.903 |     4.3
   61 |   1.3049 |     43.525 |   1.3273 |     43.873 |     4.3
   62 |   1.3016 |     43.276 |   1.3279 |     43.842 |     4.4
   63 |   1.2979 |     43.238 |   1.3221 |     43.199 |     4.5
   64 |   1.2904 |     43.119 |   1.3182 |     43.168 |     4.5
   65 |   1.2855 |     42.777 |   1.3149 |     42.862 |     4.6
   66 |   1.2800 |     42.864 |   1.3090 |     42.678 |     4.7
   67 |   1.2772 |     42.799 |   1.3045 |     42.678 |     4.8
   68 |   1.2715 |     42.637 |   1.3019 |     42.555 |     4.8
   69 |   1.2685 |     42.653 |   1.2978 |     42.586 |     4.9
   70 |   1.2610 |     42.404 |   1.2990 |     42.800 |     4.9
   71 |   1.2605 |     42.349 |   1.2939 |     42.647 |     5.0
   72 |   1.2566 |     42.301 |   1.2912 |     42.586 |     5.1
   73 |   1.2503 |     42.062 |   1.2904 |     42.923 |     5.1
   74 |   1.2452 |     41.905 |   1.2891 |     42.586 |     5.2
   75 |   1.2447 |     41.851 |   1.2873 |     42.494 |     5.3
   76 |   1.2412 |     41.867 |   1.2821 |     42.218 |     5.4
   77 |   1.2355 |     41.607 |   1.2804 |     42.218 |     5.4
   78 |   1.2319 |     41.710 |   1.2812 |     42.279 |     5.5
   79 |   1.2279 |     41.683 |   1.2765 |     42.034 |     5.6
   80 |   1.2264 |     41.575 |   1.2746 |     42.126 |     5.6
   81 |   1.2241 |     41.705 |   1.2758 |     42.402 |     5.7
   82 |   1.2243 |     41.510 |   1.2717 |     42.433 |     5.8
   83 |   1.2185 |     41.309 |   1.2714 |     42.279 |     5.8
   84 |   1.2155 |     41.477 |   1.2672 |     42.096 |     5.9
   85 |   1.2144 |     41.320 |   1.2646 |     42.034 |     6.0
   86 |   1.2102 |     41.125 |   1.2658 |     42.034 |     6.0
   87 |   1.2093 |     41.277 |   1.2611 |     42.065 |     6.1
   88 |   1.2068 |     41.266 |   1.2614 |     42.004 |     6.2
   89 |   1.2071 |     41.244 |   1.2599 |     42.034 |     6.3
   90 |   1.2017 |     41.022 |   1.2656 |     42.157 |     6.3
   91 |   1.1980 |     41.049 |   1.2607 |     41.973 |     6.4
   92 |   1.1943 |     40.941 |   1.2606 |     42.004 |     6.5
   93 |   1.1938 |     40.865 |   1.2627 |     41.881 |     6.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 179,426

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.9098 |     79.492 |   2.4312 |     66.575 |     0.0
    2 |   2.1553 |     58.344 |   1.9679 |     58.824 |     0.0
    3 |   1.8355 |     50.081 |   1.7172 |     48.346 |     0.1
    4 |   1.6381 |     47.984 |   1.5601 |     45.466 |     0.1
    5 |   1.5212 |     46.213 |   1.4771 |     45.466 |     0.1
    6 |   1.4636 |     46.234 |   1.4421 |     45.466 |     0.1
    7 |   1.4356 |     46.288 |   1.4178 |     45.466 |     0.1
    8 |   1.4121 |     46.408 |   1.4037 |     45.987 |     0.1
    9 |   1.3951 |     46.126 |   1.3810 |     45.466 |     0.2
   10 |   1.3781 |     45.974 |   1.3654 |     45.282 |     0.2
   11 |   1.3633 |     45.730 |   1.3590 |     44.516 |     0.2
   12 |   1.3513 |     45.438 |   1.3459 |     44.700 |     0.2
   13 |   1.3435 |     45.416 |   1.3521 |     44.210 |     0.2
   14 |   1.3350 |     45.199 |   1.3350 |     44.547 |     0.2
   15 |   1.3276 |     45.059 |   1.3374 |     44.393 |     0.3
   16 |   1.3218 |     44.999 |   1.3369 |     44.179 |     0.3
   17 |   1.3151 |     44.993 |   1.3287 |     43.903 |     0.3
   18 |   1.3047 |     44.647 |   1.3245 |     44.547 |     0.3
   19 |   1.2994 |     44.934 |   1.3212 |     44.485 |     0.3
   20 |   1.2951 |     44.880 |   1.3120 |     44.210 |     0.3
   21 |   1.2884 |     44.761 |   1.3042 |     44.210 |     0.4
   22 |   1.2812 |     44.425 |   1.3044 |     43.903 |     0.4
   23 |   1.2738 |     44.240 |   1.2986 |     44.271 |     0.4
   24 |   1.2621 |     43.964 |   1.2933 |     43.781 |     0.4
   25 |   1.2580 |     43.276 |   1.2898 |     43.842 |     0.4
   26 |   1.2504 |     43.249 |   1.2829 |     42.862 |     0.4
   27 |   1.2381 |     42.680 |   1.2838 |     43.689 |     0.4
   28 |   1.2301 |     42.593 |   1.2738 |     43.199 |     0.5
   29 |   1.2246 |     42.490 |   1.2761 |     43.045 |     0.5
   30 |   1.2185 |     42.214 |   1.2613 |     42.708 |     0.5
   31 |   1.2040 |     41.537 |   1.2539 |     42.402 |     0.5
   32 |   1.1986 |     40.832 |   1.2475 |     41.820 |     0.5
   33 |   1.1798 |     39.705 |   1.2365 |     40.625 |     0.5
   34 |   1.1653 |     39.467 |   1.2373 |     40.901 |     0.6
   35 |   1.1474 |     38.708 |   1.2261 |     40.748 |     0.6
   36 |   1.1402 |     38.584 |   1.2061 |     39.828 |     0.6
   37 |   1.1312 |     38.058 |   1.2102 |     40.319 |     0.6
   38 |   1.1107 |     37.560 |   1.1945 |     40.411 |     0.6
   39 |   1.0976 |     36.985 |   1.1945 |     39.614 |     0.6
   40 |   1.0832 |     36.476 |   1.1872 |     39.216 |     0.7
   41 |   1.0697 |     35.994 |   1.1841 |     38.879 |     0.7
   42 |   1.0607 |     35.549 |   1.1850 |     38.909 |     0.7
   43 |   1.0525 |     35.219 |   1.1766 |     38.879 |     0.7
   44 |   1.0360 |     34.753 |   1.1619 |     38.450 |     0.7
   45 |   1.0223 |     34.059 |   1.1743 |     39.124 |     0.7
   46 |   1.0089 |     33.783 |   1.1668 |     38.235 |     0.8
   47 |   0.9997 |     33.366 |   1.1588 |     37.653 |     0.8
   48 |   0.9938 |     33.062 |   1.1764 |     38.787 |     0.8
   49 |   0.9976 |     33.241 |   1.1661 |     37.776 |     0.8
   50 |   0.9758 |     32.526 |   1.1663 |     37.071 |     0.8
   51 |   0.9642 |     32.071 |   1.1711 |     37.684 |     0.8
   52 |   0.9519 |     31.361 |   1.1551 |     37.102 |     0.8
   53 |   0.9355 |     30.846 |   1.1538 |     37.408 |     0.9
   54 |   0.9217 |     30.207 |   1.1584 |     36.366 |     0.9
   55 |   0.9460 |     31.226 |   1.1558 |     36.183 |     0.9
   56 |   0.9205 |     30.104 |   1.1438 |     36.213 |     0.9
   57 |   0.9020 |     29.562 |   1.1599 |     36.305 |     0.9
   58 |   0.8950 |     29.221 |   1.1577 |     35.754 |     0.9
   59 |   0.8802 |     28.451 |   1.1606 |     35.631 |     1.0
   60 |   0.8700 |     28.099 |   1.1475 |     35.294 |     1.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 566,082

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5179 |     97.578 |   3.4662 |     96.569 |     0.0
    2 |   3.3017 |     82.997 |   3.0961 |     80.974 |     0.1
    3 |   2.9493 |     82.298 |   2.8476 |     83.333 |     0.1
    4 |   2.7550 |     72.193 |   2.6908 |     67.341 |     0.1
    5 |   2.6145 |     66.499 |   2.5658 |     66.912 |     0.1
    6 |   2.5038 |     60.598 |   2.4613 |     58.824 |     0.2
    7 |   2.4099 |     59.162 |   2.3728 |     58.824 |     0.2
    8 |   2.3305 |     58.686 |   2.3004 |     58.824 |     0.2
    9 |   2.2650 |     58.144 |   2.2384 |     58.824 |     0.3
   10 |   2.2090 |     57.792 |   2.1856 |     58.824 |     0.3
   11 |   2.1582 |     57.358 |   2.1382 |     58.824 |     0.3
   12 |   2.1151 |     56.719 |   2.0957 |     58.824 |     0.4
   13 |   2.0764 |     56.404 |   2.0577 |     58.762 |     0.4
   14 |   2.0410 |     55.993 |   2.0217 |     53.768 |     0.4
   15 |   2.0068 |     55.603 |   1.9876 |     53.768 |     0.4
   16 |   1.9731 |     55.500 |   1.9544 |     53.768 |     0.5
   17 |   1.9409 |     55.120 |   1.9199 |     53.768 |     0.5
   18 |   1.9105 |     54.611 |   1.8862 |     53.768 |     0.5
   19 |   1.8772 |     52.086 |   1.8544 |     48.376 |     0.6
   20 |   1.8467 |     49.875 |   1.8241 |     48.346 |     0.6
   21 |   1.8170 |     49.279 |   1.7947 |     48.346 |     0.6
   22 |   1.7877 |     48.889 |   1.7656 |     48.346 |     0.6
   23 |   1.7612 |     48.911 |   1.7373 |     48.346 |     0.7
   24 |   1.7356 |     48.748 |   1.7124 |     48.346 |     0.7
   25 |   1.7127 |     48.179 |   1.6894 |     48.346 |     0.7
   26 |   1.6885 |     47.031 |   1.6679 |     45.466 |     0.8
   27 |   1.6703 |     46.370 |   1.6483 |     45.466 |     0.8
   28 |   1.6506 |     46.261 |   1.6305 |     45.466 |     0.8
   29 |   1.6349 |     46.272 |   1.6141 |     45.466 |     0.9
   30 |   1.6160 |     46.251 |   1.5988 |     45.466 |     0.9
   31 |   1.6053 |     46.256 |   1.5851 |     45.466 |     0.9
   32 |   1.5886 |     46.223 |   1.5721 |     45.466 |     0.9
   33 |   1.5757 |     46.218 |   1.5603 |     45.466 |     1.0
   34 |   1.5657 |     46.218 |   1.5497 |     45.466 |     1.0
   35 |   1.5559 |     46.213 |   1.5400 |     45.466 |     1.0
   36 |   1.5430 |     46.218 |   1.5313 |     45.466 |     1.1
   37 |   1.5362 |     46.223 |   1.5231 |     45.435 |     1.1
   38 |   1.5307 |     46.207 |   1.5156 |     45.466 |     1.1
   39 |   1.5197 |     46.229 |   1.5082 |     45.435 |     1.1
   40 |   1.5137 |     46.218 |   1.5014 |     45.496 |     1.2
   41 |   1.5082 |     46.261 |   1.4947 |     45.496 |     1.2
   42 |   1.5008 |     46.202 |   1.4889 |     45.558 |     1.2
   43 |   1.4970 |     46.218 |   1.4828 |     45.558 |     1.3
   44 |   1.4887 |     46.240 |   1.4774 |     45.558 |     1.3
   45 |   1.4801 |     46.142 |   1.4719 |     45.496 |     1.3
   46 |   1.4789 |     46.175 |   1.4669 |     45.466 |     1.3
   47 |   1.4714 |     46.288 |   1.4624 |     45.558 |     1.4
   48 |   1.4695 |     46.213 |   1.4579 |     45.588 |     1.4
   49 |   1.4625 |     46.175 |   1.4536 |     45.588 |     1.4
   50 |   1.4582 |     46.207 |   1.4498 |     45.619 |     1.5
   51 |   1.4548 |     46.164 |   1.4466 |     45.588 |     1.5
   52 |   1.4492 |     46.294 |   1.4439 |     45.588 |     1.5
   53 |   1.4464 |     46.186 |   1.4396 |     45.558 |     1.6
   54 |   1.4427 |     46.110 |   1.4361 |     45.588 |     1.6
   55 |   1.4364 |     46.202 |   1.4326 |     45.558 |     1.6
   56 |   1.4346 |     46.104 |   1.4296 |     45.588 |     1.6
   57 |   1.4306 |     46.256 |   1.4265 |     45.558 |     1.7
   58 |   1.4288 |     46.169 |   1.4239 |     45.558 |     1.7
   59 |   1.4233 |     46.077 |   1.4221 |     45.588 |     1.7
   60 |   1.4206 |     45.985 |   1.4198 |     45.558 |     1.7
   61 |   1.4175 |     46.001 |   1.4176 |     45.496 |     1.8
   62 |   1.4157 |     45.768 |   1.4149 |     45.343 |     1.8
   63 |   1.4113 |     45.541 |   1.4134 |     44.945 |     1.8
   64 |   1.4085 |     45.465 |   1.4124 |     45.006 |     1.9
   65 |   1.4068 |     45.270 |   1.4103 |     44.761 |     1.9
   66 |   1.4053 |     45.134 |   1.4084 |     44.792 |     1.9
   67 |   1.4013 |     45.026 |   1.4086 |     44.884 |     1.9
   68 |   1.4008 |     44.972 |   1.4053 |     44.853 |     2.0
   69 |   1.3983 |     44.869 |   1.4028 |     44.638 |     2.0
   70 |   1.3936 |     44.679 |   1.4025 |     44.884 |     2.0
   71 |   1.3901 |     44.690 |   1.4000 |     44.945 |     2.0
   72 |   1.3900 |     44.620 |   1.3997 |     45.067 |     2.1
   73 |   1.3871 |     44.609 |   1.3992 |     45.159 |     2.1
   74 |   1.3879 |     44.582 |   1.3954 |     44.669 |     2.1
   75 |   1.3819 |     44.468 |   1.3955 |     45.006 |     2.2
   76 |   1.3802 |     44.430 |   1.3927 |     44.945 |     2.2
   77 |   1.3787 |     44.479 |   1.3910 |     44.853 |     2.2
   78 |   1.3773 |     44.235 |   1.3890 |     44.853 |     2.2
   79 |   1.3764 |     44.376 |   1.3875 |     44.822 |     2.3
   80 |   1.3714 |     44.354 |   1.3894 |     44.822 |     2.3
   81 |   1.3729 |     44.213 |   1.3877 |     44.914 |     2.3
   82 |   1.3658 |     44.110 |   1.3882 |     45.006 |     2.3
   83 |   1.3663 |     44.045 |   1.3871 |     45.067 |     2.4
   84 |   1.3644 |     43.997 |   1.3847 |     44.822 |     2.4
   85 |   1.3619 |     43.861 |   1.3819 |     44.669 |     2.4
   86 |   1.3603 |     43.823 |   1.3797 |     44.700 |     2.4
   87 |   1.3590 |     43.856 |   1.3789 |     44.638 |     2.5
   88 |   1.3553 |     43.655 |   1.3766 |     44.638 |     2.5
   89 |   1.3563 |     43.677 |   1.3785 |     44.792 |     2.5
   90 |   1.3517 |     43.585 |   1.3771 |     44.914 |     2.6
   91 |   1.3511 |     43.617 |   1.3762 |     44.730 |     2.6
   92 |   1.3494 |     43.433 |   1.3759 |     44.700 |     2.6
   93 |   1.3497 |     43.438 |   1.3746 |     44.884 |     2.6
   94 |   1.3454 |     43.319 |   1.3736 |     44.792 |     2.7
   95 |   1.3438 |     43.227 |   1.3701 |     44.608 |     2.7
   96 |   1.3420 |     43.211 |   1.3708 |     44.577 |     2.7
   97 |   1.3399 |     43.151 |   1.3728 |     44.730 |     2.8
   98 |   1.3419 |     43.254 |   1.3672 |     44.485 |     2.8
   99 |   1.3380 |     43.119 |   1.3679 |     44.393 |     2.8
  100 |   1.3350 |     43.010 |   1.3677 |     44.638 |     2.8
  101 |   1.3331 |     42.918 |   1.3666 |     44.577 |     2.9
  102 |   1.3310 |     42.767 |   1.3665 |     44.424 |     2.9
  103 |   1.3306 |     42.826 |   1.3627 |     44.424 |     2.9
  104 |   1.3287 |     42.799 |   1.3611 |     44.638 |     3.0
  105 |   1.3262 |     42.712 |   1.3648 |     44.485 |     3.0
  106 |   1.3248 |     42.723 |   1.3612 |     43.934 |     3.0
  107 |   1.3202 |     42.669 |   1.3594 |     44.087 |     3.0
  108 |   1.3204 |     42.588 |   1.3596 |     44.332 |     3.1
  109 |   1.3152 |     42.550 |   1.3587 |     44.210 |     3.1
  110 |   1.3160 |     42.414 |   1.3581 |     43.842 |     3.1
  111 |   1.3154 |     42.458 |   1.3533 |     43.873 |     3.2
  112 |   1.3085 |     42.382 |   1.3576 |     43.781 |     3.2
  113 |   1.3090 |     42.328 |   1.3565 |     43.750 |     3.2
  114 |   1.3054 |     42.252 |   1.3532 |     43.719 |     3.2
  115 |   1.3074 |     42.241 |   1.3528 |     43.627 |     3.3
  116 |   1.3033 |     42.393 |   1.3484 |     43.566 |     3.3
  117 |   1.2996 |     42.122 |   1.3492 |     43.597 |     3.3
  118 |   1.2986 |     42.225 |   1.3502 |     43.597 |     3.4
  119 |   1.2988 |     42.279 |   1.3488 |     43.413 |     3.4
  120 |   1.2956 |     42.127 |   1.3472 |     43.750 |     3.4
  121 |   1.2939 |     42.138 |   1.3468 |     43.627 |     3.5
  122 |   1.2909 |     42.030 |   1.3481 |     43.627 |     3.5
  123 |   1.2871 |     41.992 |   1.3425 |     43.566 |     3.5
  124 |   1.2854 |     41.932 |   1.3411 |     43.597 |     3.5
  125 |   1.2819 |     41.645 |   1.3403 |     43.566 |     3.6
  126 |   1.2816 |     41.824 |   1.3405 |     43.719 |     3.6
  127 |   1.2815 |     41.612 |   1.3441 |     43.536 |     3.6
  128 |   1.2747 |     41.737 |   1.3463 |     43.658 |     3.7
  129 |   1.2760 |     41.678 |   1.3427 |     43.444 |     3.7
  130 |   1.2749 |     41.612 |   1.3399 |     43.719 |     3.7
  131 |   1.2702 |     41.558 |   1.3391 |     43.505 |     3.8
  132 |   1.2667 |     41.542 |   1.3382 |     43.536 |     3.8
  133 |   1.2634 |     41.401 |   1.3385 |     43.290 |     3.8
  134 |   1.2630 |     41.277 |   1.3309 |     43.444 |     3.8
  135 |   1.2595 |     41.266 |   1.3375 |     43.260 |     3.9
  136 |   1.2598 |     41.239 |   1.3328 |     43.444 |     3.9
  137 |   1.2632 |     41.304 |   1.3332 |     43.536 |     3.9
  138 |   1.2587 |     40.968 |   1.3335 |     43.045 |     4.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,008,834

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2078 |     82.943 |   2.6648 |     81.893 |     0.0
    2 |   2.3547 |     61.671 |   2.1562 |     56.556 |     0.1
    3 |   2.0298 |     54.817 |   1.9383 |     51.287 |     0.1
    4 |   1.8506 |     49.063 |   1.7840 |     48.346 |     0.1
    5 |   1.7220 |     48.607 |   1.6655 |     48.346 |     0.1
    6 |   1.6208 |     47.925 |   1.5789 |     45.466 |     0.2
    7 |   1.5538 |     46.202 |   1.5227 |     45.466 |     0.2
    8 |   1.5085 |     46.207 |   1.4868 |     45.466 |     0.2
    9 |   1.4759 |     46.213 |   1.4598 |     45.466 |     0.3
   10 |   1.4513 |     46.245 |   1.4389 |     45.466 |     0.3
   11 |   1.4315 |     46.207 |   1.4215 |     45.466 |     0.3
   12 |   1.4104 |     46.191 |   1.4045 |     45.159 |     0.3
   13 |   1.3916 |     46.164 |   1.3822 |     45.466 |     0.4
   14 |   1.3715 |     45.947 |   1.3679 |     45.251 |     0.4
   15 |   1.3556 |     45.438 |   1.3553 |     45.221 |     0.4
   16 |   1.3396 |     44.928 |   1.3410 |     44.608 |     0.4
   17 |   1.3218 |     44.338 |   1.3305 |     43.229 |     0.5
   18 |   1.3138 |     43.639 |   1.3233 |     43.627 |     0.5
   19 |   1.3003 |     42.945 |   1.3254 |     43.474 |     0.5
   20 |   1.2852 |     42.376 |   1.3105 |     43.689 |     0.6
   21 |   1.2721 |     42.219 |   1.3027 |     42.831 |     0.6
   22 |   1.2617 |     41.992 |   1.2893 |     42.310 |     0.6
   23 |   1.2510 |     41.407 |   1.2791 |     42.034 |     0.6
   24 |   1.2413 |     41.401 |   1.2736 |     42.433 |     0.7
   25 |   1.2301 |     40.919 |   1.2708 |     41.176 |     0.7
   26 |   1.2167 |     40.561 |   1.2602 |     41.422 |     0.7
   27 |   1.2051 |     40.518 |   1.2453 |     40.931 |     0.7
   28 |   1.1961 |     40.160 |   1.2417 |     41.575 |     0.8
   29 |   1.1798 |     40.036 |   1.2431 |     41.850 |     0.8
   30 |   1.1685 |     39.586 |   1.2273 |     41.575 |     0.8
   31 |   1.1577 |     39.212 |   1.2165 |     40.196 |     0.8
   32 |   1.1429 |     38.893 |   1.2115 |     40.349 |     0.9
   33 |   1.1361 |     38.296 |   1.2117 |     40.104 |     0.9
   34 |   1.1287 |     38.502 |   1.1960 |     39.583 |     0.9
   35 |   1.1090 |     37.728 |   1.1861 |     39.706 |     0.9
   36 |   1.0966 |     37.018 |   1.1821 |     39.338 |     1.0
   37 |   1.0877 |     36.828 |   1.1771 |     39.338 |     1.0
   38 |   1.0705 |     36.032 |   1.1771 |     39.338 |     1.0
   39 |   1.0578 |     35.549 |   1.1694 |     39.706 |     1.0
   40 |   1.0475 |     35.197 |   1.1620 |     38.909 |     1.1
   41 |   1.0368 |     34.986 |   1.1489 |     37.776 |     1.1
   42 |   1.0209 |     34.406 |   1.1627 |     38.143 |     1.1
   43 |   1.0093 |     33.848 |   1.1403 |     37.806 |     1.1
   44 |   0.9977 |     33.447 |   1.1408 |     38.756 |     1.2
   45 |   0.9856 |     33.052 |   1.1259 |     37.439 |     1.2
   46 |   0.9713 |     32.239 |   1.1189 |     37.469 |     1.2
   47 |   0.9635 |     32.309 |   1.1272 |     38.297 |     1.3
   48 |   0.9521 |     31.664 |   1.1118 |     36.397 |     1.3
   49 |   0.9340 |     30.982 |   1.1142 |     36.428 |     1.3
   50 |   0.9215 |     30.527 |   1.0992 |     36.366 |     1.3
   51 |   0.9117 |     30.072 |   1.0978 |     36.673 |     1.4
   52 |   0.8989 |     29.687 |   1.0920 |     36.612 |     1.4
   53 |   0.8867 |     29.004 |   1.0910 |     35.570 |     1.4
   54 |   0.8761 |     28.273 |   1.0778 |     35.570 |     1.5
   55 |   0.8659 |     27.725 |   1.0768 |     35.509 |     1.5
   56 |   0.8516 |     27.189 |   1.0875 |     35.417 |     1.5
   57 |   0.8434 |     26.853 |   1.0757 |     34.436 |     1.5
   58 |   0.8366 |     26.674 |   1.0580 |     34.252 |     1.6
   59 |   0.8220 |     26.160 |   1.0573 |     33.762 |     1.6
   60 |   0.8067 |     25.276 |   1.0584 |     33.946 |     1.6
   61 |   0.8000 |     24.962 |   1.0557 |     33.333 |     1.7
   62 |   0.7853 |     24.718 |   1.0565 |     33.762 |     1.7
   63 |   0.7774 |     24.447 |   1.0425 |     33.088 |     1.7
   64 |   0.7644 |     23.868 |   1.0366 |     32.598 |     1.7
   65 |   0.7552 |     23.380 |   1.0299 |     32.966 |     1.8
   66 |   0.7466 |     23.001 |   1.0310 |     32.292 |     1.8
   67 |   0.7385 |     23.060 |   1.0325 |     32.016 |     1.8
   68 |   0.7301 |     22.508 |   1.0464 |     32.751 |     1.8
   69 |   0.7208 |     22.188 |   1.0224 |     31.893 |     1.9
   70 |   0.7127 |     21.792 |   1.0320 |     32.077 |     1.9
   71 |   0.6994 |     21.343 |   1.0194 |     31.985 |     1.9
   72 |   0.6908 |     21.153 |   1.0202 |     31.679 |     2.0
   73 |   0.6845 |     20.877 |   1.0138 |     31.158 |     2.0
   74 |   0.6748 |     20.362 |   1.0222 |     31.066 |     2.0
   75 |   0.6700 |     20.487 |   1.0151 |     31.495 |     2.0
   76 |   0.6651 |     19.999 |   1.0141 |     30.821 |     2.1
   77 |   0.6575 |     19.972 |   1.0138 |     31.066 |     2.1
   78 |   0.6444 |     19.782 |   1.0054 |     30.729 |     2.1
   79 |   0.6338 |     19.186 |   1.0150 |     30.729 |     2.2
   80 |   0.6314 |     19.175 |   1.0203 |     30.576 |     2.2
   81 |   0.6245 |     18.980 |   1.0089 |     30.300 |     2.2
   82 |   0.6177 |     18.736 |   1.0130 |     30.147 |     2.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 1,880,610

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3971 |     88.031 |   3.0656 |     83.333 |     0.1
    2 |   2.7718 |     79.703 |   2.6119 |     59.252 |     0.1
    3 |   2.4861 |     59.336 |   2.3963 |     58.824 |     0.2
    4 |   2.3005 |     59.049 |   2.2411 |     58.824 |     0.3
    5 |   2.1732 |     59.049 |   2.1306 |     58.824 |     0.3
    6 |   2.0762 |     58.642 |   2.0412 |     58.824 |     0.4
    7 |   1.9979 |     56.215 |   1.9629 |     53.768 |     0.5
    8 |   1.9238 |     54.129 |   1.8919 |     53.768 |     0.6
    9 |   1.8611 |     49.621 |   1.8290 |     48.346 |     0.6
   10 |   1.7976 |     48.613 |   1.7694 |     48.346 |     0.7
   11 |   1.7428 |     48.602 |   1.7138 |     48.346 |     0.8
   12 |   1.6899 |     48.607 |   1.6655 |     48.346 |     0.8
   13 |   1.6506 |     48.607 |   1.6241 |     48.346 |     0.9
   14 |   1.6090 |     47.405 |   1.5872 |     45.466 |     1.0
   15 |   1.5760 |     46.213 |   1.5590 |     45.466 |     1.0
   16 |   1.5525 |     46.213 |   1.5357 |     45.466 |     1.1
   17 |   1.5335 |     46.213 |   1.5167 |     45.466 |     1.2
   18 |   1.5138 |     46.213 |   1.5013 |     45.466 |     1.3
   19 |   1.4992 |     46.213 |   1.4884 |     45.466 |     1.3
   20 |   1.4864 |     46.191 |   1.4754 |     45.466 |     1.4
   21 |   1.4769 |     46.213 |   1.4655 |     45.466 |     1.5
   22 |   1.4648 |     46.213 |   1.4570 |     45.466 |     1.5
   23 |   1.4566 |     46.142 |   1.4489 |     45.466 |     1.6
   24 |   1.4487 |     46.218 |   1.4412 |     45.496 |     1.7
   25 |   1.4422 |     46.207 |   1.4353 |     45.496 |     1.7
   26 |   1.4334 |     46.207 |   1.4291 |     45.527 |     1.8
   27 |   1.4280 |     46.142 |   1.4228 |     45.527 |     1.9
   28 |   1.4194 |     46.131 |   1.4189 |     45.527 |     1.9
   29 |   1.4132 |     45.682 |   1.4134 |     45.558 |     2.0
   30 |   1.4043 |     45.476 |   1.4114 |     45.588 |     2.1
   31 |   1.4002 |     45.302 |   1.4069 |     45.404 |     2.2
   32 |   1.3955 |     44.988 |   1.4018 |     45.619 |     2.2
   33 |   1.3884 |     44.815 |   1.4005 |     45.251 |     2.3
   34 |   1.3815 |     44.869 |   1.3950 |     45.466 |     2.4
   35 |   1.3765 |     44.695 |   1.3930 |     45.374 |     2.4
   36 |   1.3691 |     44.517 |   1.3969 |     45.558 |     2.5
   37 |   1.3661 |     44.517 |   1.3919 |     45.527 |     2.6
   38 |   1.3622 |     44.463 |   1.3886 |     45.129 |     2.6
   39 |   1.3587 |     44.305 |   1.3893 |     45.312 |     2.7
   40 |   1.3530 |     44.154 |   1.3800 |     45.374 |     2.8
   41 |   1.3472 |     44.007 |   1.3793 |     45.129 |     2.8
   42 |   1.3422 |     43.877 |   1.3803 |     45.098 |     2.9
   43 |   1.3350 |     43.731 |   1.3740 |     44.853 |     3.0
   44 |   1.3389 |     43.764 |   1.3756 |     44.761 |     3.0
   45 |   1.3300 |     43.466 |   1.3760 |     44.975 |     3.1
   46 |   1.3299 |     43.541 |   1.3753 |     45.221 |     3.2
   47 |   1.3250 |     43.390 |   1.3707 |     44.884 |     3.2
   48 |   1.3184 |     43.184 |   1.3702 |     45.312 |     3.3
   49 |   1.3150 |     43.243 |   1.3697 |     44.975 |     3.4
   50 |   1.3121 |     43.086 |   1.3651 |     45.159 |     3.4
   51 |   1.3094 |     42.956 |   1.3630 |     45.006 |     3.5
   52 |   1.3053 |     42.870 |   1.3611 |     44.271 |     3.6
   53 |   1.3010 |     42.555 |   1.3596 |     44.516 |     3.6
   54 |   1.2986 |     42.712 |   1.3618 |     44.945 |     3.7
   55 |   1.2909 |     42.490 |   1.3562 |     44.301 |     3.8
   56 |   1.2892 |     42.290 |   1.3557 |     44.301 |     3.8
   57 |   1.2836 |     42.241 |   1.3559 |     45.037 |     3.9
   58 |   1.2821 |     42.143 |   1.3527 |     43.995 |     4.0
   59 |   1.2746 |     41.927 |   1.3514 |     44.700 |     4.0
   60 |   1.2676 |     41.678 |   1.3496 |     44.271 |     4.1
   61 |   1.2644 |     41.715 |   1.3452 |     43.903 |     4.2
   62 |   1.2580 |     41.721 |   1.3402 |     44.240 |     4.2
   63 |   1.2517 |     41.894 |   1.3468 |     43.781 |     4.3
   64 |   1.2454 |     41.558 |   1.3439 |     43.873 |     4.4
   65 |   1.2400 |     41.445 |   1.3431 |     43.842 |     4.5
   66 |   1.2362 |     41.287 |   1.3401 |     44.118 |     4.5
   67 |   1.2322 |     41.179 |   1.3380 |     44.087 |     4.6
   68 |   1.2242 |     41.049 |   1.3351 |     43.750 |     4.7
   69 |   1.2187 |     40.897 |   1.3334 |     43.321 |     4.7
   70 |   1.2157 |     40.995 |   1.3272 |     43.382 |     4.8
   71 |   1.2086 |     40.556 |   1.3270 |     43.597 |     4.9
   72 |   1.2072 |     40.626 |   1.3229 |     43.290 |     4.9
   73 |   1.1973 |     40.252 |   1.3196 |     43.229 |     5.0
   74 |   1.1923 |     40.198 |   1.3121 |     42.525 |     5.1
   75 |   1.1871 |     39.938 |   1.3161 |     42.678 |     5.1
   76 |   1.1801 |     39.597 |   1.3184 |     42.678 |     5.2
   77 |   1.1748 |     39.331 |   1.3186 |     43.045 |     5.3
   78 |   1.1723 |     39.131 |   1.3122 |     42.800 |     5.4
   79 |   1.1685 |     39.104 |   1.3101 |     42.923 |     5.4
   80 |   1.1591 |     38.497 |   1.3086 |     42.034 |     5.5
   81 |   1.1575 |     38.410 |   1.3039 |     42.371 |     5.6
   82 |   1.1489 |     38.269 |   1.3098 |     42.034 |     5.6
   83 |   1.1480 |     38.107 |   1.3076 |     42.647 |     5.7
   84 |   1.1373 |     38.058 |   1.3097 |     42.555 |     5.8
   85 |   1.1344 |     37.798 |   1.3094 |     42.463 |     5.8
   86 |   1.1278 |     37.538 |   1.2985 |     42.402 |     5.9
   87 |   1.1245 |     37.641 |   1.3014 |     42.463 |     6.0
   88 |   1.1156 |     37.218 |   1.2992 |     42.586 |     6.0
   89 |   1.1153 |     37.435 |   1.3027 |     41.973 |     6.1
   90 |   1.1069 |     37.224 |   1.3016 |     42.004 |     6.2
   91 |   1.1027 |     37.061 |   1.2951 |     41.605 |     6.2
   92 |   1.0955 |     36.769 |   1.2888 |     41.422 |     6.3
   93 |   1.0891 |     36.411 |   1.3016 |     41.850 |     6.4
   94 |   1.0883 |     36.416 |   1.2906 |     41.360 |     6.5
   95 |   1.0805 |     36.460 |   1.2871 |     41.513 |     6.5
   96 |   1.0751 |     36.292 |   1.2918 |     41.605 |     6.6
   97 |   1.0720 |     35.869 |   1.2981 |     41.942 |     6.7
   98 |   1.0660 |     35.799 |   1.3029 |     41.513 |     6.7
   99 |   1.0591 |     35.679 |   1.3014 |     41.789 |     6.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.1
Trainable parameters: 2,316,386

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3527 |     83.940 |   3.1877 |     81.863 |     0.1
    2 |   3.0415 |     81.491 |   2.9009 |     81.863 |     0.2
    3 |   2.7887 |     78.847 |   2.6995 |     67.341 |     0.3
    4 |   2.6281 |     66.569 |   2.5740 |     63.971 |     0.4
    5 |   2.5220 |     63.405 |   2.4896 |     62.469 |     0.5
    6 |   2.4468 |     61.194 |   2.4221 |     59.130 |     0.6
    7 |   2.3894 |     60.051 |   2.3700 |     58.854 |     0.7
    8 |   2.3407 |     59.173 |   2.3195 |     58.762 |     0.8
    9 |   2.2915 |     58.821 |   2.2703 |     58.732 |     0.9
   10 |   2.2454 |     58.344 |   2.2305 |     58.670 |     1.0
   11 |   2.2087 |     58.057 |   2.1933 |     57.812 |     1.1
   12 |   2.1721 |     57.705 |   2.1570 |     57.782 |     1.2
   13 |   2.1373 |     57.466 |   2.1244 |     57.138 |     1.3
   14 |   2.1050 |     57.244 |   2.0897 |     57.322 |     1.3
   15 |   2.0695 |     56.654 |   2.0525 |     57.047 |     1.4
   16 |   2.0356 |     56.161 |   2.0191 |     55.852 |     1.5
   17 |   2.0039 |     55.917 |   1.9879 |     54.381 |     1.6
   18 |   1.9738 |     55.391 |   1.9569 |     55.300 |     1.7
   19 |   1.9410 |     55.229 |   1.9253 |     54.044 |     1.8
   20 |   1.9133 |     54.069 |   1.8937 |     51.103 |     1.9
   21 |   1.8803 |     51.403 |   1.8609 |     48.652 |     2.0
   22 |   1.8478 |     49.577 |   1.8282 |     48.499 |     2.1
   23 |   1.8147 |     48.998 |   1.7938 |     48.070 |     2.2
   24 |   1.7819 |     48.689 |   1.7601 |     48.100 |     2.3
   25 |   1.7469 |     48.651 |   1.7271 |     48.254 |     2.4
   26 |   1.7131 |     48.662 |   1.6957 |     48.070 |     2.5
   27 |   1.6830 |     48.499 |   1.6652 |     48.192 |     2.6
   28 |   1.6549 |     47.443 |   1.6370 |     45.680 |     2.7
   29 |   1.6305 |     46.478 |   1.6122 |     45.496 |     2.8
   30 |   1.6030 |     46.202 |   1.5893 |     45.466 |     2.9
   31 |   1.5795 |     46.321 |   1.5636 |     45.527 |     3.0
   32 |   1.5545 |     46.142 |   1.5433 |     45.558 |     3.1
   33 |   1.5349 |     46.007 |   1.5258 |     45.251 |     3.2
   34 |   1.5188 |     45.655 |   1.5106 |     45.190 |     3.3
   35 |   1.5020 |     45.541 |   1.4968 |     45.067 |     3.4
   36 |   1.4902 |     45.443 |   1.4831 |     44.547 |     3.4
   37 |   1.4761 |     45.243 |   1.4706 |     44.179 |     3.5
   38 |   1.4630 |     45.080 |   1.4581 |     44.056 |     3.6
   39 |   1.4516 |     44.988 |   1.4504 |     44.240 |     3.7
   40 |   1.4381 |     44.945 |   1.4398 |     44.118 |     3.8
   41 |   1.4323 |     44.896 |   1.4296 |     43.964 |     3.9
   42 |   1.4202 |     44.490 |   1.4204 |     43.811 |     4.0
   43 |   1.4113 |     44.511 |   1.4132 |     43.444 |     4.1
   44 |   1.4004 |     44.235 |   1.4044 |     43.382 |     4.2
   45 |   1.3928 |     44.251 |   1.3966 |     43.413 |     4.3
   46 |   1.3814 |     44.029 |   1.3901 |     43.474 |     4.4
   47 |   1.3758 |     43.807 |   1.3843 |     42.953 |     4.5
   48 |   1.3680 |     43.617 |   1.3777 |     42.953 |     4.6
   49 |   1.3571 |     43.514 |   1.3735 |     43.045 |     4.7
   50 |   1.3542 |     43.406 |   1.3648 |     42.984 |     4.8
   51 |   1.3475 |     43.227 |   1.3620 |     42.831 |     4.9
   52 |   1.3410 |     43.070 |   1.3543 |     42.678 |     5.0
   53 |   1.3299 |     43.151 |   1.3517 |     42.371 |     5.1
   54 |   1.3275 |     42.886 |   1.3468 |     42.923 |     5.2
   55 |   1.3182 |     42.821 |   1.3402 |     42.616 |     5.3
   56 |   1.3094 |     42.767 |   1.3340 |     42.279 |     5.4
   57 |   1.3077 |     42.588 |   1.3297 |     42.157 |     5.5
   58 |   1.2976 |     42.517 |   1.3260 |     42.463 |     5.5
   59 |   1.2923 |     42.333 |   1.3226 |     42.525 |     5.6
   60 |   1.2853 |     42.333 |   1.3150 |     41.789 |     5.7
   61 |   1.2796 |     42.013 |   1.3112 |     41.636 |     5.8
   62 |   1.2727 |     41.845 |   1.3070 |     41.789 |     5.9
   63 |   1.2675 |     41.667 |   1.3030 |     41.881 |     6.0
   64 |   1.2644 |     41.607 |   1.2984 |     41.636 |     6.1
   65 |   1.2536 |     41.255 |   1.2956 |     41.452 |     6.2
   66 |   1.2515 |     41.157 |   1.2903 |     41.422 |     6.3
   67 |   1.2448 |     40.762 |   1.2880 |     41.452 |     6.4
   68 |   1.2387 |     40.556 |   1.2817 |     41.054 |     6.5
   69 |   1.2306 |     40.177 |   1.2786 |     40.931 |     6.6
   70 |   1.2235 |     40.095 |   1.2759 |     40.901 |     6.7
   71 |   1.2149 |     39.846 |   1.2722 |     40.319 |     6.8
   72 |   1.2114 |     39.667 |   1.2672 |     40.380 |     6.9
   73 |   1.2039 |     39.353 |   1.2651 |     39.951 |     7.0
   74 |   1.1970 |     38.920 |   1.2573 |     39.798 |     7.1
   75 |   1.1918 |     38.708 |   1.2552 |     39.522 |     7.2
   76 |   1.1862 |     38.687 |   1.2572 |     39.920 |     7.2
   77 |   1.1803 |     38.204 |   1.2499 |     38.879 |     7.3
   78 |   1.1770 |     37.977 |   1.2473 |     39.369 |     7.4
   79 |   1.1693 |     37.890 |   1.2406 |     39.032 |     7.5
   80 |   1.1648 |     37.397 |   1.2380 |     38.848 |     7.6
   81 |   1.1559 |     37.224 |   1.2310 |     38.542 |     7.7
   82 |   1.1506 |     36.958 |   1.2304 |     38.450 |     7.8
   83 |   1.1479 |     37.104 |   1.2246 |     38.235 |     7.9
   84 |   1.1457 |     36.790 |   1.2241 |     38.848 |     8.0
   85 |   1.1363 |     36.476 |   1.2180 |     38.266 |     8.1
   86 |   1.1253 |     36.313 |   1.2143 |     38.082 |     8.2
   87 |   1.1223 |     36.097 |   1.2162 |     38.174 |     8.3
   88 |   1.1178 |     35.972 |   1.2122 |     38.480 |     8.4
   89 |   1.1141 |     35.636 |   1.2100 |     37.776 |     8.5
   90 |   1.1067 |     35.446 |   1.2059 |     38.358 |     8.6
   91 |   1.1000 |     35.365 |   1.2028 |     37.531 |     8.7
   92 |   1.0945 |     35.018 |   1.1987 |     37.010 |     8.7
   93 |   1.0894 |     34.769 |   1.1936 |     36.887 |     8.8
   94 |   1.0834 |     34.590 |   1.1923 |     37.286 |     8.9
   95 |   1.0783 |     34.233 |   1.1895 |     37.316 |     9.0
   96 |   1.0729 |     34.249 |   1.1837 |     36.826 |     9.1
   97 |   1.0699 |     33.761 |   1.1851 |     36.458 |     9.2
   98 |   1.0607 |     33.794 |   1.1800 |     36.305 |     9.3
   99 |   1.0626 |     33.805 |   1.1744 |     36.550 |     9.4
  100 |   1.0566 |     33.469 |   1.1782 |     36.366 |     9.5
  101 |   1.0485 |     33.469 |   1.1735 |     36.275 |     9.6
  102 |   1.0448 |     33.079 |   1.1728 |     36.121 |     9.7
  103 |   1.0348 |     32.905 |   1.1672 |     35.999 |     9.8
  104 |   1.0354 |     32.726 |   1.1652 |     35.662 |     9.9
  105 |   1.0289 |     32.483 |   1.1616 |     35.938 |    10.0
  106 |   1.0238 |     32.483 |   1.1626 |     36.305 |    10.1
  107 |   1.0199 |     32.385 |   1.1614 |     36.244 |    10.2
  108 |   1.0184 |     32.250 |   1.1571 |     35.509 |    10.3
  109 |   1.0106 |     32.168 |   1.1574 |     35.876 |    10.4
  110 |   1.0066 |     31.811 |   1.1531 |     35.600 |    10.5
  111 |   1.0025 |     31.610 |   1.1492 |     35.723 |    10.6
  112 |   0.9983 |     31.605 |   1.1505 |     35.355 |    10.7
  113 |   0.9910 |     31.188 |   1.1513 |     35.355 |    10.8
  114 |   0.9854 |     30.944 |   1.1444 |     35.202 |    10.9
  115 |   0.9821 |     31.036 |   1.1407 |     35.141 |    10.9
  116 |   0.9808 |     30.825 |   1.1464 |     35.110 |    11.0
  117 |   0.9744 |     30.700 |   1.1379 |     35.141 |    11.1
  118 |   0.9706 |     30.435 |   1.1362 |     35.294 |    11.2
  119 |   0.9640 |     30.380 |   1.1333 |     35.110 |    11.3
  120 |   0.9608 |     30.397 |   1.1358 |     34.283 |    11.4
  121 |   0.9571 |     29.969 |   1.1322 |     34.467 |    11.5
  122 |   0.9540 |     29.811 |   1.1294 |     34.773 |    11.6
  123 |   0.9515 |     30.044 |   1.1292 |     34.804 |    11.7
  124 |   0.9434 |     29.801 |   1.1300 |     35.049 |    11.8
  125 |   0.9407 |     29.627 |   1.1251 |     34.926 |    11.9
  126 |   0.9354 |     29.383 |   1.1232 |     34.559 |    12.0
  127 |   0.9332 |     29.448 |   1.1255 |     34.712 |    12.1
  128 |   0.9247 |     29.069 |   1.1220 |     34.957 |    12.2
  129 |   0.9204 |     28.912 |   1.1195 |     34.467 |    12.3
  130 |   0.9172 |     28.614 |   1.1217 |     35.263 |    12.4
  131 |   0.9178 |     28.798 |   1.1199 |     34.712 |    12.5
  132 |   0.9134 |     28.630 |   1.1212 |     35.355 |    12.6
  133 |   0.9115 |     28.403 |   1.1115 |     34.283 |    12.7
  134 |   0.9029 |     28.419 |   1.1174 |     34.467 |    12.8
  135 |   0.9043 |     28.311 |   1.1134 |     34.589 |    12.9
  136 |   0.8952 |     27.769 |   1.1103 |     34.651 |    12.9
  137 |   0.8964 |     28.148 |   1.1145 |     34.314 |    13.0
  138 |   0.8897 |     27.888 |   1.1127 |     34.743 |    13.1
  139 |   0.8855 |     27.530 |   1.1070 |     34.375 |    13.2
  140 |   0.8859 |     27.747 |   1.1055 |     34.467 |    13.3
  141 |   0.8806 |     27.308 |   1.1103 |     34.038 |    13.4
  142 |   0.8727 |     27.259 |   1.1025 |     33.824 |    13.5
  143 |   0.8700 |     27.221 |   1.0986 |     34.161 |    13.6
  144 |   0.8651 |     26.967 |   1.1057 |     34.191 |    13.7
  145 |   0.8657 |     26.896 |   1.0994 |     34.344 |    13.8
  146 |   0.8674 |     27.184 |   1.0981 |     33.762 |    13.9
  147 |   0.8584 |     26.869 |   1.0974 |     33.885 |    14.0
  148 |   0.8556 |     26.642 |   1.0948 |     34.069 |    14.1
  149 |   0.8479 |     26.382 |   1.0972 |     34.038 |    14.2
  150 |   0.8472 |     26.127 |   1.0964 |     34.130 |    14.3
  151 |   0.8454 |     26.284 |   1.0906 |     33.333 |    14.4
  152 |   0.8376 |     26.143 |   1.0951 |     33.517 |    14.5
  153 |   0.8363 |     25.780 |   1.0969 |     33.640 |    14.6
  154 |   0.8317 |     25.872 |   1.0936 |     33.517 |    14.7
  155 |   0.8308 |     25.759 |   1.0936 |     33.578 |    14.8
  156 |   0.8252 |     25.553 |   1.0881 |     33.517 |    14.9
  157 |   0.8236 |     25.282 |   1.0897 |     33.456 |    15.0
  158 |   0.8204 |     25.352 |   1.0949 |     33.732 |    15.0
  159 |   0.8192 |     25.325 |   1.0906 |     33.456 |    15.1
  160 |   0.8185 |     25.450 |   1.0978 |     33.670 |    15.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 602,850

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.7449 |     72.150 |   2.3146 |     57.659 |     0.0
    2 |   2.1234 |     57.401 |   1.9770 |     53.952 |     0.1
    3 |   1.8452 |     51.338 |   1.6976 |     48.346 |     0.1
    4 |   1.6117 |     47.080 |   1.5345 |     45.466 |     0.1
    5 |   1.5025 |     46.218 |   1.4618 |     45.466 |     0.2
    6 |   1.4499 |     46.093 |   1.4251 |     45.098 |     0.2
    7 |   1.4168 |     45.638 |   1.3936 |     44.240 |     0.2
    8 |   1.3764 |     44.078 |   1.3642 |     43.750 |     0.2
    9 |   1.3491 |     43.845 |   1.3413 |     42.923 |     0.3
   10 |   1.3212 |     43.216 |   1.3162 |     42.402 |     0.3
   11 |   1.3014 |     42.566 |   1.3035 |     42.218 |     0.3
   12 |   1.2816 |     42.176 |   1.2847 |     41.544 |     0.4
   13 |   1.2596 |     41.640 |   1.2733 |     41.330 |     0.4
   14 |   1.2443 |     41.195 |   1.2605 |     40.472 |     0.4
   15 |   1.2284 |     40.670 |   1.2478 |     39.890 |     0.5
   16 |   1.2097 |     39.965 |   1.2399 |     39.338 |     0.5
   17 |   1.1910 |     39.326 |   1.2308 |     38.971 |     0.5
   18 |   1.1743 |     38.811 |   1.2221 |     39.154 |     0.5
   19 |   1.1566 |     38.015 |   1.2110 |     38.879 |     0.6
   20 |   1.1440 |     37.592 |   1.1984 |     38.572 |     0.6
   21 |   1.1311 |     36.801 |   1.1932 |     38.143 |     0.6
   22 |   1.1091 |     36.286 |   1.1843 |     37.837 |     0.7
   23 |   1.0892 |     35.815 |   1.1728 |     37.316 |     0.7
   24 |   1.0808 |     35.436 |   1.1831 |     37.316 |     0.7
   25 |   1.0583 |     34.607 |   1.1671 |     36.612 |     0.8
   26 |   1.0483 |     33.859 |   1.1520 |     36.550 |     0.8
   27 |   1.0346 |     33.604 |   1.1487 |     36.703 |     0.8
   28 |   1.0099 |     32.651 |   1.1426 |     35.968 |     0.8
   29 |   1.0039 |     32.385 |   1.1467 |     36.244 |     0.9
   30 |   0.9835 |     31.637 |   1.1254 |     36.029 |     0.9
   31 |   0.9642 |     30.868 |   1.1186 |     35.355 |     0.9
   32 |   0.9499 |     30.705 |   1.1273 |     35.876 |     1.0
   33 |   0.9401 |     29.920 |   1.1028 |     35.018 |     1.0
   34 |   0.9268 |     29.714 |   1.1172 |     34.835 |     1.0
   35 |   0.9112 |     28.961 |   1.1071 |     34.283 |     1.0
   36 |   0.9007 |     28.549 |   1.1059 |     34.528 |     1.1
   37 |   0.8938 |     28.533 |   1.0735 |     33.824 |     1.1
   38 |   0.8834 |     28.246 |   1.0949 |     33.762 |     1.1
   39 |   0.8551 |     27.005 |   1.0849 |     33.946 |     1.2
   40 |   0.8546 |     27.221 |   1.0965 |     34.344 |     1.2
   41 |   0.8869 |     28.820 |   1.0852 |     34.681 |     1.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 3,047,138

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2528 |     87.169 |   2.7337 |     83.333 |     0.1
    2 |   2.4950 |     65.003 |   2.2845 |     58.824 |     0.2
    3 |   2.1559 |     57.309 |   2.0420 |     58.824 |     0.3
    4 |   1.9739 |     54.595 |   1.8886 |     48.376 |     0.4
    5 |   1.8437 |     49.518 |   1.7735 |     48.346 |     0.5
    6 |   1.7404 |     48.786 |   1.6793 |     48.346 |     0.6
    7 |   1.6530 |     48.618 |   1.6002 |     48.346 |     0.7
    8 |   1.5829 |     47.464 |   1.5402 |     45.496 |     0.8
    9 |   1.5303 |     46.218 |   1.4973 |     45.466 |     0.9
   10 |   1.4936 |     46.207 |   1.4695 |     45.466 |     1.0
   11 |   1.4725 |     46.245 |   1.4503 |     45.466 |     1.1
   12 |   1.4516 |     46.245 |   1.4381 |     45.466 |     1.2
   13 |   1.4438 |     46.261 |   1.4285 |     45.466 |     1.3
   14 |   1.4330 |     46.191 |   1.4215 |     45.466 |     1.4
   15 |   1.4259 |     46.218 |   1.4160 |     45.466 |     1.5
   16 |   1.4220 |     46.288 |   1.4108 |     45.466 |     1.6
   17 |   1.4140 |     46.218 |   1.4064 |     45.466 |     1.7
   18 |   1.4098 |     46.261 |   1.4025 |     45.466 |     1.7
   19 |   1.4041 |     46.397 |   1.3954 |     45.466 |     1.8
   20 |   1.3973 |     46.288 |   1.3880 |     45.466 |     1.9
   21 |   1.3872 |     46.359 |   1.3813 |     45.466 |     2.0
   22 |   1.3792 |     46.348 |   1.3742 |     45.956 |     2.1
   23 |   1.3706 |     46.207 |   1.3656 |     45.588 |     2.2
   24 |   1.3588 |     45.709 |   1.3550 |     45.129 |     2.3
   25 |   1.3502 |     45.459 |   1.3467 |     44.608 |     2.4
   26 |   1.3406 |     45.319 |   1.3387 |     44.393 |     2.5
   27 |   1.3317 |     44.977 |   1.3318 |     44.210 |     2.6
   28 |   1.3257 |     44.793 |   1.3281 |     43.627 |     2.7
   29 |   1.3160 |     44.202 |   1.3174 |     43.750 |     2.8
   30 |   1.3097 |     44.403 |   1.3096 |     43.137 |     2.9
   31 |   1.3009 |     43.829 |   1.3048 |     42.647 |     3.0
   32 |   1.2910 |     43.482 |   1.2977 |     42.708 |     3.1
   33 |   1.2828 |     43.368 |   1.2889 |     42.678 |     3.2
   34 |   1.2769 |     43.103 |   1.2850 |     42.892 |     3.3
   35 |   1.2657 |     42.842 |   1.2767 |     42.371 |     3.4
   36 |   1.2640 |     42.788 |   1.2784 |     42.065 |     3.5
   37 |   1.2509 |     42.436 |   1.2725 |     42.065 |     3.6
   38 |   1.2410 |     42.306 |   1.2663 |     42.004 |     3.7
   39 |   1.2360 |     41.916 |   1.2603 |     42.096 |     3.8
   40 |   1.2306 |     41.656 |   1.2554 |     41.544 |     3.9
   41 |   1.2197 |     41.260 |   1.2543 |     41.820 |     3.9
   42 |   1.2117 |     40.995 |   1.2507 |     41.299 |     4.0
   43 |   1.2084 |     40.816 |   1.2468 |     41.605 |     4.1
   44 |   1.1979 |     40.556 |   1.2403 |     41.513 |     4.2
   45 |   1.1942 |     40.345 |   1.2367 |     41.176 |     4.3
   46 |   1.1910 |     40.009 |   1.2388 |     41.146 |     4.4
   47 |   1.1820 |     39.792 |   1.2344 |     41.636 |     4.5
   48 |   1.1754 |     39.727 |   1.2330 |     41.667 |     4.6
   49 |   1.1677 |     39.369 |   1.2230 |     41.605 |     4.7
   50 |   1.1591 |     39.234 |   1.2216 |     41.115 |     4.8
   51 |   1.1563 |     38.827 |   1.2243 |     40.380 |     4.9
   52 |   1.1536 |     38.681 |   1.2194 |     40.349 |     5.0
   53 |   1.1408 |     38.269 |   1.2156 |     39.706 |     5.1
   54 |   1.1356 |     38.129 |   1.2143 |     40.135 |     5.2
   55 |   1.1337 |     38.194 |   1.2053 |     39.645 |     5.3
   56 |   1.1214 |     37.717 |   1.2147 |     40.839 |     5.4
   57 |   1.1156 |     37.397 |   1.2013 |     39.338 |     5.5
   58 |   1.1110 |     37.218 |   1.1956 |     39.583 |     5.6
   59 |   1.1031 |     36.985 |   1.1912 |     39.124 |     5.7
   60 |   1.0937 |     36.579 |   1.1899 |     39.308 |     5.8
   61 |   1.0868 |     36.292 |   1.1839 |     38.603 |     5.9
   62 |   1.0777 |     36.238 |   1.1852 |     38.879 |     6.0
   63 |   1.0738 |     36.075 |   1.1846 |     38.664 |     6.1
   64 |   1.0691 |     35.679 |   1.1765 |     39.001 |     6.2
   65 |   1.0620 |     35.463 |   1.1672 |     38.542 |     6.3
   66 |   1.0518 |     35.159 |   1.1634 |     38.542 |     6.4
   67 |   1.0447 |     34.829 |   1.1605 |     37.561 |     6.5
   68 |   1.0394 |     34.531 |   1.1557 |     37.439 |     6.6
   69 |   1.0316 |     34.341 |   1.1570 |     38.174 |     6.7
   70 |   1.0277 |     34.217 |   1.1534 |     38.235 |     6.7
   71 |   1.0147 |     33.897 |   1.1427 |     37.868 |     6.8
   72 |   1.0114 |     33.675 |   1.1432 |     38.021 |     6.9
   73 |   1.0027 |     33.409 |   1.1388 |     37.623 |     7.0
   74 |   0.9947 |     32.867 |   1.1386 |     37.286 |     7.1
   75 |   0.9907 |     33.079 |   1.1382 |     37.531 |     7.2
   76 |   0.9883 |     32.678 |   1.1349 |     37.745 |     7.3
   77 |   0.9806 |     32.591 |   1.1380 |     37.531 |     7.4
   78 |   0.9760 |     32.380 |   1.1365 |     37.623 |     7.5
   79 |   0.9670 |     32.255 |   1.1289 |     37.194 |     7.6
   80 |   0.9588 |     31.832 |   1.1241 |     37.040 |     7.7
   81 |   0.9568 |     31.513 |   1.1282 |     37.194 |     7.8
   82 |   0.9497 |     31.659 |   1.1241 |     37.347 |     7.9
   83 |   0.9466 |     31.534 |   1.1160 |     36.489 |     8.0
   84 |   0.9420 |     31.518 |   1.1179 |     37.224 |     8.1
   85 |   0.9308 |     30.928 |   1.1193 |     37.010 |     8.2
   86 |   0.9283 |     30.668 |   1.1096 |     36.581 |     8.3
   87 |   0.9234 |     30.543 |   1.1085 |     36.703 |     8.4
   88 |   0.9194 |     30.239 |   1.1073 |     36.060 |     8.5
   89 |   0.9147 |     30.153 |   1.1111 |     36.152 |     8.6
   90 |   0.9106 |     29.866 |   1.1054 |     36.550 |     8.7
   91 |   0.9044 |     29.979 |   1.1080 |     36.336 |     8.8
   92 |   0.8958 |     29.362 |   1.1036 |     35.999 |     8.9
   93 |   0.8925 |     29.481 |   1.1081 |     36.183 |     9.0
   94 |   0.8868 |     29.221 |   1.1092 |     35.509 |     9.1
   95 |   0.8809 |     29.107 |   1.1067 |     35.417 |     9.2
   96 |   0.8762 |     28.798 |   1.1033 |     35.999 |     9.3
   97 |   0.8742 |     28.657 |   1.0875 |     35.355 |     9.4
   98 |   0.8742 |     29.010 |   1.1007 |     35.080 |     9.5
   99 |   0.8659 |     28.544 |   1.1055 |     35.570 |     9.6
  100 |   0.8600 |     28.121 |   1.0993 |     34.896 |     9.7
  101 |   0.8575 |     28.121 |   1.0923 |     35.018 |     9.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 833,474

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.3213 |     81.832 |   2.9102 |     83.241 |     0.0
    2 |   2.6908 |     71.191 |   2.5513 |     61.428 |     0.1
    3 |   2.4228 |     59.151 |   2.3337 |     58.517 |     0.1
    4 |   2.2470 |     58.377 |   2.1991 |     57.966 |     0.1
    5 |   2.1342 |     57.082 |   2.0927 |     55.147 |     0.2
    6 |   2.0391 |     55.440 |   1.9975 |     53.156 |     0.2
    7 |   1.9448 |     50.070 |   1.9058 |     48.560 |     0.3
    8 |   1.8586 |     48.602 |   1.8238 |     48.315 |     0.3
    9 |   1.7843 |     48.477 |   1.7487 |     48.346 |     0.3
   10 |   1.7114 |     48.537 |   1.6791 |     48.284 |     0.4
   11 |   1.6487 |     48.597 |   1.6223 |     48.346 |     0.4
   12 |   1.5955 |     48.618 |   1.5734 |     48.315 |     0.4
   13 |   1.5519 |     47.871 |   1.5364 |     45.527 |     0.5
   14 |   1.5180 |     46.251 |   1.5020 |     45.496 |     0.5
   15 |   1.4878 |     46.229 |   1.4749 |     45.588 |     0.6
   16 |   1.4592 |     46.191 |   1.4523 |     45.374 |     0.6
   17 |   1.4377 |     45.936 |   1.4331 |     45.098 |     0.6
   18 |   1.4175 |     45.286 |   1.4144 |     44.087 |     0.7
   19 |   1.3976 |     44.425 |   1.4022 |     44.516 |     0.7
   20 |   1.3832 |     43.883 |   1.3886 |     43.964 |     0.7
   21 |   1.3658 |     43.466 |   1.3743 |     43.107 |     0.8
   22 |   1.3548 |     43.048 |   1.3650 |     41.881 |     0.8
   23 |   1.3403 |     42.257 |   1.3549 |     42.096 |     0.8
   24 |   1.3276 |     41.845 |   1.3490 |     41.942 |     0.9
   25 |   1.3156 |     41.352 |   1.3367 |     41.513 |     0.9
   26 |   1.3023 |     40.903 |   1.3256 |     40.594 |     0.9
   27 |   1.2921 |     40.653 |   1.3217 |     40.380 |     1.0
   28 |   1.2835 |     40.133 |   1.3121 |     40.502 |     1.0
   29 |   1.2716 |     39.602 |   1.3029 |     40.074 |     1.0
   30 |   1.2577 |     39.223 |   1.2930 |     39.798 |     1.1
   31 |   1.2464 |     38.735 |   1.2877 |     39.737 |     1.1
   32 |   1.2360 |     38.562 |   1.2818 |     39.400 |     1.1
   33 |   1.2242 |     38.096 |   1.2712 |     38.664 |     1.2
   34 |   1.2134 |     37.738 |   1.2612 |     38.756 |     1.2
   35 |   1.1999 |     37.305 |   1.2619 |     39.185 |     1.3
   36 |   1.1876 |     37.305 |   1.2468 |     38.419 |     1.3
   37 |   1.1797 |     36.769 |   1.2451 |     38.664 |     1.3
   38 |   1.1655 |     36.498 |   1.2345 |     38.051 |     1.4
   39 |   1.1549 |     36.145 |   1.2298 |     38.388 |     1.4
   40 |   1.1431 |     36.107 |   1.2212 |     38.021 |     1.4
   41 |   1.1343 |     35.506 |   1.2147 |     37.776 |     1.5
   42 |   1.1196 |     35.105 |   1.2082 |     37.194 |     1.5
   43 |   1.1071 |     34.634 |   1.2016 |     37.500 |     1.5
   44 |   1.0983 |     34.444 |   1.1941 |     36.703 |     1.6
   45 |   1.0877 |     33.929 |   1.1891 |     37.102 |     1.6
   46 |   1.0785 |     33.761 |   1.1836 |     36.795 |     1.6
   47 |   1.0642 |     33.290 |   1.1760 |     36.918 |     1.7
   48 |   1.0553 |     32.959 |   1.1719 |     36.581 |     1.7
   49 |   1.0459 |     32.607 |   1.1603 |     35.999 |     1.7
   50 |   1.0368 |     32.147 |   1.1562 |     36.213 |     1.8
   51 |   1.0236 |     31.767 |   1.1515 |     36.397 |     1.8
   52 |   1.0171 |     31.464 |   1.1423 |     35.815 |     1.9
   53 |   1.0046 |     31.291 |   1.1450 |     36.152 |     1.9
   54 |   0.9949 |     30.559 |   1.1335 |     35.968 |     1.9
   55 |   0.9849 |     30.451 |   1.1272 |     35.325 |     2.0
   56 |   0.9773 |     29.947 |   1.1208 |     35.049 |     2.0
   57 |   0.9700 |     30.061 |   1.1171 |     35.202 |     2.0
   58 |   0.9576 |     29.383 |   1.1118 |     34.988 |     2.1
   59 |   0.9471 |     29.194 |   1.1071 |     35.018 |     2.1
   60 |   0.9395 |     28.674 |   1.1048 |     34.712 |     2.1
   61 |   0.9287 |     28.354 |   1.1021 |     34.559 |     2.2
   62 |   0.9212 |     28.181 |   1.0949 |     34.589 |     2.2
   63 |   0.9121 |     27.774 |   1.0912 |     33.977 |     2.2
   64 |   0.9038 |     27.487 |   1.0855 |     33.885 |     2.3
   65 |   0.8911 |     27.032 |   1.0845 |     33.701 |     2.3
   66 |   0.8896 |     27.054 |   1.0797 |     33.946 |     2.3
   67 |   0.8765 |     26.620 |   1.0704 |     33.027 |     2.4
   68 |   0.8676 |     26.517 |   1.0717 |     32.904 |     2.4
   69 |   0.8607 |     25.845 |   1.0666 |     32.567 |     2.4
   70 |   0.8512 |     25.927 |   1.0702 |     33.548 |     2.5
   71 |   0.8470 |     25.596 |   1.0606 |     32.537 |     2.5
   72 |   0.8363 |     25.276 |   1.0629 |     33.701 |     2.6
   73 |   0.8289 |     25.168 |   1.0543 |     32.782 |     2.6
   74 |   0.8236 |     24.908 |   1.0499 |     32.353 |     2.6
   75 |   0.8121 |     24.420 |   1.0600 |     32.874 |     2.7
   76 |   0.8068 |     24.393 |   1.0483 |     32.047 |     2.7
   77 |   0.7977 |     24.204 |   1.0470 |     32.138 |     2.7
   78 |   0.7881 |     23.673 |   1.0387 |     32.016 |     2.8
   79 |   0.7802 |     23.488 |   1.0425 |     31.955 |     2.8
   80 |   0.7756 |     23.212 |   1.0373 |     31.771 |     2.8
   81 |   0.7681 |     22.995 |   1.0444 |     31.832 |     2.9
   82 |   0.7620 |     22.643 |   1.0408 |     31.679 |     2.9
   83 |   0.7584 |     22.898 |   1.0269 |     31.127 |     2.9
   84 |   0.7482 |     22.237 |   1.0325 |     31.066 |     3.0
   85 |   0.7405 |     21.847 |   1.0349 |     31.066 |     3.0
   86 |   0.7322 |     21.749 |   1.0198 |     30.821 |     3.0
   87 |   0.7263 |     21.630 |   1.0137 |     30.699 |     3.1
   88 |   0.7185 |     21.321 |   1.0138 |     30.515 |     3.1
   89 |   0.7173 |     21.158 |   1.0199 |     30.576 |     3.2
   90 |   0.7081 |     20.893 |   1.0110 |     30.116 |     3.2
   91 |   0.7005 |     20.833 |   1.0120 |     30.270 |     3.2
   92 |   0.6923 |     20.508 |   1.0059 |     30.208 |     3.3
   93 |   0.6848 |     20.248 |   1.0112 |     29.841 |     3.3
   94 |   0.6816 |     20.183 |   1.0058 |     30.055 |     3.3
   95 |   0.6740 |     20.048 |   1.0050 |     30.178 |     3.4
   96 |   0.6703 |     19.761 |   1.0082 |     30.300 |     3.4
   97 |   0.6629 |     19.755 |   1.0199 |     30.453 |     3.4
   98 |   0.6621 |     19.197 |   1.0080 |     30.453 |     3.5
   99 |   0.6549 |     19.127 |   1.0015 |     29.657 |     3.5
  100 |   0.6502 |     19.072 |   1.0035 |     30.025 |     3.5
  101 |   0.6434 |     18.997 |   1.0097 |     30.239 |     3.6
  102 |   0.6374 |     18.601 |   1.0031 |     29.749 |     3.6
  103 |   0.6332 |     18.693 |   0.9962 |     29.902 |     3.6
  104 |   0.6242 |     18.254 |   1.0071 |     29.810 |     3.7
  105 |   0.6213 |     18.168 |   0.9995 |     29.565 |     3.7
  106 |   0.6141 |     18.000 |   1.0123 |     30.178 |     3.7
  107 |   0.6092 |     17.891 |   1.0126 |     29.381 |     3.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,666,594

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8989 |     74.800 |   2.4835 |     66.881 |     0.1
    2 |   2.2510 |     59.796 |   2.0537 |     58.824 |     0.1
    3 |   1.9220 |     53.538 |   1.7710 |     48.346 |     0.2
    4 |   1.6778 |     48.320 |   1.5810 |     45.466 |     0.3
    5 |   1.5446 |     46.245 |   1.5008 |     45.466 |     0.3
    6 |   1.4840 |     46.316 |   1.4545 |     45.466 |     0.4
    7 |   1.4489 |     46.229 |   1.4314 |     45.466 |     0.5
    8 |   1.4325 |     46.272 |   1.4186 |     45.987 |     0.5
    9 |   1.4188 |     46.186 |   1.4102 |     45.987 |     0.6
   10 |   1.4135 |     46.278 |   1.4030 |     45.987 |     0.7
   11 |   1.4067 |     46.223 |   1.3982 |     45.129 |     0.7
   12 |   1.4021 |     46.045 |   1.3946 |     45.190 |     0.8
   13 |   1.3978 |     45.795 |   1.3894 |     45.221 |     0.9
   14 |   1.3950 |     45.573 |   1.3843 |     45.251 |     0.9
   15 |   1.3873 |     45.546 |   1.3788 |     45.221 |     1.0
   16 |   1.3835 |     45.470 |   1.3736 |     45.343 |     1.1
   17 |   1.3780 |     45.340 |   1.3677 |     44.914 |     1.1
   18 |   1.3703 |     45.026 |   1.3587 |     44.301 |     1.2
   19 |   1.3700 |     45.064 |   1.3515 |     44.271 |     1.3
   20 |   1.3598 |     44.744 |   1.3466 |     43.873 |     1.3
   21 |   1.3577 |     44.853 |   1.3467 |     44.271 |     1.4
   22 |   1.3521 |     44.880 |   1.3381 |     44.056 |     1.5
   23 |   1.3463 |     44.658 |   1.3359 |     44.087 |     1.6
   24 |   1.3418 |     44.663 |   1.3255 |     44.118 |     1.6
   25 |   1.3369 |     44.614 |   1.3213 |     43.597 |     1.7
   26 |   1.3349 |     44.506 |   1.3213 |     44.087 |     1.8
   27 |   1.3244 |     44.506 |   1.3144 |     43.658 |     1.8
   28 |   1.3248 |     44.235 |   1.3148 |     43.689 |     1.9
   29 |   1.3210 |     44.376 |   1.3208 |     43.781 |     2.0
   30 |   1.3159 |     44.045 |   1.3038 |     43.229 |     2.0
   31 |   1.3107 |     43.780 |   1.3093 |     43.290 |     2.1
   32 |   1.3066 |     43.655 |   1.2924 |     42.218 |     2.2
   33 |   1.3040 |     43.726 |   1.2929 |     42.065 |     2.3
   34 |   1.3011 |     43.774 |   1.2952 |     42.800 |     2.3
   35 |   1.2987 |     43.298 |   1.2895 |     42.463 |     2.4
   36 |   1.2923 |     43.238 |   1.2933 |     42.616 |     2.5
   37 |   1.2950 |     43.379 |   1.2818 |     41.759 |     2.5
   38 |   1.2890 |     43.140 |   1.2833 |     42.371 |     2.6
   39 |   1.2883 |     43.016 |   1.2840 |     42.616 |     2.7
   40 |   1.2849 |     42.837 |   1.2850 |     42.126 |     2.8
   41 |   1.2818 |     42.691 |   1.2792 |     41.881 |     2.8
   42 |   1.2757 |     42.815 |   1.2903 |     42.402 |     2.9
   43 |   1.2838 |     43.184 |   1.2794 |     42.647 |     3.0
   44 |   1.2760 |     42.696 |   1.2822 |     41.973 |     3.0
   45 |   1.2781 |     43.108 |   1.2760 |     42.310 |     3.1
   46 |   1.2749 |     42.669 |   1.2749 |     41.942 |     3.2
   47 |   1.2760 |     42.555 |   1.2645 |     40.962 |     3.2
   48 |   1.2722 |     42.745 |   1.2649 |     40.962 |     3.3
   49 |   1.2674 |     42.409 |   1.2646 |     41.667 |     3.4
   50 |   1.2641 |     42.425 |   1.2738 |     42.341 |     3.5
   51 |   1.2698 |     42.582 |   1.2695 |     41.636 |     3.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.1
Trainable parameters: 333,218

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.8801 |     77.129 |   2.4179 |     58.824 |     0.0
    2 |   2.2045 |     58.788 |   2.0273 |     58.824 |     0.1
    3 |   1.8997 |     53.088 |   1.7555 |     48.438 |     0.1
    4 |   1.6695 |     48.071 |   1.5756 |     45.466 |     0.1
    5 |   1.5414 |     46.353 |   1.4930 |     45.466 |     0.1
    6 |   1.4779 |     46.223 |   1.4521 |     45.466 |     0.2
    7 |   1.4461 |     46.218 |   1.4306 |     45.466 |     0.2
    8 |   1.4288 |     46.326 |   1.4161 |     45.466 |     0.2
    9 |   1.4193 |     46.294 |   1.4062 |     45.803 |     0.2
   10 |   1.4106 |     46.256 |   1.3987 |     45.466 |     0.3
   11 |   1.4026 |     46.229 |   1.3959 |     45.741 |     0.3
   12 |   1.3975 |     46.083 |   1.3905 |     45.987 |     0.3
   13 |   1.3888 |     46.045 |   1.3859 |     45.864 |     0.3
   14 |   1.3870 |     46.028 |   1.3804 |     45.006 |     0.4
   15 |   1.3792 |     45.622 |   1.3735 |     45.129 |     0.4
   16 |   1.3760 |     45.882 |   1.3702 |     45.190 |     0.4
   17 |   1.3687 |     45.898 |   1.3655 |     45.343 |     0.5
   18 |   1.3627 |     45.665 |   1.3579 |     45.190 |     0.5
   19 |   1.3541 |     45.600 |   1.3516 |     45.251 |     0.5
   20 |   1.3493 |     45.221 |   1.3426 |     44.730 |     0.5
   21 |   1.3387 |     44.793 |   1.3325 |     44.026 |     0.6
   22 |   1.3226 |     44.376 |   1.3117 |     42.678 |     0.6
   23 |   1.3115 |     43.661 |   1.3053 |     42.494 |     0.6
   24 |   1.3009 |     43.471 |   1.2945 |     42.463 |     0.7
   25 |   1.2869 |     42.897 |   1.2834 |     41.820 |     0.7
   26 |   1.2856 |     43.070 |   1.2800 |     42.371 |     0.7
   27 |   1.2763 |     42.875 |   1.2801 |     42.249 |     0.7
   28 |   1.2680 |     42.723 |   1.2609 |     42.096 |     0.8
   29 |   1.2632 |     42.447 |   1.2592 |     41.605 |     0.8
   30 |   1.2586 |     42.225 |   1.2565 |     41.422 |     0.8
   31 |   1.2505 |     42.084 |   1.2605 |     41.912 |     0.9
   32 |   1.2396 |     41.770 |   1.2415 |     41.238 |     0.9
   33 |   1.2300 |     41.862 |   1.2400 |     41.391 |     0.9
   34 |   1.2295 |     41.537 |   1.2341 |     41.850 |     0.9
   35 |   1.2213 |     41.260 |   1.2354 |     41.422 |     1.0
   36 |   1.2165 |     41.336 |   1.2257 |     41.023 |     1.0
   37 |   1.2054 |     40.756 |   1.2266 |     41.513 |     1.0
   38 |   1.1974 |     40.572 |   1.2115 |     40.839 |     1.0
   39 |   1.1889 |     40.480 |   1.2108 |     40.288 |     1.1
   40 |   1.1807 |     40.280 |   1.2072 |     40.319 |     1.1
   41 |   1.1793 |     40.014 |   1.2044 |     40.074 |     1.1
   42 |   1.1646 |     39.624 |   1.1963 |     39.369 |     1.1
   43 |   1.1593 |     39.245 |   1.1944 |     39.430 |     1.2
   44 |   1.1593 |     39.337 |   1.1880 |     39.798 |     1.2
   45 |   1.1488 |     39.082 |   1.1829 |     39.308 |     1.2
   46 |   1.1438 |     38.887 |   1.1818 |     39.216 |     1.2
   47 |   1.1357 |     38.594 |   1.1776 |     38.787 |     1.3
   48 |   1.1287 |     38.573 |   1.1786 |     39.277 |     1.3
   49 |   1.1273 |     38.497 |   1.1757 |     38.817 |     1.3
   50 |   1.1202 |     38.389 |   1.1692 |     38.787 |     1.3
   51 |   1.1145 |     38.296 |   1.1721 |     39.185 |     1.4
   52 |   1.1077 |     38.074 |   1.1687 |     39.124 |     1.4
   53 |   1.0979 |     37.901 |   1.1654 |     39.062 |     1.4
   54 |   1.1022 |     37.809 |   1.1777 |     39.369 |     1.4
   55 |   1.0944 |     37.711 |   1.1664 |     38.572 |     1.5
   56 |   1.0870 |     37.522 |   1.1672 |     40.104 |     1.5
   57 |   1.0886 |     37.402 |   1.1533 |     39.277 |     1.5
   58 |   1.0800 |     37.083 |   1.1489 |     38.940 |     1.5
   59 |   1.0723 |     37.159 |   1.1493 |     38.909 |     1.6
   60 |   1.0721 |     37.153 |   1.1464 |     38.082 |     1.6
   61 |   1.0623 |     36.693 |   1.1362 |     38.082 |     1.6
   62 |   1.0608 |     36.454 |   1.1371 |     38.450 |     1.6
   63 |   1.0596 |     36.292 |   1.1334 |     37.684 |     1.6
   64 |   1.0536 |     36.384 |   1.1379 |     37.224 |     1.7
   65 |   1.0444 |     35.446 |   1.1327 |     36.949 |     1.7
   66 |   1.0512 |     35.858 |   1.1274 |     36.734 |     1.7
   67 |   1.0340 |     35.213 |   1.1163 |     36.458 |     1.7
   68 |   1.0274 |     35.132 |   1.1101 |     37.010 |     1.8
   69 |   1.0248 |     35.008 |   1.1066 |     37.194 |     1.8
   70 |   1.0132 |     34.525 |   1.1083 |     37.377 |     1.8
   71 |   1.0095 |     34.661 |   1.1119 |     36.734 |     1.8
   72 |   1.0054 |     34.249 |   1.0994 |     35.907 |     1.9
   73 |   0.9954 |     34.021 |   1.1050 |     36.612 |     1.9
   74 |   1.0003 |     34.162 |   1.1012 |     35.938 |     1.9
   75 |   1.0041 |     34.070 |   1.0910 |     35.846 |     1.9
   76 |   0.9888 |     33.734 |   1.0901 |     35.938 |     2.0
   77 |   0.9888 |     33.648 |   1.0882 |     36.213 |     2.0
   78 |   0.9811 |     33.355 |   1.0943 |     36.366 |     2.0
   79 |   0.9667 |     32.873 |   1.0850 |     35.754 |     2.0
   80 |   0.9675 |     32.764 |   1.0734 |     35.662 |     2.1
   81 |   0.9723 |     33.122 |   1.0928 |     35.938 |     2.1
   82 |   0.9621 |     32.678 |   1.0736 |     35.233 |     2.1
   83 |   0.9565 |     32.212 |   1.0696 |     35.386 |     2.2
   84 |   0.9473 |     32.049 |   1.0817 |     35.754 |     2.2
   85 |   0.9539 |     32.407 |   1.0919 |     35.049 |     2.2
   86 |   0.9430 |     32.065 |   1.0679 |     35.141 |     2.2
   87 |   0.9398 |     31.735 |   1.0581 |     34.498 |     2.3
   88 |   0.9331 |     31.502 |   1.0608 |     34.528 |     2.3
   89 |   0.9351 |     31.795 |   1.0742 |     34.681 |     2.3
   90 |   0.9441 |     31.963 |   1.0618 |     34.436 |     2.3
   91 |   0.9333 |     31.540 |   1.0674 |     34.835 |     2.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 588,194

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.1875 |     83.160 |   2.6739 |     80.270 |     0.0
    2 |   2.3534 |     60.604 |   2.1421 |     57.353 |     0.0
    3 |   2.0184 |     54.660 |   1.9339 |     53.462 |     0.1
    4 |   1.8471 |     49.247 |   1.7766 |     48.070 |     0.1
    5 |   1.7098 |     48.331 |   1.6527 |     48.070 |     0.1
    6 |   1.6065 |     47.995 |   1.5625 |     45.282 |     0.2
    7 |   1.5312 |     45.947 |   1.5019 |     45.098 |     0.2
    8 |   1.4756 |     45.882 |   1.4577 |     45.067 |     0.2
    9 |   1.4350 |     45.638 |   1.4250 |     45.037 |     0.2
   10 |   1.4004 |     45.308 |   1.3948 |     44.700 |     0.3
   11 |   1.3705 |     43.839 |   1.3673 |     44.455 |     0.3
   12 |   1.3446 |     43.032 |   1.3481 |     43.811 |     0.3
   13 |   1.3220 |     42.360 |   1.3290 |     42.800 |     0.3
   14 |   1.2960 |     41.016 |   1.3089 |     41.728 |     0.4
   15 |   1.2767 |     40.215 |   1.2911 |     41.085 |     0.4
   16 |   1.2577 |     39.591 |   1.2735 |     40.165 |     0.4
   17 |   1.2360 |     38.681 |   1.2557 |     39.553 |     0.4
   18 |   1.2161 |     38.291 |   1.2398 |     38.787 |     0.5
   19 |   1.2009 |     37.793 |   1.2337 |     38.787 |     0.5
   20 |   1.1850 |     37.359 |   1.2215 |     38.419 |     0.5
   21 |   1.1644 |     36.698 |   1.2078 |     38.358 |     0.5
   22 |   1.1540 |     36.395 |   1.1929 |     37.163 |     0.6
   23 |   1.1364 |     36.005 |   1.1860 |     37.439 |     0.6
   24 |   1.1222 |     35.577 |   1.1733 |     36.703 |     0.6
   25 |   1.1092 |     35.078 |   1.1623 |     36.520 |     0.6
   26 |   1.0930 |     34.607 |   1.1539 |     36.642 |     0.7
   27 |   1.0787 |     34.309 |   1.1478 |     35.999 |     0.7
   28 |   1.0650 |     34.043 |   1.1347 |     35.938 |     0.7
   29 |   1.0558 |     33.723 |   1.1275 |     35.570 |     0.7
   30 |   1.0419 |     33.252 |   1.1204 |     36.275 |     0.8
   31 |   1.0290 |     32.759 |   1.1172 |     35.417 |     0.8
   32 |   1.0161 |     32.596 |   1.1055 |     35.417 |     0.8
   33 |   1.0055 |     32.065 |   1.1006 |     35.539 |     0.8
   34 |   0.9930 |     31.930 |   1.0919 |     34.835 |     0.9
   35 |   0.9814 |     31.285 |   1.0842 |     35.110 |     0.9
   36 |   0.9697 |     30.857 |   1.0834 |     34.743 |     0.9
   37 |   0.9622 |     30.760 |   1.0765 |     34.467 |     0.9
   38 |   0.9481 |     29.790 |   1.0605 |     34.130 |     1.0
   39 |   0.9347 |     29.394 |   1.0622 |     34.283 |     1.0
   40 |   0.9265 |     28.988 |   1.0493 |     33.487 |     1.0
   41 |   0.9152 |     28.701 |   1.0492 |     33.701 |     1.0
   42 |   0.9055 |     28.283 |   1.0408 |     33.670 |     1.1
   43 |   0.8933 |     27.725 |   1.0361 |     33.119 |     1.1
   44 |   0.8818 |     27.335 |   1.0258 |     32.751 |     1.1
   45 |   0.8726 |     27.070 |   1.0251 |     32.966 |     1.1
   46 |   0.8636 |     26.799 |   1.0148 |     32.537 |     1.2
   47 |   0.8505 |     26.284 |   1.0142 |     32.567 |     1.2
   48 |   0.8406 |     25.824 |   1.0054 |     32.598 |     1.2
   49 |   0.8287 |     25.536 |   1.0015 |     31.955 |     1.2
   50 |   0.8188 |     25.228 |   0.9959 |     31.679 |     1.3
   51 |   0.8084 |     24.789 |   0.9928 |     31.924 |     1.3
   52 |   0.7994 |     24.545 |   0.9958 |     31.373 |     1.3
   53 |   0.7902 |     24.117 |   0.9897 |     31.587 |     1.3
   54 |   0.7821 |     23.965 |   0.9831 |     31.036 |     1.4
   55 |   0.7724 |     23.505 |   0.9763 |     30.699 |     1.4
   56 |   0.7583 |     23.066 |   0.9762 |     30.821 |     1.4
   57 |   0.7512 |     22.952 |   0.9685 |     30.882 |     1.4
   58 |   0.7409 |     22.405 |   0.9701 |     30.607 |     1.5
   59 |   0.7273 |     22.069 |   0.9663 |     30.576 |     1.5
   60 |   0.7213 |     22.031 |   0.9660 |     30.760 |     1.5
   61 |   0.7126 |     21.462 |   0.9613 |     30.086 |     1.5
   62 |   0.7046 |     21.039 |   0.9577 |     30.086 |     1.6
   63 |   0.6955 |     20.801 |   0.9627 |     29.933 |     1.6
   64 |   0.6849 |     20.524 |   0.9525 |     29.688 |     1.6
   65 |   0.6810 |     20.405 |   0.9554 |     29.381 |     1.6
   66 |   0.6679 |     20.216 |   0.9562 |     29.871 |     1.7
   67 |   0.6562 |     19.652 |   0.9527 |     29.320 |     1.7
   68 |   0.6552 |     19.365 |   0.9583 |     29.565 |     1.7
   69 |   0.6418 |     19.246 |   0.9484 |     29.534 |     1.7
   70 |   0.6373 |     18.899 |   0.9610 |     29.442 |     1.8
   71 |   0.6328 |     18.904 |   0.9517 |     29.013 |     1.8
   72 |   0.6172 |     18.065 |   0.9470 |     29.504 |     1.8
   73 |   0.6103 |     18.325 |   0.9408 |     28.554 |     1.8
   74 |   0.6029 |     17.799 |   0.9509 |     29.289 |     1.9
   75 |   0.5951 |     17.637 |   0.9349 |     28.615 |     1.9
   76 |   0.5876 |     17.295 |   0.9386 |     28.401 |     1.9
   77 |   0.5798 |     16.992 |   0.9465 |     28.401 |     1.9
   78 |   0.5720 |     16.959 |   0.9405 |     28.278 |     1.9
   79 |   0.5654 |     16.607 |   0.9411 |     28.615 |     2.0
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 32
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 368,834

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4590 |     90.160 |   3.3094 |     83.333 |     0.0
    2 |   2.8995 |     83.035 |   2.6672 |     82.721 |     0.0
    3 |   2.5140 |     64.472 |   2.4084 |     58.824 |     0.1
    4 |   2.3078 |     59.049 |   2.2562 |     58.824 |     0.1
    5 |   2.1877 |     59.049 |   2.1536 |     58.824 |     0.1
    6 |   2.0974 |     59.049 |   2.0688 |     58.824 |     0.1
    7 |   2.0218 |     56.605 |   1.9928 |     53.799 |     0.1
    8 |   1.9504 |     53.137 |   1.9224 |     48.376 |     0.2
    9 |   1.8863 |     48.602 |   1.8575 |     48.346 |     0.2
   10 |   1.8273 |     48.597 |   1.7973 |     48.346 |     0.2
   11 |   1.7678 |     48.602 |   1.7370 |     48.346 |     0.2
   12 |   1.7153 |     48.602 |   1.6845 |     48.346 |     0.2
   13 |   1.6664 |     47.361 |   1.6404 |     45.466 |     0.3
   14 |   1.6265 |     46.213 |   1.6043 |     45.466 |     0.3
   15 |   1.5942 |     46.207 |   1.5749 |     45.466 |     0.3
   16 |   1.5698 |     46.207 |   1.5489 |     45.466 |     0.3
   17 |   1.5455 |     46.213 |   1.5273 |     45.466 |     0.3
   18 |   1.5245 |     46.223 |   1.5091 |     45.466 |     0.3
   19 |   1.5088 |     46.213 |   1.4938 |     45.466 |     0.4
   20 |   1.4933 |     46.207 |   1.4802 |     45.466 |     0.4
   21 |   1.4778 |     46.207 |   1.4688 |     45.466 |     0.4
   22 |   1.4704 |     46.213 |   1.4599 |     45.466 |     0.4
   23 |   1.4609 |     46.202 |   1.4519 |     45.466 |     0.4
   24 |   1.4538 |     46.213 |   1.4455 |     45.466 |     0.5
   25 |   1.4469 |     46.207 |   1.4397 |     45.466 |     0.5
   26 |   1.4406 |     46.207 |   1.4349 |     45.466 |     0.5
   27 |   1.4376 |     46.202 |   1.4305 |     45.466 |     0.5
   28 |   1.4340 |     46.207 |   1.4266 |     45.466 |     0.5
   29 |   1.4289 |     46.278 |   1.4233 |     45.466 |     0.5
   30 |   1.4281 |     46.207 |   1.4200 |     45.466 |     0.6
   31 |   1.4240 |     46.207 |   1.4176 |     45.619 |     0.6
   32 |   1.4211 |     46.240 |   1.4148 |     45.466 |     0.6
   33 |   1.4186 |     46.283 |   1.4125 |     45.466 |     0.6
   34 |   1.4168 |     46.207 |   1.4102 |     45.466 |     0.6
   35 |   1.4128 |     46.169 |   1.4084 |     45.466 |     0.6
   36 |   1.4113 |     46.213 |   1.4061 |     45.466 |     0.7
   37 |   1.4086 |     46.121 |   1.4013 |     45.466 |     0.7
   38 |   1.4006 |     46.207 |   1.3938 |     44.792 |     0.7
   39 |   1.3887 |     46.191 |   1.3808 |     45.466 |     0.7
   40 |   1.3758 |     46.175 |   1.3725 |     45.466 |     0.7
   41 |   1.3644 |     45.947 |   1.3610 |     45.129 |     0.8
   42 |   1.3562 |     45.682 |   1.3573 |     44.363 |     0.8
   43 |   1.3466 |     45.562 |   1.3503 |     45.006 |     0.8
   44 |   1.3375 |     45.438 |   1.3439 |     44.424 |     0.8
   45 |   1.3325 |     45.405 |   1.3417 |     44.393 |     0.8
   46 |   1.3267 |     45.102 |   1.3362 |     44.087 |     0.8
   47 |   1.3203 |     44.891 |   1.3327 |     43.811 |     0.9
   48 |   1.3154 |     44.647 |   1.3309 |     43.658 |     0.9
   49 |   1.3077 |     44.500 |   1.3254 |     43.903 |     0.9
   50 |   1.3022 |     44.181 |   1.3220 |     43.382 |     0.9
   51 |   1.2988 |     43.785 |   1.3185 |     43.658 |     0.9
   52 |   1.2927 |     43.655 |   1.3181 |     43.842 |     1.0
   53 |   1.2861 |     43.471 |   1.3140 |     43.750 |     1.0
   54 |   1.2841 |     43.243 |   1.3162 |     43.842 |     1.0
   55 |   1.2793 |     43.059 |   1.3084 |     43.995 |     1.0
   56 |   1.2709 |     42.880 |   1.3088 |     43.873 |     1.0
   57 |   1.2654 |     42.474 |   1.3057 |     44.026 |     1.0
   58 |   1.2629 |     42.528 |   1.3032 |     43.137 |     1.1
   59 |   1.2593 |     42.078 |   1.3006 |     43.382 |     1.1
   60 |   1.2551 |     41.954 |   1.2993 |     43.045 |     1.1
   61 |   1.2490 |     41.873 |   1.2950 |     43.413 |     1.1
   62 |   1.2463 |     41.715 |   1.2926 |     43.015 |     1.1
   63 |   1.2408 |     41.504 |   1.2909 |     42.831 |     1.2
   64 |   1.2367 |     41.645 |   1.2907 |     42.984 |     1.2
   65 |   1.2340 |     41.493 |   1.2874 |     43.199 |     1.2
   66 |   1.2304 |     41.282 |   1.2860 |     43.107 |     1.2
   67 |   1.2235 |     41.260 |   1.2855 |     42.953 |     1.2
   68 |   1.2216 |     41.033 |   1.2828 |     43.045 |     1.3
   69 |   1.2141 |     41.065 |   1.2809 |     42.770 |     1.3
   70 |   1.2100 |     40.930 |   1.2779 |     42.463 |     1.3
   71 |   1.2081 |     40.827 |   1.2775 |     42.616 |     1.3
   72 |   1.2044 |     40.653 |   1.2758 |     42.647 |     1.3
   73 |   1.2005 |     40.778 |   1.2722 |     42.494 |     1.4
   74 |   1.1952 |     40.496 |   1.2721 |     42.923 |     1.4
   75 |   1.1926 |     40.377 |   1.2737 |     42.126 |     1.4
   76 |   1.1858 |     40.171 |   1.2696 |     42.371 |     1.4
   77 |   1.1841 |     40.193 |   1.2643 |     42.279 |     1.4
   78 |   1.1803 |     40.144 |   1.2628 |     42.279 |     1.5
   79 |   1.1789 |     39.900 |   1.2631 |     42.096 |     1.5
   80 |   1.1712 |     39.678 |   1.2621 |     41.973 |     1.5
   81 |   1.1693 |     39.770 |   1.2627 |     41.942 |     1.5
   82 |   1.1679 |     39.537 |   1.2573 |     42.096 |     1.5
   83 |   1.1646 |     39.526 |   1.2603 |     41.759 |     1.6
   84 |   1.1586 |     39.272 |   1.2587 |     42.157 |     1.6
   85 |   1.1560 |     39.348 |   1.2640 |     42.188 |     1.6
   86 |   1.1518 |     39.115 |   1.2582 |     42.004 |     1.6
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 566,082

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4845 |     93.969 |   3.4380 |     86.857 |     0.0
    2 |   3.2845 |     80.044 |   3.0824 |     81.893 |     0.0
    3 |   2.9010 |     81.491 |   2.7867 |     81.893 |     0.1
    4 |   2.6929 |     80.294 |   2.6316 |     65.901 |     0.1
    5 |   2.5489 |     64.461 |   2.4974 |     58.824 |     0.1
    6 |   2.4248 |     59.049 |   2.3925 |     58.824 |     0.1
    7 |   2.3372 |     59.049 |   2.3150 |     58.824 |     0.2
    8 |   2.2703 |     59.049 |   2.2544 |     58.824 |     0.2
    9 |   2.2192 |     59.049 |   2.2028 |     58.824 |     0.2
   10 |   2.1704 |     59.049 |   2.1567 |     58.824 |     0.2
   11 |   2.1267 |     59.049 |   2.1150 |     58.824 |     0.3
   12 |   2.0897 |     59.049 |   2.0770 |     58.793 |     0.3
   13 |   2.0541 |     58.984 |   2.0407 |     57.935 |     0.3
   14 |   2.0191 |     58.512 |   2.0065 |     58.517 |     0.3
   15 |   1.9888 |     58.095 |   1.9738 |     56.648 |     0.4
   16 |   1.9553 |     56.697 |   1.9422 |     54.167 |     0.4
   17 |   1.9224 |     55.288 |   1.9110 |     53.768 |     0.4
   18 |   1.8944 |     52.395 |   1.8794 |     48.989 |     0.4
   19 |   1.8625 |     48.960 |   1.8477 |     48.346 |     0.5
   20 |   1.8350 |     48.651 |   1.8163 |     48.346 |     0.5
   21 |   1.8019 |     48.635 |   1.7851 |     48.376 |     0.5
   22 |   1.7720 |     48.624 |   1.7531 |     48.346 |     0.5
   23 |   1.7400 |     48.607 |   1.7222 |     48.346 |     0.6
   24 |   1.7122 |     48.602 |   1.6935 |     48.346 |     0.6
   25 |   1.6835 |     47.112 |   1.6672 |     45.466 |     0.6
   26 |   1.6626 |     46.202 |   1.6436 |     45.466 |     0.7
   27 |   1.6364 |     46.207 |   1.6229 |     45.466 |     0.7
   28 |   1.6205 |     46.202 |   1.6047 |     45.466 |     0.7
   29 |   1.5994 |     46.196 |   1.5885 |     45.466 |     0.7
   30 |   1.5853 |     46.196 |   1.5739 |     45.466 |     0.8
   31 |   1.5731 |     46.202 |   1.5614 |     45.466 |     0.8
   32 |   1.5593 |     46.196 |   1.5496 |     45.466 |     0.8
   33 |   1.5480 |     46.196 |   1.5390 |     45.466 |     0.8
   34 |   1.5395 |     46.196 |   1.5292 |     45.466 |     0.9
   35 |   1.5294 |     46.196 |   1.5199 |     45.466 |     0.9
   36 |   1.5237 |     46.202 |   1.5115 |     45.466 |     0.9
   37 |   1.5130 |     46.207 |   1.5044 |     45.466 |     1.0
   38 |   1.5045 |     46.202 |   1.4974 |     45.466 |     1.0
   39 |   1.4981 |     46.196 |   1.4910 |     45.466 |     1.0
   40 |   1.4903 |     46.207 |   1.4850 |     45.466 |     1.0
   41 |   1.4865 |     46.196 |   1.4790 |     45.466 |     1.1
   42 |   1.4760 |     46.213 |   1.4716 |     45.466 |     1.1
   43 |   1.4689 |     46.207 |   1.4652 |     45.466 |     1.1
   44 |   1.4648 |     46.202 |   1.4592 |     45.466 |     1.1
   45 |   1.4560 |     46.213 |   1.4525 |     45.466 |     1.2
   46 |   1.4504 |     46.207 |   1.4471 |     45.466 |     1.2
   47 |   1.4431 |     46.202 |   1.4422 |     45.466 |     1.2
   48 |   1.4354 |     46.202 |   1.4361 |     45.466 |     1.2
   49 |   1.4301 |     46.213 |   1.4318 |     45.466 |     1.3
   50 |   1.4272 |     46.213 |   1.4249 |     45.466 |     1.3
   51 |   1.4168 |     46.202 |   1.4216 |     45.466 |     1.3
   52 |   1.4114 |     46.213 |   1.4132 |     45.466 |     1.4
   53 |   1.4026 |     46.196 |   1.4065 |     45.466 |     1.4
   54 |   1.3955 |     46.202 |   1.4009 |     45.466 |     1.4
   55 |   1.3870 |     46.180 |   1.3987 |     45.466 |     1.4
   56 |   1.3828 |     46.164 |   1.3896 |     45.282 |     1.5
   57 |   1.3721 |     46.299 |   1.3865 |     45.772 |     1.5
   58 |   1.3659 |     45.915 |   1.3797 |     45.098 |     1.5
   59 |   1.3577 |     45.795 |   1.3756 |     44.884 |     1.5
   60 |   1.3551 |     45.405 |   1.3746 |     45.129 |     1.6
   61 |   1.3480 |     45.178 |   1.3694 |     44.975 |     1.6
   62 |   1.3404 |     44.853 |   1.3663 |     45.221 |     1.6
   63 |   1.3345 |     44.495 |   1.3628 |     44.577 |     1.7
   64 |   1.3313 |     44.267 |   1.3614 |     44.363 |     1.7
   65 |   1.3246 |     44.154 |   1.3586 |     44.669 |     1.7
   66 |   1.3211 |     43.975 |   1.3564 |     44.914 |     1.7
   67 |   1.3166 |     43.861 |   1.3514 |     44.455 |     1.8
   68 |   1.3121 |     43.666 |   1.3508 |     44.730 |     1.8
   69 |   1.3108 |     43.709 |   1.3539 |     44.669 |     1.8
   70 |   1.3016 |     43.487 |   1.3479 |     44.730 |     1.9
   71 |   1.2964 |     43.384 |   1.3487 |     44.638 |     1.9
   72 |   1.2940 |     43.341 |   1.3416 |     44.393 |     1.9
   73 |   1.2894 |     43.205 |   1.3433 |     44.547 |     1.9
   74 |   1.2865 |     43.092 |   1.3436 |     44.638 |     2.0
   75 |   1.2791 |     43.119 |   1.3420 |     44.608 |     2.0
   76 |   1.2785 |     43.081 |   1.3401 |     44.393 |     2.0
   77 |   1.2761 |     42.983 |   1.3400 |     44.669 |     2.0
   78 |   1.2709 |     42.945 |   1.3410 |     44.638 |     2.1
   79 |   1.2673 |     42.842 |   1.3402 |     44.577 |     2.1
   80 |   1.2646 |     42.837 |   1.3358 |     44.485 |     2.1
   81 |   1.2615 |     42.604 |   1.3371 |     44.516 |     2.2
   82 |   1.2568 |     42.718 |   1.3376 |     44.761 |     2.2
   83 |   1.2525 |     42.566 |   1.3354 |     44.179 |     2.2
   84 |   1.2521 |     42.376 |   1.3309 |     44.148 |     2.2
   85 |   1.2497 |     42.371 |   1.3347 |     44.271 |     2.3
   86 |   1.2441 |     42.322 |   1.3324 |     44.240 |     2.3
   87 |   1.2395 |     42.339 |   1.3340 |     44.301 |     2.3
   88 |   1.2394 |     41.970 |   1.3284 |     44.332 |     2.4
   89 |   1.2354 |     42.116 |   1.3305 |     44.210 |     2.4
   90 |   1.2330 |     42.138 |   1.3303 |     44.210 |     2.4
   91 |   1.2293 |     41.965 |   1.3260 |     43.934 |     2.4
   92 |   1.2249 |     41.694 |   1.3268 |     44.087 |     2.5
   93 |   1.2211 |     41.466 |   1.3288 |     43.811 |     2.5
   94 |   1.2189 |     41.629 |   1.3281 |     43.750 |     2.5
   95 |   1.2195 |     41.390 |   1.3308 |     44.087 |     2.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 64
Decoder layers: 2
Dropout: 0.0
Trainable parameters: 1,177,122

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2303 |     84.189 |   2.8742 |     83.333 |     0.0
    2 |   2.6758 |     69.609 |   2.5506 |     64.062 |     0.1
    3 |   2.4413 |     59.466 |   2.3719 |     58.793 |     0.1
    4 |   2.2873 |     58.756 |   2.2383 |     58.517 |     0.2
    5 |   2.1699 |     58.339 |   2.1288 |     56.556 |     0.2
    6 |   2.0678 |     57.152 |   2.0252 |     55.025 |     0.3
    7 |   1.9620 |     54.568 |   1.9140 |     50.551 |     0.3
    8 |   1.8558 |     49.220 |   1.8103 |     48.468 |     0.4
    9 |   1.7640 |     48.467 |   1.7262 |     48.192 |     0.4
   10 |   1.6924 |     48.494 |   1.6587 |     48.346 |     0.5
   11 |   1.6316 |     48.120 |   1.6067 |     45.619 |     0.5
   12 |   1.5846 |     46.229 |   1.5635 |     45.404 |     0.6
   13 |   1.5508 |     46.175 |   1.5298 |     45.496 |     0.6
   14 |   1.5160 |     46.186 |   1.5017 |     45.466 |     0.7
   15 |   1.4904 |     46.153 |   1.4786 |     45.435 |     0.7
   16 |   1.4677 |     46.110 |   1.4574 |     45.190 |     0.8
   17 |   1.4433 |     46.001 |   1.4397 |     45.221 |     0.8
   18 |   1.4239 |     45.795 |   1.4242 |     45.251 |     0.9
   19 |   1.4075 |     45.747 |   1.4076 |     45.312 |     0.9
   20 |   1.3948 |     45.351 |   1.3974 |     45.221 |     1.0
   21 |   1.3772 |     45.053 |   1.3814 |     44.914 |     1.0
   22 |   1.3587 |     44.641 |   1.3704 |     44.148 |     1.1
   23 |   1.3457 |     44.034 |   1.3528 |     43.505 |     1.1
   24 |   1.3262 |     43.205 |   1.3420 |     43.352 |     1.2
   25 |   1.3124 |     42.582 |   1.3295 |     42.433 |     1.2
   26 |   1.2985 |     41.938 |   1.3167 |     41.973 |     1.2
   27 |   1.2793 |     40.897 |   1.3052 |     41.299 |     1.3
   28 |   1.2642 |     40.269 |   1.2893 |     40.319 |     1.3
   29 |   1.2469 |     39.678 |   1.2799 |     40.625 |     1.4
   30 |   1.2322 |     39.104 |   1.2649 |     40.227 |     1.4
   31 |   1.2153 |     38.822 |   1.2549 |     39.828 |     1.5
   32 |   1.2024 |     38.188 |   1.2414 |     39.461 |     1.5
   33 |   1.1861 |     37.608 |   1.2283 |     39.062 |     1.6
   34 |   1.1692 |     37.023 |   1.2182 |     39.062 |     1.6
   35 |   1.1547 |     36.433 |   1.2064 |     38.634 |     1.7
   36 |   1.1380 |     36.064 |   1.1966 |     37.898 |     1.7
   37 |   1.1232 |     35.213 |   1.1819 |     36.918 |     1.8
   38 |   1.1036 |     34.206 |   1.1731 |     36.734 |     1.8
   39 |   1.0902 |     33.599 |   1.1614 |     35.846 |     1.9
   40 |   1.0758 |     33.014 |   1.1528 |     36.213 |     1.9
   41 |   1.0613 |     32.315 |   1.1456 |     36.121 |     2.0
   42 |   1.0435 |     31.664 |   1.1368 |     35.692 |     2.0
   43 |   1.0300 |     31.199 |   1.1250 |     35.202 |     2.1
   44 |   1.0188 |     31.036 |   1.1170 |     35.386 |     2.1
   45 |   0.9985 |     30.467 |   1.1048 |     34.436 |     2.2
   46 |   0.9863 |     29.909 |   1.1016 |     34.804 |     2.2
   47 |   0.9735 |     29.400 |   1.0965 |     34.620 |     2.2
   48 |   0.9588 |     28.896 |   1.0781 |     34.007 |     2.3
   49 |   0.9447 |     28.235 |   1.0766 |     34.099 |     2.3
   50 |   0.9330 |     27.975 |   1.0725 |     34.467 |     2.4
   51 |   0.9185 |     27.422 |   1.0609 |     33.946 |     2.4
   52 |   0.9060 |     27.156 |   1.0574 |     33.548 |     2.5
   53 |   0.8967 |     26.588 |   1.0528 |     33.578 |     2.5
   54 |   0.8831 |     26.013 |   1.0401 |     32.537 |     2.6
   55 |   0.8702 |     25.645 |   1.0391 |     32.047 |     2.6
   56 |   0.8573 |     25.363 |   1.0338 |     32.353 |     2.7
   57 |   0.8500 |     24.897 |   1.0298 |     31.801 |     2.7
   58 |   0.8386 |     24.377 |   1.0238 |     31.955 |     2.8
   59 |   0.8256 |     24.231 |   1.0219 |     31.648 |     2.8
   60 |   0.8138 |     23.602 |   1.0175 |     31.526 |     2.9
   61 |   0.8021 |     23.353 |   1.0112 |     31.066 |     2.9
   62 |   0.7914 |     22.773 |   1.0043 |     31.311 |     2.9
   63 |   0.7815 |     22.648 |   1.0066 |     31.066 |     3.0
   64 |   0.7673 |     22.134 |   1.0054 |     30.882 |     3.0
   65 |   0.7590 |     21.895 |   1.0064 |     30.852 |     3.1
   66 |   0.7494 |     21.608 |   0.9945 |     30.515 |     3.1
   67 |   0.7410 |     21.435 |   0.9990 |     30.790 |     3.2
   68 |   0.7309 |     21.018 |   0.9927 |     30.362 |     3.2
   69 |   0.7194 |     20.606 |   0.9893 |     29.779 |     3.3
   70 |   0.7085 |     20.129 |   0.9813 |     29.841 |     3.3
   71 |   0.7004 |     20.026 |   0.9828 |     29.902 |     3.4
   72 |   0.6922 |     19.896 |   0.9912 |     29.688 |     3.4
   73 |   0.6842 |     19.500 |   0.9781 |     29.504 |     3.5
   74 |   0.6756 |     19.230 |   0.9776 |     29.228 |     3.5
   75 |   0.6827 |     19.522 |   0.9718 |     29.228 |     3.6
   76 |   0.6598 |     18.709 |   0.9776 |     28.983 |     3.6
   77 |   0.6472 |     18.298 |   0.9737 |     29.044 |     3.7
   78 |   0.6439 |     18.092 |   0.9676 |     28.523 |     3.7
   79 |   0.6369 |     18.243 |   0.9649 |     28.860 |     3.8
   80 |   0.6244 |     17.447 |   0.9665 |     28.585 |     3.8
   81 |   0.6187 |     17.398 |   0.9578 |     28.768 |     3.9
   82 |   0.6122 |     17.404 |   0.9593 |     28.922 |     3.9
   83 |   0.6069 |     16.867 |   0.9535 |     28.156 |     4.0
   84 |   0.5929 |     16.921 |   0.9660 |     28.554 |     4.0
   85 |   0.5852 |     16.282 |   0.9657 |     28.217 |     4.1
   86 |   0.5795 |     16.428 |   0.9624 |     27.665 |     4.1
   87 |   0.5717 |     15.794 |   0.9697 |     28.156 |     4.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 1,357,410

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2831 |     61.828 |   1.6321 |     48.407 |     0.0
    2 |   1.4908 |     46.381 |   1.4185 |     45.466 |     0.1
    3 |   1.4178 |     46.348 |   1.3980 |     45.466 |     0.1
    4 |   1.3983 |     46.419 |   1.3797 |     45.466 |     0.2
    5 |   1.3729 |     46.039 |   1.3721 |     46.170 |     0.2
    6 |   1.3331 |     45.199 |   1.3131 |     43.627 |     0.2
    7 |   1.2900 |     44.024 |   1.2797 |     43.107 |     0.3
    8 |   1.2623 |     43.032 |   1.2624 |     42.770 |     0.3
    9 |   1.2403 |     42.485 |   1.2394 |     41.759 |     0.3
   10 |   1.2145 |     41.547 |   1.2105 |     40.043 |     0.4
   11 |   1.1891 |     40.345 |   1.2128 |     40.564 |     0.4
   12 |   1.1700 |     39.857 |   1.1953 |     40.043 |     0.4
   13 |   1.1575 |     39.602 |   1.1682 |     38.664 |     0.5
   14 |   1.1365 |     38.665 |   1.1678 |     38.909 |     0.5
   15 |   1.1205 |     38.307 |   1.1522 |     37.929 |     0.6
   16 |   1.1076 |     37.668 |   1.1590 |     39.062 |     0.6
   17 |   1.0893 |     37.527 |   1.1366 |     38.113 |     0.6
   18 |   1.0672 |     36.503 |   1.1260 |     37.286 |     0.7
   19 |   1.0606 |     36.205 |   1.1124 |     37.561 |     0.7
   20 |   1.0384 |     35.755 |   1.0755 |     36.428 |     0.7
   21 |   1.0092 |     34.840 |   1.0855 |     36.642 |     0.8
   22 |   0.9906 |     33.940 |   1.0633 |     35.876 |     0.8
   23 |   0.9758 |     33.593 |   1.0548 |     35.754 |     0.9
   24 |   0.9517 |     32.434 |   1.0495 |     35.417 |     0.9
   25 |   0.9301 |     31.529 |   1.0577 |     33.946 |     0.9
   26 |   0.9243 |     31.627 |   1.0560 |     34.896 |     1.0
   27 |   0.9159 |     31.155 |   1.0285 |     33.732 |     1.0
   28 |   0.8813 |     30.185 |   1.0583 |     35.417 |     1.0
   29 |   0.8734 |     29.730 |   0.9777 |     32.935 |     1.1
   30 |   0.8486 |     28.852 |   1.0119 |     33.180 |     1.1
   31 |   0.8397 |     28.533 |   0.9976 |     33.303 |     1.2
   32 |   0.8262 |     28.305 |   0.9589 |     31.648 |     1.2
   33 |   0.7826 |     26.625 |   0.9677 |     31.342 |     1.2
   34 |   0.7894 |     26.761 |   1.0006 |     32.506 |     1.3
   35 |   0.7718 |     26.008 |   0.9601 |     31.679 |     1.3
   36 |   0.7643 |     26.008 |   0.9743 |     31.556 |     1.3
   37 |   0.7366 |     25.033 |   0.9533 |     31.495 |     1.4
   38 |   0.6983 |     23.570 |   0.9716 |     31.955 |     1.4
   39 |   0.6992 |     23.624 |   0.9733 |     31.801 |     1.4
   40 |   0.7056 |     24.225 |   0.9584 |     30.668 |     1.5
   41 |   0.6712 |     22.703 |   0.9673 |     30.852 |     1.5
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 64
Decoder layers: 3
Dropout: 0.1
Trainable parameters: 420,834

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.6060 |     68.731 |   2.0919 |     53.983 |     0.0
    2 |   1.8269 |     51.430 |   1.6068 |     45.466 |     0.1
    3 |   1.5271 |     46.256 |   1.4616 |     45.466 |     0.1
    4 |   1.4488 |     46.234 |   1.4229 |     45.987 |     0.1
    5 |   1.4186 |     46.186 |   1.4037 |     45.466 |     0.1
    6 |   1.4032 |     46.261 |   1.3891 |     45.466 |     0.2
    7 |   1.3902 |     46.272 |   1.3745 |     45.987 |     0.2
    8 |   1.3791 |     46.180 |   1.3606 |     45.435 |     0.2
    9 |   1.3612 |     45.904 |   1.3487 |     44.730 |     0.3
   10 |   1.3479 |     45.362 |   1.3321 |     44.301 |     0.3
   11 |   1.3214 |     44.376 |   1.3027 |     43.566 |     0.3
   12 |   1.2929 |     43.455 |   1.2793 |     43.045 |     0.3
   13 |   1.2722 |     42.994 |   1.2555 |     41.575 |     0.4
   14 |   1.2453 |     41.753 |   1.2504 |     41.544 |     0.4
   15 |   1.2216 |     41.260 |   1.2226 |     40.962 |     0.4
   16 |   1.1955 |     40.697 |   1.2073 |     40.594 |     0.4
   17 |   1.1746 |     39.992 |   1.1860 |     40.104 |     0.5
   18 |   1.1579 |     39.044 |   1.1674 |     38.572 |     0.5
   19 |   1.1347 |     38.567 |   1.1630 |     38.297 |     0.5
   20 |   1.1121 |     37.646 |   1.1356 |     38.143 |     0.6
   21 |   1.0884 |     37.229 |   1.1335 |     37.500 |     0.6
   22 |   1.0719 |     36.693 |   1.1115 |     36.765 |     0.6
   23 |   1.0656 |     36.384 |   1.1120 |     37.500 |     0.6
   24 |   1.0379 |     35.544 |   1.0871 |     36.489 |     0.7
   25 |   1.0068 |     34.563 |   1.0817 |     35.692 |     0.7
   26 |   0.9946 |     33.891 |   1.0750 |     36.091 |     0.7
   27 |   0.9746 |     33.431 |   1.0637 |     34.589 |     0.7
   28 |   0.9482 |     32.493 |   1.0669 |     35.355 |     0.7
   29 |   0.9316 |     31.876 |   1.0419 |     34.344 |     0.8
   30 |   0.9302 |     31.849 |   1.0447 |     34.957 |     0.8
   31 |   0.9003 |     30.462 |   1.0295 |     33.762 |     0.8
   32 |   0.8815 |     29.790 |   1.0261 |     33.824 |     0.9
   33 |   0.8705 |     29.578 |   1.0074 |     32.935 |     0.9
   34 |   0.8440 |     28.246 |   1.0031 |     32.384 |     0.9
   35 |   0.8320 |     27.969 |   1.0019 |     32.292 |     0.9
   36 |   0.8207 |     27.384 |   0.9890 |     30.699 |     1.0
   37 |   0.7919 |     26.290 |   0.9776 |     31.495 |     1.0
   38 |   0.8012 |     27.043 |   0.9900 |     31.618 |     1.0
   39 |   0.7771 |     25.721 |   0.9786 |     31.097 |     1.0
   40 |   0.7517 |     25.081 |   0.9661 |     31.005 |     1.1
   41 |   0.7337 |     24.187 |   0.9571 |     29.657 |     1.1
   42 |   0.7267 |     24.128 |   0.9649 |     30.729 |     1.1
   43 |   0.7133 |     23.358 |   0.9660 |     29.381 |     1.2
   44 |   0.6874 |     22.762 |   0.9499 |     29.228 |     1.2
   45 |   0.6780 |     22.248 |   0.9356 |     29.136 |     1.2
   46 |   0.6668 |     22.296 |   0.9365 |     29.320 |     1.2
   47 |   0.6482 |     21.202 |   0.9365 |     28.707 |     1.3
   48 |   0.6380 |     21.142 |   0.9397 |     28.094 |     1.3
   49 |   0.6277 |     20.757 |   0.9201 |     28.309 |     1.3
   50 |   0.6148 |     20.470 |   0.9399 |     27.574 |     1.3
   51 |   0.6056 |     20.199 |   0.9374 |     28.707 |     1.3
   52 |   0.5865 |     19.164 |   0.9457 |     28.768 |     1.4
   53 |   0.5792 |     19.034 |   0.9337 |     27.788 |     1.4
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 32
Descriptors dimension: 1136
Decoder embedding dimension: 32
Encoder hidden units: 128
Encoder layers: 3
Decoder hidden units: 32
Decoder layers: 3
Dropout: 0.0
Trainable parameters: 1,618,690

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.4100 |     79.562 |   3.3193 |     83.303 |     0.1
    2 |   3.1307 |     83.317 |   2.9345 |     83.333 |     0.1
    3 |   2.8028 |     77.254 |   2.7167 |     67.341 |     0.2
    4 |   2.6375 |     67.295 |   2.5887 |     66.912 |     0.3
    5 |   2.5251 |     67.057 |   2.4903 |     66.912 |     0.3
    6 |   2.4379 |     66.000 |   2.4150 |     59.375 |     0.4
    7 |   2.3716 |     59.211 |   2.3554 |     58.946 |     0.5
    8 |   2.3175 |     59.054 |   2.3050 |     58.824 |     0.5
    9 |   2.2706 |     59.049 |   2.2595 |     58.824 |     0.6
   10 |   2.2258 |     59.049 |   2.2176 |     58.824 |     0.7
   11 |   2.1884 |     59.049 |   2.1764 |     58.824 |     0.7
   12 |   2.1486 |     59.049 |   2.1362 |     58.793 |     0.8
   13 |   2.1097 |     59.027 |   2.0943 |     58.609 |     0.9
   14 |   2.0626 |     58.843 |   2.0445 |     58.272 |     0.9
   15 |   2.0197 |     58.453 |   2.0015 |     58.272 |     1.0
   16 |   1.9789 |     58.236 |   1.9634 |     56.893 |     1.1
   17 |   1.9413 |     57.141 |   1.9277 |     56.250 |     1.1
   18 |   1.9088 |     53.099 |   1.8940 |     50.674 |     1.2
   19 |   1.8762 |     49.772 |   1.8611 |     48.744 |     1.3
   20 |   1.8423 |     48.960 |   1.8291 |     48.376 |     1.3
   21 |   1.8140 |     48.640 |   1.7988 |     48.376 |     1.4
   22 |   1.7851 |     48.635 |   1.7709 |     48.376 |     1.4
   23 |   1.7581 |     48.618 |   1.7449 |     48.376 |     1.5
   24 |   1.7367 |     48.613 |   1.7207 |     48.376 |     1.6
   25 |   1.7107 |     48.607 |   1.6977 |     48.376 |     1.6
   26 |   1.6877 |     48.607 |   1.6763 |     48.376 |     1.7
   27 |   1.6683 |     48.613 |   1.6562 |     48.346 |     1.8
   28 |   1.6500 |     46.695 |   1.6381 |     45.466 |     1.8
   29 |   1.6313 |     46.218 |   1.6203 |     45.496 |     1.9
   30 |   1.6136 |     46.218 |   1.6042 |     45.466 |     2.0
   31 |   1.5994 |     46.218 |   1.5886 |     45.466 |     2.0
   32 |   1.5835 |     46.213 |   1.5749 |     45.466 |     2.1
   33 |   1.5704 |     46.213 |   1.5623 |     45.466 |     2.2
   34 |   1.5585 |     46.213 |   1.5504 |     45.466 |     2.2
   35 |   1.5482 |     46.213 |   1.5398 |     45.466 |     2.3
   36 |   1.5359 |     46.213 |   1.5295 |     45.466 |     2.4
   37 |   1.5258 |     46.213 |   1.5196 |     45.466 |     2.5
   38 |   1.5186 |     46.213 |   1.5102 |     45.466 |     2.5
   39 |   1.5072 |     46.213 |   1.5011 |     45.466 |     2.6
   40 |   1.4991 |     46.213 |   1.4931 |     45.466 |     2.7
   41 |   1.4915 |     46.213 |   1.4857 |     45.466 |     2.7
   42 |   1.4851 |     46.207 |   1.4793 |     45.466 |     2.8
   43 |   1.4766 |     46.294 |   1.4730 |     45.527 |     2.9
   44 |   1.4712 |     46.213 |   1.4676 |     45.466 |     2.9
   45 |   1.4656 |     46.218 |   1.4639 |     45.466 |     3.0
   46 |   1.4612 |     46.115 |   1.4578 |     45.466 |     3.1
   47 |   1.4568 |     46.202 |   1.4534 |     45.466 |     3.1
   48 |   1.4526 |     46.213 |   1.4492 |     45.466 |     3.2
   49 |   1.4478 |     46.104 |   1.4450 |     45.466 |     3.3
   50 |   1.4411 |     46.202 |   1.4406 |     45.466 |     3.3
   51 |   1.4389 |     46.207 |   1.4369 |     45.588 |     3.4
   52 |   1.4328 |     46.202 |   1.4332 |     45.466 |     3.5
   53 |   1.4289 |     46.175 |   1.4288 |     44.792 |     3.5
   54 |   1.4277 |     45.963 |   1.4258 |     45.466 |     3.6
   55 |   1.4229 |     45.709 |   1.4224 |     45.221 |     3.7
   56 |   1.4183 |     45.795 |   1.4187 |     44.638 |     3.7
   57 |   1.4134 |     45.730 |   1.4150 |     44.700 |     3.8
   58 |   1.4120 |     45.611 |   1.4128 |     44.424 |     3.9
   59 |   1.4068 |     45.302 |   1.4084 |     44.516 |     3.9
   60 |   1.4013 |     45.134 |   1.4040 |     44.669 |     4.0
   61 |   1.3955 |     45.292 |   1.4004 |     44.975 |     4.1
   62 |   1.3913 |     45.178 |   1.3970 |     44.761 |     4.1
   63 |   1.3863 |     45.102 |   1.3931 |     44.730 |     4.2
   64 |   1.3818 |     44.993 |   1.3888 |     44.210 |     4.3
   65 |   1.3783 |     44.798 |   1.3867 |     43.903 |     4.4
   66 |   1.3717 |     44.349 |   1.3824 |     43.934 |     4.4
   67 |   1.3659 |     44.175 |   1.3785 |     43.719 |     4.5
   68 |   1.3619 |     43.942 |   1.3751 |     43.903 |     4.6
   69 |   1.3581 |     43.585 |   1.3699 |     43.536 |     4.6
   70 |   1.3519 |     43.590 |   1.3656 |     43.627 |     4.7
   71 |   1.3463 |     43.368 |   1.3626 |     43.413 |     4.8
   72 |   1.3392 |     43.103 |   1.3579 |     43.199 |     4.8
   73 |   1.3366 |     42.940 |   1.3531 |     43.290 |     4.9
   74 |   1.3301 |     42.794 |   1.3515 |     43.199 |     5.0
   75 |   1.3230 |     42.702 |   1.3467 |     43.107 |     5.0
   76 |   1.3176 |     42.734 |   1.3391 |     43.015 |     5.1
   77 |   1.3075 |     42.311 |   1.3351 |     42.647 |     5.2
   78 |   1.3025 |     42.176 |   1.3321 |     42.678 |     5.2
   79 |   1.2966 |     42.008 |   1.3292 |     42.371 |     5.3
   80 |   1.2911 |     41.770 |   1.3208 |     42.279 |     5.4
   81 |   1.2851 |     41.537 |   1.3201 |     42.310 |     5.4
   82 |   1.2784 |     41.445 |   1.3180 |     41.942 |     5.5
   83 |   1.2706 |     40.957 |   1.3133 |     41.544 |     5.6
   84 |   1.2649 |     40.827 |   1.3083 |     41.728 |     5.6
   85 |   1.2580 |     40.431 |   1.3081 |     41.728 |     5.7
   86 |   1.2504 |     40.312 |   1.3008 |     41.850 |     5.8
   87 |   1.2460 |     40.047 |   1.2980 |     41.544 |     5.8
   88 |   1.2407 |     40.057 |   1.2947 |     41.697 |     5.9
   89 |   1.2352 |     39.808 |   1.2941 |     41.728 |     6.0
   90 |   1.2251 |     39.521 |   1.2910 |     41.667 |     6.1
   91 |   1.2206 |     39.358 |   1.2866 |     41.513 |     6.1
   92 |   1.2139 |     39.391 |   1.2845 |     41.636 |     6.2
   93 |   1.2055 |     39.082 |   1.2842 |     41.912 |     6.3
   94 |   1.1996 |     38.974 |   1.2811 |     41.360 |     6.3
   95 |   1.1928 |     38.654 |   1.2783 |     41.544 |     6.4
   96 |   1.1880 |     38.589 |   1.2734 |     41.391 |     6.5
   97 |   1.1825 |     38.383 |   1.2705 |     41.085 |     6.5
   98 |   1.1747 |     37.955 |   1.2679 |     41.085 |     6.6
   99 |   1.1707 |     37.825 |   1.2675 |     41.360 |     6.7
  100 |   1.1648 |     37.803 |   1.2674 |     40.778 |     6.7
  101 |   1.1580 |     37.635 |   1.2660 |     41.023 |     6.8
  102 |   1.1526 |     37.262 |   1.2554 |     40.533 |     6.9
  103 |   1.1448 |     37.251 |   1.2552 |     40.686 |     6.9
  104 |   1.1405 |     37.077 |   1.2568 |     40.594 |     7.0
  105 |   1.1343 |     36.709 |   1.2567 |     40.717 |     7.1
  106 |   1.1268 |     36.899 |   1.2517 |     40.901 |     7.1
  107 |   1.1227 |     36.590 |   1.2470 |     40.809 |     7.2
  108 |   1.1214 |     36.557 |   1.2492 |     40.411 |     7.3
  109 |   1.1112 |     36.232 |   1.2449 |     40.319 |     7.3
  110 |   1.1055 |     36.162 |   1.2451 |     40.625 |     7.4
  111 |   1.1029 |     36.189 |   1.2416 |     40.319 |     9.2
  112 |   1.0969 |     35.820 |   1.2422 |     40.411 |    11.2
  113 |   1.0905 |     35.728 |   1.2350 |     40.257 |    13.0
  114 |   1.0851 |     35.398 |   1.2375 |     40.288 |    14.8
  115 |   1.0813 |     35.409 |   1.2351 |     39.859 |    16.6
  116 |   1.0775 |     35.127 |   1.2349 |     40.257 |    18.5
  117 |   1.0695 |     34.986 |   1.2351 |     40.165 |    20.4
  118 |   1.0627 |     34.569 |   1.2330 |     40.012 |    22.3
  119 |   1.0576 |     34.450 |   1.2295 |     39.982 |    24.2
  120 |   1.0541 |     34.314 |   1.2291 |     39.951 |    26.1
  121 |   1.0491 |     34.054 |   1.2300 |     39.522 |    27.8
  122 |   1.0446 |     33.610 |   1.2334 |     39.706 |    29.7
  123 |   1.0388 |     33.528 |   1.2261 |     39.583 |    31.6
  124 |   1.0351 |     33.393 |   1.2290 |     39.400 |    33.3
  125 |   1.0275 |     33.155 |   1.2269 |     39.920 |    35.1
  126 |   1.0204 |     32.710 |   1.2198 |     39.124 |    37.0
  127 |   1.0166 |     32.710 |   1.2254 |     38.940 |    38.8
  128 |   1.0123 |     32.651 |   1.2213 |     38.817 |    40.7
  129 |   1.0099 |     32.401 |   1.2264 |     39.491 |    42.6
  130 |   1.0039 |     32.412 |   1.2189 |     38.664 |    44.6
  131 |   0.9990 |     31.892 |   1.2163 |     38.756 |    46.5
  132 |   0.9919 |     31.664 |   1.2199 |     38.817 |    48.4
  133 |   0.9866 |     31.502 |   1.2213 |     38.695 |    50.3
  134 |   0.9814 |     31.020 |   1.2192 |     38.511 |    52.2
  135 |   0.9783 |     30.922 |   1.2146 |     38.082 |    54.1
  136 |   0.9764 |     30.592 |   1.2172 |     38.297 |    55.8
  137 |   0.9696 |     30.370 |   1.2096 |     37.745 |    57.5
  138 |   0.9592 |     30.039 |   1.2146 |     37.837 |    59.8
  139 |   0.9583 |     29.898 |   1.2113 |     37.531 |    61.7
  140 |   0.9500 |     29.606 |   1.2117 |     37.592 |    63.0
  141 |   0.9464 |     29.383 |   1.2201 |     37.745 |    63.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,393,186

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.2868 |     62.289 |   1.6634 |     48.346 |     0.0
    2 |   1.5051 |     46.500 |   1.4258 |     45.466 |     0.1
    3 |   1.4206 |     46.337 |   1.4008 |     45.466 |     0.2
    4 |   1.4066 |     46.121 |   1.3951 |     45.987 |     0.2
    5 |   1.3961 |     46.148 |   1.3818 |     45.404 |     0.3
    6 |   1.3738 |     45.107 |   1.3518 |     43.995 |     0.3
    7 |   1.3540 |     44.896 |   1.3384 |     44.301 |     0.4
    8 |   1.3413 |     44.820 |   1.3268 |     44.240 |     0.5
    9 |   1.3319 |     44.674 |   1.3229 |     44.301 |     0.5
   10 |   1.3263 |     44.587 |   1.3230 |     43.995 |     0.6
   11 |   1.3168 |     44.322 |   1.3141 |     44.393 |     0.7
   12 |   1.3067 |     44.105 |   1.3005 |     44.118 |     0.7
   13 |   1.2979 |     43.650 |   1.2942 |     43.321 |     0.8
   14 |   1.2774 |     42.897 |   1.2701 |     41.575 |     0.8
   15 |   1.2649 |     42.474 |   1.2620 |     41.636 |     0.9
   16 |   1.2504 |     42.203 |   1.2490 |     41.483 |     0.9
   17 |   1.2333 |     41.862 |   1.2363 |     41.085 |     1.0
   18 |   1.2210 |     41.065 |   1.2269 |     41.268 |     1.1
   19 |   1.2076 |     40.697 |   1.2235 |     40.901 |     1.2
   20 |   1.1957 |     40.637 |   1.2089 |     39.645 |     1.3
   21 |   1.1867 |     39.965 |   1.2022 |     39.093 |     1.4
   22 |   1.1708 |     39.245 |   1.1874 |     39.032 |     1.5
   23 |   1.1556 |     38.985 |   1.1751 |     38.450 |     1.6
   24 |   1.1386 |     38.524 |   1.1754 |     38.572 |     1.7
   25 |   1.1260 |     38.242 |   1.1690 |     38.542 |     1.8
   26 |   1.1216 |     37.663 |   1.1619 |     38.572 |     1.9
   27 |   1.1150 |     37.793 |   1.1669 |     38.971 |     1.9
   28 |   1.0977 |     37.115 |   1.1595 |     38.174 |     2.0
   29 |   1.0839 |     36.638 |   1.1453 |     38.266 |     2.1
   30 |   1.0828 |     36.682 |   1.1444 |     38.419 |     2.2
   31 |   1.0681 |     36.319 |   1.1324 |     37.960 |     2.2
   32 |   1.0450 |     35.387 |   1.1389 |     37.898 |     2.3
   33 |   1.0303 |     35.203 |   1.1187 |     37.194 |     2.4
   34 |   1.0281 |     35.284 |   1.1082 |     36.765 |     2.4
   35 |   1.0208 |     34.682 |   1.1173 |     37.255 |     2.5
   36 |   1.0172 |     34.813 |   1.1172 |     37.040 |     2.5
   37 |   1.0051 |     34.303 |   1.0899 |     36.918 |     2.6
   38 |   0.9890 |     33.816 |   1.0986 |     36.918 |     2.6
   39 |   0.9764 |     33.203 |   1.0957 |     36.336 |     2.7
   40 |   0.9730 |     33.285 |   1.0967 |     36.734 |     2.8
   41 |   0.9562 |     32.559 |   1.0902 |     36.458 |     2.9
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 4
Decoder hidden units: 128
Decoder layers: 3
Dropout: 0.2
Trainable parameters: 735,842

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2373 |     85.967 |   2.7146 |     81.893 |     0.0
    2 |   2.4413 |     63.578 |   2.2046 |     58.824 |     0.1
    3 |   2.0835 |     56.345 |   1.9681 |     55.055 |     0.1
    4 |   1.9029 |     51.046 |   1.8219 |     48.376 |     0.2
    5 |   1.7758 |     48.943 |   1.7102 |     48.346 |     0.2
    6 |   1.6797 |     48.635 |   1.6196 |     48.315 |     0.3
    7 |   1.5951 |     48.472 |   1.5505 |     48.131 |     0.3
    8 |   1.5345 |     46.397 |   1.5014 |     45.404 |     0.4
    9 |   1.4926 |     46.099 |   1.4656 |     45.404 |     0.4
   10 |   1.4612 |     46.240 |   1.4395 |     45.343 |     0.4
   11 |   1.4358 |     45.882 |   1.4206 |     45.251 |     0.5
   12 |   1.4192 |     45.855 |   1.4064 |     45.343 |     0.5
   13 |   1.4005 |     45.611 |   1.3905 |     44.455 |     0.6
   14 |   1.3864 |     45.161 |   1.3770 |     45.404 |     0.6
   15 |   1.3726 |     45.297 |   1.3675 |     44.700 |     0.6
   16 |   1.3599 |     45.053 |   1.3556 |     44.945 |     0.7
   17 |   1.3485 |     44.755 |   1.3460 |     44.455 |     0.7
   18 |   1.3367 |     44.533 |   1.3323 |     44.179 |     0.8
   19 |   1.3226 |     44.360 |   1.3260 |     44.577 |     0.8
   20 |   1.3136 |     44.062 |   1.3162 |     44.424 |     0.8
   21 |   1.3033 |     43.818 |   1.3091 |     43.260 |     0.9
   22 |   1.2930 |     43.601 |   1.3035 |     43.168 |     0.9
   23 |   1.2853 |     43.298 |   1.2940 |     42.800 |     0.9
   24 |   1.2736 |     42.994 |   1.2884 |     42.678 |     1.0
   25 |   1.2629 |     42.631 |   1.2803 |     42.249 |     1.0
   26 |   1.2498 |     42.490 |   1.2800 |     42.953 |     1.0
   27 |   1.2415 |     42.539 |   1.2671 |     42.402 |     1.1
   28 |   1.2309 |     41.775 |   1.2596 |     41.881 |     1.1
   29 |   1.2227 |     42.003 |   1.2534 |     42.004 |     1.1
   30 |   1.2131 |     41.504 |   1.2447 |     41.667 |     1.2
   31 |   1.2016 |     41.157 |   1.2382 |     41.422 |     1.2
   32 |   1.1927 |     40.735 |   1.2328 |     41.268 |     1.2
   33 |   1.1853 |     40.328 |   1.2244 |     40.717 |     1.3
   34 |   1.1757 |     40.047 |   1.2184 |     40.165 |     1.3
   35 |   1.1689 |     39.651 |   1.2086 |     40.472 |     1.4
   36 |   1.1581 |     39.293 |   1.2045 |     40.257 |     1.4
   37 |   1.1466 |     38.930 |   1.1963 |     40.196 |     1.4
   38 |   1.1388 |     38.502 |   1.1905 |     39.798 |     1.5
   39 |   1.1264 |     38.112 |   1.1851 |     39.767 |     1.5
   40 |   1.1146 |     37.527 |   1.1771 |     39.491 |     1.5
   41 |   1.1047 |     37.381 |   1.1707 |     38.909 |     1.6
   42 |   1.1008 |     36.823 |   1.1621 |     38.695 |     1.6
   43 |   1.0883 |     36.173 |   1.1554 |     38.542 |     1.6
   44 |   1.0760 |     35.885 |   1.1428 |     37.316 |     1.7
   45 |   1.0637 |     35.495 |   1.1407 |     37.745 |     1.7
   46 |   1.0578 |     35.159 |   1.1295 |     37.592 |     1.7
   47 |   1.0420 |     34.943 |   1.1208 |     37.163 |     1.8
   48 |   1.0342 |     34.531 |   1.1184 |     37.224 |     1.8
   49 |   1.0214 |     34.059 |   1.1088 |     37.194 |     1.8
   50 |   1.0126 |     33.935 |   1.1010 |     36.489 |     1.9
   51 |   0.9998 |     33.241 |   1.0904 |     36.397 |     1.9
   52 |   0.9915 |     33.117 |   1.0894 |     35.784 |     1.9
   53 |   0.9818 |     32.732 |   1.1104 |     36.857 |     2.0
   54 |   0.9710 |     32.466 |   1.0770 |     35.539 |     2.0
   55 |   0.9576 |     32.179 |   1.0697 |     35.539 |     2.1
   56 |   0.9532 |     31.892 |   1.0660 |     35.080 |     2.1
   57 |   0.9512 |     31.589 |   1.0542 |     35.202 |     2.1
   58 |   0.9381 |     30.955 |   1.0626 |     34.957 |     2.1
   59 |   0.9314 |     31.009 |   1.0598 |     34.988 |     2.2
   60 |   0.9276 |     30.700 |   1.0516 |     35.202 |     2.2
   61 |   0.9191 |     30.424 |   1.0455 |     34.559 |     2.2
   62 |   0.9070 |     29.958 |   1.0401 |     34.559 |     2.3
   63 |   0.8992 |     29.551 |   1.0298 |     33.885 |     2.3
   64 |   0.8913 |     29.264 |   1.0456 |     34.743 |     2.3
   65 |   0.8859 |     29.064 |   1.0256 |     34.130 |     2.4
   66 |   0.8788 |     28.917 |   1.0365 |     33.885 |     2.4
   67 |   0.8699 |     28.614 |   1.0288 |     33.701 |     2.4
   68 |   0.8620 |     28.256 |   1.0169 |     33.272 |     2.5
   69 |   0.8564 |     28.267 |   1.0107 |     33.180 |     2.5
   70 |   0.8490 |     27.796 |   1.0125 |     32.812 |     2.5
   71 |   0.8411 |     27.276 |   1.0023 |     32.230 |     2.6
   72 |   0.8334 |     27.156 |   1.0073 |     32.384 |     2.6
   73 |   0.8258 |     27.200 |   1.0032 |     32.077 |     2.6
   74 |   0.8219 |     26.685 |   0.9961 |     32.016 |     2.7
   75 |   0.8190 |     26.783 |   0.9949 |     32.322 |     2.7
   76 |   0.8093 |     26.219 |   0.9900 |     32.077 |     2.7
   77 |   0.8042 |     26.371 |   0.9904 |     31.924 |     2.8
   78 |   0.7958 |     25.737 |   0.9869 |     31.771 |     2.8
   79 |   0.7921 |     25.840 |   0.9876 |     31.679 |     2.8
   80 |   0.7864 |     25.564 |   0.9809 |     31.342 |     2.9
   81 |   0.7867 |     25.639 |   0.9884 |     31.679 |     2.9
   82 |   0.7778 |     25.016 |   0.9832 |     31.219 |     2.9
   83 |   0.7773 |     25.195 |   0.9773 |     30.882 |     3.0
   84 |   0.7669 |     24.881 |   0.9773 |     30.821 |     3.0
   85 |   0.7583 |     24.713 |   0.9698 |     30.699 |     3.0
   86 |   0.7481 |     24.290 |   0.9895 |     30.515 |     3.1
   87 |   0.7505 |     24.615 |   0.9754 |     30.729 |     3.1
   88 |   0.7416 |     24.214 |   0.9723 |     30.086 |     3.2
   89 |   0.7367 |     23.900 |   0.9767 |     30.423 |     3.2
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 128
Encoder hidden units: 32
Encoder layers: 2
Decoder hidden units: 128
Decoder layers: 2
Dropout: 0.2
Trainable parameters: 455,138

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.2697 |     77.920 |   2.6214 |     68.842 |     0.0
    2 |   2.2344 |     57.472 |   1.9935 |     54.350 |     0.0
    3 |   1.8835 |     49.529 |   1.7787 |     48.039 |     0.1
    4 |   1.7137 |     48.385 |   1.6379 |     48.162 |     0.1
    5 |   1.5972 |     48.315 |   1.5447 |     48.192 |     0.1
    6 |   1.5184 |     47.508 |   1.4771 |     45.190 |     0.1
    7 |   1.4572 |     45.660 |   1.4278 |     44.945 |     0.2
    8 |   1.4111 |     45.156 |   1.3885 |     44.547 |     0.2
    9 |   1.3742 |     44.511 |   1.3594 |     43.597 |     0.2
   10 |   1.3465 |     44.045 |   1.3353 |     43.045 |     0.2
   11 |   1.3221 |     43.466 |   1.3129 |     42.494 |     0.2
   12 |   1.2993 |     42.962 |   1.2957 |     42.004 |     0.3
   13 |   1.2835 |     42.133 |   1.2827 |     41.513 |     0.3
   14 |   1.2659 |     41.575 |   1.2654 |     41.360 |     0.3
   15 |   1.2496 |     41.325 |   1.2511 |     40.686 |     0.3
   16 |   1.2338 |     40.686 |   1.2382 |     40.043 |     0.4
   17 |   1.2163 |     40.437 |   1.2311 |     40.380 |     0.4
   18 |   1.2058 |     40.036 |   1.2188 |     40.257 |     0.4
   19 |   1.1920 |     39.581 |   1.2051 |     39.400 |     0.4
   20 |   1.1792 |     39.234 |   1.1950 |     39.737 |     0.4
   21 |   1.1679 |     39.055 |   1.1828 |     38.909 |     0.5
   22 |   1.1548 |     38.589 |   1.1748 |     38.542 |     0.5
   23 |   1.1426 |     38.204 |   1.1670 |     38.542 |     0.5
   24 |   1.1295 |     37.706 |   1.1564 |     38.021 |     0.5
   25 |   1.1222 |     37.262 |   1.1448 |     37.960 |     0.5
   26 |   1.1101 |     36.893 |   1.1357 |     37.806 |     0.6
   27 |   1.0994 |     36.503 |   1.1268 |     36.949 |     0.6
   28 |   1.0880 |     35.707 |   1.1267 |     37.255 |     0.6
   29 |   1.0758 |     35.165 |   1.1166 |     36.826 |     0.6
   30 |   1.0628 |     34.715 |   1.1056 |     36.642 |     0.6
   31 |   1.0516 |     34.363 |   1.0976 |     35.110 |     0.7
   32 |   1.0449 |     33.946 |   1.0883 |     34.498 |     0.7
   33 |   1.0321 |     33.523 |   1.0764 |     34.651 |     0.7
   34 |   1.0237 |     33.247 |   1.0709 |     34.191 |     0.7
   35 |   1.0123 |     32.689 |   1.0616 |     34.191 |     0.8
   36 |   1.0017 |     32.320 |   1.0496 |     33.793 |     0.8
   37 |   0.9896 |     31.887 |   1.0515 |     34.436 |     0.8
   38 |   0.9799 |     31.675 |   1.0394 |     33.885 |     0.8
   39 |   0.9706 |     31.236 |   1.0310 |     33.517 |     0.8
   40 |   0.9607 |     30.906 |   1.0302 |     33.333 |     0.9
   41 |   0.9563 |     31.041 |   1.0157 |     33.272 |     0.9
   42 |   0.9464 |     30.570 |   1.0076 |     32.506 |     0.9
   43 |   0.9364 |     30.169 |   1.0101 |     33.088 |     0.9
   44 |   0.9312 |     29.947 |   1.0004 |     32.567 |     0.9
   45 |   0.9194 |     29.443 |   0.9989 |     32.353 |     1.0
   46 |   0.9138 |     29.248 |   0.9962 |     32.445 |     1.0
   47 |   0.9049 |     29.096 |   0.9930 |     32.230 |     1.0
   48 |   0.8973 |     28.879 |   0.9815 |     32.077 |     1.0
   49 |   0.8889 |     28.484 |   0.9736 |     31.158 |     1.0
   50 |   0.8781 |     28.446 |   0.9712 |     31.403 |     1.1
   51 |   0.8694 |     27.964 |   0.9677 |     31.526 |     1.1
   52 |   0.8634 |     27.682 |   0.9656 |     31.311 |     1.1
   53 |   0.8569 |     27.698 |   0.9559 |     30.913 |     1.1
   54 |   0.8477 |     27.086 |   0.9527 |     30.974 |     1.2
   55 |   0.8421 |     26.793 |   0.9520 |     30.882 |     1.2
   56 |   0.8344 |     26.669 |   0.9422 |     30.729 |     1.2
   57 |   0.8295 |     26.571 |   0.9405 |     30.453 |     1.2
   58 |   0.8226 |     26.154 |   0.9381 |     30.392 |     1.3
   59 |   0.8141 |     25.872 |   0.9336 |     29.963 |     1.3
   60 |   0.8067 |     25.455 |   0.9292 |     29.841 |     1.3
   61 |   0.7983 |     25.482 |   0.9269 |     29.994 |     1.3
   62 |   0.7937 |     25.461 |   0.9188 |     29.442 |     1.3
   63 |   0.7888 |     25.265 |   0.9204 |     29.841 |     1.4
   64 |   0.7832 |     25.092 |   0.9165 |     29.289 |     1.4
   65 |   0.7757 |     24.762 |   0.9193 |     29.534 |     1.4
   66 |   0.7719 |     24.285 |   0.9033 |     29.167 |     1.4
   67 |   0.7628 |     24.236 |   0.9023 |     28.860 |     1.4
   68 |   0.7548 |     24.106 |   0.9074 |     28.523 |     1.5
   69 |   0.7459 |     23.808 |   0.8984 |     28.860 |     1.5
   70 |   0.7485 |     23.683 |   0.8923 |     28.922 |     1.5
   71 |   0.7398 |     23.645 |   0.8971 |     29.105 |     1.5
   72 |   0.7294 |     23.234 |   0.8878 |     28.585 |     1.5
   73 |   0.7250 |     22.854 |   0.8863 |     28.278 |     1.6
   74 |   0.7182 |     22.746 |   0.8855 |     28.339 |     1.6
   75 |   0.7123 |     22.437 |   0.8920 |     28.462 |     1.6
   76 |   0.7035 |     22.161 |   0.8842 |     28.094 |     1.6
   77 |   0.7004 |     22.345 |   0.8816 |     28.309 |     1.7
   78 |   0.6948 |     21.950 |   0.8697 |     27.665 |     1.7
   79 |   0.6884 |     21.646 |   0.8833 |     28.309 |     1.7
   80 |   0.6860 |     21.581 |   0.8698 |     27.849 |     1.7
   81 |   0.6757 |     21.326 |   0.8726 |     27.727 |     1.8
   82 |   0.6706 |     21.186 |   0.8767 |     28.125 |     1.8
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 64
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 64
Encoder layers: 3
Decoder hidden units: 64
Decoder layers: 4
Dropout: 0.0
Trainable parameters: 710,050

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.001
Weight decay: 0.0001
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   2.5919 |     67.572 |   2.0530 |     58.824 |     0.0
    2 |   1.8170 |     51.008 |   1.6215 |     45.466 |     0.0
    3 |   1.5313 |     46.261 |   1.4662 |     45.466 |     0.1
    4 |   1.4543 |     46.196 |   1.4283 |     45.987 |     0.1
    5 |   1.4262 |     46.278 |   1.4122 |     45.987 |     0.1
    6 |   1.4133 |     46.321 |   1.4011 |     45.466 |     0.1
    7 |   1.4096 |     46.299 |   1.3954 |     45.466 |     0.2
    8 |   1.4015 |     46.256 |   1.3934 |     45.404 |     0.2
    9 |   1.3944 |     46.066 |   1.3875 |     45.895 |     0.2
   10 |   1.3850 |     46.034 |   1.3769 |     45.404 |     0.2
   11 |   1.3700 |     46.045 |   1.3546 |     45.006 |     0.3
   12 |   1.3367 |     45.535 |   1.3115 |     44.730 |     0.3
   13 |   1.2997 |     44.847 |   1.2891 |     44.240 |     0.3
   14 |   1.2850 |     44.430 |   1.2881 |     43.168 |     0.4
   15 |   1.2654 |     43.531 |   1.2674 |     43.045 |     0.4
   16 |   1.2494 |     42.880 |   1.2522 |     41.942 |     0.4
   17 |   1.2270 |     42.187 |   1.2269 |     40.993 |     0.5
   18 |   1.2083 |     41.358 |   1.2281 |     41.085 |     0.5
   19 |   1.1870 |     40.675 |   1.1992 |     41.299 |     0.5
   20 |   1.1825 |     40.670 |   1.1935 |     41.023 |     0.6
   21 |   1.1690 |     40.561 |   1.1862 |     40.165 |     0.6
   22 |   1.1469 |     39.879 |   1.1799 |     39.982 |     0.6
   23 |   1.1248 |     39.185 |   1.1631 |     39.032 |     0.7
   24 |   1.1181 |     38.627 |   1.1544 |     39.614 |     0.7
   25 |   1.1039 |     38.069 |   1.1249 |     38.388 |     0.7
   26 |   1.0885 |     37.673 |   1.1398 |     39.185 |     0.8
   27 |   1.0763 |     37.413 |   1.1193 |     37.623 |     0.8
   28 |   1.0663 |     36.259 |   1.1260 |     37.684 |     0.8
   29 |   1.0581 |     35.940 |   1.0957 |     37.347 |     0.9
   30 |   1.0369 |     35.349 |   1.0988 |     35.907 |     0.9
   31 |   1.0285 |     34.807 |   1.1020 |     36.305 |     0.9
   32 |   1.0490 |     35.235 |   1.0951 |     36.489 |     0.9
   33 |   1.0064 |     33.940 |   1.0745 |     36.029 |     1.0
   34 |   0.9919 |     33.160 |   1.0826 |     35.876 |     1.0
   35 |   0.9694 |     32.726 |   1.0795 |     36.121 |     1.0
   36 |   0.9719 |     32.716 |   1.0504 |     34.804 |     1.1
   37 |   0.9407 |     31.507 |   1.0449 |     34.222 |     1.1
   38 |   0.9372 |     31.307 |   1.0699 |     35.294 |     1.1
   39 |   0.9273 |     31.068 |   1.0480 |     35.141 |     1.1
   40 |   0.9328 |     31.404 |   1.0458 |     35.600 |     1.2
   41 |   0.9149 |     30.527 |   1.0355 |     35.080 |     1.2
   42 |   0.8887 |     29.931 |   1.0159 |     33.824 |     1.2
   43 |   0.8804 |     29.324 |   1.0520 |     35.049 |     1.2
   44 |   0.8853 |     29.410 |   1.0338 |     34.130 |     1.3
   45 |   0.8589 |     28.712 |   1.0252 |     34.161 |     1.3
   46 |   0.8454 |     28.294 |   1.0165 |     33.762 |     1.3
Early stopping

Model: Seq2Seq Multimodal Bi-LSTM
Source index: <Seq2Seq Index with 47 items>
Target index: <Seq2Seq Index with 34 items>
Encoder embedding dimension: 128
Descriptors dimension: 1136
Decoder embedding dimension: 64
Encoder hidden units: 128
Encoder layers: 2
Decoder hidden units: 32
Decoder layers: 4
Dropout: 0.2
Trainable parameters: 1,096,034

Training started
X_train.shape: torch.Size([3076, 702])
Y_train.shape: torch.Size([3076, 7])
X_dev.shape: torch.Size([544, 337])
Y_dev.shape: torch.Size([544, 7])
Epochs: 500
Learning rate: 0.0001
Weight decay: 1e-05
Epoch | Train                 | Development           | Minutes
      | Loss     | Error Rate | Loss     | Error Rate |
---------------------------------------------------------------
    1 |   3.5170 |     90.453 |   3.4574 |     83.333 |     0.6
    2 |   3.3060 |     83.333 |   3.1070 |     83.333 |     1.3
    3 |   2.9532 |     83.333 |   2.8375 |     83.333 |     2.2
    4 |   2.7543 |     82.927 |   2.6957 |     83.333 |     2.9
    5 |   2.6252 |     68.336 |   2.5727 |     66.912 |     4.8
    6 |   2.5161 |     62.224 |   2.4663 |     58.824 |     5.7
    7 |   2.4194 |     59.059 |   2.3743 |     58.824 |     6.4
    8 |   2.3384 |     58.095 |   2.2970 |     58.824 |     7.1
    9 |   2.2712 |     57.586 |   2.2333 |     58.824 |     7.8
   10 |   2.2148 |     57.190 |   2.1791 |     58.824 |     8.5
   11 |   2.1676 |     56.957 |   2.1334 |     58.824 |     9.1
   12 |   2.1219 |     56.805 |   2.0927 |     58.824 |    10.1
   13 |   2.0840 |     56.529 |   2.0551 |     58.793 |    10.9
   14 |   2.0496 |     56.117 |   2.0211 |     53.768 |    11.6
   15 |   2.0159 |     56.074 |   1.9888 |     53.768 |    12.2
   16 |   1.9861 |     55.781 |   1.9568 |     53.768 |    12.9
   17 |   1.9570 |     55.548 |   1.9279 |     53.768 |    13.6
   18 |   1.9268 |     55.174 |   1.9012 |     53.768 |    14.3
   19 |   1.9025 |     54.622 |   1.8738 |     53.768 |    14.9
   20 |   1.8743 |     53.782 |   1.8484 |     49.571 |    15.7
   21 |   1.8496 |     52.384 |   1.8215 |     48.346 |    16.4
   22 |   1.8230 |     50.786 |   1.7960 |     48.346 |    17.1
   23 |   1.7976 |     49.843 |   1.7722 |     48.346 |    17.9
   24 |   1.7728 |     49.339 |   1.7487 |     48.346 |    18.6
   25 |   1.7504 |     49.014 |   1.7272 |     48.346 |    19.4
   26 |   1.7324 |     49.057 |   1.7052 |     48.346 |    20.0
   27 |   1.7098 |     48.906 |   1.6854 |     48.346 |    20.7
   28 |   1.6900 |     48.835 |   1.6671 |     48.346 |    21.6
   29 |   1.6691 |     48.770 |   1.6499 |     48.346 |    22.3
   30 |   1.6544 |     48.429 |   1.6335 |     48.346 |    23.0
   31 |   1.6371 |     47.497 |   1.6179 |     45.466 |    23.7
   32 |   1.6226 |     46.679 |   1.6024 |     45.466 |    24.7
   33 |   1.6064 |     46.348 |   1.5883 |     45.466 |    25.3
   34 |   1.5941 |     46.256 |   1.5750 |     45.466 |    26.0
   35 |   1.5776 |     46.234 |   1.5626 |     45.466 |    26.9
   36 |   1.5659 |     46.229 |   1.5510 |     45.466 |    27.6
   37 |   1.5563 |     46.213 |   1.5406 |     45.466 |    28.5
   38 |   1.5458 |     46.223 |   1.5309 |     45.466 |    29.7
   39 |   1.5341 |     46.218 |   1.5219 |     45.466 |    30.3
   40 |   1.5269 |     46.213 |   1.5134 |     45.466 |    31.1
   41 |   1.5195 |     46.218 |   1.5054 |     45.466 |    31.9
   42 |   1.5096 |     46.202 |   1.4981 |     45.466 |    32.5
   43 |   1.5026 |     46.213 |   1.4913 |     45.466 |    33.0
   44 |   1.4951 |     46.202 |   1.4852 |     45.466 |    33.5
   45 |   1.4911 |     46.207 |   1.4797 |     45.466 |    33.5
   46 |   1.4869 |     46.207 |   1.4745 |     45.466 |    33.6
   47 |   1.4821 |     46.191 |   1.4700 |     45.466 |    33.6
   48 |   1.4783 |     46.256 |   1.4658 |     45.466 |    33.7
   49 |   1.4714 |     46.213 |   1.4620 |     45.466 |    33.7
   50 |   1.4694 |     46.218 |   1.4580 |     45.466 |    33.7
   51 |   1.4658 |     46.337 |   1.4545 |     45.466 |    33.8
   52 |   1.4614 |     46.294 |   1.4512 |     45.466 |    33.8
   53 |   1.4572 |     46.137 |   1.4481 |     45.466 |    33.9
   54 |   1.4549 |     46.397 |   1.4451 |     45.466 |    33.9
   55 |   1.4507 |     46.158 |   1.4423 |     45.466 |    34.0
   56 |   1.4483 |     46.158 |   1.4397 |     45.466 |    34.0
   57 |   1.4442 |     46.240 |   1.4373 |     45.466 |    34.1
   58 |   1.4411 |     46.245 |   1.4348 |     45.466 |    34.1
   59 |   1.4404 |     46.142 |   1.4326 |     45.466 |    34.1
   60 |   1.4406 |     46.240 |   1.4306 |     45.466 |    34.2
   61 |   1.4337 |     46.288 |   1.4286 |     45.466 |    34.2
   62 |   1.4326 |     46.348 |   1.4269 |     45.466 |    34.3
   63 |   1.4327 |     46.055 |   1.4251 |     45.466 |    34.3
   64 |   1.4292 |     46.196 |   1.4234 |     45.466 |    34.4
   65 |   1.4269 |     46.202 |   1.4216 |     45.466 |    34.4
   66 |   1.4275 |     46.218 |   1.4202 |     45.466 |    34.5
   67 |   1.4267 |     46.196 |   1.4188 |     45.466 |    34.5
   68 |   1.4261 |     46.121 |   1.4174 |     45.466 |    34.5
   69 |   1.4243 |     46.272 |   1.4163 |     45.466 |    34.6
   70 |   1.4240 |     46.245 |   1.4150 |     45.466 |    34.6
   71 |   1.4200 |     46.272 |   1.4139 |     45.466 |    34.7
