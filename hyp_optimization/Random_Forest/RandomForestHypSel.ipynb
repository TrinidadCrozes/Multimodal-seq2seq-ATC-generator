{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a48897e-d235-4413-81d0-7b77eee3e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "from seq2seq import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39207352-1c15-41db-8945-782ec4d90ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "# Convert a string that simulates a list to a real list\n",
    "def convert_string_list(element):\n",
    "    # Delete [] of the string\n",
    "    element = element[1:len(element)-1]\n",
    "    # Create a list that contains each code as e.g. 'A'\n",
    "    ATC_list = list(element.split(', '))\n",
    "    for index, code in enumerate(ATC_list):\n",
    "        # Delete '' of the code\n",
    "        ATC_list[index] = code[1:len(code)-1]\n",
    "    return ATC_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b861b1-d4f9-4899-9249-e1d82b6d5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 78\n",
    "set_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4cc27c-f570-4897-ba94-6703df1e56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_grid = { \n",
    "    'n_estimators': [50, 100, 500],\n",
    "    'max_depth': [None, 20, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b35f23b-c61b-4e0c-a645-3ae02b5e26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_1(seed):\n",
    "    train_set = pd.read_csv(f'../../Data/train_set.csv')\n",
    "    val_set = pd.read_csv(f'../../Data/val_set.csv')\n",
    "    # test_set = pd.read_csv(f'../../Data/test_set.csv')\n",
    "    # train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns from train set\n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns from test set\n",
    "    val_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    val_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = val_set.reset_index(drop=True)\n",
    "    # Divide in X and y\n",
    "    X_train = train_set.drop('ATC_level1', axis = 1)\n",
    "    y_train = train_set['ATC_level1']\n",
    "    X_test = test_set.drop('ATC_level1', axis = 1)\n",
    "    y_test = test_set['ATC_level1']\n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_2(seed):\n",
    "    train_set = pd.read_csv(f'../../Data/train_set.csv')\n",
    "    val_set = pd.read_csv(f'../../Data/val_set.csv')\n",
    "    # test_set = pd.read_csv(f'../../Data/test_set.csv')\n",
    "    # train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    val_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level3', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    val_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = val_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC level 1 code\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level1_list = convert_string_list(row['ATC_level1'])\n",
    "        for code in ATC_level1_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level1'] = code\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level2_list = convert_string_list(row['ATC_level2'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level2_list:\n",
    "            if code[0] == row['ATC_level1']:\n",
    "                new_row = row.copy()\n",
    "                new_row['ATC_level2'] = code[1:len(code)]\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "    \n",
    "    new_test_set2 = test_set\n",
    "    \n",
    "    X_train = new_train_set2.drop('ATC_level2', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level2']\n",
    "    X_test = new_test_set2.drop('ATC_level2', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level2']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_3(seed):\n",
    "    train_set = pd.read_csv(f'../../Data/train_set.csv')\n",
    "    val_set = pd.read_csv(f'../../Data/val_set.csv')\n",
    "    # test_set = pd.read_csv(f'../../Data/test_set.csv')\n",
    "    # train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    val_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level4', axis = 1, inplace = True)\n",
    "    val_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = val_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC code\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level2_list = convert_string_list(row['ATC_level2'])\n",
    "        for code in ATC_level2_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level2'] = code\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level3_list = convert_string_list(row['ATC_level3'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level3_list:\n",
    "            if code[0:3] == row['ATC_level2']:\n",
    "                new_row = row.copy()\n",
    "                new_row['ATC_level3'] = code[3:len(code)]\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "\n",
    "    new_test_set2 = test_set\n",
    "\n",
    "    X_train = new_train_set2.drop('ATC_level3', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level3']\n",
    "    X_test = new_test_set2.drop('ATC_level3', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level3']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "def train_test_4(seed):\n",
    "    train_set = pd.read_csv(f'../../Data/train_set.csv')\n",
    "    val_set = pd.read_csv(f'../../Data/val_set.csv')\n",
    "    # test_set = pd.read_csv(f'../../Data/test_set.csv')\n",
    "    # train_set = pd.concat([train_set, val_set], ignore_index=True)\n",
    "    # Delete unnecessary columns \n",
    "    train_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    train_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    train_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    # Delete unnecessary columns \n",
    "    val_set.drop('Neutralized SMILES', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC Codes', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level1', axis = 1, inplace = True)\n",
    "    val_set.drop('ATC_level2', axis = 1, inplace = True)\n",
    "    val_set.drop('multiple_ATC', axis = 1, inplace = True)\n",
    "    test_set = val_set.reset_index(drop=True)\n",
    "    # Replicate compounds that have more than 1 ATC code\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in train_set.iterrows():\n",
    "        ATC_level3_list = convert_string_list(row['ATC_level3'])\n",
    "        for code in ATC_level3_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['ATC_level3'] = code\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    new_train_set = pd.DataFrame(new_rows)\n",
    "    new_train_set = new_train_set.reset_index(drop=True)\n",
    "    # Delete level 1 letter from ATC_level2\n",
    "    new_rows = [] \n",
    "    for _, row in new_train_set.iterrows():\n",
    "        ATC_level4_list = convert_string_list(row['ATC_level4'])\n",
    "        # Split ATC code if they have more than 1 code at level 2\n",
    "        for code in ATC_level4_list:\n",
    "            if code[0:4] == row['ATC_level3']:\n",
    "                if code[4:len(code)] != '':\n",
    "                    new_row = row.copy()\n",
    "                    new_row['ATC_level4'] = code[4:len(code)]\n",
    "                    new_rows.append(new_row)\n",
    "\n",
    "    new_train_set2 = pd.DataFrame(new_rows)\n",
    "    new_train_set2 = new_train_set2.reset_index(drop=True)\n",
    "\n",
    "    new_test_set2 = test_set\n",
    "\n",
    "    X_train = new_train_set2.drop('ATC_level4', axis = 1)\n",
    "    y_train = new_train_set2['ATC_level4']\n",
    "    X_test = new_test_set2.drop('ATC_level4', axis = 1)\n",
    "    y_test = new_test_set2['ATC_level4']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c211c8c-12f4-4ece-9a32-d363acd2efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_level1(X_test):\n",
    "    X_test = np.asarray(X_test).astype(np.float32)\n",
    "    X_test[pd.isna(X_test)] = np.nanmedian(X_test)\n",
    "    X_test = scaler1.transform(X_test)\n",
    "    return X_test\n",
    "def test_set_level2(X_test1, pred_df_level1):\n",
    "    X_test1.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "    X_test1['ATC_level1'] = pred_df_level1['pred_1']\n",
    "    ATC_level1 = X_test1['ATC_level1']\n",
    "    categorical_atc = atc_level1_labels_encoder2.transform(ATC_level1)\n",
    "    df_level1 = pd.DataFrame(categorical_atc, columns=atc_level1_labels2)\n",
    "    X_test1 = pd.concat([X_test1, df_level1], axis = 1)\n",
    "    X_test1.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "\n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler2.transform(X_test1)\n",
    "\n",
    "    return X_test1\n",
    "def test_set_level3(X_test1, pred_df_level1, pred_df_level2):\n",
    "    predicted_codes = []\n",
    "    \n",
    "    for index, pred1 in enumerate(pred_df_level1['pred_1']):\n",
    "        pred2 = str(pred_df_level2.at[pred_df_level2.index[index], 'pred_2']).zfill(2)\n",
    "        prediction = pred1 + '' + pred2\n",
    "        predicted_codes.append(prediction)\n",
    "        \n",
    "    X_test1['ATC_level2'] = predicted_codes\n",
    "    \n",
    "    ATC_level22 = X_test1['ATC_level2']\n",
    "    ATC_level1_3 = ATC_level22.copy()\n",
    "    ATC_level2_3 = ATC_level22.copy()\n",
    "    for index, atc in enumerate(ATC_level22):\n",
    "        ATC_level1_3[index] = []\n",
    "        ATC_level1_3[index].append(atc[0:1])\n",
    "        ATC_level2_3[index] = []\n",
    "        ATC_level2_3[index].append(atc[1:3])\n",
    "\n",
    "    X_test1 = X_test1.drop(labels=['ATC_level2'], axis=\"columns\")\n",
    "    \n",
    "    categorical_atc1_3 = atc_level1_labels_encoder3.transform(ATC_level1_3)\n",
    "    categorical_atc2_3 = atc_level2_labels_encoder3.transform(ATC_level2_3)\n",
    "    df_level1_3 = pd.DataFrame(categorical_atc1_3, columns=atc_level1_labels3)\n",
    "    df_level2_3 = pd.DataFrame(categorical_atc2_3, columns=atc_level2_labels3)\n",
    "    X_test1 = pd.concat([X_test1, df_level1_3, df_level2_3], axis = 1)                         \n",
    "    \n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler3.transform(X_test1)\n",
    "\n",
    "    return X_test1\n",
    "def test_set_level4(X_test1, pred_df_level1, pred_df_level2, pred_df_level3):\n",
    "    predicted_codes = []\n",
    "    \n",
    "    for index, pred1 in enumerate(pred_df_level1['pred_1']):\n",
    "        pred2 = str(pred_df_level2.at[pred_df_level2.index[index], 'pred_2']).zfill(2)\n",
    "        pred3 = pred_df_level3.at[pred_df_level3.index[index], 'pred_3']\n",
    "        prediction = pred1 + '' + pred2 + '' + pred3\n",
    "        predicted_codes.append(prediction)\n",
    "        \n",
    "    X_test1['ATC_level3'] = predicted_codes\n",
    "    \n",
    "    ATC_level33 = X_test1['ATC_level3']\n",
    "    ATC_level1_4 = ATC_level33.copy()\n",
    "    ATC_level2_4 = ATC_level33.copy()\n",
    "    ATC_level3_4 = ATC_level33.copy()\n",
    "    for index, atc in enumerate(ATC_level33):\n",
    "        ATC_level1_4[index] = []\n",
    "        ATC_level1_4[index].append(atc[0:1])\n",
    "        ATC_level2_4[index] = []\n",
    "        ATC_level2_4[index].append(atc[1:3])\n",
    "        ATC_level3_4[index] = []\n",
    "        ATC_level3_4[index].append(atc[3:4])\n",
    "\n",
    "    X_test1 = X_test1.drop(labels=['ATC_level3'], axis=\"columns\")\n",
    "\n",
    "    categorical_atc1_4 = atc_level1_labels_encoder4.transform(ATC_level1_4)\n",
    "    categorical_atc2_4 = atc_level2_labels_encoder4.transform(ATC_level2_4)\n",
    "    categorical_atc3_4 = atc_level3_labels_encoder4.transform(ATC_level3_4)\n",
    "    df_level1_4 = pd.DataFrame(categorical_atc1_4, columns=atc_level1_labels4)\n",
    "    df_level2_4 = pd.DataFrame(categorical_atc2_4, columns=atc_level2_labels4)\n",
    "    df_level3_4 = pd.DataFrame(categorical_atc3_4, columns=atc_level3_labels4)\n",
    "    X_test1 = pd.concat([X_test1, df_level1_4, df_level2_4, df_level3_4], axis = 1)                         \n",
    "   \n",
    "    X_test1 = np.asarray(X_test1).astype(np.float32)\n",
    "    X_test1[pd.isna(X_test1)] = np.nanmedian(X_test1)\n",
    "    X_test1 = scaler4.transform(X_test1)\n",
    "\n",
    "    return X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fecbef18-315d-4494-8f79-2dd7d8f2bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, previous_predictions = None, index = None):\n",
    "    \"\"\"Genera predicciones para todos los niveles, o solo para un índice si se repite\"\"\"\n",
    "    \n",
    "    def sample_prediction(model, X_test, encoder, previous_predictions = None, index = None):\n",
    "        \"\"\"Calcula la predicción con probabilidad ponderada para un índice o todos\"\"\"\n",
    "        if previous_predictions is None:\n",
    "            # Primera vez: calcular todas las predicciones\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            y_prob_matrix = np.array(y_prob)[:, :, 1].T\n",
    "            predictions = [\n",
    "                random.choices(encoder.classes_, weights=row, k=1)[0] for row in y_prob_matrix\n",
    "            ]\n",
    "        else:\n",
    "            # Mantener las predicciones anteriores y cambiar solo la del índice dado\n",
    "            predictions = previous_predictions.copy()\n",
    "            if index is not None:\n",
    "                y_prob = model.predict_proba(X_test)\n",
    "                row = np.array(y_prob)[:, :, 1].T[index]\n",
    "                predictions[index] = random.choices(encoder.classes_, weights=row, k=1)[0]\n",
    "    \n",
    "        return predictions\n",
    "    \n",
    "    # Nivel 1\n",
    "    X_test1 = test_set_level1(X_test1)\n",
    "    pred_1 = sample_prediction(model1, X_test1, mlb, previous_predictions['pred_1'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level1 = pd.DataFrame(pred_1, columns=['pred_1'])\n",
    "\n",
    "    # Nivel 2\n",
    "    X_test2 = test_set_level2(X_test2, pred_df_level1)\n",
    "    pred_2 = sample_prediction(model2, X_test2, y_labels_encoder2, previous_predictions['pred_2'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level2 = pd.DataFrame(pred_2, columns=['pred_2'])\n",
    "\n",
    "    # Nivel 3\n",
    "    X_test3 = test_set_level3(X_test3, pred_df_level1, pred_df_level2)\n",
    "    pred_3 = sample_prediction(model3, X_test3, y_labels_encoder3, previous_predictions['pred_3'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level3 = pd.DataFrame(pred_3, columns=['pred_3'])\n",
    "\n",
    "    # Nivel 4\n",
    "    X_test4 = test_set_level4(X_test4, pred_df_level1, pred_df_level2, pred_df_level3)\n",
    "    pred_4 = sample_prediction(model4, X_test4, y_labels_encoder4, previous_predictions['pred_4'] if previous_predictions is not None else None, index)\n",
    "    pred_df_level4 = pd.DataFrame(pred_4, columns=['pred_4'])\n",
    "\n",
    "    return pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc79abb-f0ed-4ceb-9923-c25f1d0d7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "def random_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4):\n",
    "    final_predictions = [[] for _ in range(len(X_test1))]\n",
    "    max_attempts = 30\n",
    "    for i in range(3):\n",
    "        pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4 = generate_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4)\n",
    "        for index in range(len(pred_df_level1)):\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                pred1 = pred_df_level1.at[index, 'pred_1']\n",
    "                pred2 = str(pred_df_level2.at[index, \"pred_2\"]).zfill(2)\n",
    "                pred3 = pred_df_level3.at[index, \"pred_3\"]\n",
    "                pred4 = pred_df_level4.at[index, \"pred_4\"]\n",
    "                prediction = pred1 + pred2 + pred3 + pred4\n",
    "                \n",
    "                if prediction not in final_predictions[index]:\n",
    "                    final_predictions[index].append(prediction)\n",
    "                    break  # Salimos del bucle cuando obtenemos una predicción nueva\n",
    "\n",
    "                # print(f\"{attempts}- Prediction {prediction} already found in {final_predictions[index]}, reclassifying index {index}...\")\n",
    "                previous_predictions = pd.concat([pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4], axis = 1)\n",
    "                # Recalcular la clasificación completa solo para este índice\n",
    "                pred_df_level1, pred_df_level2, pred_df_level3, pred_df_level4 = generate_predictions(model1, X_test1, model2, X_test2, model3, X_test3, model4, X_test4, previous_predictions, index)\n",
    "                attempts += 1\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c8bd25e-f238-4c60-880f-27ec045e045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "combinations = list(itertools.product(*hyperparameters_grid.values()))\n",
    "if len(combinations)>200:\n",
    "    max_evals = 200\n",
    "else:\n",
    "    max_evals = len(combinations)\n",
    "tested_params = set()\n",
    "df_tests = pd.DataFrame(columns = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'class_weight', 'Precision nivel1', 'Precision nivel2', 'Precision nivel3', 'Precision nivel4', 'Recall nivel1', 'Recall nivel2', 'Recall nivel3', 'Recall nivel4', 'Drugs that have at least one match'], index = list(range(max_evals)))\n",
    "sys.stdout = open('log.txt', 'w')\n",
    "for comb in range(max_evals):\n",
    "    while True:\n",
    "        random_params = {k: random.sample(v, 1)[0] for k, v in hyperparameters_grid.items()}\n",
    "        params_tuple = tuple(random_params.values())\n",
    "        if params_tuple not in tested_params:\n",
    "            tested_params.add(params_tuple)\n",
    "            break   \n",
    "    X_train1, y_train1, X_test1, y_test1 = train_test_1(seed)\n",
    "    X_train2, y_train2, X_test2, y_test2 = train_test_2(seed)\n",
    "    X_train3, y_train3, X_test3, y_test3 = train_test_3(seed)\n",
    "    X_train4, y_train4, X_test4, y_test4 = train_test_4(seed)\n",
    "    # LEVEL1\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels1 = set()\n",
    "    for lista in y_train1:\n",
    "        lista = convert_string_list(lista)\n",
    "        for code in lista:\n",
    "            labels1.add(code)\n",
    "    for lista in y_test1:\n",
    "        lista = convert_string_list(lista)\n",
    "        for code in lista:\n",
    "            labels1.add(code)\n",
    "            \n",
    "    labels1 = sorted(list(labels1))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit([labels1])\n",
    "    y_new = y_train1.copy()\n",
    "    for index, lista in enumerate(y_train1):\n",
    "        y_new[index] = []\n",
    "        lista = convert_string_list(lista)\n",
    "        for i, label in enumerate(lista):\n",
    "            y_new[index].append(lista[i])\n",
    "    y_categorical1 = mlb.transform(y_new)\n",
    "    \n",
    "    X_train1 = np.asarray(X_train1).astype(np.float32)\n",
    "    y_categorical1 = np.asarray(y_categorical1).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train1[pd.isna(X_train1)] = np.nanmedian(X_train1)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler1 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train1 = scaler1.fit_transform(X_train1)\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=random_params['n_estimators'], max_depth=random_params['max_depth'], min_samples_split=random_params['min_samples_split'], min_samples_leaf=random_params['min_samples_leaf'], class_weight = random_params['class_weight'], random_state=seed)\n",
    "    # Train the model\n",
    "    rf1.fit(X_train1, y_categorical1)\n",
    "    \n",
    "    #LEVEL 2\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels2 = set()\n",
    "    for code in y_train2:\n",
    "        labels2.add(code)\n",
    "            \n",
    "    labels2 = sorted(list(labels2))\n",
    "    y_labels_encoder2 = MultiLabelBinarizer()\n",
    "    y_labels_encoder2.fit([labels2])\n",
    "    encoded_y_train2 = y_labels_encoder2.transform(y_train2.values.reshape(-1, 1))\n",
    "    atc_level1_labels2 = set()\n",
    "    for _, row in X_train2.iterrows():\n",
    "        atc_level1_labels2.add(row['ATC_level1'])\n",
    "            \n",
    "    atc_level1_labels2 = sorted(list(atc_level1_labels2))\n",
    "    atc_level1_labels_encoder2 = MultiLabelBinarizer()\n",
    "    atc_level1_labels_encoder2.fit([atc_level1_labels2])\n",
    "    ATC_level11 = X_train2['ATC_level1']\n",
    "    ATC_level1_2 = ATC_level11.copy()\n",
    "    for index, lista in enumerate(ATC_level11):\n",
    "        ATC_level1_2[index] = []\n",
    "        ATC_level1_2[index].append(lista)\n",
    "    categorical_atc2 = atc_level1_labels_encoder2.transform(ATC_level1_2)\n",
    "    X_train2.drop(labels=['ATC_level1'], axis=\"columns\", inplace=True)\n",
    "    df_level1_2 = pd.DataFrame(categorical_atc2, columns=atc_level1_labels2)\n",
    "    X_train2 = pd.concat([X_train2, df_level1_2], axis = 1)\n",
    "    X_train2 = np.asarray(X_train2).astype(np.float32)\n",
    "    encoded_y_train2 = np.asarray(encoded_y_train2).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train2[pd.isna(X_train2)] = np.nanmedian(X_train2)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler2 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train2 = scaler2.fit_transform(X_train2)\n",
    "    \n",
    "    rf2 = RandomForestClassifier(n_estimators=random_params['n_estimators'], max_depth=random_params['max_depth'], min_samples_split=random_params['min_samples_split'], min_samples_leaf=random_params['min_samples_leaf'], class_weight = random_params['class_weight'], random_state=seed)\n",
    "    # Train the model\n",
    "    rf2.fit(X_train2, encoded_y_train2)\n",
    "    \n",
    "    #LEVEL 3\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels3 = set()\n",
    "    for code in y_train3:\n",
    "        labels3.add(code)\n",
    "            \n",
    "    labels3 = sorted(list(labels3))\n",
    "    y_labels_encoder3 = MultiLabelBinarizer()\n",
    "    y_labels_encoder3.fit([labels3])\n",
    "    encoded_y_train3 = y_labels_encoder3.transform(y_train3.values.reshape(-1, 1))\n",
    "    \n",
    "    atc_level1_labels3 = set()\n",
    "    atc_level2_labels3 = set()\n",
    "    for _, row in X_train3.iterrows():\n",
    "        atc_level1_labels3.add(row['ATC_level2'][0:1])\n",
    "        atc_level2_labels3.add(row['ATC_level2'][1:3])\n",
    "    for _, row in X_test3.iterrows():\n",
    "        lista = convert_string_list(row['ATC_level2'])\n",
    "        for code in lista:\n",
    "            atc_level1_labels3.add(code[0:1])\n",
    "            atc_level2_labels3.add(code[1:3])\n",
    "            \n",
    "    atc_level1_labels3 = sorted(list(atc_level1_labels3))\n",
    "    atc_level2_labels3 = sorted(list(atc_level2_labels3))\n",
    "    atc_level1_labels_encoder3 = MultiLabelBinarizer()\n",
    "    atc_level1_labels_encoder3.fit([atc_level1_labels3])\n",
    "    atc_level2_labels_encoder3 = MultiLabelBinarizer()\n",
    "    atc_level2_labels_encoder3.fit([atc_level2_labels3])\n",
    "    ATC_level22 = X_train3['ATC_level2']\n",
    "    ATC_level1_3 = ATC_level22.copy()\n",
    "    ATC_level2_3 = ATC_level22.copy()\n",
    "    for index, lista in enumerate(ATC_level22):\n",
    "        ATC_level1_3[index] = []\n",
    "        ATC_level1_3[index].append(lista[0:1])\n",
    "    for index, lista in enumerate(ATC_level22):\n",
    "        ATC_level2_3[index] = []\n",
    "        ATC_level2_3[index].append(lista[1:3])\n",
    "    X_train3.drop(labels=['ATC_level2'], axis=\"columns\", inplace=True)\n",
    "    categorical_atc1_3 = atc_level1_labels_encoder3.transform(ATC_level1_3)\n",
    "    categorical_atc2_3 = atc_level2_labels_encoder3.transform(ATC_level2_3)\n",
    "    df_level1_3 = pd.DataFrame(categorical_atc1_3, columns=atc_level1_labels3)\n",
    "    df_level2_3 = pd.DataFrame(categorical_atc2_3, columns=atc_level2_labels3)\n",
    "    X_train3 = pd.concat([X_train3, df_level1_3, df_level2_3], axis = 1)\n",
    "    X_train3 = np.asarray(X_train3).astype(np.float32)\n",
    "    encoded_y_train3 = np.asarray(encoded_y_train3).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train3[pd.isna(X_train3)] = np.nanmedian(X_train3)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler3 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train3 = scaler3.fit_transform(X_train3)\n",
    "    \n",
    "    rf3 = RandomForestClassifier(n_estimators=random_params['n_estimators'], max_depth=random_params['max_depth'], min_samples_split=random_params['min_samples_split'], min_samples_leaf=random_params['min_samples_leaf'], class_weight = random_params['class_weight'], random_state=seed)\n",
    "    # Train the model\n",
    "    rf3.fit(X_train3, encoded_y_train3)\n",
    "    #LEVEL 4\n",
    "    # Get all available labels describing the level 1 ATC code\n",
    "    labels4 = set()\n",
    "    for code in y_train4:\n",
    "        labels4.add(code)\n",
    "            \n",
    "    labels4 = sorted(list(labels4))\n",
    "    y_labels_encoder4 = MultiLabelBinarizer()\n",
    "    y_labels_encoder4.fit([labels4])\n",
    "    encoded_y_train4 = y_labels_encoder4.transform(y_train4.values.reshape(-1, 1))\n",
    "    \n",
    "    atc_level1_labels4 = set()\n",
    "    atc_level2_labels4 = set()\n",
    "    atc_level3_labels4 = set()\n",
    "    for _, row in X_train4.iterrows():\n",
    "        atc_level1_labels4.add(row['ATC_level3'][0:1])\n",
    "        atc_level2_labels4.add(row['ATC_level3'][1:3])\n",
    "        atc_level3_labels4.add(row['ATC_level3'][3:4])\n",
    "    for _, row in X_test4.iterrows():\n",
    "        lista = convert_string_list(row['ATC_level3'])\n",
    "        for code in lista:\n",
    "            atc_level1_labels4.add(code[0:1])\n",
    "            atc_level2_labels4.add(code[1:3])\n",
    "            atc_level3_labels4.add(code[3:4])\n",
    "    \n",
    "    atc_level1_labels4 = sorted(list(atc_level1_labels4))\n",
    "    atc_level2_labels4 = sorted(list(atc_level2_labels4))\n",
    "    atc_level3_labels4 = sorted(list(atc_level3_labels4))\n",
    "    atc_level1_labels_encoder4 = MultiLabelBinarizer()\n",
    "    atc_level1_labels_encoder4.fit([atc_level1_labels4])\n",
    "    atc_level2_labels_encoder4 = MultiLabelBinarizer()\n",
    "    atc_level2_labels_encoder4.fit([atc_level2_labels4])\n",
    "    atc_level3_labels_encoder4 = MultiLabelBinarizer()\n",
    "    atc_level3_labels_encoder4.fit([atc_level3_labels4])\n",
    "    ATC_level33 = X_train4['ATC_level3']\n",
    "    ATC_level1_4 = ATC_level33.copy()\n",
    "    for index, lista in enumerate(ATC_level33):\n",
    "        ATC_level1_4[index] = []\n",
    "        ATC_level1_4[index].append(lista[0:1])\n",
    "    ATC_level2_4 = ATC_level33.copy()\n",
    "    for index, lista in enumerate(ATC_level33):\n",
    "        ATC_level2_4[index] = []\n",
    "        ATC_level2_4[index].append(lista[1:3])\n",
    "    ATC_level3_4 = ATC_level33.copy()\n",
    "    for index, lista in enumerate(ATC_level33):\n",
    "        ATC_level3_4[index] = []\n",
    "        ATC_level3_4[index].append(lista[3:4])\n",
    "    X_train4.drop(labels=['ATC_level3'], axis=\"columns\", inplace=True)\n",
    "    categorical_atc1_4 = atc_level1_labels_encoder4.transform(ATC_level1_4)\n",
    "    categorical_atc2_4 = atc_level2_labels_encoder4.transform(ATC_level2_4)\n",
    "    categorical_atc3_4 = atc_level3_labels_encoder4.transform(ATC_level3_4)\n",
    "    df_level1_4 = pd.DataFrame(categorical_atc1_4, columns=atc_level1_labels4)\n",
    "    df_level2_4 = pd.DataFrame(categorical_atc2_4, columns=atc_level2_labels4)\n",
    "    df_level3_4 = pd.DataFrame(categorical_atc3_4, columns=atc_level3_labels4)\n",
    "    X_train4 = pd.concat([X_train4, df_level1_4, df_level2_4, df_level3_4], axis = 1)\n",
    "    X_train4 = np.asarray(X_train4).astype(np.float32)\n",
    "    encoded_y_train4 = np.asarray(encoded_y_train4).astype(np.float32)\n",
    "    # Complete NaN values in each column with the median\n",
    "    X_train4[pd.isna(X_train4)] = np.nanmedian(X_train4)\n",
    "    # Define an instance of the MinMaxScaler\n",
    "    scaler4 = MinMaxScaler()\n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_train4 = scaler4.fit_transform(X_train4)\n",
    "    \n",
    "    rf4 = RandomForestClassifier(n_estimators=random_params['n_estimators'], max_depth=random_params['max_depth'], min_samples_split=random_params['min_samples_split'], min_samples_leaf=random_params['min_samples_leaf'], class_weight = random_params['class_weight'], random_state=seed)\n",
    "\n",
    "    rf4.fit(X_train4, encoded_y_train4)\n",
    "    \n",
    "    #TEST\n",
    "    X_test3.drop(labels=['ATC_level2'], axis=\"columns\", inplace=True)\n",
    "    X_test4.drop(labels=['ATC_level3'], axis=\"columns\", inplace = True)\n",
    "\n",
    "    output = random_predictions(rf1, X_test1, rf2, X_test2, rf3, X_test3, rf4, X_test4)\n",
    "    predictions = []\n",
    "    for preds in output:\n",
    "        interm = []\n",
    "        for pred in preds:\n",
    "            clean_pred = pred.replace('<START>', '').replace('<END>', '')\n",
    "            if len(clean_pred) == 5:\n",
    "                interm.append(clean_pred)\n",
    "        predictions.append(interm)\n",
    "            \n",
    "    precision_1, precision_2, precision_3, precision_4 = defined_metrics.precision(predictions, \"../../Data/val_set.csv\", 'ATC Codes')\n",
    "    recall_1, recall_2, recall_3, recall_4, comp = defined_metrics.recall(predictions, \"../../Data/val_set.csv\", 'ATC Codes')\n",
    "    df_tests.iloc[comb, :] = [f\"{random_params['n_estimators']}\", f\"{random_params['max_depth']}\", f\"{random_params['min_samples_split']}\", f\"{random_params['min_samples_leaf']}\", f\"{random_params['class_weight']}\", f\"{precision_1}\", f\"{precision_2}\", f\"{precision_3}\", f\"{precision_4}\", f\"{recall_1}\", f\"{recall_2}\", f\"{recall_3}\", f\"{recall_4}\", f\"{comp}\"]\n",
    "    df_tests.to_csv(\"randomforest_results.csv\", index = False)\n",
    "sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "533b08b8-97a6-41c9-9351-b3ba995727fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>Precision nivel1</th>\n",
       "      <th>Precision nivel2</th>\n",
       "      <th>Precision nivel3</th>\n",
       "      <th>Precision nivel4</th>\n",
       "      <th>Recall nivel1</th>\n",
       "      <th>Recall nivel2</th>\n",
       "      <th>Recall nivel3</th>\n",
       "      <th>Recall nivel4</th>\n",
       "      <th>Drugs that have at least one match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.20327304048234301</td>\n",
       "      <td>0.37058823529411755</td>\n",
       "      <td>0.3917748917748918</td>\n",
       "      <td>0.2222222222222222</td>\n",
       "      <td>0.40766580534022395</td>\n",
       "      <td>0.4211111111111111</td>\n",
       "      <td>0.42207792207792205</td>\n",
       "      <td>0.2222222222222222</td>\n",
       "      <td>[170, 77, 36, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>500</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.21533161068044765</td>\n",
       "      <td>0.40037243947858475</td>\n",
       "      <td>0.39837398373983735</td>\n",
       "      <td>0.3108108108108108</td>\n",
       "      <td>0.424978466838932</td>\n",
       "      <td>0.42737430167597756</td>\n",
       "      <td>0.4146341463414634</td>\n",
       "      <td>0.35135135135135137</td>\n",
       "      <td>[179, 82, 37, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.22049956933677853</td>\n",
       "      <td>0.42602495543672014</td>\n",
       "      <td>0.46099290780141844</td>\n",
       "      <td>0.26595744680851063</td>\n",
       "      <td>0.43996554694229123</td>\n",
       "      <td>0.4746286393345217</td>\n",
       "      <td>0.4485815602836879</td>\n",
       "      <td>0.2872340425531915</td>\n",
       "      <td>[187, 94, 47, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.22049956933677858</td>\n",
       "      <td>0.393854748603352</td>\n",
       "      <td>0.5325670498084292</td>\n",
       "      <td>0.20588235294117646</td>\n",
       "      <td>0.42693080677576795</td>\n",
       "      <td>0.44674115456238356</td>\n",
       "      <td>0.5680076628352491</td>\n",
       "      <td>0.23529411764705882</td>\n",
       "      <td>[179, 87, 51, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>500</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.22222222222222218</td>\n",
       "      <td>0.39336917562724016</td>\n",
       "      <td>0.4578544061302681</td>\n",
       "      <td>0.34848484848484845</td>\n",
       "      <td>0.4368791271892047</td>\n",
       "      <td>0.43201911589008357</td>\n",
       "      <td>0.47701149425287354</td>\n",
       "      <td>0.3977272727272727</td>\n",
       "      <td>[186, 87, 44, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>50</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.40051679586563316</td>\n",
       "      <td>0.5076335877862597</td>\n",
       "      <td>0.6212121212121213</td>\n",
       "      <td>0.6209677419354839</td>\n",
       "      <td>0.6306345104794718</td>\n",
       "      <td>0.6056509754028837</td>\n",
       "      <td>0.7110101010101011</td>\n",
       "      <td>0.7123655913978494</td>\n",
       "      <td>[262, 165, 124, 91]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>500</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.40654608096468564</td>\n",
       "      <td>0.5426208651399492</td>\n",
       "      <td>0.6245098039215685</td>\n",
       "      <td>0.5374677002583979</td>\n",
       "      <td>0.642736146999713</td>\n",
       "      <td>0.6147158608990669</td>\n",
       "      <td>0.7157843137254902</td>\n",
       "      <td>0.6589147286821704</td>\n",
       "      <td>[262, 170, 129, 87]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4112833763996557</td>\n",
       "      <td>0.4987468671679199</td>\n",
       "      <td>0.6807228915662651</td>\n",
       "      <td>0.5751879699248121</td>\n",
       "      <td>0.6537898363479758</td>\n",
       "      <td>0.5938178780284044</td>\n",
       "      <td>0.7595381526104418</td>\n",
       "      <td>0.650375939849624</td>\n",
       "      <td>[266, 166, 133, 90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4194659776055123</td>\n",
       "      <td>0.5082382762991129</td>\n",
       "      <td>0.6734892787524366</td>\n",
       "      <td>0.5259259259259259</td>\n",
       "      <td>0.6411714039621017</td>\n",
       "      <td>0.6269961977186312</td>\n",
       "      <td>0.7543859649122807</td>\n",
       "      <td>0.6308641975308642</td>\n",
       "      <td>[263, 171, 135, 89]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4194659776055126</td>\n",
       "      <td>0.514575411913815</td>\n",
       "      <td>0.6264705882352943</td>\n",
       "      <td>0.5794871794871794</td>\n",
       "      <td>0.6449038185472294</td>\n",
       "      <td>0.6212505280946345</td>\n",
       "      <td>0.7348039215686275</td>\n",
       "      <td>0.6897435897435896</td>\n",
       "      <td>[263, 170, 130, 93]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators max_depth min_samples_split min_samples_leaf class_weight  \\\n",
       "142           50        20                10                4     balanced   \n",
       "145          500        20                10                4     balanced   \n",
       "82            50        20                 2                4     balanced   \n",
       "57           100        20                 5                4     balanced   \n",
       "113          500        20                 2                4     balanced   \n",
       "..           ...       ...               ...              ...          ...   \n",
       "108           50      None                 2                1         None   \n",
       "144          500        40                 2                1         None   \n",
       "76            50        40                 2                1         None   \n",
       "109          100        20                 2                1         None   \n",
       "52           100        40                 2                1         None   \n",
       "\n",
       "        Precision nivel1     Precision nivel2     Precision nivel3  \\\n",
       "142  0.20327304048234301  0.37058823529411755   0.3917748917748918   \n",
       "145  0.21533161068044765  0.40037243947858475  0.39837398373983735   \n",
       "82   0.22049956933677853  0.42602495543672014  0.46099290780141844   \n",
       "57   0.22049956933677858    0.393854748603352   0.5325670498084292   \n",
       "113  0.22222222222222218  0.39336917562724016   0.4578544061302681   \n",
       "..                   ...                  ...                  ...   \n",
       "108  0.40051679586563316   0.5076335877862597   0.6212121212121213   \n",
       "144  0.40654608096468564   0.5426208651399492   0.6245098039215685   \n",
       "76    0.4112833763996557   0.4987468671679199   0.6807228915662651   \n",
       "109   0.4194659776055123   0.5082382762991129   0.6734892787524366   \n",
       "52    0.4194659776055126    0.514575411913815   0.6264705882352943   \n",
       "\n",
       "        Precision nivel4        Recall nivel1        Recall nivel2  \\\n",
       "142   0.2222222222222222  0.40766580534022395   0.4211111111111111   \n",
       "145   0.3108108108108108    0.424978466838932  0.42737430167597756   \n",
       "82   0.26595744680851063  0.43996554694229123   0.4746286393345217   \n",
       "57   0.20588235294117646  0.42693080677576795  0.44674115456238356   \n",
       "113  0.34848484848484845   0.4368791271892047  0.43201911589008357   \n",
       "..                   ...                  ...                  ...   \n",
       "108   0.6209677419354839   0.6306345104794718   0.6056509754028837   \n",
       "144   0.5374677002583979    0.642736146999713   0.6147158608990669   \n",
       "76    0.5751879699248121   0.6537898363479758   0.5938178780284044   \n",
       "109   0.5259259259259259   0.6411714039621017   0.6269961977186312   \n",
       "52    0.5794871794871794   0.6449038185472294   0.6212505280946345   \n",
       "\n",
       "           Recall nivel3        Recall nivel4  \\\n",
       "142  0.42207792207792205   0.2222222222222222   \n",
       "145   0.4146341463414634  0.35135135135135137   \n",
       "82    0.4485815602836879   0.2872340425531915   \n",
       "57    0.5680076628352491  0.23529411764705882   \n",
       "113  0.47701149425287354   0.3977272727272727   \n",
       "..                   ...                  ...   \n",
       "108   0.7110101010101011   0.7123655913978494   \n",
       "144   0.7157843137254902   0.6589147286821704   \n",
       "76    0.7595381526104418    0.650375939849624   \n",
       "109   0.7543859649122807   0.6308641975308642   \n",
       "52    0.7348039215686275   0.6897435897435896   \n",
       "\n",
       "    Drugs that have at least one match  \n",
       "142                   [170, 77, 36, 8]  \n",
       "145                  [179, 82, 37, 13]  \n",
       "82                   [187, 94, 47, 14]  \n",
       "57                   [179, 87, 51, 12]  \n",
       "113                  [186, 87, 44, 18]  \n",
       "..                                 ...  \n",
       "108                [262, 165, 124, 91]  \n",
       "144                [262, 170, 129, 87]  \n",
       "76                 [266, 166, 133, 90]  \n",
       "109                [263, 171, 135, 89]  \n",
       "52                 [263, 170, 130, 93]  \n",
       "\n",
       "[162 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tests.sort_values(by = \"Precision nivel1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e89c03-506b-4793-a97c-fc8cc837f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_tests.sort_values(by = \"Precision nivel1\")).to_csv(\"randomforest_sortedresults.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
